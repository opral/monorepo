"use strict"
var __create = Object.create
var __defProp = Object.defineProperty
var __getOwnPropDesc = Object.getOwnPropertyDescriptor
var __getOwnPropNames = Object.getOwnPropertyNames
var __getProtoOf = Object.getPrototypeOf
var __hasOwnProp = Object.prototype.hasOwnProperty
var __esm = (fn, res) =>
	function __init() {
		return fn && (res = (0, fn[__getOwnPropNames(fn)[0]])((fn = 0))), res
	}
var __commonJS = (cb, mod) =>
	function __require() {
		return (
			mod || (0, cb[__getOwnPropNames(cb)[0]])((mod = { exports: {} }).exports, mod), mod.exports
		)
	}
var __export = (target, all) => {
	for (var name in all) __defProp(target, name, { get: all[name], enumerable: true })
}
var __copyProps = (to, from2, except, desc) => {
	if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
		for (let key of __getOwnPropNames(from2))
			if (!__hasOwnProp.call(to, key) && key !== except)
				__defProp(to, key, {
					get: () => from2[key],
					enumerable: !(desc = __getOwnPropDesc(from2, key)) || desc.enumerable,
				})
	}
	return to
}
var __toESM = (mod, isNodeMode, target) => (
	(target = mod != undefined ? __create(__getProtoOf(mod)) : {}),
	__copyProps(
		// If the importer is in node compatibility mode or this is not an ESM
		// file that has been converted to a CommonJS file using a Babel-
		// compatible transform (i.e. "__esModule" has not been set), then set
		// "default" to the CommonJS "module.exports" for node compatibility.
		isNodeMode || !mod || !mod.__esModule
			? __defProp(target, "default", { value: mod, enumerable: true })
			: target,
		mod
	)
)
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod)

// ../../../node_modules/.pnpm/@actions+core@1.10.1/node_modules/@actions/core/lib/utils.js
var require_utils = __commonJS({
	"../../../node_modules/.pnpm/@actions+core@1.10.1/node_modules/@actions/core/lib/utils.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.toCommandProperties = exports2.toCommandValue = void 0
		function toCommandValue(input) {
			if (input === null || input === void 0) {
				return ""
			} else if (typeof input === "string" || input instanceof String) {
				return input
			}
			return JSON.stringify(input)
		}
		exports2.toCommandValue = toCommandValue
		function toCommandProperties(annotationProperties) {
			if (!Object.keys(annotationProperties).length) {
				return {}
			}
			return {
				title: annotationProperties.title,
				file: annotationProperties.file,
				line: annotationProperties.startLine,
				endLine: annotationProperties.endLine,
				col: annotationProperties.startColumn,
				endColumn: annotationProperties.endColumn,
			}
		}
		exports2.toCommandProperties = toCommandProperties
	},
})

// ../../../node_modules/.pnpm/@actions+core@1.10.1/node_modules/@actions/core/lib/command.js
var require_command = __commonJS({
	"../../../node_modules/.pnpm/@actions+core@1.10.1/node_modules/@actions/core/lib/command.js"(
		exports2
	) {
		"use strict"
		var __createBinding =
			(exports2 && exports2.__createBinding) ||
			(Object.create
				? function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						Object.defineProperty(o, k2, {
							enumerable: true,
							get: function () {
								return m[k]
							},
						})
				  }
				: function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						o[k2] = m[k]
				  })
		var __setModuleDefault =
			(exports2 && exports2.__setModuleDefault) ||
			(Object.create
				? function (o, v) {
						Object.defineProperty(o, "default", { enumerable: true, value: v })
				  }
				: function (o, v) {
						o["default"] = v
				  })
		var __importStar =
			(exports2 && exports2.__importStar) ||
			function (mod) {
				if (mod && mod.__esModule) return mod
				var result = {}
				if (mod != undefined) {
					for (var k in mod)
						if (k !== "default" && Object.hasOwnProperty.call(mod, k))
							__createBinding(result, mod, k)
				}
				__setModuleDefault(result, mod)
				return result
			}
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.issue = exports2.issueCommand = void 0
		var os = __importStar(require("node:os"))
		var utils_1 = require_utils()
		function issueCommand(command, properties, message) {
			const cmd = new Command(command, properties, message)
			process.stdout.write(cmd.toString() + os.EOL)
		}
		exports2.issueCommand = issueCommand
		function issue(name, message = "") {
			issueCommand(name, {}, message)
		}
		exports2.issue = issue
		var CMD_STRING = "::"
		var Command = class {
			constructor(command, properties, message) {
				if (!command) {
					command = "missing.command"
				}
				this.command = command
				this.properties = properties
				this.message = message
			}
			toString() {
				let cmdStr = CMD_STRING + this.command
				if (this.properties && Object.keys(this.properties).length > 0) {
					cmdStr += " "
					let first = true
					for (const key in this.properties) {
						if (this.properties.hasOwnProperty(key)) {
							const val = this.properties[key]
							if (val) {
								if (first) {
									first = false
								} else {
									cmdStr += ","
								}
								cmdStr += `${key}=${escapeProperty(val)}`
							}
						}
					}
				}
				cmdStr += `${CMD_STRING}${escapeData(this.message)}`
				return cmdStr
			}
		}
		function escapeData(s) {
			return utils_1
				.toCommandValue(s)
				.replace(/%/g, "%25")
				.replace(/\r/g, "%0D")
				.replace(/\n/g, "%0A")
		}
		function escapeProperty(s) {
			return utils_1
				.toCommandValue(s)
				.replace(/%/g, "%25")
				.replace(/\r/g, "%0D")
				.replace(/\n/g, "%0A")
				.replace(/:/g, "%3A")
				.replace(/,/g, "%2C")
		}
	},
})

// ../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/rng.js
function rng() {
	if (poolPtr > rnds8Pool.length - 16) {
		import_crypto.default.randomFillSync(rnds8Pool)
		poolPtr = 0
	}
	return rnds8Pool.slice(poolPtr, (poolPtr += 16))
}
var import_crypto, rnds8Pool, poolPtr
var init_rng = __esm({
	"../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/rng.js"() {
		import_crypto = __toESM(require("node:crypto"))
		rnds8Pool = new Uint8Array(256)
		poolPtr = rnds8Pool.length
	},
})

// ../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/regex.js
var regex_default
var init_regex = __esm({
	"../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/regex.js"() {
		regex_default =
			/^(?:[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}|00000000-0000-0000-0000-000000000000)$/i
	},
})

// ../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/validate.js
function validate(uuid) {
	return typeof uuid === "string" && regex_default.test(uuid)
}
var validate_default
var init_validate = __esm({
	"../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/validate.js"() {
		init_regex()
		validate_default = validate
	},
})

// ../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/stringify.js
function stringify(arr, offset = 0) {
	const uuid = (
		byteToHex[arr[offset + 0]] +
		byteToHex[arr[offset + 1]] +
		byteToHex[arr[offset + 2]] +
		byteToHex[arr[offset + 3]] +
		"-" +
		byteToHex[arr[offset + 4]] +
		byteToHex[arr[offset + 5]] +
		"-" +
		byteToHex[arr[offset + 6]] +
		byteToHex[arr[offset + 7]] +
		"-" +
		byteToHex[arr[offset + 8]] +
		byteToHex[arr[offset + 9]] +
		"-" +
		byteToHex[arr[offset + 10]] +
		byteToHex[arr[offset + 11]] +
		byteToHex[arr[offset + 12]] +
		byteToHex[arr[offset + 13]] +
		byteToHex[arr[offset + 14]] +
		byteToHex[arr[offset + 15]]
	).toLowerCase()
	if (!validate_default(uuid)) {
		throw TypeError("Stringified UUID is invalid")
	}
	return uuid
}
var byteToHex, stringify_default
var init_stringify = __esm({
	"../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/stringify.js"() {
		init_validate()
		byteToHex = []
		for (let i = 0; i < 256; ++i) {
			byteToHex.push((i + 256).toString(16).slice(1))
		}
		stringify_default = stringify
	},
})

// ../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/v1.js
function v1(options, buf, offset) {
	let i = (buf && offset) || 0
	const b = buf || new Array(16)
	options = options || {}
	let node = options.node || _nodeId
	let clockseq = options.clockseq !== void 0 ? options.clockseq : _clockseq
	if (node == undefined || clockseq == undefined) {
		const seedBytes = options.random || (options.rng || rng)()
		if (node == undefined) {
			node = _nodeId = [
				seedBytes[0] | 1,
				seedBytes[1],
				seedBytes[2],
				seedBytes[3],
				seedBytes[4],
				seedBytes[5],
			]
		}
		if (clockseq == undefined) {
			clockseq = _clockseq = ((seedBytes[6] << 8) | seedBytes[7]) & 16383
		}
	}
	let msecs = options.msecs !== void 0 ? options.msecs : Date.now()
	let nsecs = options.nsecs !== void 0 ? options.nsecs : _lastNSecs + 1
	const dt = msecs - _lastMSecs + (nsecs - _lastNSecs) / 1e4
	if (dt < 0 && options.clockseq === void 0) {
		clockseq = (clockseq + 1) & 16383
	}
	if ((dt < 0 || msecs > _lastMSecs) && options.nsecs === void 0) {
		nsecs = 0
	}
	if (nsecs >= 1e4) {
		throw new Error("uuid.v1(): Can't create more than 10M uuids/sec")
	}
	_lastMSecs = msecs
	_lastNSecs = nsecs
	_clockseq = clockseq
	msecs += 122192928e5
	const tl = ((msecs & 268435455) * 1e4 + nsecs) % 4294967296
	b[i++] = (tl >>> 24) & 255
	b[i++] = (tl >>> 16) & 255
	b[i++] = (tl >>> 8) & 255
	b[i++] = tl & 255
	const tmh = ((msecs / 4294967296) * 1e4) & 268435455
	b[i++] = (tmh >>> 8) & 255
	b[i++] = tmh & 255
	b[i++] = ((tmh >>> 24) & 15) | 16
	b[i++] = (tmh >>> 16) & 255
	b[i++] = (clockseq >>> 8) | 128
	b[i++] = clockseq & 255
	for (let n = 0; n < 6; ++n) {
		b[i + n] = node[n]
	}
	return buf || stringify_default(b)
}
var _nodeId, _clockseq, _lastMSecs, _lastNSecs, v1_default
var init_v1 = __esm({
	"../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/v1.js"() {
		init_rng()
		init_stringify()
		_lastMSecs = 0
		_lastNSecs = 0
		v1_default = v1
	},
})

// ../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/parse.js
function parse(uuid) {
	if (!validate_default(uuid)) {
		throw TypeError("Invalid UUID")
	}
	let v
	const arr = new Uint8Array(16)
	arr[0] = (v = parseInt(uuid.slice(0, 8), 16)) >>> 24
	arr[1] = (v >>> 16) & 255
	arr[2] = (v >>> 8) & 255
	arr[3] = v & 255
	arr[4] = (v = parseInt(uuid.slice(9, 13), 16)) >>> 8
	arr[5] = v & 255
	arr[6] = (v = parseInt(uuid.slice(14, 18), 16)) >>> 8
	arr[7] = v & 255
	arr[8] = (v = parseInt(uuid.slice(19, 23), 16)) >>> 8
	arr[9] = v & 255
	arr[10] = ((v = parseInt(uuid.slice(24, 36), 16)) / 1099511627776) & 255
	arr[11] = (v / 4294967296) & 255
	arr[12] = (v >>> 24) & 255
	arr[13] = (v >>> 16) & 255
	arr[14] = (v >>> 8) & 255
	arr[15] = v & 255
	return arr
}
var parse_default
var init_parse = __esm({
	"../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/parse.js"() {
		init_validate()
		parse_default = parse
	},
})

// ../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/v35.js
function stringToBytes(str) {
	str = unescape(encodeURIComponent(str))
	const bytes = []
	for (let i = 0; i < str.length; ++i) {
		bytes.push(str.charCodeAt(i))
	}
	return bytes
}
function v35_default(name, version3, hashfunc) {
	function generateUUID(value, namespace, buf, offset) {
		if (typeof value === "string") {
			value = stringToBytes(value)
		}
		if (typeof namespace === "string") {
			namespace = parse_default(namespace)
		}
		if (namespace.length !== 16) {
			throw TypeError("Namespace must be array-like (16 iterable integer values, 0-255)")
		}
		let bytes = new Uint8Array(16 + value.length)
		bytes.set(namespace)
		bytes.set(value, namespace.length)
		bytes = hashfunc(bytes)
		bytes[6] = (bytes[6] & 15) | version3
		bytes[8] = (bytes[8] & 63) | 128
		if (buf) {
			offset = offset || 0
			for (let i = 0; i < 16; ++i) {
				buf[offset + i] = bytes[i]
			}
			return buf
		}
		return stringify_default(bytes)
	}
	try {
		generateUUID.name = name
	} catch (err) {}
	generateUUID.DNS = DNS
	generateUUID.URL = URL2
	return generateUUID
}
var DNS, URL2
var init_v35 = __esm({
	"../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/v35.js"() {
		init_stringify()
		init_parse()
		DNS = "6ba7b810-9dad-11d1-80b4-00c04fd430c8"
		URL2 = "6ba7b811-9dad-11d1-80b4-00c04fd430c8"
	},
})

// ../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/md5.js
function md5(bytes) {
	if (Array.isArray(bytes)) {
		bytes = Buffer.from(bytes)
	} else if (typeof bytes === "string") {
		bytes = Buffer.from(bytes, "utf8")
	}
	return import_crypto2.default.createHash("md5").update(bytes).digest()
}
var import_crypto2, md5_default
var init_md5 = __esm({
	"../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/md5.js"() {
		import_crypto2 = __toESM(require("node:crypto"))
		md5_default = md5
	},
})

// ../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/v3.js
var v3, v3_default
var init_v3 = __esm({
	"../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/v3.js"() {
		init_v35()
		init_md5()
		v3 = v35_default("v3", 48, md5_default)
		v3_default = v3
	},
})

// ../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/v4.js
function v4(options, buf, offset) {
	options = options || {}
	const rnds = options.random || (options.rng || rng)()
	rnds[6] = (rnds[6] & 15) | 64
	rnds[8] = (rnds[8] & 63) | 128
	if (buf) {
		offset = offset || 0
		for (let i = 0; i < 16; ++i) {
			buf[offset + i] = rnds[i]
		}
		return buf
	}
	return stringify_default(rnds)
}
var v4_default
var init_v4 = __esm({
	"../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/v4.js"() {
		init_rng()
		init_stringify()
		v4_default = v4
	},
})

// ../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/sha1.js
function sha1(bytes) {
	if (Array.isArray(bytes)) {
		bytes = Buffer.from(bytes)
	} else if (typeof bytes === "string") {
		bytes = Buffer.from(bytes, "utf8")
	}
	return import_crypto3.default.createHash("sha1").update(bytes).digest()
}
var import_crypto3, sha1_default
var init_sha1 = __esm({
	"../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/sha1.js"() {
		import_crypto3 = __toESM(require("node:crypto"))
		sha1_default = sha1
	},
})

// ../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/v5.js
var v5, v5_default
var init_v5 = __esm({
	"../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/v5.js"() {
		init_v35()
		init_sha1()
		v5 = v35_default("v5", 80, sha1_default)
		v5_default = v5
	},
})

// ../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/nil.js
var nil_default
var init_nil = __esm({
	"../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/nil.js"() {
		nil_default = "00000000-0000-0000-0000-000000000000"
	},
})

// ../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/version.js
function version(uuid) {
	if (!validate_default(uuid)) {
		throw TypeError("Invalid UUID")
	}
	return parseInt(uuid.slice(14, 15), 16)
}
var version_default
var init_version = __esm({
	"../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/version.js"() {
		init_validate()
		version_default = version
	},
})

// ../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/index.js
var esm_node_exports = {}
__export(esm_node_exports, {
	NIL: () => nil_default,
	parse: () => parse_default,
	stringify: () => stringify_default,
	v1: () => v1_default,
	v3: () => v3_default,
	v4: () => v4_default,
	v5: () => v5_default,
	validate: () => validate_default,
	version: () => version_default,
})
var init_esm_node = __esm({
	"../../../node_modules/.pnpm/uuid@8.3.2/node_modules/uuid/dist/esm-node/index.js"() {
		init_v1()
		init_v3()
		init_v4()
		init_v5()
		init_nil()
		init_version()
		init_validate()
		init_stringify()
		init_parse()
	},
})

// ../../../node_modules/.pnpm/@actions+core@1.10.1/node_modules/@actions/core/lib/file-command.js
var require_file_command = __commonJS({
	"../../../node_modules/.pnpm/@actions+core@1.10.1/node_modules/@actions/core/lib/file-command.js"(
		exports2
	) {
		"use strict"
		var __createBinding =
			(exports2 && exports2.__createBinding) ||
			(Object.create
				? function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						Object.defineProperty(o, k2, {
							enumerable: true,
							get: function () {
								return m[k]
							},
						})
				  }
				: function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						o[k2] = m[k]
				  })
		var __setModuleDefault =
			(exports2 && exports2.__setModuleDefault) ||
			(Object.create
				? function (o, v) {
						Object.defineProperty(o, "default", { enumerable: true, value: v })
				  }
				: function (o, v) {
						o["default"] = v
				  })
		var __importStar =
			(exports2 && exports2.__importStar) ||
			function (mod) {
				if (mod && mod.__esModule) return mod
				var result = {}
				if (mod != undefined) {
					for (var k in mod)
						if (k !== "default" && Object.hasOwnProperty.call(mod, k))
							__createBinding(result, mod, k)
				}
				__setModuleDefault(result, mod)
				return result
			}
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.prepareKeyValueMessage = exports2.issueFileCommand = void 0
		var fs2 = __importStar(require("node:fs"))
		var os = __importStar(require("node:os"))
		var uuid_1 = (init_esm_node(), __toCommonJS(esm_node_exports))
		var utils_1 = require_utils()
		function issueFileCommand(command, message) {
			const filePath = process.env[`GITHUB_${command}`]
			if (!filePath) {
				throw new Error(`Unable to find environment variable for file command ${command}`)
			}
			if (!fs2.existsSync(filePath)) {
				throw new Error(`Missing file at path: ${filePath}`)
			}
			fs2.appendFileSync(filePath, `${utils_1.toCommandValue(message)}${os.EOL}`, {
				encoding: "utf8",
			})
		}
		exports2.issueFileCommand = issueFileCommand
		function prepareKeyValueMessage(key, value) {
			const delimiter = `ghadelimiter_${uuid_1.v4()}`
			const convertedValue = utils_1.toCommandValue(value)
			if (key.includes(delimiter)) {
				throw new Error(`Unexpected input: name should not contain the delimiter "${delimiter}"`)
			}
			if (convertedValue.includes(delimiter)) {
				throw new Error(`Unexpected input: value should not contain the delimiter "${delimiter}"`)
			}
			return `${key}<<${delimiter}${os.EOL}${convertedValue}${os.EOL}${delimiter}`
		}
		exports2.prepareKeyValueMessage = prepareKeyValueMessage
	},
})

// ../../../node_modules/.pnpm/@actions+http-client@2.2.1/node_modules/@actions/http-client/lib/proxy.js
var require_proxy = __commonJS({
	"../../../node_modules/.pnpm/@actions+http-client@2.2.1/node_modules/@actions/http-client/lib/proxy.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.checkBypass = exports2.getProxyUrl = void 0
		function getProxyUrl(reqUrl) {
			const usingSsl = reqUrl.protocol === "https:"
			if (checkBypass(reqUrl)) {
				return void 0
			}
			const proxyVar = (() => {
				if (usingSsl) {
					return process.env["https_proxy"] || process.env["HTTPS_PROXY"]
				} else {
					return process.env["http_proxy"] || process.env["HTTP_PROXY"]
				}
			})()
			if (proxyVar) {
				try {
					return new URL(proxyVar)
				} catch (_a) {
					if (!proxyVar.startsWith("http://") && !proxyVar.startsWith("https://"))
						return new URL(`http://${proxyVar}`)
				}
			} else {
				return void 0
			}
		}
		exports2.getProxyUrl = getProxyUrl
		function checkBypass(reqUrl) {
			if (!reqUrl.hostname) {
				return false
			}
			const reqHost = reqUrl.hostname
			if (isLoopbackAddress(reqHost)) {
				return true
			}
			const noProxy = process.env["no_proxy"] || process.env["NO_PROXY"] || ""
			if (!noProxy) {
				return false
			}
			let reqPort
			if (reqUrl.port) {
				reqPort = Number(reqUrl.port)
			} else if (reqUrl.protocol === "http:") {
				reqPort = 80
			} else if (reqUrl.protocol === "https:") {
				reqPort = 443
			}
			const upperReqHosts = [reqUrl.hostname.toUpperCase()]
			if (typeof reqPort === "number") {
				upperReqHosts.push(`${upperReqHosts[0]}:${reqPort}`)
			}
			for (const upperNoProxyItem of noProxy
				.split(",")
				.map((x) => x.trim().toUpperCase())
				.filter((x) => x)) {
				if (
					upperNoProxyItem === "*" ||
					upperReqHosts.some(
						(x) =>
							x === upperNoProxyItem ||
							x.endsWith(`.${upperNoProxyItem}`) ||
							(upperNoProxyItem.startsWith(".") && x.endsWith(`${upperNoProxyItem}`))
					)
				) {
					return true
				}
			}
			return false
		}
		exports2.checkBypass = checkBypass
		function isLoopbackAddress(host) {
			const hostLower = host.toLowerCase()
			return (
				hostLower === "localhost" ||
				hostLower.startsWith("127.") ||
				hostLower.startsWith("[::1]") ||
				hostLower.startsWith("[0:0:0:0:0:0:0:1]")
			)
		}
	},
})

// ../../../node_modules/.pnpm/tunnel@0.0.6/node_modules/tunnel/lib/tunnel.js
var require_tunnel = __commonJS({
	"../../../node_modules/.pnpm/tunnel@0.0.6/node_modules/tunnel/lib/tunnel.js"(exports2) {
		"use strict"
		var net = require("node:net")
		var tls = require("node:tls")
		var http = require("node:http")
		var https = require("node:https")
		var events = require("node:events")
		var assert = require("node:assert")
		var util = require("node:util")
		exports2.httpOverHttp = httpOverHttp
		exports2.httpsOverHttp = httpsOverHttp
		exports2.httpOverHttps = httpOverHttps
		exports2.httpsOverHttps = httpsOverHttps
		function httpOverHttp(options) {
			var agent = new TunnelingAgent(options)
			agent.request = http.request
			return agent
		}
		function httpsOverHttp(options) {
			var agent = new TunnelingAgent(options)
			agent.request = http.request
			agent.createSocket = createSecureSocket
			agent.defaultPort = 443
			return agent
		}
		function httpOverHttps(options) {
			var agent = new TunnelingAgent(options)
			agent.request = https.request
			return agent
		}
		function httpsOverHttps(options) {
			var agent = new TunnelingAgent(options)
			agent.request = https.request
			agent.createSocket = createSecureSocket
			agent.defaultPort = 443
			return agent
		}
		function TunnelingAgent(options) {
			var self2 = this
			self2.options = options || {}
			self2.proxyOptions = self2.options.proxy || {}
			self2.maxSockets = self2.options.maxSockets || http.Agent.defaultMaxSockets
			self2.requests = []
			self2.sockets = []
			self2.on("free", function onFree(socket, host, port, localAddress) {
				var options2 = toOptions(host, port, localAddress)
				for (var i = 0, len = self2.requests.length; i < len; ++i) {
					var pending = self2.requests[i]
					if (pending.host === options2.host && pending.port === options2.port) {
						self2.requests.splice(i, 1)
						pending.request.onSocket(socket)
						return
					}
				}
				socket.destroy()
				self2.removeSocket(socket)
			})
		}
		util.inherits(TunnelingAgent, events.EventEmitter)
		TunnelingAgent.prototype.addRequest = function addRequest(req, host, port, localAddress) {
			var self2 = this
			var options = mergeOptions(
				{ request: req },
				self2.options,
				toOptions(host, port, localAddress)
			)
			if (self2.sockets.length >= this.maxSockets) {
				self2.requests.push(options)
				return
			}
			self2.createSocket(options, function (socket) {
				socket.on("free", onFree)
				socket.on("close", onCloseOrRemove)
				socket.on("agentRemove", onCloseOrRemove)
				req.onSocket(socket)
				function onFree() {
					self2.emit("free", socket, options)
				}
				function onCloseOrRemove(err) {
					self2.removeSocket(socket)
					socket.removeListener("free", onFree)
					socket.removeListener("close", onCloseOrRemove)
					socket.removeListener("agentRemove", onCloseOrRemove)
				}
			})
		}
		TunnelingAgent.prototype.createSocket = function createSocket(options, cb) {
			var self2 = this
			var placeholder = {}
			self2.sockets.push(placeholder)
			var connectOptions = mergeOptions({}, self2.proxyOptions, {
				method: "CONNECT",
				path: options.host + ":" + options.port,
				agent: false,
				headers: {
					host: options.host + ":" + options.port,
				},
			})
			if (options.localAddress) {
				connectOptions.localAddress = options.localAddress
			}
			if (connectOptions.proxyAuth) {
				connectOptions.headers = connectOptions.headers || {}
				connectOptions.headers["Proxy-Authorization"] =
					"Basic " + new Buffer(connectOptions.proxyAuth).toString("base64")
			}
			debug10("making CONNECT request")
			var connectReq = self2.request(connectOptions)
			connectReq.useChunkedEncodingByDefault = false
			connectReq.once("response", onResponse)
			connectReq.once("upgrade", onUpgrade)
			connectReq.once("connect", onConnect)
			connectReq.once("error", onError)
			connectReq.end()
			function onResponse(res) {
				res.upgrade = true
			}
			function onUpgrade(res, socket, head) {
				process.nextTick(function () {
					onConnect(res, socket, head)
				})
			}
			function onConnect(res, socket, head) {
				connectReq.removeAllListeners()
				socket.removeAllListeners()
				if (res.statusCode !== 200) {
					debug10("tunneling socket could not be established, statusCode=%d", res.statusCode)
					socket.destroy()
					var error = new Error(
						"tunneling socket could not be established, statusCode=" + res.statusCode
					)
					error.code = "ECONNRESET"
					options.request.emit("error", error)
					self2.removeSocket(placeholder)
					return
				}
				if (head.length > 0) {
					debug10("got illegal response body from proxy")
					socket.destroy()
					var error = new Error("got illegal response body from proxy")
					error.code = "ECONNRESET"
					options.request.emit("error", error)
					self2.removeSocket(placeholder)
					return
				}
				debug10("tunneling connection has established")
				self2.sockets[self2.sockets.indexOf(placeholder)] = socket
				return cb(socket)
			}
			function onError(cause) {
				connectReq.removeAllListeners()
				debug10("tunneling socket could not be established, cause=%s\n", cause.message, cause.stack)
				var error = new Error("tunneling socket could not be established, cause=" + cause.message)
				error.code = "ECONNRESET"
				options.request.emit("error", error)
				self2.removeSocket(placeholder)
			}
		}
		TunnelingAgent.prototype.removeSocket = function removeSocket(socket) {
			var pos = this.sockets.indexOf(socket)
			if (pos === -1) {
				return
			}
			this.sockets.splice(pos, 1)
			var pending = this.requests.shift()
			if (pending) {
				this.createSocket(pending, function (socket2) {
					pending.request.onSocket(socket2)
				})
			}
		}
		function createSecureSocket(options, cb) {
			var self2 = this
			TunnelingAgent.prototype.createSocket.call(self2, options, function (socket) {
				var hostHeader = options.request.getHeader("host")
				var tlsOptions = mergeOptions({}, self2.options, {
					socket,
					servername: hostHeader ? hostHeader.replace(/:.*$/, "") : options.host,
				})
				var secureSocket = tls.connect(0, tlsOptions)
				self2.sockets[self2.sockets.indexOf(socket)] = secureSocket
				cb(secureSocket)
			})
		}
		function toOptions(host, port, localAddress) {
			if (typeof host === "string") {
				return {
					host,
					port,
					localAddress,
				}
			}
			return host
		}
		function mergeOptions(target) {
			for (var i = 1, len = arguments.length; i < len; ++i) {
				var overrides = arguments[i]
				if (typeof overrides === "object") {
					var keys = Object.keys(overrides)
					for (var j = 0, keyLen = keys.length; j < keyLen; ++j) {
						var k = keys[j]
						if (overrides[k] !== void 0) {
							target[k] = overrides[k]
						}
					}
				}
			}
			return target
		}
		var debug10
		if (process.env.NODE_DEBUG && /\btunnel\b/.test(process.env.NODE_DEBUG)) {
			debug10 = function () {
				var args = Array.prototype.slice.call(arguments)
				if (typeof args[0] === "string") {
					args[0] = "TUNNEL: " + args[0]
				} else {
					args.unshift("TUNNEL:")
				}
				console.error.apply(console, args)
			}
		} else {
			debug10 = function () {}
		}
		exports2.debug = debug10
	},
})

// ../../../node_modules/.pnpm/tunnel@0.0.6/node_modules/tunnel/index.js
var require_tunnel2 = __commonJS({
	"../../../node_modules/.pnpm/tunnel@0.0.6/node_modules/tunnel/index.js"(exports2, module2) {
		module2.exports = require_tunnel()
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/core/symbols.js
var require_symbols = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/core/symbols.js"(
		exports2,
		module2
	) {
		module2.exports = {
			kClose: Symbol("close"),
			kDestroy: Symbol("destroy"),
			kDispatch: Symbol("dispatch"),
			kUrl: Symbol("url"),
			kWriting: Symbol("writing"),
			kResuming: Symbol("resuming"),
			kQueue: Symbol("queue"),
			kConnect: Symbol("connect"),
			kConnecting: Symbol("connecting"),
			kHeadersList: Symbol("headers list"),
			kKeepAliveDefaultTimeout: Symbol("default keep alive timeout"),
			kKeepAliveMaxTimeout: Symbol("max keep alive timeout"),
			kKeepAliveTimeoutThreshold: Symbol("keep alive timeout threshold"),
			kKeepAliveTimeoutValue: Symbol("keep alive timeout"),
			kKeepAlive: Symbol("keep alive"),
			kHeadersTimeout: Symbol("headers timeout"),
			kBodyTimeout: Symbol("body timeout"),
			kServerName: Symbol("server name"),
			kLocalAddress: Symbol("local address"),
			kHost: Symbol("host"),
			kNoRef: Symbol("no ref"),
			kBodyUsed: Symbol("used"),
			kRunning: Symbol("running"),
			kBlocking: Symbol("blocking"),
			kPending: Symbol("pending"),
			kSize: Symbol("size"),
			kBusy: Symbol("busy"),
			kQueued: Symbol("queued"),
			kFree: Symbol("free"),
			kConnected: Symbol("connected"),
			kClosed: Symbol("closed"),
			kNeedDrain: Symbol("need drain"),
			kReset: Symbol("reset"),
			kDestroyed: Symbol.for("nodejs.stream.destroyed"),
			kMaxHeadersSize: Symbol("max headers size"),
			kRunningIdx: Symbol("running index"),
			kPendingIdx: Symbol("pending index"),
			kError: Symbol("error"),
			kClients: Symbol("clients"),
			kClient: Symbol("client"),
			kParser: Symbol("parser"),
			kOnDestroyed: Symbol("destroy callbacks"),
			kPipelining: Symbol("pipelining"),
			kSocket: Symbol("socket"),
			kHostHeader: Symbol("host header"),
			kConnector: Symbol("connector"),
			kStrictContentLength: Symbol("strict content length"),
			kMaxRedirections: Symbol("maxRedirections"),
			kMaxRequests: Symbol("maxRequestsPerClient"),
			kProxy: Symbol("proxy agent options"),
			kCounter: Symbol("socket request counter"),
			kInterceptors: Symbol("dispatch interceptors"),
			kMaxResponseSize: Symbol("max response size"),
			kHTTP2Session: Symbol("http2Session"),
			kHTTP2SessionState: Symbol("http2Session state"),
			kHTTP2BuildRequest: Symbol("http2 build request"),
			kHTTP1BuildRequest: Symbol("http1 build request"),
			kHTTP2CopyHeaders: Symbol("http2 copy headers"),
			kHTTPConnVersion: Symbol("http connection version"),
			kRetryHandlerDefaultRetry: Symbol("retry agent default retry"),
			kConstruct: Symbol("constructable"),
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/core/errors.js
var require_errors = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/core/errors.js"(
		exports2,
		module2
	) {
		"use strict"
		var UndiciError = class extends Error {
			constructor(message) {
				super(message)
				this.name = "UndiciError"
				this.code = "UND_ERR"
			}
		}
		var ConnectTimeoutError = class _ConnectTimeoutError extends UndiciError {
			constructor(message) {
				super(message)
				Error.captureStackTrace(this, _ConnectTimeoutError)
				this.name = "ConnectTimeoutError"
				this.message = message || "Connect Timeout Error"
				this.code = "UND_ERR_CONNECT_TIMEOUT"
			}
		}
		var HeadersTimeoutError = class _HeadersTimeoutError extends UndiciError {
			constructor(message) {
				super(message)
				Error.captureStackTrace(this, _HeadersTimeoutError)
				this.name = "HeadersTimeoutError"
				this.message = message || "Headers Timeout Error"
				this.code = "UND_ERR_HEADERS_TIMEOUT"
			}
		}
		var HeadersOverflowError = class _HeadersOverflowError extends UndiciError {
			constructor(message) {
				super(message)
				Error.captureStackTrace(this, _HeadersOverflowError)
				this.name = "HeadersOverflowError"
				this.message = message || "Headers Overflow Error"
				this.code = "UND_ERR_HEADERS_OVERFLOW"
			}
		}
		var BodyTimeoutError = class _BodyTimeoutError extends UndiciError {
			constructor(message) {
				super(message)
				Error.captureStackTrace(this, _BodyTimeoutError)
				this.name = "BodyTimeoutError"
				this.message = message || "Body Timeout Error"
				this.code = "UND_ERR_BODY_TIMEOUT"
			}
		}
		var ResponseStatusCodeError = class _ResponseStatusCodeError extends UndiciError {
			constructor(message, statusCode, headers, body) {
				super(message)
				Error.captureStackTrace(this, _ResponseStatusCodeError)
				this.name = "ResponseStatusCodeError"
				this.message = message || "Response Status Code Error"
				this.code = "UND_ERR_RESPONSE_STATUS_CODE"
				this.body = body
				this.status = statusCode
				this.statusCode = statusCode
				this.headers = headers
			}
		}
		var InvalidArgumentError = class _InvalidArgumentError extends UndiciError {
			constructor(message) {
				super(message)
				Error.captureStackTrace(this, _InvalidArgumentError)
				this.name = "InvalidArgumentError"
				this.message = message || "Invalid Argument Error"
				this.code = "UND_ERR_INVALID_ARG"
			}
		}
		var InvalidReturnValueError = class _InvalidReturnValueError extends UndiciError {
			constructor(message) {
				super(message)
				Error.captureStackTrace(this, _InvalidReturnValueError)
				this.name = "InvalidReturnValueError"
				this.message = message || "Invalid Return Value Error"
				this.code = "UND_ERR_INVALID_RETURN_VALUE"
			}
		}
		var RequestAbortedError = class _RequestAbortedError extends UndiciError {
			constructor(message) {
				super(message)
				Error.captureStackTrace(this, _RequestAbortedError)
				this.name = "AbortError"
				this.message = message || "Request aborted"
				this.code = "UND_ERR_ABORTED"
			}
		}
		var InformationalError = class _InformationalError extends UndiciError {
			constructor(message) {
				super(message)
				Error.captureStackTrace(this, _InformationalError)
				this.name = "InformationalError"
				this.message = message || "Request information"
				this.code = "UND_ERR_INFO"
			}
		}
		var RequestContentLengthMismatchError = class _RequestContentLengthMismatchError extends UndiciError {
			constructor(message) {
				super(message)
				Error.captureStackTrace(this, _RequestContentLengthMismatchError)
				this.name = "RequestContentLengthMismatchError"
				this.message = message || "Request body length does not match content-length header"
				this.code = "UND_ERR_REQ_CONTENT_LENGTH_MISMATCH"
			}
		}
		var ResponseContentLengthMismatchError = class _ResponseContentLengthMismatchError extends UndiciError {
			constructor(message) {
				super(message)
				Error.captureStackTrace(this, _ResponseContentLengthMismatchError)
				this.name = "ResponseContentLengthMismatchError"
				this.message = message || "Response body length does not match content-length header"
				this.code = "UND_ERR_RES_CONTENT_LENGTH_MISMATCH"
			}
		}
		var ClientDestroyedError = class _ClientDestroyedError extends UndiciError {
			constructor(message) {
				super(message)
				Error.captureStackTrace(this, _ClientDestroyedError)
				this.name = "ClientDestroyedError"
				this.message = message || "The client is destroyed"
				this.code = "UND_ERR_DESTROYED"
			}
		}
		var ClientClosedError = class _ClientClosedError extends UndiciError {
			constructor(message) {
				super(message)
				Error.captureStackTrace(this, _ClientClosedError)
				this.name = "ClientClosedError"
				this.message = message || "The client is closed"
				this.code = "UND_ERR_CLOSED"
			}
		}
		var SocketError = class _SocketError extends UndiciError {
			constructor(message, socket) {
				super(message)
				Error.captureStackTrace(this, _SocketError)
				this.name = "SocketError"
				this.message = message || "Socket error"
				this.code = "UND_ERR_SOCKET"
				this.socket = socket
			}
		}
		var NotSupportedError = class _NotSupportedError extends UndiciError {
			constructor(message) {
				super(message)
				Error.captureStackTrace(this, _NotSupportedError)
				this.name = "NotSupportedError"
				this.message = message || "Not supported error"
				this.code = "UND_ERR_NOT_SUPPORTED"
			}
		}
		var BalancedPoolMissingUpstreamError = class extends UndiciError {
			constructor(message) {
				super(message)
				Error.captureStackTrace(this, NotSupportedError)
				this.name = "MissingUpstreamError"
				this.message = message || "No upstream has been added to the BalancedPool"
				this.code = "UND_ERR_BPL_MISSING_UPSTREAM"
			}
		}
		var HTTPParserError = class _HTTPParserError extends Error {
			constructor(message, code, data) {
				super(message)
				Error.captureStackTrace(this, _HTTPParserError)
				this.name = "HTTPParserError"
				this.code = code ? `HPE_${code}` : void 0
				this.data = data ? data.toString() : void 0
			}
		}
		var ResponseExceededMaxSizeError = class _ResponseExceededMaxSizeError extends UndiciError {
			constructor(message) {
				super(message)
				Error.captureStackTrace(this, _ResponseExceededMaxSizeError)
				this.name = "ResponseExceededMaxSizeError"
				this.message = message || "Response content exceeded max size"
				this.code = "UND_ERR_RES_EXCEEDED_MAX_SIZE"
			}
		}
		var RequestRetryError = class _RequestRetryError extends UndiciError {
			constructor(message, code, { headers, data }) {
				super(message)
				Error.captureStackTrace(this, _RequestRetryError)
				this.name = "RequestRetryError"
				this.message = message || "Request retry error"
				this.code = "UND_ERR_REQ_RETRY"
				this.statusCode = code
				this.data = data
				this.headers = headers
			}
		}
		module2.exports = {
			HTTPParserError,
			UndiciError,
			HeadersTimeoutError,
			HeadersOverflowError,
			BodyTimeoutError,
			RequestContentLengthMismatchError,
			ConnectTimeoutError,
			ResponseStatusCodeError,
			InvalidArgumentError,
			InvalidReturnValueError,
			RequestAbortedError,
			ClientDestroyedError,
			ClientClosedError,
			InformationalError,
			SocketError,
			NotSupportedError,
			ResponseContentLengthMismatchError,
			BalancedPoolMissingUpstreamError,
			ResponseExceededMaxSizeError,
			RequestRetryError,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/core/constants.js
var require_constants = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/core/constants.js"(
		exports2,
		module2
	) {
		"use strict"
		var headerNameLowerCasedRecord = {}
		var wellknownHeaderNames = [
			"Accept",
			"Accept-Encoding",
			"Accept-Language",
			"Accept-Ranges",
			"Access-Control-Allow-Credentials",
			"Access-Control-Allow-Headers",
			"Access-Control-Allow-Methods",
			"Access-Control-Allow-Origin",
			"Access-Control-Expose-Headers",
			"Access-Control-Max-Age",
			"Access-Control-Request-Headers",
			"Access-Control-Request-Method",
			"Age",
			"Allow",
			"Alt-Svc",
			"Alt-Used",
			"Authorization",
			"Cache-Control",
			"Clear-Site-Data",
			"Connection",
			"Content-Disposition",
			"Content-Encoding",
			"Content-Language",
			"Content-Length",
			"Content-Location",
			"Content-Range",
			"Content-Security-Policy",
			"Content-Security-Policy-Report-Only",
			"Content-Type",
			"Cookie",
			"Cross-Origin-Embedder-Policy",
			"Cross-Origin-Opener-Policy",
			"Cross-Origin-Resource-Policy",
			"Date",
			"Device-Memory",
			"Downlink",
			"ECT",
			"ETag",
			"Expect",
			"Expect-CT",
			"Expires",
			"Forwarded",
			"From",
			"Host",
			"If-Match",
			"If-Modified-Since",
			"If-None-Match",
			"If-Range",
			"If-Unmodified-Since",
			"Keep-Alive",
			"Last-Modified",
			"Link",
			"Location",
			"Max-Forwards",
			"Origin",
			"Permissions-Policy",
			"Pragma",
			"Proxy-Authenticate",
			"Proxy-Authorization",
			"RTT",
			"Range",
			"Referer",
			"Referrer-Policy",
			"Refresh",
			"Retry-After",
			"Sec-WebSocket-Accept",
			"Sec-WebSocket-Extensions",
			"Sec-WebSocket-Key",
			"Sec-WebSocket-Protocol",
			"Sec-WebSocket-Version",
			"Server",
			"Server-Timing",
			"Service-Worker-Allowed",
			"Service-Worker-Navigation-Preload",
			"Set-Cookie",
			"SourceMap",
			"Strict-Transport-Security",
			"Supports-Loading-Mode",
			"TE",
			"Timing-Allow-Origin",
			"Trailer",
			"Transfer-Encoding",
			"Upgrade",
			"Upgrade-Insecure-Requests",
			"User-Agent",
			"Vary",
			"Via",
			"WWW-Authenticate",
			"X-Content-Type-Options",
			"X-DNS-Prefetch-Control",
			"X-Frame-Options",
			"X-Permitted-Cross-Domain-Policies",
			"X-Powered-By",
			"X-Requested-With",
			"X-XSS-Protection",
		]
		for (const key of wellknownHeaderNames) {
			const lowerCasedKey = key.toLowerCase()
			headerNameLowerCasedRecord[key] = headerNameLowerCasedRecord[lowerCasedKey] = lowerCasedKey
		}
		Object.setPrototypeOf(headerNameLowerCasedRecord, null)
		module2.exports = {
			wellknownHeaderNames,
			headerNameLowerCasedRecord,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/core/util.js
var require_util = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/core/util.js"(
		exports2,
		module2
	) {
		"use strict"
		var assert = require("node:assert")
		var { kDestroyed, kBodyUsed } = require_symbols()
		var { IncomingMessage } = require("node:http")
		var stream = require("node:stream")
		var net = require("node:net")
		var { InvalidArgumentError } = require_errors()
		var { Blob: Blob2 } = require("node:buffer")
		var nodeUtil = require("node:util")
		var { stringify: stringify2 } = require("node:querystring")
		var { headerNameLowerCasedRecord } = require_constants()
		var [nodeMajor, nodeMinor] = process.versions.node.split(".").map((v) => Number(v))
		function nop() {}
		function isStream(obj) {
			return (
				obj &&
				typeof obj === "object" &&
				typeof obj.pipe === "function" &&
				typeof obj.on === "function"
			)
		}
		function isBlobLike(object) {
			return (
				(Blob2 && object instanceof Blob2) ||
				(object &&
					typeof object === "object" &&
					(typeof object.stream === "function" || typeof object.arrayBuffer === "function") &&
					/^(Blob|File)$/.test(object[Symbol.toStringTag]))
			)
		}
		function buildURL(url, queryParams) {
			if (url.includes("?") || url.includes("#")) {
				throw new Error('Query params cannot be passed when url already contains "?" or "#".')
			}
			const stringified = stringify2(queryParams)
			if (stringified) {
				url += "?" + stringified
			}
			return url
		}
		function parseURL(url) {
			if (typeof url === "string") {
				url = new URL(url)
				if (!/^https?:/.test(url.origin || url.protocol)) {
					throw new InvalidArgumentError(
						"Invalid URL protocol: the URL must start with `http:` or `https:`."
					)
				}
				return url
			}
			if (!url || typeof url !== "object") {
				throw new InvalidArgumentError("Invalid URL: The URL argument must be a non-null object.")
			}
			if (!/^https?:/.test(url.origin || url.protocol)) {
				throw new InvalidArgumentError(
					"Invalid URL protocol: the URL must start with `http:` or `https:`."
				)
			}
			if (!(url instanceof URL)) {
				if (url.port != undefined && url.port !== "" && !Number.isFinite(parseInt(url.port))) {
					throw new InvalidArgumentError(
						"Invalid URL: port must be a valid integer or a string representation of an integer."
					)
				}
				if (url.path != undefined && typeof url.path !== "string") {
					throw new InvalidArgumentError(
						"Invalid URL path: the path must be a string or null/undefined."
					)
				}
				if (url.pathname != undefined && typeof url.pathname !== "string") {
					throw new InvalidArgumentError(
						"Invalid URL pathname: the pathname must be a string or null/undefined."
					)
				}
				if (url.hostname != undefined && typeof url.hostname !== "string") {
					throw new InvalidArgumentError(
						"Invalid URL hostname: the hostname must be a string or null/undefined."
					)
				}
				if (url.origin != undefined && typeof url.origin !== "string") {
					throw new InvalidArgumentError(
						"Invalid URL origin: the origin must be a string or null/undefined."
					)
				}
				const port = url.port != undefined ? url.port : url.protocol === "https:" ? 443 : 80
				let origin =
					url.origin != undefined ? url.origin : `${url.protocol}//${url.hostname}:${port}`
				let path = url.path != undefined ? url.path : `${url.pathname || ""}${url.search || ""}`
				if (origin.endsWith("/")) {
					origin = origin.slice(0, Math.max(0, origin.length - 1))
				}
				if (path && !path.startsWith("/")) {
					path = `/${path}`
				}
				url = new URL(origin + path)
			}
			return url
		}
		function parseOrigin2(url) {
			url = parseURL(url)
			if (url.pathname !== "/" || url.search || url.hash) {
				throw new InvalidArgumentError("invalid url")
			}
			return url
		}
		function getHostname(host) {
			if (host[0] === "[") {
				const idx2 = host.indexOf("]")
				assert(idx2 !== -1)
				return host.substring(1, idx2)
			}
			const idx = host.indexOf(":")
			if (idx === -1) return host
			return host.slice(0, Math.max(0, idx))
		}
		function getServerName(host) {
			if (!host) {
				return null
			}
			assert.strictEqual(typeof host, "string")
			const servername = getHostname(host)
			if (net.isIP(servername)) {
				return ""
			}
			return servername
		}
		function deepClone(obj) {
			return JSON.parse(JSON.stringify(obj))
		}
		function isAsyncIterable(obj) {
			return !!(obj != undefined && typeof obj[Symbol.asyncIterator] === "function")
		}
		function isIterable(obj) {
			return !!(
				obj != undefined &&
				(typeof obj[Symbol.iterator] === "function" ||
					typeof obj[Symbol.asyncIterator] === "function")
			)
		}
		function bodyLength(body) {
			if (body == undefined) {
				return 0
			} else if (isStream(body)) {
				const state = body._readableState
				return state &&
					state.objectMode === false &&
					state.ended === true &&
					Number.isFinite(state.length)
					? state.length
					: null
			} else if (isBlobLike(body)) {
				return body.size != undefined ? body.size : null
			} else if (isBuffer(body)) {
				return body.byteLength
			}
			return null
		}
		function isDestroyed(stream2) {
			return !stream2 || !!(stream2.destroyed || stream2[kDestroyed])
		}
		function isReadableAborted(stream2) {
			const state = stream2 && stream2._readableState
			return isDestroyed(stream2) && state && !state.endEmitted
		}
		function destroy(stream2, err) {
			if (stream2 == undefined || !isStream(stream2) || isDestroyed(stream2)) {
				return
			}
			if (typeof stream2.destroy === "function") {
				if (Object.getPrototypeOf(stream2).constructor === IncomingMessage) {
					stream2.socket = null
				}
				stream2.destroy(err)
			} else if (err) {
				process.nextTick(
					(stream3, err2) => {
						stream3.emit("error", err2)
					},
					stream2,
					err
				)
			}
			if (stream2.destroyed !== true) {
				stream2[kDestroyed] = true
			}
		}
		var KEEPALIVE_TIMEOUT_EXPR = /timeout=(\d+)/
		function parseKeepAliveTimeout(val) {
			const m = val.toString().match(KEEPALIVE_TIMEOUT_EXPR)
			return m ? parseInt(m[1], 10) * 1e3 : null
		}
		function headerNameToString(value) {
			return headerNameLowerCasedRecord[value] || value.toLowerCase()
		}
		function parseHeaders(headers, obj = {}) {
			if (!Array.isArray(headers)) return headers
			for (let i = 0; i < headers.length; i += 2) {
				const key = headers[i].toString().toLowerCase()
				let val = obj[key]
				if (!val) {
					if (Array.isArray(headers[i + 1])) {
						obj[key] = headers[i + 1].map((x) => x.toString("utf8"))
					} else {
						obj[key] = headers[i + 1].toString("utf8")
					}
				} else {
					if (!Array.isArray(val)) {
						val = [val]
						obj[key] = val
					}
					val.push(headers[i + 1].toString("utf8"))
				}
			}
			if ("content-length" in obj && "content-disposition" in obj) {
				obj["content-disposition"] = Buffer.from(obj["content-disposition"]).toString("latin1")
			}
			return obj
		}
		function parseRawHeaders(headers) {
			const ret = []
			let hasContentLength = false
			let contentDispositionIdx = -1
			for (let n = 0; n < headers.length; n += 2) {
				const key = headers[n + 0].toString()
				const val = headers[n + 1].toString("utf8")
				if (
					key.length === 14 &&
					(key === "content-length" || key.toLowerCase() === "content-length")
				) {
					ret.push(key, val)
					hasContentLength = true
				} else if (
					key.length === 19 &&
					(key === "content-disposition" || key.toLowerCase() === "content-disposition")
				) {
					contentDispositionIdx = ret.push(key, val) - 1
				} else {
					ret.push(key, val)
				}
			}
			if (hasContentLength && contentDispositionIdx !== -1) {
				ret[contentDispositionIdx] = Buffer.from(ret[contentDispositionIdx]).toString("latin1")
			}
			return ret
		}
		function isBuffer(buffer) {
			return buffer instanceof Uint8Array || Buffer.isBuffer(buffer)
		}
		function validateHandler(handler, method, upgrade) {
			if (!handler || typeof handler !== "object") {
				throw new InvalidArgumentError("handler must be an object")
			}
			if (typeof handler.onConnect !== "function") {
				throw new InvalidArgumentError("invalid onConnect method")
			}
			if (typeof handler.onError !== "function") {
				throw new InvalidArgumentError("invalid onError method")
			}
			if (typeof handler.onBodySent !== "function" && handler.onBodySent !== void 0) {
				throw new InvalidArgumentError("invalid onBodySent method")
			}
			if (upgrade || method === "CONNECT") {
				if (typeof handler.onUpgrade !== "function") {
					throw new InvalidArgumentError("invalid onUpgrade method")
				}
			} else {
				if (typeof handler.onHeaders !== "function") {
					throw new InvalidArgumentError("invalid onHeaders method")
				}
				if (typeof handler.onData !== "function") {
					throw new InvalidArgumentError("invalid onData method")
				}
				if (typeof handler.onComplete !== "function") {
					throw new InvalidArgumentError("invalid onComplete method")
				}
			}
		}
		function isDisturbed(body) {
			return !!(
				body &&
				(stream.isDisturbed
					? stream.isDisturbed(body) || body[kBodyUsed]
					: body[kBodyUsed] ||
					  body.readableDidRead ||
					  (body._readableState && body._readableState.dataEmitted) ||
					  isReadableAborted(body))
			)
		}
		function isErrored(body) {
			return !!(
				body &&
				(stream.isErrored
					? stream.isErrored(body)
					: /state: 'errored'/.test(nodeUtil.inspect(body)))
			)
		}
		function isReadable(body) {
			return !!(
				body &&
				(stream.isReadable
					? stream.isReadable(body)
					: /state: 'readable'/.test(nodeUtil.inspect(body)))
			)
		}
		function getSocketInfo(socket) {
			return {
				localAddress: socket.localAddress,
				localPort: socket.localPort,
				remoteAddress: socket.remoteAddress,
				remotePort: socket.remotePort,
				remoteFamily: socket.remoteFamily,
				timeout: socket.timeout,
				bytesWritten: socket.bytesWritten,
				bytesRead: socket.bytesRead,
			}
		}
		async function* convertIterableToBuffer(iterable) {
			for await (const chunk of iterable) {
				yield Buffer.isBuffer(chunk) ? chunk : Buffer.from(chunk)
			}
		}
		var ReadableStream
		function ReadableStreamFrom(iterable) {
			if (!ReadableStream) {
				ReadableStream = require("node:stream/web").ReadableStream
			}
			if (ReadableStream.from) {
				return ReadableStream.from(convertIterableToBuffer(iterable))
			}
			let iterator
			return new ReadableStream(
				{
					async start() {
						iterator = iterable[Symbol.asyncIterator]()
					},
					async pull(controller) {
						const { done, value } = await iterator.next()
						if (done) {
							queueMicrotask(() => {
								controller.close()
							})
						} else {
							const buf = Buffer.isBuffer(value) ? value : Buffer.from(value)
							controller.enqueue(new Uint8Array(buf))
						}
						return controller.desiredSize > 0
					},
					async cancel(reason) {
						await iterator.return()
					},
				},
				0
			)
		}
		function isFormDataLike(object) {
			return (
				object &&
				typeof object === "object" &&
				typeof object.append === "function" &&
				typeof object.delete === "function" &&
				typeof object.get === "function" &&
				typeof object.getAll === "function" &&
				typeof object.has === "function" &&
				typeof object.set === "function" &&
				object[Symbol.toStringTag] === "FormData"
			)
		}
		function throwIfAborted(signal) {
			if (!signal) {
				return
			}
			if (typeof signal.throwIfAborted === "function") {
				signal.throwIfAborted()
			} else {
				if (signal.aborted) {
					const err = new Error("The operation was aborted")
					err.name = "AbortError"
					throw err
				}
			}
		}
		function addAbortListener(signal, listener) {
			if ("addEventListener" in signal) {
				signal.addEventListener("abort", listener, { once: true })
				return () => signal.removeEventListener("abort", listener)
			}
			signal.addListener("abort", listener)
			return () => signal.removeListener("abort", listener)
		}
		var hasToWellFormed = !!String.prototype.toWellFormed
		function toUSVString(val) {
			if (hasToWellFormed) {
				return `${val}`.toWellFormed()
			} else if (nodeUtil.toUSVString) {
				return nodeUtil.toUSVString(val)
			}
			return `${val}`
		}
		function parseRangeHeader(range) {
			if (range == undefined || range === "") return { start: 0, end: null, size: null }
			const m = range ? range.match(/^bytes (\d+)-(\d+)\/(\d+)?$/) : null
			return m
				? {
						start: parseInt(m[1]),
						end: m[2] ? parseInt(m[2]) : null,
						size: m[3] ? parseInt(m[3]) : null,
				  }
				: null
		}
		var kEnumerableProperty = /* @__PURE__ */ Object.create(null)
		kEnumerableProperty.enumerable = true
		module2.exports = {
			kEnumerableProperty,
			nop,
			isDisturbed,
			isErrored,
			isReadable,
			toUSVString,
			isReadableAborted,
			isBlobLike,
			parseOrigin: parseOrigin2,
			parseURL,
			getServerName,
			isStream,
			isIterable,
			isAsyncIterable,
			isDestroyed,
			headerNameToString,
			parseRawHeaders,
			parseHeaders,
			parseKeepAliveTimeout,
			destroy,
			bodyLength,
			deepClone,
			ReadableStreamFrom,
			isBuffer,
			validateHandler,
			getSocketInfo,
			isFormDataLike,
			buildURL,
			throwIfAborted,
			addAbortListener,
			parseRangeHeader,
			nodeMajor,
			nodeMinor,
			nodeHasAutoSelectFamily: nodeMajor > 18 || (nodeMajor === 18 && nodeMinor >= 13),
			safeHTTPMethods: ["GET", "HEAD", "OPTIONS", "TRACE"],
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/timers.js
var require_timers = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/timers.js"(exports2, module2) {
		"use strict"
		var fastNow = Date.now()
		var fastNowTimeout
		var fastTimers = []
		function onTimeout() {
			fastNow = Date.now()
			let len = fastTimers.length
			let idx = 0
			while (idx < len) {
				const timer = fastTimers[idx]
				if (timer.state === 0) {
					timer.state = fastNow + timer.delay
				} else if (timer.state > 0 && fastNow >= timer.state) {
					timer.state = -1
					timer.callback(timer.opaque)
				}
				if (timer.state === -1) {
					timer.state = -2
					if (idx !== len - 1) {
						fastTimers[idx] = fastTimers.pop()
					} else {
						fastTimers.pop()
					}
					len -= 1
				} else {
					idx += 1
				}
			}
			if (fastTimers.length > 0) {
				refreshTimeout()
			}
		}
		function refreshTimeout() {
			if (fastNowTimeout && fastNowTimeout.refresh) {
				fastNowTimeout.refresh()
			} else {
				clearTimeout(fastNowTimeout)
				fastNowTimeout = setTimeout(onTimeout, 1e3)
				if (fastNowTimeout.unref) {
					fastNowTimeout.unref()
				}
			}
		}
		var Timeout = class {
			constructor(callback, delay, opaque) {
				this.callback = callback
				this.delay = delay
				this.opaque = opaque
				this.state = -2
				this.refresh()
			}
			refresh() {
				if (this.state === -2) {
					fastTimers.push(this)
					if (!fastNowTimeout || fastTimers.length === 1) {
						refreshTimeout()
					}
				}
				this.state = 0
			}
			clear() {
				this.state = -1
			}
		}
		module2.exports = {
			setTimeout(callback, delay, opaque) {
				return delay < 1e3
					? setTimeout(callback, delay, opaque)
					: new Timeout(callback, delay, opaque)
			},
			clearTimeout(timeout) {
				if (timeout instanceof Timeout) {
					timeout.clear()
				} else {
					clearTimeout(timeout)
				}
			},
		}
	},
})

// ../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/deps/streamsearch/sbmh.js
var require_sbmh = __commonJS({
	"../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/deps/streamsearch/sbmh.js"(
		exports2,
		module2
	) {
		"use strict"
		var EventEmitter = require("node:events").EventEmitter
		var inherits = require("node:util").inherits
		function SBMH(needle) {
			if (typeof needle === "string") {
				needle = Buffer.from(needle)
			}
			if (!Buffer.isBuffer(needle)) {
				throw new TypeError("The needle has to be a String or a Buffer.")
			}
			const needleLength = needle.length
			if (needleLength === 0) {
				throw new Error("The needle cannot be an empty String/Buffer.")
			}
			if (needleLength > 256) {
				throw new Error("The needle cannot have a length bigger than 256.")
			}
			this.maxMatches = Infinity
			this.matches = 0
			this._occ = new Array(256).fill(needleLength)
			this._lookbehind_size = 0
			this._needle = needle
			this._bufpos = 0
			this._lookbehind = Buffer.alloc(needleLength)
			for (var i = 0; i < needleLength - 1; ++i) {
				this._occ[needle[i]] = needleLength - 1 - i
			}
		}
		inherits(SBMH, EventEmitter)
		SBMH.prototype.reset = function () {
			this._lookbehind_size = 0
			this.matches = 0
			this._bufpos = 0
		}
		SBMH.prototype.push = function (chunk, pos) {
			if (!Buffer.isBuffer(chunk)) {
				chunk = Buffer.from(chunk, "binary")
			}
			const chlen = chunk.length
			this._bufpos = pos || 0
			let r
			while (r !== chlen && this.matches < this.maxMatches) {
				r = this._sbmh_feed(chunk)
			}
			return r
		}
		SBMH.prototype._sbmh_feed = function (data) {
			const len = data.length
			const needle = this._needle
			const needleLength = needle.length
			const lastNeedleChar = needle[needleLength - 1]
			let pos = -this._lookbehind_size
			let ch
			if (pos < 0) {
				while (pos < 0 && pos <= len - needleLength) {
					ch = this._sbmh_lookup_char(data, pos + needleLength - 1)
					if (ch === lastNeedleChar && this._sbmh_memcmp(data, pos, needleLength - 1)) {
						this._lookbehind_size = 0
						++this.matches
						this.emit("info", true)
						return (this._bufpos = pos + needleLength)
					}
					pos += this._occ[ch]
				}
				if (pos < 0) {
					while (pos < 0 && !this._sbmh_memcmp(data, pos, len - pos)) {
						++pos
					}
				}
				if (pos >= 0) {
					this.emit("info", false, this._lookbehind, 0, this._lookbehind_size)
					this._lookbehind_size = 0
				} else {
					const bytesToCutOff = this._lookbehind_size + pos
					if (bytesToCutOff > 0) {
						this.emit("info", false, this._lookbehind, 0, bytesToCutOff)
					}
					this._lookbehind.copy(
						this._lookbehind,
						0,
						bytesToCutOff,
						this._lookbehind_size - bytesToCutOff
					)
					this._lookbehind_size -= bytesToCutOff
					data.copy(this._lookbehind, this._lookbehind_size)
					this._lookbehind_size += len
					this._bufpos = len
					return len
				}
			}
			pos += (pos >= 0) * this._bufpos
			if (data.includes(needle, pos)) {
				pos = data.indexOf(needle, pos)
				++this.matches
				if (pos > 0) {
					this.emit("info", true, data, this._bufpos, pos)
				} else {
					this.emit("info", true)
				}
				return (this._bufpos = pos + needleLength)
			} else {
				pos = len - needleLength
			}
			while (
				pos < len &&
				(data[pos] !== needle[0] ||
					Buffer.compare(data.subarray(pos, pos + len - pos), needle.subarray(0, len - pos)) !== 0)
			) {
				++pos
			}
			if (pos < len) {
				data.copy(this._lookbehind, 0, pos, pos + (len - pos))
				this._lookbehind_size = len - pos
			}
			if (pos > 0) {
				this.emit("info", false, data, this._bufpos, pos < len ? pos : len)
			}
			this._bufpos = len
			return len
		}
		SBMH.prototype._sbmh_lookup_char = function (data, pos) {
			return pos < 0 ? this._lookbehind[this._lookbehind_size + pos] : data[pos]
		}
		SBMH.prototype._sbmh_memcmp = function (data, pos, len) {
			for (var i = 0; i < len; ++i) {
				if (this._sbmh_lookup_char(data, pos + i) !== this._needle[i]) {
					return false
				}
			}
			return true
		}
		module2.exports = SBMH
	},
})

// ../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/deps/dicer/lib/PartStream.js
var require_PartStream = __commonJS({
	"../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/deps/dicer/lib/PartStream.js"(
		exports2,
		module2
	) {
		"use strict"
		var inherits = require("node:util").inherits
		var ReadableStream = require("node:stream").Readable
		function PartStream(opts) {
			ReadableStream.call(this, opts)
		}
		inherits(PartStream, ReadableStream)
		PartStream.prototype._read = function (n) {}
		module2.exports = PartStream
	},
})

// ../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/lib/utils/getLimit.js
var require_getLimit = __commonJS({
	"../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/lib/utils/getLimit.js"(
		exports2,
		module2
	) {
		"use strict"
		module2.exports = function getLimit(limits, name, defaultLimit) {
			if (!limits || limits[name] === void 0 || limits[name] === null) {
				return defaultLimit
			}
			if (typeof limits[name] !== "number" || isNaN(limits[name])) {
				throw new TypeError("Limit " + name + " is not a valid number")
			}
			return limits[name]
		}
	},
})

// ../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/deps/dicer/lib/HeaderParser.js
var require_HeaderParser = __commonJS({
	"../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/deps/dicer/lib/HeaderParser.js"(
		exports2,
		module2
	) {
		"use strict"
		var EventEmitter = require("node:events").EventEmitter
		var inherits = require("node:util").inherits
		var getLimit = require_getLimit()
		var StreamSearch = require_sbmh()
		var B_DCRLF = Buffer.from("\r\n\r\n")
		var RE_CRLF = /\r\n/g
		var RE_HDR = /^([^:]+):[ \t]?([\x00-\xFF]+)?$/
		function HeaderParser(cfg) {
			EventEmitter.call(this)
			cfg = cfg || {}
			const self2 = this
			this.nread = 0
			this.maxed = false
			this.npairs = 0
			this.maxHeaderPairs = getLimit(cfg, "maxHeaderPairs", 2e3)
			this.maxHeaderSize = getLimit(cfg, "maxHeaderSize", 80 * 1024)
			this.buffer = ""
			this.header = {}
			this.finished = false
			this.ss = new StreamSearch(B_DCRLF)
			this.ss.on("info", function (isMatch, data, start, end) {
				if (data && !self2.maxed) {
					if (self2.nread + end - start >= self2.maxHeaderSize) {
						end = self2.maxHeaderSize - self2.nread + start
						self2.nread = self2.maxHeaderSize
						self2.maxed = true
					} else {
						self2.nread += end - start
					}
					self2.buffer += data.toString("binary", start, end)
				}
				if (isMatch) {
					self2._finish()
				}
			})
		}
		inherits(HeaderParser, EventEmitter)
		HeaderParser.prototype.push = function (data) {
			const r = this.ss.push(data)
			if (this.finished) {
				return r
			}
		}
		HeaderParser.prototype.reset = function () {
			this.finished = false
			this.buffer = ""
			this.header = {}
			this.ss.reset()
		}
		HeaderParser.prototype._finish = function () {
			if (this.buffer) {
				this._parseHeader()
			}
			this.ss.matches = this.ss.maxMatches
			const header = this.header
			this.header = {}
			this.buffer = ""
			this.finished = true
			this.nread = this.npairs = 0
			this.maxed = false
			this.emit("header", header)
		}
		HeaderParser.prototype._parseHeader = function () {
			if (this.npairs === this.maxHeaderPairs) {
				return
			}
			const lines = this.buffer.split(RE_CRLF)
			const len = lines.length
			let m, h
			for (var i = 0; i < len; ++i) {
				if (lines[i].length === 0) {
					continue
				}
				if ((lines[i][0] === "	" || lines[i][0] === " ") && h) {
					this.header[h][this.header[h].length - 1] += lines[i]
					continue
				}
				const posColon = lines[i].indexOf(":")
				if (posColon === -1 || posColon === 0) {
					return
				}
				m = RE_HDR.exec(lines[i])
				h = m[1].toLowerCase()
				this.header[h] = this.header[h] || []
				this.header[h].push(m[2] || "")
				if (++this.npairs === this.maxHeaderPairs) {
					break
				}
			}
		}
		module2.exports = HeaderParser
	},
})

// ../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/deps/dicer/lib/Dicer.js
var require_Dicer = __commonJS({
	"../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/deps/dicer/lib/Dicer.js"(
		exports2,
		module2
	) {
		"use strict"
		var WritableStream = require("node:stream").Writable
		var inherits = require("node:util").inherits
		var StreamSearch = require_sbmh()
		var PartStream = require_PartStream()
		var HeaderParser = require_HeaderParser()
		var DASH = 45
		var B_ONEDASH = Buffer.from("-")
		var B_CRLF = Buffer.from("\r\n")
		var EMPTY_FN = function () {}
		function Dicer(cfg) {
			if (!(this instanceof Dicer)) {
				return new Dicer(cfg)
			}
			WritableStream.call(this, cfg)
			if (!cfg || (!cfg.headerFirst && typeof cfg.boundary !== "string")) {
				throw new TypeError("Boundary required")
			}
			if (typeof cfg.boundary === "string") {
				this.setBoundary(cfg.boundary)
			} else {
				this._bparser = void 0
			}
			this._headerFirst = cfg.headerFirst
			this._dashes = 0
			this._parts = 0
			this._finished = false
			this._realFinish = false
			this._isPreamble = true
			this._justMatched = false
			this._firstWrite = true
			this._inHeader = true
			this._part = void 0
			this._cb = void 0
			this._ignoreData = false
			this._partOpts = { highWaterMark: cfg.partHwm }
			this._pause = false
			const self2 = this
			this._hparser = new HeaderParser(cfg)
			this._hparser.on("header", function (header) {
				self2._inHeader = false
				self2._part.emit("header", header)
			})
		}
		inherits(Dicer, WritableStream)
		Dicer.prototype.emit = function (ev) {
			if (ev === "finish" && !this._realFinish) {
				if (!this._finished) {
					const self2 = this
					process.nextTick(function () {
						self2.emit("error", new Error("Unexpected end of multipart data"))
						if (self2._part && !self2._ignoreData) {
							const type = self2._isPreamble ? "Preamble" : "Part"
							self2._part.emit(
								"error",
								new Error(type + " terminated early due to unexpected end of multipart data")
							)
							self2._part.push(null)
							process.nextTick(function () {
								self2._realFinish = true
								self2.emit("finish")
								self2._realFinish = false
							})
							return
						}
						self2._realFinish = true
						self2.emit("finish")
						self2._realFinish = false
					})
				}
			} else {
				WritableStream.prototype.emit.apply(this, arguments)
			}
		}
		Dicer.prototype._write = function (data, encoding, cb) {
			if (!this._hparser && !this._bparser) {
				return cb()
			}
			if (this._headerFirst && this._isPreamble) {
				if (!this._part) {
					this._part = new PartStream(this._partOpts)
					if (this.listenerCount("preamble") !== 0) {
						this.emit("preamble", this._part)
					} else {
						this._ignore()
					}
				}
				const r = this._hparser.push(data)
				if (!this._inHeader && r !== void 0 && r < data.length) {
					data = data.slice(r)
				} else {
					return cb()
				}
			}
			if (this._firstWrite) {
				this._bparser.push(B_CRLF)
				this._firstWrite = false
			}
			this._bparser.push(data)
			if (this._pause) {
				this._cb = cb
			} else {
				cb()
			}
		}
		Dicer.prototype.reset = function () {
			this._part = void 0
			this._bparser = void 0
			this._hparser = void 0
		}
		Dicer.prototype.setBoundary = function (boundary) {
			const self2 = this
			this._bparser = new StreamSearch("\r\n--" + boundary)
			this._bparser.on("info", function (isMatch, data, start, end) {
				self2._oninfo(isMatch, data, start, end)
			})
		}
		Dicer.prototype._ignore = function () {
			if (this._part && !this._ignoreData) {
				this._ignoreData = true
				this._part.on("error", EMPTY_FN)
				this._part.resume()
			}
		}
		Dicer.prototype._oninfo = function (isMatch, data, start, end) {
			let buf
			const self2 = this
			let i = 0
			let r
			let shouldWriteMore = true
			if (!this._part && this._justMatched && data) {
				while (this._dashes < 2 && start + i < end) {
					if (data[start + i] === DASH) {
						++i
						++this._dashes
					} else {
						if (this._dashes) {
							buf = B_ONEDASH
						}
						this._dashes = 0
						break
					}
				}
				if (this._dashes === 2) {
					if (start + i < end && this.listenerCount("trailer") !== 0) {
						this.emit("trailer", data.slice(start + i, end))
					}
					this.reset()
					this._finished = true
					if (self2._parts === 0) {
						self2._realFinish = true
						self2.emit("finish")
						self2._realFinish = false
					}
				}
				if (this._dashes) {
					return
				}
			}
			if (this._justMatched) {
				this._justMatched = false
			}
			if (!this._part) {
				this._part = new PartStream(this._partOpts)
				this._part._read = function (n) {
					self2._unpause()
				}
				if (this._isPreamble && this.listenerCount("preamble") !== 0) {
					this.emit("preamble", this._part)
				} else if (this._isPreamble !== true && this.listenerCount("part") !== 0) {
					this.emit("part", this._part)
				} else {
					this._ignore()
				}
				if (!this._isPreamble) {
					this._inHeader = true
				}
			}
			if (data && start < end && !this._ignoreData) {
				if (this._isPreamble || !this._inHeader) {
					if (buf) {
						shouldWriteMore = this._part.push(buf)
					}
					shouldWriteMore = this._part.push(data.slice(start, end))
					if (!shouldWriteMore) {
						this._pause = true
					}
				} else if (!this._isPreamble && this._inHeader) {
					if (buf) {
						this._hparser.push(buf)
					}
					r = this._hparser.push(data.slice(start, end))
					if (!this._inHeader && r !== void 0 && r < end) {
						this._oninfo(false, data, start + r, end)
					}
				}
			}
			if (isMatch) {
				this._hparser.reset()
				if (this._isPreamble) {
					this._isPreamble = false
				} else {
					if (start !== end) {
						++this._parts
						this._part.on("end", function () {
							if (--self2._parts === 0) {
								if (self2._finished) {
									self2._realFinish = true
									self2.emit("finish")
									self2._realFinish = false
								} else {
									self2._unpause()
								}
							}
						})
					}
				}
				this._part.push(null)
				this._part = void 0
				this._ignoreData = false
				this._justMatched = true
				this._dashes = 0
			}
		}
		Dicer.prototype._unpause = function () {
			if (!this._pause) {
				return
			}
			this._pause = false
			if (this._cb) {
				const cb = this._cb
				this._cb = void 0
				cb()
			}
		}
		module2.exports = Dicer
	},
})

// ../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/lib/utils/decodeText.js
var require_decodeText = __commonJS({
	"../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/lib/utils/decodeText.js"(
		exports2,
		module2
	) {
		"use strict"
		var utf8Decoder = new TextDecoder("utf-8")
		var textDecoders = /* @__PURE__ */ new Map([
			["utf-8", utf8Decoder],
			["utf8", utf8Decoder],
		])
		function getDecoder(charset) {
			let lc
			while (true) {
				switch (charset) {
					case "utf-8":
					case "utf8":
						return decoders.utf8
					case "latin1":
					case "ascii":
					case "us-ascii":
					case "iso-8859-1":
					case "iso8859-1":
					case "iso88591":
					case "iso_8859-1":
					case "windows-1252":
					case "iso_8859-1:1987":
					case "cp1252":
					case "x-cp1252":
						return decoders.latin1
					case "utf16le":
					case "utf-16le":
					case "ucs2":
					case "ucs-2":
						return decoders.utf16le
					case "base64":
						return decoders.base64
					default:
						if (lc === void 0) {
							lc = true
							charset = charset.toLowerCase()
							continue
						}
						return decoders.other.bind(charset)
				}
			}
		}
		var decoders = {
			utf8: (data, sourceEncoding) => {
				if (data.length === 0) {
					return ""
				}
				if (typeof data === "string") {
					data = Buffer.from(data, sourceEncoding)
				}
				return data.utf8Slice(0, data.length)
			},
			latin1: (data, sourceEncoding) => {
				if (data.length === 0) {
					return ""
				}
				if (typeof data === "string") {
					return data
				}
				return data.latin1Slice(0, data.length)
			},
			utf16le: (data, sourceEncoding) => {
				if (data.length === 0) {
					return ""
				}
				if (typeof data === "string") {
					data = Buffer.from(data, sourceEncoding)
				}
				return data.ucs2Slice(0, data.length)
			},
			base64: (data, sourceEncoding) => {
				if (data.length === 0) {
					return ""
				}
				if (typeof data === "string") {
					data = Buffer.from(data, sourceEncoding)
				}
				return data.base64Slice(0, data.length)
			},
			other: (data, sourceEncoding) => {
				if (data.length === 0) {
					return ""
				}
				if (typeof data === "string") {
					data = Buffer.from(data, sourceEncoding)
				}
				if (textDecoders.has(exports2.toString())) {
					try {
						return textDecoders.get(exports2).decode(data)
					} catch {}
				}
				return typeof data === "string" ? data : data.toString()
			},
		}
		function decodeText(text, sourceEncoding, destEncoding) {
			if (text) {
				return getDecoder(destEncoding)(text, sourceEncoding)
			}
			return text
		}
		module2.exports = decodeText
	},
})

// ../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/lib/utils/parseParams.js
var require_parseParams = __commonJS({
	"../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/lib/utils/parseParams.js"(
		exports2,
		module2
	) {
		"use strict"
		var decodeText = require_decodeText()
		var RE_ENCODED = /%[a-fA-F0-9][a-fA-F0-9]/g
		var EncodedLookup = {
			"%00": "\0",
			"%01": "",
			"%02": "",
			"%03": "",
			"%04": "",
			"%05": "",
			"%06": "",
			"%07": "\x07",
			"%08": "\b",
			"%09": "	",
			"%0a": "\n",
			"%0A": "\n",
			"%0b": "\v",
			"%0B": "\v",
			"%0c": "\f",
			"%0C": "\f",
			"%0d": "\r",
			"%0D": "\r",
			"%0e": "",
			"%0E": "",
			"%0f": "",
			"%0F": "",
			"%10": "",
			"%11": "",
			"%12": "",
			"%13": "",
			"%14": "",
			"%15": "",
			"%16": "",
			"%17": "",
			"%18": "",
			"%19": "",
			"%1a": "",
			"%1A": "",
			"%1b": "\x1B",
			"%1B": "\x1B",
			"%1c": "",
			"%1C": "",
			"%1d": "",
			"%1D": "",
			"%1e": "",
			"%1E": "",
			"%1f": "",
			"%1F": "",
			"%20": " ",
			"%21": "!",
			"%22": '"',
			"%23": "#",
			"%24": "$",
			"%25": "%",
			"%26": "&",
			"%27": "'",
			"%28": "(",
			"%29": ")",
			"%2a": "*",
			"%2A": "*",
			"%2b": "+",
			"%2B": "+",
			"%2c": ",",
			"%2C": ",",
			"%2d": "-",
			"%2D": "-",
			"%2e": ".",
			"%2E": ".",
			"%2f": "/",
			"%2F": "/",
			"%30": "0",
			"%31": "1",
			"%32": "2",
			"%33": "3",
			"%34": "4",
			"%35": "5",
			"%36": "6",
			"%37": "7",
			"%38": "8",
			"%39": "9",
			"%3a": ":",
			"%3A": ":",
			"%3b": ";",
			"%3B": ";",
			"%3c": "<",
			"%3C": "<",
			"%3d": "=",
			"%3D": "=",
			"%3e": ">",
			"%3E": ">",
			"%3f": "?",
			"%3F": "?",
			"%40": "@",
			"%41": "A",
			"%42": "B",
			"%43": "C",
			"%44": "D",
			"%45": "E",
			"%46": "F",
			"%47": "G",
			"%48": "H",
			"%49": "I",
			"%4a": "J",
			"%4A": "J",
			"%4b": "K",
			"%4B": "K",
			"%4c": "L",
			"%4C": "L",
			"%4d": "M",
			"%4D": "M",
			"%4e": "N",
			"%4E": "N",
			"%4f": "O",
			"%4F": "O",
			"%50": "P",
			"%51": "Q",
			"%52": "R",
			"%53": "S",
			"%54": "T",
			"%55": "U",
			"%56": "V",
			"%57": "W",
			"%58": "X",
			"%59": "Y",
			"%5a": "Z",
			"%5A": "Z",
			"%5b": "[",
			"%5B": "[",
			"%5c": "\\",
			"%5C": "\\",
			"%5d": "]",
			"%5D": "]",
			"%5e": "^",
			"%5E": "^",
			"%5f": "_",
			"%5F": "_",
			"%60": "`",
			"%61": "a",
			"%62": "b",
			"%63": "c",
			"%64": "d",
			"%65": "e",
			"%66": "f",
			"%67": "g",
			"%68": "h",
			"%69": "i",
			"%6a": "j",
			"%6A": "j",
			"%6b": "k",
			"%6B": "k",
			"%6c": "l",
			"%6C": "l",
			"%6d": "m",
			"%6D": "m",
			"%6e": "n",
			"%6E": "n",
			"%6f": "o",
			"%6F": "o",
			"%70": "p",
			"%71": "q",
			"%72": "r",
			"%73": "s",
			"%74": "t",
			"%75": "u",
			"%76": "v",
			"%77": "w",
			"%78": "x",
			"%79": "y",
			"%7a": "z",
			"%7A": "z",
			"%7b": "{",
			"%7B": "{",
			"%7c": "|",
			"%7C": "|",
			"%7d": "}",
			"%7D": "}",
			"%7e": "~",
			"%7E": "~",
			"%7f": "\x7F",
			"%7F": "\x7F",
			"%80": "\x80",
			"%81": "\x81",
			"%82": "\x82",
			"%83": "\x83",
			"%84": "\x84",
			"%85": "\x85",
			"%86": "\x86",
			"%87": "\x87",
			"%88": "\x88",
			"%89": "\x89",
			"%8a": "\x8A",
			"%8A": "\x8A",
			"%8b": "\x8B",
			"%8B": "\x8B",
			"%8c": "\x8C",
			"%8C": "\x8C",
			"%8d": "\x8D",
			"%8D": "\x8D",
			"%8e": "\x8E",
			"%8E": "\x8E",
			"%8f": "\x8F",
			"%8F": "\x8F",
			"%90": "\x90",
			"%91": "\x91",
			"%92": "\x92",
			"%93": "\x93",
			"%94": "\x94",
			"%95": "\x95",
			"%96": "\x96",
			"%97": "\x97",
			"%98": "\x98",
			"%99": "\x99",
			"%9a": "\x9A",
			"%9A": "\x9A",
			"%9b": "\x9B",
			"%9B": "\x9B",
			"%9c": "\x9C",
			"%9C": "\x9C",
			"%9d": "\x9D",
			"%9D": "\x9D",
			"%9e": "\x9E",
			"%9E": "\x9E",
			"%9f": "\x9F",
			"%9F": "\x9F",
			"%a0": "\xA0",
			"%A0": "\xA0",
			"%a1": "\xA1",
			"%A1": "\xA1",
			"%a2": "\xA2",
			"%A2": "\xA2",
			"%a3": "\xA3",
			"%A3": "\xA3",
			"%a4": "\xA4",
			"%A4": "\xA4",
			"%a5": "\xA5",
			"%A5": "\xA5",
			"%a6": "\xA6",
			"%A6": "\xA6",
			"%a7": "\xA7",
			"%A7": "\xA7",
			"%a8": "\xA8",
			"%A8": "\xA8",
			"%a9": "\xA9",
			"%A9": "\xA9",
			"%aa": "\xAA",
			"%Aa": "\xAA",
			"%aA": "\xAA",
			"%AA": "\xAA",
			"%ab": "\xAB",
			"%Ab": "\xAB",
			"%aB": "\xAB",
			"%AB": "\xAB",
			"%ac": "\xAC",
			"%Ac": "\xAC",
			"%aC": "\xAC",
			"%AC": "\xAC",
			"%ad": "\xAD",
			"%Ad": "\xAD",
			"%aD": "\xAD",
			"%AD": "\xAD",
			"%ae": "\xAE",
			"%Ae": "\xAE",
			"%aE": "\xAE",
			"%AE": "\xAE",
			"%af": "\xAF",
			"%Af": "\xAF",
			"%aF": "\xAF",
			"%AF": "\xAF",
			"%b0": "\xB0",
			"%B0": "\xB0",
			"%b1": "\xB1",
			"%B1": "\xB1",
			"%b2": "\xB2",
			"%B2": "\xB2",
			"%b3": "\xB3",
			"%B3": "\xB3",
			"%b4": "\xB4",
			"%B4": "\xB4",
			"%b5": "\xB5",
			"%B5": "\xB5",
			"%b6": "\xB6",
			"%B6": "\xB6",
			"%b7": "\xB7",
			"%B7": "\xB7",
			"%b8": "\xB8",
			"%B8": "\xB8",
			"%b9": "\xB9",
			"%B9": "\xB9",
			"%ba": "\xBA",
			"%Ba": "\xBA",
			"%bA": "\xBA",
			"%BA": "\xBA",
			"%bb": "\xBB",
			"%Bb": "\xBB",
			"%bB": "\xBB",
			"%BB": "\xBB",
			"%bc": "\xBC",
			"%Bc": "\xBC",
			"%bC": "\xBC",
			"%BC": "\xBC",
			"%bd": "\xBD",
			"%Bd": "\xBD",
			"%bD": "\xBD",
			"%BD": "\xBD",
			"%be": "\xBE",
			"%Be": "\xBE",
			"%bE": "\xBE",
			"%BE": "\xBE",
			"%bf": "\xBF",
			"%Bf": "\xBF",
			"%bF": "\xBF",
			"%BF": "\xBF",
			"%c0": "\xC0",
			"%C0": "\xC0",
			"%c1": "\xC1",
			"%C1": "\xC1",
			"%c2": "\xC2",
			"%C2": "\xC2",
			"%c3": "\xC3",
			"%C3": "\xC3",
			"%c4": "\xC4",
			"%C4": "\xC4",
			"%c5": "\xC5",
			"%C5": "\xC5",
			"%c6": "\xC6",
			"%C6": "\xC6",
			"%c7": "\xC7",
			"%C7": "\xC7",
			"%c8": "\xC8",
			"%C8": "\xC8",
			"%c9": "\xC9",
			"%C9": "\xC9",
			"%ca": "\xCA",
			"%Ca": "\xCA",
			"%cA": "\xCA",
			"%CA": "\xCA",
			"%cb": "\xCB",
			"%Cb": "\xCB",
			"%cB": "\xCB",
			"%CB": "\xCB",
			"%cc": "\xCC",
			"%Cc": "\xCC",
			"%cC": "\xCC",
			"%CC": "\xCC",
			"%cd": "\xCD",
			"%Cd": "\xCD",
			"%cD": "\xCD",
			"%CD": "\xCD",
			"%ce": "\xCE",
			"%Ce": "\xCE",
			"%cE": "\xCE",
			"%CE": "\xCE",
			"%cf": "\xCF",
			"%Cf": "\xCF",
			"%cF": "\xCF",
			"%CF": "\xCF",
			"%d0": "\xD0",
			"%D0": "\xD0",
			"%d1": "\xD1",
			"%D1": "\xD1",
			"%d2": "\xD2",
			"%D2": "\xD2",
			"%d3": "\xD3",
			"%D3": "\xD3",
			"%d4": "\xD4",
			"%D4": "\xD4",
			"%d5": "\xD5",
			"%D5": "\xD5",
			"%d6": "\xD6",
			"%D6": "\xD6",
			"%d7": "\xD7",
			"%D7": "\xD7",
			"%d8": "\xD8",
			"%D8": "\xD8",
			"%d9": "\xD9",
			"%D9": "\xD9",
			"%da": "\xDA",
			"%Da": "\xDA",
			"%dA": "\xDA",
			"%DA": "\xDA",
			"%db": "\xDB",
			"%Db": "\xDB",
			"%dB": "\xDB",
			"%DB": "\xDB",
			"%dc": "\xDC",
			"%Dc": "\xDC",
			"%dC": "\xDC",
			"%DC": "\xDC",
			"%dd": "\xDD",
			"%Dd": "\xDD",
			"%dD": "\xDD",
			"%DD": "\xDD",
			"%de": "\xDE",
			"%De": "\xDE",
			"%dE": "\xDE",
			"%DE": "\xDE",
			"%df": "\xDF",
			"%Df": "\xDF",
			"%dF": "\xDF",
			"%DF": "\xDF",
			"%e0": "\xE0",
			"%E0": "\xE0",
			"%e1": "\xE1",
			"%E1": "\xE1",
			"%e2": "\xE2",
			"%E2": "\xE2",
			"%e3": "\xE3",
			"%E3": "\xE3",
			"%e4": "\xE4",
			"%E4": "\xE4",
			"%e5": "\xE5",
			"%E5": "\xE5",
			"%e6": "\xE6",
			"%E6": "\xE6",
			"%e7": "\xE7",
			"%E7": "\xE7",
			"%e8": "\xE8",
			"%E8": "\xE8",
			"%e9": "\xE9",
			"%E9": "\xE9",
			"%ea": "\xEA",
			"%Ea": "\xEA",
			"%eA": "\xEA",
			"%EA": "\xEA",
			"%eb": "\xEB",
			"%Eb": "\xEB",
			"%eB": "\xEB",
			"%EB": "\xEB",
			"%ec": "\xEC",
			"%Ec": "\xEC",
			"%eC": "\xEC",
			"%EC": "\xEC",
			"%ed": "\xED",
			"%Ed": "\xED",
			"%eD": "\xED",
			"%ED": "\xED",
			"%ee": "\xEE",
			"%Ee": "\xEE",
			"%eE": "\xEE",
			"%EE": "\xEE",
			"%ef": "\xEF",
			"%Ef": "\xEF",
			"%eF": "\xEF",
			"%EF": "\xEF",
			"%f0": "\xF0",
			"%F0": "\xF0",
			"%f1": "\xF1",
			"%F1": "\xF1",
			"%f2": "\xF2",
			"%F2": "\xF2",
			"%f3": "\xF3",
			"%F3": "\xF3",
			"%f4": "\xF4",
			"%F4": "\xF4",
			"%f5": "\xF5",
			"%F5": "\xF5",
			"%f6": "\xF6",
			"%F6": "\xF6",
			"%f7": "\xF7",
			"%F7": "\xF7",
			"%f8": "\xF8",
			"%F8": "\xF8",
			"%f9": "\xF9",
			"%F9": "\xF9",
			"%fa": "\xFA",
			"%Fa": "\xFA",
			"%fA": "\xFA",
			"%FA": "\xFA",
			"%fb": "\xFB",
			"%Fb": "\xFB",
			"%fB": "\xFB",
			"%FB": "\xFB",
			"%fc": "\xFC",
			"%Fc": "\xFC",
			"%fC": "\xFC",
			"%FC": "\xFC",
			"%fd": "\xFD",
			"%Fd": "\xFD",
			"%fD": "\xFD",
			"%FD": "\xFD",
			"%fe": "\xFE",
			"%Fe": "\xFE",
			"%fE": "\xFE",
			"%FE": "\xFE",
			"%ff": "\xFF",
			"%Ff": "\xFF",
			"%fF": "\xFF",
			"%FF": "\xFF",
		}
		function encodedReplacer(match) {
			return EncodedLookup[match]
		}
		var STATE_KEY = 0
		var STATE_VALUE = 1
		var STATE_CHARSET = 2
		var STATE_LANG = 3
		function parseParams(str) {
			const res = []
			let state = STATE_KEY
			let charset = ""
			let inquote = false
			let escaping = false
			let p = 0
			let tmp = ""
			const len = str.length
			for (var i = 0; i < len; ++i) {
				const char = str[i]
				if (char === "\\" && inquote) {
					if (escaping) {
						escaping = false
					} else {
						escaping = true
						continue
					}
				} else if (char === '"') {
					if (!escaping) {
						if (inquote) {
							inquote = false
							state = STATE_KEY
						} else {
							inquote = true
						}
						continue
					} else {
						escaping = false
					}
				} else {
					if (escaping && inquote) {
						tmp += "\\"
					}
					escaping = false
					if ((state === STATE_CHARSET || state === STATE_LANG) && char === "'") {
						if (state === STATE_CHARSET) {
							state = STATE_LANG
							charset = tmp.slice(1)
						} else {
							state = STATE_VALUE
						}
						tmp = ""
						continue
					} else if (state === STATE_KEY && (char === "*" || char === "=") && res.length) {
						state = char === "*" ? STATE_CHARSET : STATE_VALUE
						res[p] = [tmp, void 0]
						tmp = ""
						continue
					} else if (!inquote && char === ";") {
						state = STATE_KEY
						if (charset) {
							if (tmp.length) {
								tmp = decodeText(tmp.replace(RE_ENCODED, encodedReplacer), "binary", charset)
							}
							charset = ""
						} else if (tmp.length) {
							tmp = decodeText(tmp, "binary", "utf8")
						}
						if (res[p] === void 0) {
							res[p] = tmp
						} else {
							res[p][1] = tmp
						}
						tmp = ""
						++p
						continue
					} else if (!inquote && (char === " " || char === "	")) {
						continue
					}
				}
				tmp += char
			}
			if (charset && tmp.length) {
				tmp = decodeText(tmp.replace(RE_ENCODED, encodedReplacer), "binary", charset)
			} else if (tmp) {
				tmp = decodeText(tmp, "binary", "utf8")
			}
			if (res[p] === void 0) {
				if (tmp) {
					res[p] = tmp
				}
			} else {
				res[p][1] = tmp
			}
			return res
		}
		module2.exports = parseParams
	},
})

// ../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/lib/utils/basename.js
var require_basename = __commonJS({
	"../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/lib/utils/basename.js"(
		exports2,
		module2
	) {
		"use strict"
		module2.exports = function basename2(path) {
			if (typeof path !== "string") {
				return ""
			}
			for (var i = path.length - 1; i >= 0; --i) {
				switch (path.charCodeAt(i)) {
					case 47:
					case 92:
						path = path.slice(i + 1)
						return path === ".." || path === "." ? "" : path
				}
			}
			return path === ".." || path === "." ? "" : path
		}
	},
})

// ../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/lib/types/multipart.js
var require_multipart = __commonJS({
	"../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/lib/types/multipart.js"(
		exports2,
		module2
	) {
		"use strict"
		var { Readable } = require("node:stream")
		var { inherits } = require("node:util")
		var Dicer = require_Dicer()
		var parseParams = require_parseParams()
		var decodeText = require_decodeText()
		var basename2 = require_basename()
		var getLimit = require_getLimit()
		var RE_BOUNDARY = /^boundary$/i
		var RE_FIELD = /^form-data$/i
		var RE_CHARSET = /^charset$/i
		var RE_FILENAME = /^filename$/i
		var RE_NAME = /^name$/i
		Multipart.detect = /^multipart\/form-data/i
		function Multipart(boy, cfg) {
			let i
			let len
			const self2 = this
			let boundary
			const limits = cfg.limits
			const isPartAFile =
				cfg.isPartAFile ||
				((fieldName, contentType, fileName) =>
					contentType === "application/octet-stream" || fileName !== void 0)
			const parsedConType = cfg.parsedConType || []
			const defCharset = cfg.defCharset || "utf8"
			const preservePath = cfg.preservePath
			const fileOpts = { highWaterMark: cfg.fileHwm }
			for (i = 0, len = parsedConType.length; i < len; ++i) {
				if (Array.isArray(parsedConType[i]) && RE_BOUNDARY.test(parsedConType[i][0])) {
					boundary = parsedConType[i][1]
					break
				}
			}
			function checkFinished() {
				if (nends === 0 && finished && !boy._done) {
					finished = false
					self2.end()
				}
			}
			if (typeof boundary !== "string") {
				throw new Error("Multipart: Boundary not found")
			}
			const fieldSizeLimit = getLimit(limits, "fieldSize", 1 * 1024 * 1024)
			const fileSizeLimit = getLimit(limits, "fileSize", Infinity)
			const filesLimit = getLimit(limits, "files", Infinity)
			const fieldsLimit = getLimit(limits, "fields", Infinity)
			const partsLimit = getLimit(limits, "parts", Infinity)
			const headerPairsLimit = getLimit(limits, "headerPairs", 2e3)
			const headerSizeLimit = getLimit(limits, "headerSize", 80 * 1024)
			let nfiles = 0
			let nfields = 0
			let nends = 0
			let curFile
			let curField
			let finished = false
			this._needDrain = false
			this._pause = false
			this._cb = void 0
			this._nparts = 0
			this._boy = boy
			const parserCfg = {
				boundary,
				maxHeaderPairs: headerPairsLimit,
				maxHeaderSize: headerSizeLimit,
				partHwm: fileOpts.highWaterMark,
				highWaterMark: cfg.highWaterMark,
			}
			this.parser = new Dicer(parserCfg)
			this.parser
				.on("drain", function () {
					self2._needDrain = false
					if (self2._cb && !self2._pause) {
						const cb = self2._cb
						self2._cb = void 0
						cb()
					}
				})
				.on("part", function onPart(part) {
					if (++self2._nparts > partsLimit) {
						self2.parser.removeListener("part", onPart)
						self2.parser.on("part", skipPart)
						boy.hitPartsLimit = true
						boy.emit("partsLimit")
						return skipPart(part)
					}
					if (curField) {
						const field = curField
						field.emit("end")
						field.removeAllListeners("end")
					}
					part
						.on("header", function (header) {
							let contype
							let fieldname
							let parsed
							let charset
							let encoding
							let filename
							let nsize = 0
							if (header["content-type"]) {
								parsed = parseParams(header["content-type"][0])
								if (parsed[0]) {
									contype = parsed[0].toLowerCase()
									for (i = 0, len = parsed.length; i < len; ++i) {
										if (RE_CHARSET.test(parsed[i][0])) {
											charset = parsed[i][1].toLowerCase()
											break
										}
									}
								}
							}
							if (contype === void 0) {
								contype = "text/plain"
							}
							if (charset === void 0) {
								charset = defCharset
							}
							if (header["content-disposition"]) {
								parsed = parseParams(header["content-disposition"][0])
								if (!RE_FIELD.test(parsed[0])) {
									return skipPart(part)
								}
								for (i = 0, len = parsed.length; i < len; ++i) {
									if (RE_NAME.test(parsed[i][0])) {
										fieldname = parsed[i][1]
									} else if (RE_FILENAME.test(parsed[i][0])) {
										filename = parsed[i][1]
										if (!preservePath) {
											filename = basename2(filename)
										}
									}
								}
							} else {
								return skipPart(part)
							}
							if (header["content-transfer-encoding"]) {
								encoding = header["content-transfer-encoding"][0].toLowerCase()
							} else {
								encoding = "7bit"
							}
							let onData, onEnd
							if (isPartAFile(fieldname, contype, filename)) {
								if (nfiles === filesLimit) {
									if (!boy.hitFilesLimit) {
										boy.hitFilesLimit = true
										boy.emit("filesLimit")
									}
									return skipPart(part)
								}
								++nfiles
								if (boy.listenerCount("file") === 0) {
									self2.parser._ignore()
									return
								}
								++nends
								const file = new FileStream(fileOpts)
								curFile = file
								file.on("end", function () {
									--nends
									self2._pause = false
									checkFinished()
									if (self2._cb && !self2._needDrain) {
										const cb = self2._cb
										self2._cb = void 0
										cb()
									}
								})
								file._read = function (n) {
									if (!self2._pause) {
										return
									}
									self2._pause = false
									if (self2._cb && !self2._needDrain) {
										const cb = self2._cb
										self2._cb = void 0
										cb()
									}
								}
								boy.emit("file", fieldname, file, filename, encoding, contype)
								onData = function (data) {
									if ((nsize += data.length) > fileSizeLimit) {
										const extralen = fileSizeLimit - nsize + data.length
										if (extralen > 0) {
											file.push(data.slice(0, extralen))
										}
										file.truncated = true
										file.bytesRead = fileSizeLimit
										part.removeAllListeners("data")
										file.emit("limit")
										return
									} else if (!file.push(data)) {
										self2._pause = true
									}
									file.bytesRead = nsize
								}
								onEnd = function () {
									curFile = void 0
									file.push(null)
								}
							} else {
								if (nfields === fieldsLimit) {
									if (!boy.hitFieldsLimit) {
										boy.hitFieldsLimit = true
										boy.emit("fieldsLimit")
									}
									return skipPart(part)
								}
								++nfields
								++nends
								let buffer = ""
								let truncated = false
								curField = part
								onData = function (data) {
									if ((nsize += data.length) > fieldSizeLimit) {
										const extralen = fieldSizeLimit - (nsize - data.length)
										buffer += data.toString("binary", 0, extralen)
										truncated = true
										part.removeAllListeners("data")
									} else {
										buffer += data.toString("binary")
									}
								}
								onEnd = function () {
									curField = void 0
									if (buffer.length) {
										buffer = decodeText(buffer, "binary", charset)
									}
									boy.emit("field", fieldname, buffer, false, truncated, encoding, contype)
									--nends
									checkFinished()
								}
							}
							part._readableState.sync = false
							part.on("data", onData)
							part.on("end", onEnd)
						})
						.on("error", function (err) {
							if (curFile) {
								curFile.emit("error", err)
							}
						})
				})
				.on("error", function (err) {
					boy.emit("error", err)
				})
				.on("finish", function () {
					finished = true
					checkFinished()
				})
		}
		Multipart.prototype.write = function (chunk, cb) {
			const r = this.parser.write(chunk)
			if (r && !this._pause) {
				cb()
			} else {
				this._needDrain = !r
				this._cb = cb
			}
		}
		Multipart.prototype.end = function () {
			const self2 = this
			if (self2.parser.writable) {
				self2.parser.end()
			} else if (!self2._boy._done) {
				process.nextTick(function () {
					self2._boy._done = true
					self2._boy.emit("finish")
				})
			}
		}
		function skipPart(part) {
			part.resume()
		}
		function FileStream(opts) {
			Readable.call(this, opts)
			this.bytesRead = 0
			this.truncated = false
		}
		inherits(FileStream, Readable)
		FileStream.prototype._read = function (n) {}
		module2.exports = Multipart
	},
})

// ../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/lib/utils/Decoder.js
var require_Decoder = __commonJS({
	"../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/lib/utils/Decoder.js"(
		exports2,
		module2
	) {
		"use strict"
		var RE_PLUS = /\+/g
		var HEX = [
			0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
			0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
			0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
			0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
			0, 0, 0, 0,
		]
		function Decoder() {
			this.buffer = void 0
		}
		Decoder.prototype.write = function (str) {
			str = str.replace(RE_PLUS, " ")
			let res = ""
			let i = 0
			let p = 0
			const len = str.length
			for (; i < len; ++i) {
				if (this.buffer !== void 0) {
					if (!HEX[str.charCodeAt(i)]) {
						res += "%" + this.buffer
						this.buffer = void 0
						--i
					} else {
						this.buffer += str[i]
						++p
						if (this.buffer.length === 2) {
							res += String.fromCharCode(parseInt(this.buffer, 16))
							this.buffer = void 0
						}
					}
				} else if (str[i] === "%") {
					if (i > p) {
						res += str.substring(p, i)
						p = i
					}
					this.buffer = ""
					++p
				}
			}
			if (p < len && this.buffer === void 0) {
				res += str.slice(Math.max(0, p))
			}
			return res
		}
		Decoder.prototype.reset = function () {
			this.buffer = void 0
		}
		module2.exports = Decoder
	},
})

// ../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/lib/types/urlencoded.js
var require_urlencoded = __commonJS({
	"../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/lib/types/urlencoded.js"(
		exports2,
		module2
	) {
		"use strict"
		var Decoder = require_Decoder()
		var decodeText = require_decodeText()
		var getLimit = require_getLimit()
		var RE_CHARSET = /^charset$/i
		UrlEncoded.detect = /^application\/x-www-form-urlencoded/i
		function UrlEncoded(boy, cfg) {
			const limits = cfg.limits
			const parsedConType = cfg.parsedConType
			this.boy = boy
			this.fieldSizeLimit = getLimit(limits, "fieldSize", 1 * 1024 * 1024)
			this.fieldNameSizeLimit = getLimit(limits, "fieldNameSize", 100)
			this.fieldsLimit = getLimit(limits, "fields", Infinity)
			let charset
			for (var i = 0, len = parsedConType.length; i < len; ++i) {
				if (Array.isArray(parsedConType[i]) && RE_CHARSET.test(parsedConType[i][0])) {
					charset = parsedConType[i][1].toLowerCase()
					break
				}
			}
			if (charset === void 0) {
				charset = cfg.defCharset || "utf8"
			}
			this.decoder = new Decoder()
			this.charset = charset
			this._fields = 0
			this._state = "key"
			this._checkingBytes = true
			this._bytesKey = 0
			this._bytesVal = 0
			this._key = ""
			this._val = ""
			this._keyTrunc = false
			this._valTrunc = false
			this._hitLimit = false
		}
		UrlEncoded.prototype.write = function (data, cb) {
			if (this._fields === this.fieldsLimit) {
				if (!this.boy.hitFieldsLimit) {
					this.boy.hitFieldsLimit = true
					this.boy.emit("fieldsLimit")
				}
				return cb()
			}
			let idxeq
			let idxamp
			let i
			let p = 0
			const len = data.length
			while (p < len) {
				if (this._state === "key") {
					idxeq = idxamp = void 0
					for (i = p; i < len; ++i) {
						if (!this._checkingBytes) {
							++p
						}
						if (data[i] === 61) {
							idxeq = i
							break
						} else if (data[i] === 38) {
							idxamp = i
							break
						}
						if (this._checkingBytes && this._bytesKey === this.fieldNameSizeLimit) {
							this._hitLimit = true
							break
						} else if (this._checkingBytes) {
							++this._bytesKey
						}
					}
					if (idxeq !== void 0) {
						if (idxeq > p) {
							this._key += this.decoder.write(data.toString("binary", p, idxeq))
						}
						this._state = "val"
						this._hitLimit = false
						this._checkingBytes = true
						this._val = ""
						this._bytesVal = 0
						this._valTrunc = false
						this.decoder.reset()
						p = idxeq + 1
					} else if (idxamp !== void 0) {
						++this._fields
						let key
						const keyTrunc = this._keyTrunc
						if (idxamp > p) {
							key = this._key += this.decoder.write(data.toString("binary", p, idxamp))
						} else {
							key = this._key
						}
						this._hitLimit = false
						this._checkingBytes = true
						this._key = ""
						this._bytesKey = 0
						this._keyTrunc = false
						this.decoder.reset()
						if (key.length) {
							this.boy.emit("field", decodeText(key, "binary", this.charset), "", keyTrunc, false)
						}
						p = idxamp + 1
						if (this._fields === this.fieldsLimit) {
							return cb()
						}
					} else if (this._hitLimit) {
						if (i > p) {
							this._key += this.decoder.write(data.toString("binary", p, i))
						}
						p = i
						if ((this._bytesKey = this._key.length) === this.fieldNameSizeLimit) {
							this._checkingBytes = false
							this._keyTrunc = true
						}
					} else {
						if (p < len) {
							this._key += this.decoder.write(data.toString("binary", p))
						}
						p = len
					}
				} else {
					idxamp = void 0
					for (i = p; i < len; ++i) {
						if (!this._checkingBytes) {
							++p
						}
						if (data[i] === 38) {
							idxamp = i
							break
						}
						if (this._checkingBytes && this._bytesVal === this.fieldSizeLimit) {
							this._hitLimit = true
							break
						} else if (this._checkingBytes) {
							++this._bytesVal
						}
					}
					if (idxamp !== void 0) {
						++this._fields
						if (idxamp > p) {
							this._val += this.decoder.write(data.toString("binary", p, idxamp))
						}
						this.boy.emit(
							"field",
							decodeText(this._key, "binary", this.charset),
							decodeText(this._val, "binary", this.charset),
							this._keyTrunc,
							this._valTrunc
						)
						this._state = "key"
						this._hitLimit = false
						this._checkingBytes = true
						this._key = ""
						this._bytesKey = 0
						this._keyTrunc = false
						this.decoder.reset()
						p = idxamp + 1
						if (this._fields === this.fieldsLimit) {
							return cb()
						}
					} else if (this._hitLimit) {
						if (i > p) {
							this._val += this.decoder.write(data.toString("binary", p, i))
						}
						p = i
						if (
							(this._val === "" && this.fieldSizeLimit === 0) ||
							(this._bytesVal = this._val.length) === this.fieldSizeLimit
						) {
							this._checkingBytes = false
							this._valTrunc = true
						}
					} else {
						if (p < len) {
							this._val += this.decoder.write(data.toString("binary", p))
						}
						p = len
					}
				}
			}
			cb()
		}
		UrlEncoded.prototype.end = function () {
			if (this.boy._done) {
				return
			}
			if (this._state === "key" && this._key.length > 0) {
				this.boy.emit(
					"field",
					decodeText(this._key, "binary", this.charset),
					"",
					this._keyTrunc,
					false
				)
			} else if (this._state === "val") {
				this.boy.emit(
					"field",
					decodeText(this._key, "binary", this.charset),
					decodeText(this._val, "binary", this.charset),
					this._keyTrunc,
					this._valTrunc
				)
			}
			this.boy._done = true
			this.boy.emit("finish")
		}
		module2.exports = UrlEncoded
	},
})

// ../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/lib/main.js
var require_main = __commonJS({
	"../../../node_modules/.pnpm/@fastify+busboy@2.1.1/node_modules/@fastify/busboy/lib/main.js"(
		exports2,
		module2
	) {
		"use strict"
		var WritableStream = require("node:stream").Writable
		var { inherits } = require("node:util")
		var Dicer = require_Dicer()
		var MultipartParser = require_multipart()
		var UrlencodedParser = require_urlencoded()
		var parseParams = require_parseParams()
		function Busboy(opts) {
			if (!(this instanceof Busboy)) {
				return new Busboy(opts)
			}
			if (typeof opts !== "object") {
				throw new TypeError("Busboy expected an options-Object.")
			}
			if (typeof opts.headers !== "object") {
				throw new TypeError("Busboy expected an options-Object with headers-attribute.")
			}
			if (typeof opts.headers["content-type"] !== "string") {
				throw new TypeError("Missing Content-Type-header.")
			}
			const { headers, ...streamOptions } = opts
			this.opts = {
				autoDestroy: false,
				...streamOptions,
			}
			WritableStream.call(this, this.opts)
			this._done = false
			this._parser = this.getParserByHeaders(headers)
			this._finished = false
		}
		inherits(Busboy, WritableStream)
		Busboy.prototype.emit = function (ev) {
			if (ev === "finish") {
				if (!this._done) {
					this._parser?.end()
					return
				} else if (this._finished) {
					return
				}
				this._finished = true
			}
			WritableStream.prototype.emit.apply(this, arguments)
		}
		Busboy.prototype.getParserByHeaders = function (headers) {
			const parsed = parseParams(headers["content-type"])
			const cfg = {
				defCharset: this.opts.defCharset,
				fileHwm: this.opts.fileHwm,
				headers,
				highWaterMark: this.opts.highWaterMark,
				isPartAFile: this.opts.isPartAFile,
				limits: this.opts.limits,
				parsedConType: parsed,
				preservePath: this.opts.preservePath,
			}
			if (MultipartParser.detect.test(parsed[0])) {
				return new MultipartParser(this, cfg)
			}
			if (UrlencodedParser.detect.test(parsed[0])) {
				return new UrlencodedParser(this, cfg)
			}
			throw new Error("Unsupported Content-Type.")
		}
		Busboy.prototype._write = function (chunk, encoding, cb) {
			this._parser.write(chunk, cb)
		}
		module2.exports = Busboy
		module2.exports.default = Busboy
		module2.exports.Busboy = Busboy
		module2.exports.Dicer = Dicer
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/constants.js
var require_constants2 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/constants.js"(
		exports2,
		module2
	) {
		"use strict"
		var { MessageChannel: MessageChannel2, receiveMessageOnPort } = require("node:worker_threads")
		var corsSafeListedMethods = ["GET", "HEAD", "POST"]
		var corsSafeListedMethodsSet = new Set(corsSafeListedMethods)
		var nullBodyStatus = [101, 204, 205, 304]
		var redirectStatus = [301, 302, 303, 307, 308]
		var redirectStatusSet = new Set(redirectStatus)
		var badPorts = [
			"1",
			"7",
			"9",
			"11",
			"13",
			"15",
			"17",
			"19",
			"20",
			"21",
			"22",
			"23",
			"25",
			"37",
			"42",
			"43",
			"53",
			"69",
			"77",
			"79",
			"87",
			"95",
			"101",
			"102",
			"103",
			"104",
			"109",
			"110",
			"111",
			"113",
			"115",
			"117",
			"119",
			"123",
			"135",
			"137",
			"139",
			"143",
			"161",
			"179",
			"389",
			"427",
			"465",
			"512",
			"513",
			"514",
			"515",
			"526",
			"530",
			"531",
			"532",
			"540",
			"548",
			"554",
			"556",
			"563",
			"587",
			"601",
			"636",
			"989",
			"990",
			"993",
			"995",
			"1719",
			"1720",
			"1723",
			"2049",
			"3659",
			"4045",
			"5060",
			"5061",
			"6000",
			"6566",
			"6665",
			"6666",
			"6667",
			"6668",
			"6669",
			"6697",
			"10080",
		]
		var badPortsSet = new Set(badPorts)
		var referrerPolicy = [
			"",
			"no-referrer",
			"no-referrer-when-downgrade",
			"same-origin",
			"origin",
			"strict-origin",
			"origin-when-cross-origin",
			"strict-origin-when-cross-origin",
			"unsafe-url",
		]
		var referrerPolicySet = new Set(referrerPolicy)
		var requestRedirect = ["follow", "manual", "error"]
		var safeMethods = ["GET", "HEAD", "OPTIONS", "TRACE"]
		var safeMethodsSet = new Set(safeMethods)
		var requestMode = ["navigate", "same-origin", "no-cors", "cors"]
		var requestCredentials = ["omit", "same-origin", "include"]
		var requestCache = [
			"default",
			"no-store",
			"reload",
			"no-cache",
			"force-cache",
			"only-if-cached",
		]
		var requestBodyHeader = [
			"content-encoding",
			"content-language",
			"content-location",
			"content-type",
			// See https://github.com/nodejs/undici/issues/2021
			// 'Content-Length' is a forbidden header name, which is typically
			// removed in the Headers implementation. However, undici doesn't
			// filter out headers, so we add it here.
			"content-length",
		]
		var requestDuplex = ["half"]
		var forbiddenMethods = ["CONNECT", "TRACE", "TRACK"]
		var forbiddenMethodsSet = new Set(forbiddenMethods)
		var subresource = [
			"audio",
			"audioworklet",
			"font",
			"image",
			"manifest",
			"paintworklet",
			"script",
			"style",
			"track",
			"video",
			"xslt",
			"",
		]
		var subresourceSet = new Set(subresource)
		var DOMException2 =
			globalThis.DOMException ??
			(() => {
				try {
					atob("~")
				} catch (err) {
					return Object.getPrototypeOf(err).constructor
				}
			})()
		var channel
		var structuredClone2 =
			globalThis.structuredClone ?? // https://github.com/nodejs/node/blob/b27ae24dcc4251bad726d9d84baf678d1f707fed/lib/internal/structured_clone.js
			// structuredClone was added in v17.0.0, but fetch supports v16.8
			function structuredClone3(value, options = void 0) {
				if (arguments.length === 0) {
					throw new TypeError("missing argument")
				}
				if (!channel) {
					channel = new MessageChannel2()
				}
				channel.port1.unref()
				channel.port2.unref()
				channel.port1.postMessage(value, options?.transfer)
				return receiveMessageOnPort(channel.port2).message
			}
		module2.exports = {
			DOMException: DOMException2,
			structuredClone: structuredClone2,
			subresource,
			forbiddenMethods,
			requestBodyHeader,
			referrerPolicy,
			requestRedirect,
			requestMode,
			requestCredentials,
			requestCache,
			redirectStatus,
			corsSafeListedMethods,
			nullBodyStatus,
			safeMethods,
			badPorts,
			requestDuplex,
			subresourceSet,
			badPortsSet,
			redirectStatusSet,
			corsSafeListedMethodsSet,
			safeMethodsSet,
			forbiddenMethodsSet,
			referrerPolicySet,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/global.js
var require_global = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/global.js"(
		exports2,
		module2
	) {
		"use strict"
		var globalOrigin = Symbol.for("undici.globalOrigin.1")
		function getGlobalOrigin() {
			return globalThis[globalOrigin]
		}
		function setGlobalOrigin(newOrigin) {
			if (newOrigin === void 0) {
				Object.defineProperty(globalThis, globalOrigin, {
					value: void 0,
					writable: true,
					enumerable: false,
					configurable: false,
				})
				return
			}
			const parsedURL = new URL(newOrigin)
			if (parsedURL.protocol !== "http:" && parsedURL.protocol !== "https:") {
				throw new TypeError(`Only http & https urls are allowed, received ${parsedURL.protocol}`)
			}
			Object.defineProperty(globalThis, globalOrigin, {
				value: parsedURL,
				writable: true,
				enumerable: false,
				configurable: false,
			})
		}
		module2.exports = {
			getGlobalOrigin,
			setGlobalOrigin,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/util.js
var require_util2 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/util.js"(
		exports2,
		module2
	) {
		"use strict"
		var {
			redirectStatusSet,
			referrerPolicySet: referrerPolicyTokens,
			badPortsSet,
		} = require_constants2()
		var { getGlobalOrigin } = require_global()
		var { performance: performance2 } = require("node:perf_hooks")
		var { isBlobLike, toUSVString, ReadableStreamFrom } = require_util()
		var assert = require("node:assert")
		var { isUint8Array } = require("node:util/types")
		var supportedHashes = []
		var crypto5
		try {
			crypto5 = require("node:crypto")
			const possibleRelevantHashes = ["sha256", "sha384", "sha512"]
			supportedHashes = crypto5
				.getHashes()
				.filter((hash2) => possibleRelevantHashes.includes(hash2))
		} catch {}
		function responseURL(response) {
			const urlList = response.urlList
			const length = urlList.length
			return length === 0 ? null : urlList[length - 1].toString()
		}
		function responseLocationURL(response, requestFragment) {
			if (!redirectStatusSet.has(response.status)) {
				return null
			}
			let location = response.headersList.get("location")
			if (location !== null && isValidHeaderValue(location)) {
				location = new URL(location, responseURL(response))
			}
			if (location && !location.hash) {
				location.hash = requestFragment
			}
			return location
		}
		function requestCurrentURL(request) {
			return request.urlList.at(-1)
		}
		function requestBadPort(request) {
			const url = requestCurrentURL(request)
			if (urlIsHttpHttpsScheme(url) && badPortsSet.has(url.port)) {
				return "blocked"
			}
			return "allowed"
		}
		function isErrorLike(object) {
			return (
				object instanceof Error ||
				object?.constructor?.name === "Error" ||
				object?.constructor?.name === "DOMException"
			)
		}
		function isValidReasonPhrase(statusText) {
			for (let i = 0; i < statusText.length; ++i) {
				const c = statusText.charCodeAt(i)
				if (
					!(
						c === 9 || // HTAB
						(c >= 32 && c <= 126) || // SP / VCHAR
						(c >= 128 && c <= 255)
					)
				) {
					return false
				}
			}
			return true
		}
		function isTokenCharCode(c) {
			switch (c) {
				case 34:
				case 40:
				case 41:
				case 44:
				case 47:
				case 58:
				case 59:
				case 60:
				case 61:
				case 62:
				case 63:
				case 64:
				case 91:
				case 92:
				case 93:
				case 123:
				case 125:
					return false
				default:
					return c >= 33 && c <= 126
			}
		}
		function isValidHTTPToken(characters) {
			if (characters.length === 0) {
				return false
			}
			for (let i = 0; i < characters.length; ++i) {
				if (!isTokenCharCode(characters.charCodeAt(i))) {
					return false
				}
			}
			return true
		}
		function isValidHeaderName(potentialValue) {
			return isValidHTTPToken(potentialValue)
		}
		function isValidHeaderValue(potentialValue) {
			if (
				potentialValue.startsWith("	") ||
				potentialValue.startsWith(" ") ||
				potentialValue.endsWith("	") ||
				potentialValue.endsWith(" ")
			) {
				return false
			}
			if (
				potentialValue.includes("\0") ||
				potentialValue.includes("\r") ||
				potentialValue.includes("\n")
			) {
				return false
			}
			return true
		}
		function setRequestReferrerPolicyOnRedirect(request, actualResponse) {
			const { headersList } = actualResponse
			const policyHeader = (headersList.get("referrer-policy") ?? "").split(",")
			let policy = ""
			if (policyHeader.length > 0) {
				for (let i = policyHeader.length; i !== 0; i--) {
					const token = policyHeader[i - 1].trim()
					if (referrerPolicyTokens.has(token)) {
						policy = token
						break
					}
				}
			}
			if (policy !== "") {
				request.referrerPolicy = policy
			}
		}
		function crossOriginResourcePolicyCheck() {
			return "allowed"
		}
		function corsCheck() {
			return "success"
		}
		function TAOCheck() {
			return "success"
		}
		function appendFetchMetadata(httpRequest) {
			let header = null
			header = httpRequest.mode
			httpRequest.headersList.set("sec-fetch-mode", header)
		}
		function appendRequestOriginHeader(request) {
			let serializedOrigin = request.origin
			if (request.responseTainting === "cors" || request.mode === "websocket") {
				if (serializedOrigin) {
					request.headersList.append("origin", serializedOrigin)
				}
			} else if (request.method !== "GET" && request.method !== "HEAD") {
				switch (request.referrerPolicy) {
					case "no-referrer":
						serializedOrigin = null
						break
					case "no-referrer-when-downgrade":
					case "strict-origin":
					case "strict-origin-when-cross-origin":
						if (
							request.origin &&
							urlHasHttpsScheme(request.origin) &&
							!urlHasHttpsScheme(requestCurrentURL(request))
						) {
							serializedOrigin = null
						}
						break
					case "same-origin":
						if (!sameOrigin(request, requestCurrentURL(request))) {
							serializedOrigin = null
						}
						break
					default:
				}
				if (serializedOrigin) {
					request.headersList.append("origin", serializedOrigin)
				}
			}
		}
		function coarsenedSharedCurrentTime(crossOriginIsolatedCapability) {
			return performance2.now()
		}
		function createOpaqueTimingInfo(timingInfo) {
			return {
				startTime: timingInfo.startTime ?? 0,
				redirectStartTime: 0,
				redirectEndTime: 0,
				postRedirectStartTime: timingInfo.startTime ?? 0,
				finalServiceWorkerStartTime: 0,
				finalNetworkResponseStartTime: 0,
				finalNetworkRequestStartTime: 0,
				endTime: 0,
				encodedBodySize: 0,
				decodedBodySize: 0,
				finalConnectionTimingInfo: null,
			}
		}
		function makePolicyContainer() {
			return {
				referrerPolicy: "strict-origin-when-cross-origin",
			}
		}
		function clonePolicyContainer(policyContainer) {
			return {
				referrerPolicy: policyContainer.referrerPolicy,
			}
		}
		function determineRequestsReferrer(request) {
			const policy = request.referrerPolicy
			assert(policy)
			let referrerSource = null
			if (request.referrer === "client") {
				const globalOrigin = getGlobalOrigin()
				if (!globalOrigin || globalOrigin.origin === "null") {
					return "no-referrer"
				}
				referrerSource = new URL(globalOrigin)
			} else if (request.referrer instanceof URL) {
				referrerSource = request.referrer
			}
			let referrerURL = stripURLForReferrer(referrerSource)
			const referrerOrigin = stripURLForReferrer(referrerSource, true)
			if (referrerURL.toString().length > 4096) {
				referrerURL = referrerOrigin
			}
			const areSameOrigin = sameOrigin(request, referrerURL)
			const isNonPotentiallyTrustWorthy =
				isURLPotentiallyTrustworthy(referrerURL) && !isURLPotentiallyTrustworthy(request.url)
			switch (policy) {
				case "origin":
					return referrerOrigin != undefined
						? referrerOrigin
						: stripURLForReferrer(referrerSource, true)
				case "unsafe-url":
					return referrerURL
				case "same-origin":
					return areSameOrigin ? referrerOrigin : "no-referrer"
				case "origin-when-cross-origin":
					return areSameOrigin ? referrerURL : referrerOrigin
				case "strict-origin-when-cross-origin": {
					const currentURL = requestCurrentURL(request)
					if (sameOrigin(referrerURL, currentURL)) {
						return referrerURL
					}
					if (
						isURLPotentiallyTrustworthy(referrerURL) &&
						!isURLPotentiallyTrustworthy(currentURL)
					) {
						return "no-referrer"
					}
					return referrerOrigin
				}
				case "strict-origin":
				case "no-referrer-when-downgrade":
				default:
					return isNonPotentiallyTrustWorthy ? "no-referrer" : referrerOrigin
			}
		}
		function stripURLForReferrer(url, originOnly) {
			assert(url instanceof URL)
			if (url.protocol === "file:" || url.protocol === "about:" || url.protocol === "blank:") {
				return "no-referrer"
			}
			url.username = ""
			url.password = ""
			url.hash = ""
			if (originOnly) {
				url.pathname = ""
				url.search = ""
			}
			return url
		}
		function isURLPotentiallyTrustworthy(url) {
			if (!(url instanceof URL)) {
				return false
			}
			if (url.href === "about:blank" || url.href === "about:srcdoc") {
				return true
			}
			if (url.protocol === "data:") return true
			if (url.protocol === "file:") return true
			return isOriginPotentiallyTrustworthy(url.origin)
			function isOriginPotentiallyTrustworthy(origin) {
				if (origin == undefined || origin === "null") return false
				const originAsURL = new URL(origin)
				if (originAsURL.protocol === "https:" || originAsURL.protocol === "wss:") {
					return true
				}
				if (
					/^127(?:\.[0-9]+){0,2}\.[0-9]+$|^\[(?:0*:)*?:?0*1\]$/.test(originAsURL.hostname) ||
					originAsURL.hostname === "localhost" ||
					originAsURL.hostname.includes("localhost.") ||
					originAsURL.hostname.endsWith(".localhost")
				) {
					return true
				}
				return false
			}
		}
		function bytesMatch(bytes, metadataList) {
			if (crypto5 === void 0) {
				return true
			}
			const parsedMetadata = parseMetadata(metadataList)
			if (parsedMetadata === "no metadata") {
				return true
			}
			if (parsedMetadata.length === 0) {
				return true
			}
			const strongest = getStrongestMetadata(parsedMetadata)
			const metadata = filterMetadataListByAlgorithm(parsedMetadata, strongest)
			for (const item of metadata) {
				const algorithm = item.algo
				const expectedValue = item.hash
				let actualValue = crypto5.createHash(algorithm).update(bytes).digest("base64")
				if (actualValue.at(-1) === "=") {
					if (actualValue.at(-2) === "=") {
						actualValue = actualValue.slice(0, -2)
					} else {
						actualValue = actualValue.slice(0, -1)
					}
				}
				if (compareBase64Mixed(actualValue, expectedValue)) {
					return true
				}
			}
			return false
		}
		var parseHashWithOptions =
			/(?<algo>sha256|sha384|sha512)-((?<hash>[A-Za-z0-9+/]+|[A-Za-z0-9_-]+)={0,2}(?:\s|$)( +[!-~]*)?)?/i
		function parseMetadata(metadata) {
			const result = []
			let empty = true
			for (const token of metadata.split(" ")) {
				empty = false
				const parsedToken = parseHashWithOptions.exec(token)
				if (
					parsedToken === null ||
					parsedToken.groups === void 0 ||
					parsedToken.groups.algo === void 0
				) {
					continue
				}
				const algorithm = parsedToken.groups.algo.toLowerCase()
				if (supportedHashes.includes(algorithm)) {
					result.push(parsedToken.groups)
				}
			}
			if (empty === true) {
				return "no metadata"
			}
			return result
		}
		function getStrongestMetadata(metadataList) {
			let algorithm = metadataList[0].algo
			if (algorithm[3] === "5") {
				return algorithm
			}
			for (let i = 1; i < metadataList.length; ++i) {
				const metadata = metadataList[i]
				if (metadata.algo[3] === "5") {
					algorithm = "sha512"
					break
				} else if (algorithm[3] === "3") {
					continue
				} else if (metadata.algo[3] === "3") {
					algorithm = "sha384"
				}
			}
			return algorithm
		}
		function filterMetadataListByAlgorithm(metadataList, algorithm) {
			if (metadataList.length === 1) {
				return metadataList
			}
			let pos = 0
			for (let i = 0; i < metadataList.length; ++i) {
				if (metadataList[i].algo === algorithm) {
					metadataList[pos++] = metadataList[i]
				}
			}
			metadataList.length = pos
			return metadataList
		}
		function compareBase64Mixed(actualValue, expectedValue) {
			if (actualValue.length !== expectedValue.length) {
				return false
			}
			for (const [i, element] of actualValue.entries()) {
				if (element !== expectedValue[i]) {
					if (
						(element === "+" && expectedValue[i] === "-") ||
						(element === "/" && expectedValue[i] === "_")
					) {
						continue
					}
					return false
				}
			}
			return true
		}
		function tryUpgradeRequestToAPotentiallyTrustworthyURL(request) {}
		function sameOrigin(A, B) {
			if (A.origin === B.origin && A.origin === "null") {
				return true
			}
			if (A.protocol === B.protocol && A.hostname === B.hostname && A.port === B.port) {
				return true
			}
			return false
		}
		function createDeferredPromise() {
			let res
			let rej
			const promise = new Promise((resolve, reject) => {
				res = resolve
				rej = reject
			})
			return { promise, resolve: res, reject: rej }
		}
		function isAborted(fetchParams) {
			return fetchParams.controller.state === "aborted"
		}
		function isCancelled(fetchParams) {
			return (
				fetchParams.controller.state === "aborted" || fetchParams.controller.state === "terminated"
			)
		}
		var normalizeMethodRecord = {
			delete: "DELETE",
			DELETE: "DELETE",
			get: "GET",
			GET: "GET",
			head: "HEAD",
			HEAD: "HEAD",
			options: "OPTIONS",
			OPTIONS: "OPTIONS",
			post: "POST",
			POST: "POST",
			put: "PUT",
			PUT: "PUT",
		}
		Object.setPrototypeOf(normalizeMethodRecord, null)
		function normalizeMethod(method) {
			return normalizeMethodRecord[method.toLowerCase()] ?? method
		}
		function serializeJavascriptValueToJSONString(value) {
			const result = JSON.stringify(value)
			if (result === void 0) {
				throw new TypeError("Value is not JSON serializable")
			}
			assert(typeof result === "string")
			return result
		}
		var esIteratorPrototype = Object.getPrototypeOf(Object.getPrototypeOf([][Symbol.iterator]()))
		function makeIterator(iterator, name, kind) {
			const object = {
				index: 0,
				kind,
				target: iterator,
			}
			const i = {
				next() {
					if (Object.getPrototypeOf(this) !== i) {
						throw new TypeError(
							`'next' called on an object that does not implement interface ${name} Iterator.`
						)
					}
					const { index: index2, kind: kind2, target } = object
					const values = target()
					const len = values.length
					if (index2 >= len) {
						return { value: void 0, done: true }
					}
					const pair = values[index2]
					object.index = index2 + 1
					return iteratorResult(pair, kind2)
				},
				// The class string of an iterator prototype object for a given interface is the
				// result of concatenating the identifier of the interface and the string " Iterator".
				[Symbol.toStringTag]: `${name} Iterator`,
			}
			Object.setPrototypeOf(i, esIteratorPrototype)
			return Object.setPrototypeOf({}, i)
		}
		function iteratorResult(pair, kind) {
			let result
			switch (kind) {
				case "key": {
					result = pair[0]
					break
				}
				case "value": {
					result = pair[1]
					break
				}
				case "key+value": {
					result = pair
					break
				}
			}
			return { value: result, done: false }
		}
		async function fullyReadBody(body, processBody, processBodyError) {
			const successSteps = processBody
			const errorSteps = processBodyError
			let reader
			try {
				reader = body.stream.getReader()
			} catch (e) {
				errorSteps(e)
				return
			}
			try {
				const result = await readAllBytes(reader)
				successSteps(result)
			} catch (e) {
				errorSteps(e)
			}
		}
		var ReadableStream = globalThis.ReadableStream
		function isReadableStreamLike(stream) {
			if (!ReadableStream) {
				ReadableStream = require("node:stream/web").ReadableStream
			}
			return (
				stream instanceof ReadableStream ||
				(stream[Symbol.toStringTag] === "ReadableStream" && typeof stream.tee === "function")
			)
		}
		var MAXIMUM_ARGUMENT_LENGTH = 65535
		function isomorphicDecode(input) {
			if (input.length < MAXIMUM_ARGUMENT_LENGTH) {
				return String.fromCharCode(...input)
			}
			return input.reduce((previous, current) => previous + String.fromCharCode(current), "")
		}
		function readableStreamClose(controller) {
			try {
				controller.close()
			} catch (err) {
				if (!err.message.includes("Controller is already closed")) {
					throw err
				}
			}
		}
		function isomorphicEncode(input) {
			for (let i = 0; i < input.length; i++) {
				assert(input.charCodeAt(i) <= 255)
			}
			return input
		}
		async function readAllBytes(reader) {
			const bytes = []
			let byteLength = 0
			while (true) {
				const { done, value: chunk } = await reader.read()
				if (done) {
					return Buffer.concat(bytes, byteLength)
				}
				if (!isUint8Array(chunk)) {
					throw new TypeError("Received non-Uint8Array chunk")
				}
				bytes.push(chunk)
				byteLength += chunk.length
			}
		}
		function urlIsLocal(url) {
			assert("protocol" in url)
			const protocol = url.protocol
			return protocol === "about:" || protocol === "blob:" || protocol === "data:"
		}
		function urlHasHttpsScheme(url) {
			if (typeof url === "string") {
				return url.startsWith("https:")
			}
			return url.protocol === "https:"
		}
		function urlIsHttpHttpsScheme(url) {
			assert("protocol" in url)
			const protocol = url.protocol
			return protocol === "http:" || protocol === "https:"
		}
		var hasOwn = Object.hasOwn || ((dict, key) => Object.prototype.hasOwnProperty.call(dict, key))
		module2.exports = {
			isAborted,
			isCancelled,
			createDeferredPromise,
			ReadableStreamFrom,
			toUSVString,
			tryUpgradeRequestToAPotentiallyTrustworthyURL,
			coarsenedSharedCurrentTime,
			determineRequestsReferrer,
			makePolicyContainer,
			clonePolicyContainer,
			appendFetchMetadata,
			appendRequestOriginHeader,
			TAOCheck,
			corsCheck,
			crossOriginResourcePolicyCheck,
			createOpaqueTimingInfo,
			setRequestReferrerPolicyOnRedirect,
			isValidHTTPToken,
			requestBadPort,
			requestCurrentURL,
			responseURL,
			responseLocationURL,
			isBlobLike,
			isURLPotentiallyTrustworthy,
			isValidReasonPhrase,
			sameOrigin,
			normalizeMethod,
			serializeJavascriptValueToJSONString,
			makeIterator,
			isValidHeaderName,
			isValidHeaderValue,
			hasOwn,
			isErrorLike,
			fullyReadBody,
			bytesMatch,
			isReadableStreamLike,
			readableStreamClose,
			isomorphicEncode,
			isomorphicDecode,
			urlIsLocal,
			urlHasHttpsScheme,
			urlIsHttpHttpsScheme,
			readAllBytes,
			normalizeMethodRecord,
			parseMetadata,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/symbols.js
var require_symbols2 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/symbols.js"(
		exports2,
		module2
	) {
		"use strict"
		module2.exports = {
			kUrl: Symbol("url"),
			kHeaders: Symbol("headers"),
			kSignal: Symbol("signal"),
			kState: Symbol("state"),
			kGuard: Symbol("guard"),
			kRealm: Symbol("realm"),
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/webidl.js
var require_webidl = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/webidl.js"(
		exports2,
		module2
	) {
		"use strict"
		var { types: types2 } = require("node:util")
		var { hasOwn, toUSVString } = require_util2()
		var webidl = {}
		webidl.converters = {}
		webidl.util = {}
		webidl.errors = {}
		webidl.errors.exception = function (message) {
			return new TypeError(`${message.header}: ${message.message}`)
		}
		webidl.errors.conversionFailed = function (context2) {
			const plural = context2.types.length === 1 ? "" : " one of"
			const message = `${
				context2.argument
			} could not be converted to${plural}: ${context2.types.join(", ")}.`
			return webidl.errors.exception({
				header: context2.prefix,
				message,
			})
		}
		webidl.errors.invalidArgument = function (context2) {
			return webidl.errors.exception({
				header: context2.prefix,
				message: `"${context2.value}" is an invalid ${context2.type}.`,
			})
		}
		webidl.brandCheck = function (V, I, opts = void 0) {
			if (opts?.strict !== false && !(V instanceof I)) {
				throw new TypeError("Illegal invocation")
			} else {
				return V?.[Symbol.toStringTag] === I.prototype[Symbol.toStringTag]
			}
		}
		webidl.argumentLengthCheck = function ({ length }, min, ctx) {
			if (length < min) {
				throw webidl.errors.exception({
					message: `${min} argument${min !== 1 ? "s" : ""} required, but${
						length ? " only" : ""
					} ${length} found.`,
					...ctx,
				})
			}
		}
		webidl.illegalConstructor = function () {
			throw webidl.errors.exception({
				header: "TypeError",
				message: "Illegal constructor",
			})
		}
		webidl.util.Type = function (V) {
			switch (typeof V) {
				case "undefined":
					return "Undefined"
				case "boolean":
					return "Boolean"
				case "string":
					return "String"
				case "symbol":
					return "Symbol"
				case "number":
					return "Number"
				case "bigint":
					return "BigInt"
				case "function":
				case "object": {
					if (V === null) {
						return "Null"
					}
					return "Object"
				}
			}
		}
		webidl.util.ConvertToInt = function (V, bitLength, signedness, opts = {}) {
			let upperBound
			let lowerBound
			if (bitLength === 64) {
				upperBound = Math.pow(2, 53) - 1
				if (signedness === "unsigned") {
					lowerBound = 0
				} else {
					lowerBound = Math.pow(-2, 53) + 1
				}
			} else if (signedness === "unsigned") {
				lowerBound = 0
				upperBound = Math.pow(2, bitLength) - 1
			} else {
				lowerBound = Math.pow(-2, bitLength) - 1
				upperBound = Math.pow(2, bitLength - 1) - 1
			}
			let x = Number(V)
			if (x === 0) {
				x = 0
			}
			if (opts.enforceRange === true) {
				if (Number.isNaN(x) || x === Number.POSITIVE_INFINITY || x === Number.NEGATIVE_INFINITY) {
					throw webidl.errors.exception({
						header: "Integer conversion",
						message: `Could not convert ${V} to an integer.`,
					})
				}
				x = webidl.util.IntegerPart(x)
				if (x < lowerBound || x > upperBound) {
					throw webidl.errors.exception({
						header: "Integer conversion",
						message: `Value must be between ${lowerBound}-${upperBound}, got ${x}.`,
					})
				}
				return x
			}
			if (!Number.isNaN(x) && opts.clamp === true) {
				x = Math.min(Math.max(x, lowerBound), upperBound)
				if (Math.floor(x) % 2 === 0) {
					x = Math.floor(x)
				} else {
					x = Math.ceil(x)
				}
				return x
			}
			if (
				Number.isNaN(x) ||
				(x === 0 && Object.is(0, x)) ||
				x === Number.POSITIVE_INFINITY ||
				x === Number.NEGATIVE_INFINITY
			) {
				return 0
			}
			x = webidl.util.IntegerPart(x)
			x = x % Math.pow(2, bitLength)
			if (signedness === "signed" && x >= Math.pow(2, bitLength) - 1) {
				return x - Math.pow(2, bitLength)
			}
			return x
		}
		webidl.util.IntegerPart = function (n) {
			const r = Math.floor(Math.abs(n))
			if (n < 0) {
				return -1 * r
			}
			return r
		}
		webidl.sequenceConverter = function (converter) {
			return (V) => {
				if (webidl.util.Type(V) !== "Object") {
					throw webidl.errors.exception({
						header: "Sequence",
						message: `Value of type ${webidl.util.Type(V)} is not an Object.`,
					})
				}
				const method = V?.[Symbol.iterator]?.()
				const seq = []
				if (method === void 0 || typeof method.next !== "function") {
					throw webidl.errors.exception({
						header: "Sequence",
						message: "Object is not an iterator.",
					})
				}
				while (true) {
					const { done, value } = method.next()
					if (done) {
						break
					}
					seq.push(converter(value))
				}
				return seq
			}
		}
		webidl.recordConverter = function (keyConverter, valueConverter) {
			return (O) => {
				if (webidl.util.Type(O) !== "Object") {
					throw webidl.errors.exception({
						header: "Record",
						message: `Value of type ${webidl.util.Type(O)} is not an Object.`,
					})
				}
				const result = {}
				if (!types2.isProxy(O)) {
					const keys2 = Object.keys(O)
					for (const key of keys2) {
						const typedKey = keyConverter(key)
						const typedValue = valueConverter(O[key])
						result[typedKey] = typedValue
					}
					return result
				}
				const keys = Reflect.ownKeys(O)
				for (const key of keys) {
					const desc = Reflect.getOwnPropertyDescriptor(O, key)
					if (desc?.enumerable) {
						const typedKey = keyConverter(key)
						const typedValue = valueConverter(O[key])
						result[typedKey] = typedValue
					}
				}
				return result
			}
		}
		webidl.interfaceConverter = function (i) {
			return (V, opts = {}) => {
				if (opts.strict !== false && !(V instanceof i)) {
					throw webidl.errors.exception({
						header: i.name,
						message: `Expected ${V} to be an instance of ${i.name}.`,
					})
				}
				return V
			}
		}
		webidl.dictionaryConverter = function (converters) {
			return (dictionary) => {
				const type = webidl.util.Type(dictionary)
				const dict = {}
				if (type === "Null" || type === "Undefined") {
					return dict
				} else if (type !== "Object") {
					throw webidl.errors.exception({
						header: "Dictionary",
						message: `Expected ${dictionary} to be one of: Null, Undefined, Object.`,
					})
				}
				for (const options of converters) {
					const { key, defaultValue, required, converter } = options
					if (required === true && !hasOwn(dictionary, key)) {
						throw webidl.errors.exception({
							header: "Dictionary",
							message: `Missing required key "${key}".`,
						})
					}
					let value = dictionary[key]
					const hasDefault = hasOwn(options, "defaultValue")
					if (hasDefault && value !== null) {
						value = value ?? defaultValue
					}
					if (required || hasDefault || value !== void 0) {
						value = converter(value)
						if (options.allowedValues && !options.allowedValues.includes(value)) {
							throw webidl.errors.exception({
								header: "Dictionary",
								message: `${value} is not an accepted type. Expected one of ${options.allowedValues.join(
									", "
								)}.`,
							})
						}
						dict[key] = value
					}
				}
				return dict
			}
		}
		webidl.nullableConverter = function (converter) {
			return (V) => {
				if (V === null) {
					return V
				}
				return converter(V)
			}
		}
		webidl.converters.DOMString = function (V, opts = {}) {
			if (V === null && opts.legacyNullToEmptyString) {
				return ""
			}
			if (typeof V === "symbol") {
				throw new TypeError("Could not convert argument of type symbol to string.")
			}
			return String(V)
		}
		webidl.converters.ByteString = function (V) {
			const x = webidl.converters.DOMString(V)
			for (let index2 = 0; index2 < x.length; index2++) {
				if (x.charCodeAt(index2) > 255) {
					throw new TypeError(
						`Cannot convert argument to a ByteString because the character at index ${index2} has a value of ${x.charCodeAt(
							index2
						)} which is greater than 255.`
					)
				}
			}
			return x
		}
		webidl.converters.USVString = toUSVString
		webidl.converters.boolean = function (V) {
			const x = Boolean(V)
			return x
		}
		webidl.converters.any = function (V) {
			return V
		}
		webidl.converters["long long"] = function (V) {
			const x = webidl.util.ConvertToInt(V, 64, "signed")
			return x
		}
		webidl.converters["unsigned long long"] = function (V) {
			const x = webidl.util.ConvertToInt(V, 64, "unsigned")
			return x
		}
		webidl.converters["unsigned long"] = function (V) {
			const x = webidl.util.ConvertToInt(V, 32, "unsigned")
			return x
		}
		webidl.converters["unsigned short"] = function (V, opts) {
			const x = webidl.util.ConvertToInt(V, 16, "unsigned", opts)
			return x
		}
		webidl.converters.ArrayBuffer = function (V, opts = {}) {
			if (webidl.util.Type(V) !== "Object" || !types2.isAnyArrayBuffer(V)) {
				throw webidl.errors.conversionFailed({
					prefix: `${V}`,
					argument: `${V}`,
					types: ["ArrayBuffer"],
				})
			}
			if (opts.allowShared === false && types2.isSharedArrayBuffer(V)) {
				throw webidl.errors.exception({
					header: "ArrayBuffer",
					message: "SharedArrayBuffer is not allowed.",
				})
			}
			return V
		}
		webidl.converters.TypedArray = function (V, T, opts = {}) {
			if (
				webidl.util.Type(V) !== "Object" ||
				!types2.isTypedArray(V) ||
				V.constructor.name !== T.name
			) {
				throw webidl.errors.conversionFailed({
					prefix: `${T.name}`,
					argument: `${V}`,
					types: [T.name],
				})
			}
			if (opts.allowShared === false && types2.isSharedArrayBuffer(V.buffer)) {
				throw webidl.errors.exception({
					header: "ArrayBuffer",
					message: "SharedArrayBuffer is not allowed.",
				})
			}
			return V
		}
		webidl.converters.DataView = function (V, opts = {}) {
			if (webidl.util.Type(V) !== "Object" || !types2.isDataView(V)) {
				throw webidl.errors.exception({
					header: "DataView",
					message: "Object is not a DataView.",
				})
			}
			if (opts.allowShared === false && types2.isSharedArrayBuffer(V.buffer)) {
				throw webidl.errors.exception({
					header: "ArrayBuffer",
					message: "SharedArrayBuffer is not allowed.",
				})
			}
			return V
		}
		webidl.converters.BufferSource = function (V, opts = {}) {
			if (types2.isAnyArrayBuffer(V)) {
				return webidl.converters.ArrayBuffer(V, opts)
			}
			if (types2.isTypedArray(V)) {
				return webidl.converters.TypedArray(V, V.constructor)
			}
			if (types2.isDataView(V)) {
				return webidl.converters.DataView(V, opts)
			}
			throw new TypeError(`Could not convert ${V} to a BufferSource.`)
		}
		webidl.converters["sequence<ByteString>"] = webidl.sequenceConverter(
			webidl.converters.ByteString
		)
		webidl.converters["sequence<sequence<ByteString>>"] = webidl.sequenceConverter(
			webidl.converters["sequence<ByteString>"]
		)
		webidl.converters["record<ByteString, ByteString>"] = webidl.recordConverter(
			webidl.converters.ByteString,
			webidl.converters.ByteString
		)
		module2.exports = {
			webidl,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/dataURL.js
var require_dataURL = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/dataURL.js"(
		exports2,
		module2
	) {
		var assert = require("node:assert")
		var { atob: atob2 } = require("node:buffer")
		var { isomorphicDecode } = require_util2()
		var encoder = new TextEncoder()
		var HTTP_TOKEN_CODEPOINTS = /^[!#$%&'*+-.^_|~A-Za-z0-9]+$/
		var HTTP_WHITESPACE_REGEX = /(\u000A|\u000D|\u0009|\u0020)/
		var HTTP_QUOTED_STRING_TOKENS = /[\u0009|\u0020-\u007E|\u0080-\u00FF]/
		function dataURLProcessor(dataURL) {
			assert(dataURL.protocol === "data:")
			let input = URLSerializer(dataURL, true)
			input = input.slice(5)
			const position = { position: 0 }
			let mimeType = collectASequenceOfCodePointsFast(",", input, position)
			const mimeTypeLength = mimeType.length
			mimeType = removeASCIIWhitespace(mimeType, true, true)
			if (position.position >= input.length) {
				return "failure"
			}
			position.position++
			const encodedBody = input.slice(mimeTypeLength + 1)
			let body = stringPercentDecode(encodedBody)
			if (/;(\u0020){0,}base64$/i.test(mimeType)) {
				const stringBody = isomorphicDecode(body)
				body = forgivingBase64(stringBody)
				if (body === "failure") {
					return "failure"
				}
				mimeType = mimeType.slice(0, -6)
				mimeType = mimeType.replace(/(\u0020)+$/, "")
				mimeType = mimeType.slice(0, -1)
			}
			if (mimeType.startsWith(";")) {
				mimeType = "text/plain" + mimeType
			}
			let mimeTypeRecord = parseMIMEType(mimeType)
			if (mimeTypeRecord === "failure") {
				mimeTypeRecord = parseMIMEType("text/plain;charset=US-ASCII")
			}
			return { mimeType: mimeTypeRecord, body }
		}
		function URLSerializer(url, excludeFragment = false) {
			if (!excludeFragment) {
				return url.href
			}
			const href = url.href
			const hashLength = url.hash.length
			return hashLength === 0 ? href : href.slice(0, Math.max(0, href.length - hashLength))
		}
		function collectASequenceOfCodePoints(condition, input, position) {
			let result = ""
			while (position.position < input.length && condition(input[position.position])) {
				result += input[position.position]
				position.position++
			}
			return result
		}
		function collectASequenceOfCodePointsFast(char, input, position) {
			const idx = input.indexOf(char, position.position)
			const start = position.position
			if (idx === -1) {
				position.position = input.length
				return input.slice(start)
			}
			position.position = idx
			return input.slice(start, position.position)
		}
		function stringPercentDecode(input) {
			const bytes = encoder.encode(input)
			return percentDecode(bytes)
		}
		function percentDecode(input) {
			const output = []
			for (let i = 0; i < input.length; i++) {
				const byte = input[i]
				if (byte !== 37) {
					output.push(byte)
				} else if (
					byte === 37 &&
					!/^[0-9A-Fa-f]{2}$/i.test(String.fromCharCode(input[i + 1], input[i + 2]))
				) {
					output.push(37)
				} else {
					const nextTwoBytes = String.fromCharCode(input[i + 1], input[i + 2])
					const bytePoint = Number.parseInt(nextTwoBytes, 16)
					output.push(bytePoint)
					i += 2
				}
			}
			return Uint8Array.from(output)
		}
		function parseMIMEType(input) {
			input = removeHTTPWhitespace(input, true, true)
			const position = { position: 0 }
			const type = collectASequenceOfCodePointsFast("/", input, position)
			if (type.length === 0 || !HTTP_TOKEN_CODEPOINTS.test(type)) {
				return "failure"
			}
			if (position.position > input.length) {
				return "failure"
			}
			position.position++
			let subtype = collectASequenceOfCodePointsFast(";", input, position)
			subtype = removeHTTPWhitespace(subtype, false, true)
			if (subtype.length === 0 || !HTTP_TOKEN_CODEPOINTS.test(subtype)) {
				return "failure"
			}
			const typeLowercase = type.toLowerCase()
			const subtypeLowercase = subtype.toLowerCase()
			const mimeType = {
				type: typeLowercase,
				subtype: subtypeLowercase,
				/** @type {Map<string, string>} */
				parameters: /* @__PURE__ */ new Map(),
				// https://mimesniff.spec.whatwg.org/#mime-type-essence
				essence: `${typeLowercase}/${subtypeLowercase}`,
			}
			while (position.position < input.length) {
				position.position++
				collectASequenceOfCodePoints(
					// https://fetch.spec.whatwg.org/#http-whitespace
					(char) => HTTP_WHITESPACE_REGEX.test(char),
					input,
					position
				)
				let parameterName = collectASequenceOfCodePoints(
					(char) => char !== ";" && char !== "=",
					input,
					position
				)
				parameterName = parameterName.toLowerCase()
				if (position.position < input.length) {
					if (input[position.position] === ";") {
						continue
					}
					position.position++
				}
				if (position.position > input.length) {
					break
				}
				let parameterValue = null
				if (input[position.position] === '"') {
					parameterValue = collectAnHTTPQuotedString(input, position, true)
					collectASequenceOfCodePointsFast(";", input, position)
				} else {
					parameterValue = collectASequenceOfCodePointsFast(";", input, position)
					parameterValue = removeHTTPWhitespace(parameterValue, false, true)
					if (parameterValue.length === 0) {
						continue
					}
				}
				if (
					parameterName.length !== 0 &&
					HTTP_TOKEN_CODEPOINTS.test(parameterName) &&
					(parameterValue.length === 0 || HTTP_QUOTED_STRING_TOKENS.test(parameterValue)) &&
					!mimeType.parameters.has(parameterName)
				) {
					mimeType.parameters.set(parameterName, parameterValue)
				}
			}
			return mimeType
		}
		function forgivingBase64(data) {
			data = data.replace(/[\u0009\u000A\u000C\u000D\u0020]/g, "")
			if (data.length % 4 === 0) {
				data = data.replace(/=?=$/, "")
			}
			if (data.length % 4 === 1) {
				return "failure"
			}
			if (/[^+/0-9A-Za-z]/.test(data)) {
				return "failure"
			}
			const binary = atob2(data)
			const bytes = new Uint8Array(binary.length)
			for (let byte = 0; byte < binary.length; byte++) {
				bytes[byte] = binary.charCodeAt(byte)
			}
			return bytes
		}
		function collectAnHTTPQuotedString(input, position, extractValue) {
			const positionStart = position.position
			let value = ""
			assert(input[position.position] === '"')
			position.position++
			while (true) {
				value += collectASequenceOfCodePoints(
					(char) => char !== '"' && char !== "\\",
					input,
					position
				)
				if (position.position >= input.length) {
					break
				}
				const quoteOrBackslash = input[position.position]
				position.position++
				if (quoteOrBackslash === "\\") {
					if (position.position >= input.length) {
						value += "\\"
						break
					}
					value += input[position.position]
					position.position++
				} else {
					assert(quoteOrBackslash === '"')
					break
				}
			}
			if (extractValue) {
				return value
			}
			return input.slice(positionStart, position.position)
		}
		function serializeAMimeType(mimeType) {
			assert(mimeType !== "failure")
			const { parameters, essence } = mimeType
			let serialization = essence
			for (let [name, value] of parameters.entries()) {
				serialization += ";"
				serialization += name
				serialization += "="
				if (!HTTP_TOKEN_CODEPOINTS.test(value)) {
					value = value.replace(/(\\|")/g, "\\$1")
					value = '"' + value
					value += '"'
				}
				serialization += value
			}
			return serialization
		}
		function isHTTPWhiteSpace(char) {
			return char === "\r" || char === "\n" || char === "	" || char === " "
		}
		function removeHTTPWhitespace(str, leading = true, trailing = true) {
			let lead = 0
			let trail = str.length - 1
			if (leading) {
				for (; lead < str.length && isHTTPWhiteSpace(str[lead]); lead++);
			}
			if (trailing) {
				for (; trail > 0 && isHTTPWhiteSpace(str[trail]); trail--);
			}
			return str.slice(lead, trail + 1)
		}
		function isASCIIWhitespace(char) {
			return char === "\r" || char === "\n" || char === "	" || char === "\f" || char === " "
		}
		function removeASCIIWhitespace(str, leading = true, trailing = true) {
			let lead = 0
			let trail = str.length - 1
			if (leading) {
				for (; lead < str.length && isASCIIWhitespace(str[lead]); lead++);
			}
			if (trailing) {
				for (; trail > 0 && isASCIIWhitespace(str[trail]); trail--);
			}
			return str.slice(lead, trail + 1)
		}
		module2.exports = {
			dataURLProcessor,
			URLSerializer,
			collectASequenceOfCodePoints,
			collectASequenceOfCodePointsFast,
			stringPercentDecode,
			parseMIMEType,
			collectAnHTTPQuotedString,
			serializeAMimeType,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/file.js
var require_file = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/file.js"(
		exports2,
		module2
	) {
		"use strict"
		var { Blob: Blob2, File: NativeFile } = require("node:buffer")
		var { types: types2 } = require("node:util")
		var { kState } = require_symbols2()
		var { isBlobLike } = require_util2()
		var { webidl } = require_webidl()
		var { parseMIMEType, serializeAMimeType } = require_dataURL()
		var { kEnumerableProperty } = require_util()
		var encoder = new TextEncoder()
		var File = class _File extends Blob2 {
			constructor(fileBits, fileName, options = {}) {
				webidl.argumentLengthCheck(arguments, 2, { header: "File constructor" })
				fileBits = webidl.converters["sequence<BlobPart>"](fileBits)
				fileName = webidl.converters.USVString(fileName)
				options = webidl.converters.FilePropertyBag(options)
				const n = fileName
				let t = options.type
				let d
				substep: {
					if (t) {
						t = parseMIMEType(t)
						if (t === "failure") {
							t = ""
							break substep
						}
						t = serializeAMimeType(t).toLowerCase()
					}
					d = options.lastModified
				}
				super(processBlobParts(fileBits, options), { type: t })
				this[kState] = {
					name: n,
					lastModified: d,
					type: t,
				}
			}
			get name() {
				webidl.brandCheck(this, _File)
				return this[kState].name
			}
			get lastModified() {
				webidl.brandCheck(this, _File)
				return this[kState].lastModified
			}
			get type() {
				webidl.brandCheck(this, _File)
				return this[kState].type
			}
		}
		var FileLike = class _FileLike {
			constructor(blobLike, fileName, options = {}) {
				const n = fileName
				const t = options.type
				const d = options.lastModified ?? Date.now()
				this[kState] = {
					blobLike,
					name: n,
					type: t,
					lastModified: d,
				}
			}
			stream(...args) {
				webidl.brandCheck(this, _FileLike)
				return this[kState].blobLike.stream(...args)
			}
			arrayBuffer(...args) {
				webidl.brandCheck(this, _FileLike)
				return this[kState].blobLike.arrayBuffer(...args)
			}
			slice(...args) {
				webidl.brandCheck(this, _FileLike)
				return this[kState].blobLike.slice(...args)
			}
			text(...args) {
				webidl.brandCheck(this, _FileLike)
				return this[kState].blobLike.text(...args)
			}
			get size() {
				webidl.brandCheck(this, _FileLike)
				return this[kState].blobLike.size
			}
			get type() {
				webidl.brandCheck(this, _FileLike)
				return this[kState].blobLike.type
			}
			get name() {
				webidl.brandCheck(this, _FileLike)
				return this[kState].name
			}
			get lastModified() {
				webidl.brandCheck(this, _FileLike)
				return this[kState].lastModified
			}
			get [Symbol.toStringTag]() {
				return "File"
			}
		}
		Object.defineProperties(File.prototype, {
			[Symbol.toStringTag]: {
				value: "File",
				configurable: true,
			},
			name: kEnumerableProperty,
			lastModified: kEnumerableProperty,
		})
		webidl.converters.Blob = webidl.interfaceConverter(Blob2)
		webidl.converters.BlobPart = function (V, opts) {
			if (webidl.util.Type(V) === "Object") {
				if (isBlobLike(V)) {
					return webidl.converters.Blob(V, { strict: false })
				}
				if (ArrayBuffer.isView(V) || types2.isAnyArrayBuffer(V)) {
					return webidl.converters.BufferSource(V, opts)
				}
			}
			return webidl.converters.USVString(V, opts)
		}
		webidl.converters["sequence<BlobPart>"] = webidl.sequenceConverter(webidl.converters.BlobPart)
		webidl.converters.FilePropertyBag = webidl.dictionaryConverter([
			{
				key: "lastModified",
				converter: webidl.converters["long long"],
				get defaultValue() {
					return Date.now()
				},
			},
			{
				key: "type",
				converter: webidl.converters.DOMString,
				defaultValue: "",
			},
			{
				key: "endings",
				converter: (value) => {
					value = webidl.converters.DOMString(value)
					value = value.toLowerCase()
					if (value !== "native") {
						value = "transparent"
					}
					return value
				},
				defaultValue: "transparent",
			},
		])
		function processBlobParts(parts, options) {
			const bytes = []
			for (const element of parts) {
				if (typeof element === "string") {
					let s = element
					if (options.endings === "native") {
						s = convertLineEndingsNative(s)
					}
					bytes.push(encoder.encode(s))
				} else if (types2.isAnyArrayBuffer(element) || types2.isTypedArray(element)) {
					if (!element.buffer) {
						bytes.push(new Uint8Array(element))
					} else {
						bytes.push(new Uint8Array(element.buffer, element.byteOffset, element.byteLength))
					}
				} else if (isBlobLike(element)) {
					bytes.push(element)
				}
			}
			return bytes
		}
		function convertLineEndingsNative(s) {
			let nativeLineEnding = "\n"
			if (process.platform === "win32") {
				nativeLineEnding = "\r\n"
			}
			return s.replace(/\r?\n/g, nativeLineEnding)
		}
		function isFileLike(object) {
			return (
				(NativeFile && object instanceof NativeFile) ||
				object instanceof File ||
				(object &&
					(typeof object.stream === "function" || typeof object.arrayBuffer === "function") &&
					object[Symbol.toStringTag] === "File")
			)
		}
		module2.exports = { File, FileLike, isFileLike }
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/formdata.js
var require_formdata = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/formdata.js"(
		exports2,
		module2
	) {
		"use strict"
		var { isBlobLike, toUSVString, makeIterator } = require_util2()
		var { kState } = require_symbols2()
		var { File: UndiciFile, FileLike, isFileLike } = require_file()
		var { webidl } = require_webidl()
		var { Blob: Blob2, File: NativeFile } = require("node:buffer")
		var File = NativeFile ?? UndiciFile
		var FormData = class _FormData {
			constructor(form) {
				if (form !== void 0) {
					throw webidl.errors.conversionFailed({
						prefix: "FormData constructor",
						argument: "Argument 1",
						types: ["undefined"],
					})
				}
				this[kState] = []
			}
			append(name, value, filename = void 0) {
				webidl.brandCheck(this, _FormData)
				webidl.argumentLengthCheck(arguments, 2, { header: "FormData.append" })
				if (arguments.length === 3 && !isBlobLike(value)) {
					throw new TypeError(
						"Failed to execute 'append' on 'FormData': parameter 2 is not of type 'Blob'"
					)
				}
				name = webidl.converters.USVString(name)
				value = isBlobLike(value)
					? webidl.converters.Blob(value, { strict: false })
					: webidl.converters.USVString(value)
				filename = arguments.length === 3 ? webidl.converters.USVString(filename) : void 0
				const entry = makeEntry(name, value, filename)
				this[kState].push(entry)
			}
			delete(name) {
				webidl.brandCheck(this, _FormData)
				webidl.argumentLengthCheck(arguments, 1, { header: "FormData.delete" })
				name = webidl.converters.USVString(name)
				this[kState] = this[kState].filter((entry) => entry.name !== name)
			}
			get(name) {
				webidl.brandCheck(this, _FormData)
				webidl.argumentLengthCheck(arguments, 1, { header: "FormData.get" })
				name = webidl.converters.USVString(name)
				const idx = this[kState].findIndex((entry) => entry.name === name)
				if (idx === -1) {
					return null
				}
				return this[kState][idx].value
			}
			getAll(name) {
				webidl.brandCheck(this, _FormData)
				webidl.argumentLengthCheck(arguments, 1, { header: "FormData.getAll" })
				name = webidl.converters.USVString(name)
				return this[kState].filter((entry) => entry.name === name).map((entry) => entry.value)
			}
			has(name) {
				webidl.brandCheck(this, _FormData)
				webidl.argumentLengthCheck(arguments, 1, { header: "FormData.has" })
				name = webidl.converters.USVString(name)
				return this[kState].findIndex((entry) => entry.name === name) !== -1
			}
			set(name, value, filename = void 0) {
				webidl.brandCheck(this, _FormData)
				webidl.argumentLengthCheck(arguments, 2, { header: "FormData.set" })
				if (arguments.length === 3 && !isBlobLike(value)) {
					throw new TypeError(
						"Failed to execute 'set' on 'FormData': parameter 2 is not of type 'Blob'"
					)
				}
				name = webidl.converters.USVString(name)
				value = isBlobLike(value)
					? webidl.converters.Blob(value, { strict: false })
					: webidl.converters.USVString(value)
				filename = arguments.length === 3 ? toUSVString(filename) : void 0
				const entry = makeEntry(name, value, filename)
				const idx = this[kState].findIndex((entry2) => entry2.name === name)
				if (idx !== -1) {
					this[kState] = [
						...this[kState].slice(0, idx),
						entry,
						...this[kState].slice(idx + 1).filter((entry2) => entry2.name !== name),
					]
				} else {
					this[kState].push(entry)
				}
			}
			entries() {
				webidl.brandCheck(this, _FormData)
				return makeIterator(
					() => this[kState].map((pair) => [pair.name, pair.value]),
					"FormData",
					"key+value"
				)
			}
			keys() {
				webidl.brandCheck(this, _FormData)
				return makeIterator(
					() => this[kState].map((pair) => [pair.name, pair.value]),
					"FormData",
					"key"
				)
			}
			values() {
				webidl.brandCheck(this, _FormData)
				return makeIterator(
					() => this[kState].map((pair) => [pair.name, pair.value]),
					"FormData",
					"value"
				)
			}
			/**
			 * @param {(value: string, key: string, self: FormData) => void} callbackFn
			 * @param {unknown} thisArg
			 */
			forEach(callbackFn, thisArg = globalThis) {
				webidl.brandCheck(this, _FormData)
				webidl.argumentLengthCheck(arguments, 1, { header: "FormData.forEach" })
				if (typeof callbackFn !== "function") {
					throw new TypeError(
						"Failed to execute 'forEach' on 'FormData': parameter 1 is not of type 'Function'."
					)
				}
				for (const [key, value] of this) {
					callbackFn.apply(thisArg, [value, key, this])
				}
			}
		}
		FormData.prototype[Symbol.iterator] = FormData.prototype.entries
		Object.defineProperties(FormData.prototype, {
			[Symbol.toStringTag]: {
				value: "FormData",
				configurable: true,
			},
		})
		function makeEntry(name, value, filename) {
			name = Buffer.from(name).toString("utf8")
			if (typeof value === "string") {
				value = Buffer.from(value).toString("utf8")
			} else {
				if (!isFileLike(value)) {
					value =
						value instanceof Blob2
							? new File([value], "blob", { type: value.type })
							: new FileLike(value, "blob", { type: value.type })
				}
				if (filename !== void 0) {
					const options = {
						type: value.type,
						lastModified: value.lastModified,
					}
					value =
						(NativeFile && value instanceof NativeFile) || value instanceof UndiciFile
							? new File([value], filename, options)
							: new FileLike(value, filename, options)
				}
			}
			return { name, value }
		}
		module2.exports = { FormData }
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/body.js
var require_body = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/body.js"(
		exports2,
		module2
	) {
		"use strict"
		var Busboy = require_main()
		var util = require_util()
		var {
			ReadableStreamFrom,
			isBlobLike,
			isReadableStreamLike,
			readableStreamClose,
			createDeferredPromise,
			fullyReadBody,
		} = require_util2()
		var { FormData } = require_formdata()
		var { kState } = require_symbols2()
		var { webidl } = require_webidl()
		var { DOMException: DOMException2, structuredClone: structuredClone2 } = require_constants2()
		var { Blob: Blob2, File: NativeFile } = require("node:buffer")
		var { kBodyUsed } = require_symbols()
		var assert = require("node:assert")
		var { isErrored } = require_util()
		var { isUint8Array, isArrayBuffer } = require("node:util/types")
		var { File: UndiciFile } = require_file()
		var { parseMIMEType, serializeAMimeType } = require_dataURL()
		var ReadableStream = globalThis.ReadableStream
		var File = NativeFile ?? UndiciFile
		var textEncoder = new TextEncoder()
		var textDecoder = new TextDecoder()
		function extractBody(object, keepalive = false) {
			if (!ReadableStream) {
				ReadableStream = require("node:stream/web").ReadableStream
			}
			let stream = null
			if (object instanceof ReadableStream) {
				stream = object
			} else if (isBlobLike(object)) {
				stream = object.stream()
			} else {
				stream = new ReadableStream({
					async pull(controller) {
						controller.enqueue(typeof source === "string" ? textEncoder.encode(source) : source)
						queueMicrotask(() => readableStreamClose(controller))
					},
					start() {},
					type: void 0,
				})
			}
			assert(isReadableStreamLike(stream))
			let action = null
			let source = null
			let length = null
			let type = null
			if (typeof object === "string") {
				source = object
				type = "text/plain;charset=UTF-8"
			} else if (object instanceof URLSearchParams) {
				source = object.toString()
				type = "application/x-www-form-urlencoded;charset=UTF-8"
			} else if (isArrayBuffer(object)) {
				source = new Uint8Array([...object])
			} else if (ArrayBuffer.isView(object)) {
				source = new Uint8Array(
					object.buffer.slice(object.byteOffset, object.byteOffset + object.byteLength)
				)
			} else if (util.isFormDataLike(object)) {
				const boundary = `----formdata-undici-0${`${Math.floor(Math.random() * 1e11)}`.padStart(
					11,
					"0"
				)}`
				const prefix = `--${boundary}\r
Content-Disposition: form-data`
				const escape2 = (str) =>
					str.replace(/\n/g, "%0A").replace(/\r/g, "%0D").replace(/"/g, "%22")
				const normalizeLinefeeds = (value) => value.replace(/\r?\n|\r/g, "\r\n")
				const blobParts = []
				const rn = new Uint8Array([13, 10])
				length = 0
				let hasUnknownSizeValue = false
				for (const [name, value] of object) {
					if (typeof value === "string") {
						const chunk2 = textEncoder.encode(
							prefix +
								`; name="${escape2(normalizeLinefeeds(name))}"\r
\r
${normalizeLinefeeds(value)}\r
`
						)
						blobParts.push(chunk2)
						length += chunk2.byteLength
					} else {
						const chunk2 = textEncoder.encode(
							`${prefix}; name="${escape2(normalizeLinefeeds(name))}"` +
								(value.name ? `; filename="${escape2(value.name)}"` : "") +
								`\r
Content-Type: ${value.type || "application/octet-stream"}\r
\r
`
						)
						blobParts.push(chunk2, value, rn)
						if (typeof value.size === "number") {
							length += chunk2.byteLength + value.size + rn.byteLength
						} else {
							hasUnknownSizeValue = true
						}
					}
				}
				const chunk = textEncoder.encode(`--${boundary}--`)
				blobParts.push(chunk)
				length += chunk.byteLength
				if (hasUnknownSizeValue) {
					length = null
				}
				source = object
				action = async function* () {
					for (const part of blobParts) {
						if (part.stream) {
							yield* part.stream()
						} else {
							yield part
						}
					}
				}
				type = "multipart/form-data; boundary=" + boundary
			} else if (isBlobLike(object)) {
				source = object
				length = object.size
				if (object.type) {
					type = object.type
				}
			} else if (typeof object[Symbol.asyncIterator] === "function") {
				if (keepalive) {
					throw new TypeError("keepalive")
				}
				if (util.isDisturbed(object) || object.locked) {
					throw new TypeError("Response body object should not be disturbed or locked")
				}
				stream = object instanceof ReadableStream ? object : ReadableStreamFrom(object)
			}
			if (typeof source === "string" || util.isBuffer(source)) {
				length = Buffer.byteLength(source)
			}
			if (action != undefined) {
				let iterator
				stream = new ReadableStream({
					async start() {
						iterator = action(object)[Symbol.asyncIterator]()
					},
					async pull(controller) {
						const { value, done } = await iterator.next()
						if (done) {
							queueMicrotask(() => {
								controller.close()
							})
						} else {
							if (!isErrored(stream)) {
								controller.enqueue(new Uint8Array(value))
							}
						}
						return controller.desiredSize > 0
					},
					async cancel(reason) {
						await iterator.return()
					},
					type: void 0,
				})
			}
			const body = { stream, source, length }
			return [body, type]
		}
		function safelyExtractBody(object, keepalive = false) {
			if (!ReadableStream) {
				ReadableStream = require("node:stream/web").ReadableStream
			}
			if (object instanceof ReadableStream) {
				assert(!util.isDisturbed(object), "The body has already been consumed.")
				assert(!object.locked, "The stream is locked.")
			}
			return extractBody(object, keepalive)
		}
		function cloneBody(body) {
			const [out1, out2] = body.stream.tee()
			const out2Clone = structuredClone2(out2, { transfer: [out2] })
			const [, finalClone] = out2Clone.tee()
			body.stream = out1
			return {
				stream: finalClone,
				length: body.length,
				source: body.source,
			}
		}
		async function* consumeBody(body) {
			if (body) {
				if (isUint8Array(body)) {
					yield body
				} else {
					const stream = body.stream
					if (util.isDisturbed(stream)) {
						throw new TypeError("The body has already been consumed.")
					}
					if (stream.locked) {
						throw new TypeError("The stream is locked.")
					}
					stream[kBodyUsed] = true
					yield* stream
				}
			}
		}
		function throwIfAborted(state) {
			if (state.aborted) {
				throw new DOMException2("The operation was aborted.", "AbortError")
			}
		}
		function bodyMixinMethods(instance) {
			const methods = {
				blob() {
					return specConsumeBody(
						this,
						(bytes) => {
							let mimeType = bodyMimeType(this)
							if (mimeType === "failure") {
								mimeType = ""
							} else if (mimeType) {
								mimeType = serializeAMimeType(mimeType)
							}
							return new Blob2([bytes], { type: mimeType })
						},
						instance
					)
				},
				arrayBuffer() {
					return specConsumeBody(
						this,
						(bytes) => {
							return new Uint8Array(bytes).buffer
						},
						instance
					)
				},
				text() {
					return specConsumeBody(this, utf8DecodeBytes, instance)
				},
				json() {
					return specConsumeBody(this, parseJSONFromBytes, instance)
				},
				async formData() {
					webidl.brandCheck(this, instance)
					throwIfAborted(this[kState])
					const contentType = this.headers.get("Content-Type")
					if (/multipart\/form-data/.test(contentType)) {
						const headers = {}
						for (const [key, value] of this.headers) headers[key.toLowerCase()] = value
						const responseFormData = new FormData()
						let busboy
						try {
							busboy = new Busboy({
								headers,
								preservePath: true,
							})
						} catch (err) {
							throw new DOMException2(`${err}`, "AbortError")
						}
						busboy.on("field", (name, value) => {
							responseFormData.append(name, value)
						})
						busboy.on("file", (name, value, filename, encoding, mimeType) => {
							const chunks = []
							if (encoding === "base64" || encoding.toLowerCase() === "base64") {
								let base64chunk = ""
								value.on("data", (chunk) => {
									base64chunk += chunk.toString().replace(/[\r\n]/gm, "")
									const end = base64chunk.length - (base64chunk.length % 4)
									chunks.push(Buffer.from(base64chunk.slice(0, end), "base64"))
									base64chunk = base64chunk.slice(end)
								})
								value.on("end", () => {
									chunks.push(Buffer.from(base64chunk, "base64"))
									responseFormData.append(name, new File(chunks, filename, { type: mimeType }))
								})
							} else {
								value.on("data", (chunk) => {
									chunks.push(chunk)
								})
								value.on("end", () => {
									responseFormData.append(name, new File(chunks, filename, { type: mimeType }))
								})
							}
						})
						const busboyResolve = new Promise((resolve, reject) => {
							busboy.on("finish", resolve)
							busboy.on("error", (err) => reject(new TypeError(err)))
						})
						if (this.body !== null)
							for await (const chunk of consumeBody(this[kState].body)) busboy.write(chunk)
						busboy.end()
						await busboyResolve
						return responseFormData
					} else if (/application\/x-www-form-urlencoded/.test(contentType)) {
						let entries
						try {
							let text = ""
							const streamingDecoder = new TextDecoder("utf-8", { ignoreBOM: true })
							for await (const chunk of consumeBody(this[kState].body)) {
								if (!isUint8Array(chunk)) {
									throw new TypeError("Expected Uint8Array chunk")
								}
								text += streamingDecoder.decode(chunk, { stream: true })
							}
							text += streamingDecoder.decode()
							entries = new URLSearchParams(text)
						} catch (err) {
							throw Object.assign(new TypeError(), { cause: err })
						}
						const formData = new FormData()
						for (const [name, value] of entries) {
							formData.append(name, value)
						}
						return formData
					} else {
						await Promise.resolve()
						throwIfAborted(this[kState])
						throw webidl.errors.exception({
							header: `${instance.name}.formData`,
							message: "Could not parse content as FormData.",
						})
					}
				},
			}
			return methods
		}
		function mixinBody(prototype) {
			Object.assign(prototype.prototype, bodyMixinMethods(prototype))
		}
		async function specConsumeBody(object, convertBytesToJSValue, instance) {
			webidl.brandCheck(object, instance)
			throwIfAborted(object[kState])
			if (bodyUnusable(object[kState].body)) {
				throw new TypeError("Body is unusable")
			}
			const promise = createDeferredPromise()
			const errorSteps = (error) => promise.reject(error)
			const successSteps = (data) => {
				try {
					promise.resolve(convertBytesToJSValue(data))
				} catch (e) {
					errorSteps(e)
				}
			}
			if (object[kState].body == undefined) {
				successSteps(new Uint8Array())
				return promise.promise
			}
			await fullyReadBody(object[kState].body, successSteps, errorSteps)
			return promise.promise
		}
		function bodyUnusable(body) {
			return body != undefined && (body.stream.locked || util.isDisturbed(body.stream))
		}
		function utf8DecodeBytes(buffer) {
			if (buffer.length === 0) {
				return ""
			}
			if (buffer[0] === 239 && buffer[1] === 187 && buffer[2] === 191) {
				buffer = buffer.subarray(3)
			}
			const output = textDecoder.decode(buffer)
			return output
		}
		function parseJSONFromBytes(bytes) {
			return JSON.parse(utf8DecodeBytes(bytes))
		}
		function bodyMimeType(object) {
			const { headersList } = object[kState]
			const contentType = headersList.get("content-type")
			if (contentType === null) {
				return "failure"
			}
			return parseMIMEType(contentType)
		}
		module2.exports = {
			extractBody,
			safelyExtractBody,
			cloneBody,
			mixinBody,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/core/request.js
var require_request = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/core/request.js"(
		exports2,
		module2
	) {
		"use strict"
		var { InvalidArgumentError, NotSupportedError } = require_errors()
		var assert = require("node:assert")
		var { kHTTP2BuildRequest, kHTTP2CopyHeaders, kHTTP1BuildRequest } = require_symbols()
		var util = require_util()
		var tokenRegExp = /^[\^_`a-zA-Z\-0-9!#$%&'*+.|~]+$/
		var headerCharRegex = /[^\t\x20-\x7e\x80-\xff]/
		var invalidPathRegex = /[^\u0021-\u00ff]/
		var kHandler = Symbol("handler")
		var channels = {}
		var extractBody
		try {
			const diagnosticsChannel = require("node:diagnostics_channel")
			channels.create = diagnosticsChannel.channel("undici:request:create")
			channels.bodySent = diagnosticsChannel.channel("undici:request:bodySent")
			channels.headers = diagnosticsChannel.channel("undici:request:headers")
			channels.trailers = diagnosticsChannel.channel("undici:request:trailers")
			channels.error = diagnosticsChannel.channel("undici:request:error")
		} catch {
			channels.create = { hasSubscribers: false }
			channels.bodySent = { hasSubscribers: false }
			channels.headers = { hasSubscribers: false }
			channels.trailers = { hasSubscribers: false }
			channels.error = { hasSubscribers: false }
		}
		var Request = class _Request {
			constructor(
				origin,
				{
					path,
					method,
					body,
					headers,
					query,
					idempotent,
					blocking,
					upgrade,
					headersTimeout,
					bodyTimeout,
					reset: reset2,
					throwOnError,
					expectContinue,
				},
				handler
			) {
				if (typeof path !== "string") {
					throw new InvalidArgumentError("path must be a string")
				} else if (
					path[0] !== "/" &&
					!(path.startsWith("http://") || path.startsWith("https://")) &&
					method !== "CONNECT"
				) {
					throw new InvalidArgumentError("path must be an absolute URL or start with a slash")
				} else if (invalidPathRegex.exec(path) !== null) {
					throw new InvalidArgumentError("invalid request path")
				}
				if (typeof method !== "string") {
					throw new InvalidArgumentError("method must be a string")
				} else if (tokenRegExp.exec(method) === null) {
					throw new InvalidArgumentError("invalid request method")
				}
				if (upgrade && typeof upgrade !== "string") {
					throw new InvalidArgumentError("upgrade must be a string")
				}
				if (
					headersTimeout != undefined &&
					(!Number.isFinite(headersTimeout) || headersTimeout < 0)
				) {
					throw new InvalidArgumentError("invalid headersTimeout")
				}
				if (bodyTimeout != undefined && (!Number.isFinite(bodyTimeout) || bodyTimeout < 0)) {
					throw new InvalidArgumentError("invalid bodyTimeout")
				}
				if (reset2 != undefined && typeof reset2 !== "boolean") {
					throw new InvalidArgumentError("invalid reset")
				}
				if (expectContinue != undefined && typeof expectContinue !== "boolean") {
					throw new InvalidArgumentError("invalid expectContinue")
				}
				this.headersTimeout = headersTimeout
				this.bodyTimeout = bodyTimeout
				this.throwOnError = throwOnError === true
				this.method = method
				this.abort = null
				if (body == undefined) {
					this.body = null
				} else if (util.isStream(body)) {
					this.body = body
					const rState = this.body._readableState
					if (!rState || !rState.autoDestroy) {
						this.endHandler = function autoDestroy() {
							util.destroy(this)
						}
						this.body.on("end", this.endHandler)
					}
					this.errorHandler = (err) => {
						if (this.abort) {
							this.abort(err)
						} else {
							this.error = err
						}
					}
					this.body.on("error", this.errorHandler)
				} else if (util.isBuffer(body)) {
					this.body = body.byteLength ? body : null
				} else if (ArrayBuffer.isView(body)) {
					this.body = body.buffer.byteLength
						? Buffer.from(body.buffer, body.byteOffset, body.byteLength)
						: null
				} else if (body instanceof ArrayBuffer) {
					this.body = body.byteLength ? Buffer.from(body) : null
				} else if (typeof body === "string") {
					this.body = body.length ? Buffer.from(body) : null
				} else if (util.isFormDataLike(body) || util.isIterable(body) || util.isBlobLike(body)) {
					this.body = body
				} else {
					throw new InvalidArgumentError(
						"body must be a string, a Buffer, a Readable stream, an iterable, or an async iterable"
					)
				}
				this.completed = false
				this.aborted = false
				this.upgrade = upgrade || null
				this.path = query ? util.buildURL(path, query) : path
				this.origin = origin
				this.idempotent =
					idempotent == undefined ? method === "HEAD" || method === "GET" : idempotent
				this.blocking = blocking == undefined ? false : blocking
				this.reset = reset2 == undefined ? null : reset2
				this.host = null
				this.contentLength = null
				this.contentType = null
				this.headers = ""
				this.expectContinue = expectContinue != undefined ? expectContinue : false
				if (Array.isArray(headers)) {
					if (headers.length % 2 !== 0) {
						throw new InvalidArgumentError("headers array must be even")
					}
					for (let i = 0; i < headers.length; i += 2) {
						processHeader(this, headers[i], headers[i + 1])
					}
				} else if (headers && typeof headers === "object") {
					const keys = Object.keys(headers)
					for (const key of keys) {
						processHeader(this, key, headers[key])
					}
				} else if (headers != undefined) {
					throw new InvalidArgumentError("headers must be an object or an array")
				}
				if (util.isFormDataLike(this.body)) {
					if (util.nodeMajor < 16 || (util.nodeMajor === 16 && util.nodeMinor < 8)) {
						throw new InvalidArgumentError(
							"Form-Data bodies are only supported in node v16.8 and newer."
						)
					}
					if (!extractBody) {
						extractBody = require_body().extractBody
					}
					const [bodyStream, contentType] = extractBody(body)
					if (this.contentType == undefined) {
						this.contentType = contentType
						this.headers += `content-type: ${contentType}\r
`
					}
					this.body = bodyStream.stream
					this.contentLength = bodyStream.length
				} else if (util.isBlobLike(body) && this.contentType == undefined && body.type) {
					this.contentType = body.type
					this.headers += `content-type: ${body.type}\r
`
				}
				util.validateHandler(handler, method, upgrade)
				this.servername = util.getServerName(this.host)
				this[kHandler] = handler
				if (channels.create.hasSubscribers) {
					channels.create.publish({ request: this })
				}
			}
			onBodySent(chunk) {
				if (this[kHandler].onBodySent) {
					try {
						return this[kHandler].onBodySent(chunk)
					} catch (err) {
						this.abort(err)
					}
				}
			}
			onRequestSent() {
				if (channels.bodySent.hasSubscribers) {
					channels.bodySent.publish({ request: this })
				}
				if (this[kHandler].onRequestSent) {
					try {
						return this[kHandler].onRequestSent()
					} catch (err) {
						this.abort(err)
					}
				}
			}
			onConnect(abort) {
				assert(!this.aborted)
				assert(!this.completed)
				if (this.error) {
					abort(this.error)
				} else {
					this.abort = abort
					return this[kHandler].onConnect(abort)
				}
			}
			onHeaders(statusCode, headers, resume, statusText) {
				assert(!this.aborted)
				assert(!this.completed)
				if (channels.headers.hasSubscribers) {
					channels.headers.publish({ request: this, response: { statusCode, headers, statusText } })
				}
				try {
					return this[kHandler].onHeaders(statusCode, headers, resume, statusText)
				} catch (err) {
					this.abort(err)
				}
			}
			onData(chunk) {
				assert(!this.aborted)
				assert(!this.completed)
				try {
					return this[kHandler].onData(chunk)
				} catch (err) {
					this.abort(err)
					return false
				}
			}
			onUpgrade(statusCode, headers, socket) {
				assert(!this.aborted)
				assert(!this.completed)
				return this[kHandler].onUpgrade(statusCode, headers, socket)
			}
			onComplete(trailers) {
				this.onFinally()
				assert(!this.aborted)
				this.completed = true
				if (channels.trailers.hasSubscribers) {
					channels.trailers.publish({ request: this, trailers })
				}
				try {
					return this[kHandler].onComplete(trailers)
				} catch (err) {
					this.onError(err)
				}
			}
			onError(error) {
				this.onFinally()
				if (channels.error.hasSubscribers) {
					channels.error.publish({ request: this, error })
				}
				if (this.aborted) {
					return
				}
				this.aborted = true
				return this[kHandler].onError(error)
			}
			onFinally() {
				if (this.errorHandler) {
					this.body.off("error", this.errorHandler)
					this.errorHandler = null
				}
				if (this.endHandler) {
					this.body.off("end", this.endHandler)
					this.endHandler = null
				}
			}
			// TODO: adjust to support H2
			addHeader(key, value) {
				processHeader(this, key, value)
				return this
			}
			static [kHTTP1BuildRequest](origin, opts, handler) {
				return new _Request(origin, opts, handler)
			}
			static [kHTTP2BuildRequest](origin, opts, handler) {
				const headers = opts.headers
				opts = { ...opts, headers: null }
				const request = new _Request(origin, opts, handler)
				request.headers = {}
				if (Array.isArray(headers)) {
					if (headers.length % 2 !== 0) {
						throw new InvalidArgumentError("headers array must be even")
					}
					for (let i = 0; i < headers.length; i += 2) {
						processHeader(request, headers[i], headers[i + 1], true)
					}
				} else if (headers && typeof headers === "object") {
					const keys = Object.keys(headers)
					for (const key of keys) {
						processHeader(request, key, headers[key], true)
					}
				} else if (headers != undefined) {
					throw new InvalidArgumentError("headers must be an object or an array")
				}
				return request
			}
			static [kHTTP2CopyHeaders](raw) {
				const rawHeaders = raw.split("\r\n")
				const headers = {}
				for (const header of rawHeaders) {
					const [key, value] = header.split(": ")
					if (value == undefined || value.length === 0) continue
					if (headers[key]) headers[key] += `,${value}`
					else headers[key] = value
				}
				return headers
			}
		}
		function processHeaderValue(key, val, skipAppend) {
			if (val && typeof val === "object") {
				throw new InvalidArgumentError(`invalid ${key} header`)
			}
			val = val != undefined ? `${val}` : ""
			if (headerCharRegex.exec(val) !== null) {
				throw new InvalidArgumentError(`invalid ${key} header`)
			}
			return skipAppend
				? val
				: `${key}: ${val}\r
`
		}
		function processHeader(request, key, val, skipAppend = false) {
			if (val && typeof val === "object" && !Array.isArray(val)) {
				throw new InvalidArgumentError(`invalid ${key} header`)
			} else if (val === void 0) {
				return
			}
			if (request.host === null && key.length === 4 && key.toLowerCase() === "host") {
				if (headerCharRegex.exec(val) !== null) {
					throw new InvalidArgumentError(`invalid ${key} header`)
				}
				request.host = val
			} else if (
				request.contentLength === null &&
				key.length === 14 &&
				key.toLowerCase() === "content-length"
			) {
				request.contentLength = parseInt(val, 10)
				if (!Number.isFinite(request.contentLength)) {
					throw new InvalidArgumentError("invalid content-length header")
				}
			} else if (
				request.contentType === null &&
				key.length === 12 &&
				key.toLowerCase() === "content-type"
			) {
				request.contentType = val
				if (skipAppend) request.headers[key] = processHeaderValue(key, val, skipAppend)
				else request.headers += processHeaderValue(key, val)
			} else if (key.length === 17 && key.toLowerCase() === "transfer-encoding") {
				throw new InvalidArgumentError("invalid transfer-encoding header")
			} else if (key.length === 10 && key.toLowerCase() === "connection") {
				const value = typeof val === "string" ? val.toLowerCase() : null
				if (value !== "close" && value !== "keep-alive") {
					throw new InvalidArgumentError("invalid connection header")
				} else if (value === "close") {
					request.reset = true
				}
			} else if (key.length === 10 && key.toLowerCase() === "keep-alive") {
				throw new InvalidArgumentError("invalid keep-alive header")
			} else if (key.length === 7 && key.toLowerCase() === "upgrade") {
				throw new InvalidArgumentError("invalid upgrade header")
			} else if (key.length === 6 && key.toLowerCase() === "expect") {
				throw new NotSupportedError("expect header not supported")
			} else if (tokenRegExp.exec(key) === null) {
				throw new InvalidArgumentError("invalid header key")
			} else {
				if (Array.isArray(val)) {
					for (const element of val) {
						if (skipAppend) {
							if (request.headers[key])
								request.headers[key] += `,${processHeaderValue(key, element, skipAppend)}`
							else request.headers[key] = processHeaderValue(key, element, skipAppend)
						} else {
							request.headers += processHeaderValue(key, element)
						}
					}
				} else {
					if (skipAppend) request.headers[key] = processHeaderValue(key, val, skipAppend)
					else request.headers += processHeaderValue(key, val)
				}
			}
		}
		module2.exports = Request
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/dispatcher.js
var require_dispatcher = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/dispatcher.js"(
		exports2,
		module2
	) {
		"use strict"
		var EventEmitter = require("node:events")
		var Dispatcher = class extends EventEmitter {
			dispatch() {
				throw new Error("not implemented")
			}
			close() {
				throw new Error("not implemented")
			}
			destroy() {
				throw new Error("not implemented")
			}
		}
		module2.exports = Dispatcher
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/dispatcher-base.js
var require_dispatcher_base = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/dispatcher-base.js"(
		exports2,
		module2
	) {
		"use strict"
		var Dispatcher = require_dispatcher()
		var { ClientDestroyedError, ClientClosedError, InvalidArgumentError } = require_errors()
		var { kDestroy, kClose, kDispatch, kInterceptors } = require_symbols()
		var kDestroyed = Symbol("destroyed")
		var kClosed = Symbol("closed")
		var kOnDestroyed = Symbol("onDestroyed")
		var kOnClosed = Symbol("onClosed")
		var kInterceptedDispatch = Symbol("Intercepted Dispatch")
		var DispatcherBase = class extends Dispatcher {
			constructor() {
				super()
				this[kDestroyed] = false
				this[kOnDestroyed] = null
				this[kClosed] = false
				this[kOnClosed] = []
			}
			get destroyed() {
				return this[kDestroyed]
			}
			get closed() {
				return this[kClosed]
			}
			get interceptors() {
				return this[kInterceptors]
			}
			set interceptors(newInterceptors) {
				if (newInterceptors) {
					for (let i = newInterceptors.length - 1; i >= 0; i--) {
						const interceptor = this[kInterceptors][i]
						if (typeof interceptor !== "function") {
							throw new InvalidArgumentError("interceptor must be an function")
						}
					}
				}
				this[kInterceptors] = newInterceptors
			}
			close(callback) {
				if (callback === void 0) {
					return new Promise((resolve, reject) => {
						this.close((err, data) => {
							return err ? reject(err) : resolve(data)
						})
					})
				}
				if (typeof callback !== "function") {
					throw new InvalidArgumentError("invalid callback")
				}
				if (this[kDestroyed]) {
					queueMicrotask(() => callback(new ClientDestroyedError(), null))
					return
				}
				if (this[kClosed]) {
					if (this[kOnClosed]) {
						this[kOnClosed].push(callback)
					} else {
						queueMicrotask(() => callback(null, null))
					}
					return
				}
				this[kClosed] = true
				this[kOnClosed].push(callback)
				const onClosed = () => {
					const callbacks = this[kOnClosed]
					this[kOnClosed] = null
					for (const callback_ of callbacks) {
						callback_(null, null)
					}
				}
				this[kClose]()
					.then(() => this.destroy())
					.then(() => {
						queueMicrotask(onClosed)
					})
			}
			destroy(err, callback) {
				if (typeof err === "function") {
					callback = err
					err = null
				}
				if (callback === void 0) {
					return new Promise((resolve, reject) => {
						this.destroy(err, (err2, data) => {
							return err2
								? /* istanbul ignore next: should never error */
								  reject(err2)
								: resolve(data)
						})
					})
				}
				if (typeof callback !== "function") {
					throw new InvalidArgumentError("invalid callback")
				}
				if (this[kDestroyed]) {
					if (this[kOnDestroyed]) {
						this[kOnDestroyed].push(callback)
					} else {
						queueMicrotask(() => callback(null, null))
					}
					return
				}
				if (!err) {
					err = new ClientDestroyedError()
				}
				this[kDestroyed] = true
				this[kOnDestroyed] = this[kOnDestroyed] || []
				this[kOnDestroyed].push(callback)
				const onDestroyed = () => {
					const callbacks = this[kOnDestroyed]
					this[kOnDestroyed] = null
					for (const callback_ of callbacks) {
						callback_(null, null)
					}
				}
				this[kDestroy](err).then(() => {
					queueMicrotask(onDestroyed)
				})
			}
			[kInterceptedDispatch](opts, handler) {
				if (!this[kInterceptors] || this[kInterceptors].length === 0) {
					this[kInterceptedDispatch] = this[kDispatch]
					return this[kDispatch](opts, handler)
				}
				let dispatch = this[kDispatch].bind(this)
				for (let i = this[kInterceptors].length - 1; i >= 0; i--) {
					dispatch = this[kInterceptors][i](dispatch)
				}
				this[kInterceptedDispatch] = dispatch
				return dispatch(opts, handler)
			}
			dispatch(opts, handler) {
				if (!handler || typeof handler !== "object") {
					throw new InvalidArgumentError("handler must be an object")
				}
				try {
					if (!opts || typeof opts !== "object") {
						throw new InvalidArgumentError("opts must be an object.")
					}
					if (this[kDestroyed] || this[kOnDestroyed]) {
						throw new ClientDestroyedError()
					}
					if (this[kClosed]) {
						throw new ClientClosedError()
					}
					return this[kInterceptedDispatch](opts, handler)
				} catch (err) {
					if (typeof handler.onError !== "function") {
						throw new InvalidArgumentError("invalid onError method")
					}
					handler.onError(err)
					return false
				}
			}
		}
		module2.exports = DispatcherBase
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/core/connect.js
var require_connect = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/core/connect.js"(
		exports2,
		module2
	) {
		"use strict"
		var net = require("node:net")
		var assert = require("node:assert")
		var util = require_util()
		var { InvalidArgumentError, ConnectTimeoutError } = require_errors()
		var tls
		var SessionCache
		if (global.FinalizationRegistry && !process.env.NODE_V8_COVERAGE) {
			SessionCache = class WeakSessionCache {
				constructor(maxCachedSessions) {
					this._maxCachedSessions = maxCachedSessions
					this._sessionCache = /* @__PURE__ */ new Map()
					this._sessionRegistry = new global.FinalizationRegistry((key) => {
						if (this._sessionCache.size < this._maxCachedSessions) {
							return
						}
						const ref = this._sessionCache.get(key)
						if (ref !== void 0 && ref.deref() === void 0) {
							this._sessionCache.delete(key)
						}
					})
				}
				get(sessionKey) {
					const ref = this._sessionCache.get(sessionKey)
					return ref ? ref.deref() : null
				}
				set(sessionKey, session) {
					if (this._maxCachedSessions === 0) {
						return
					}
					this._sessionCache.set(sessionKey, new WeakRef(session))
					this._sessionRegistry.register(session, sessionKey)
				}
			}
		} else {
			SessionCache = class SimpleSessionCache {
				constructor(maxCachedSessions) {
					this._maxCachedSessions = maxCachedSessions
					this._sessionCache = /* @__PURE__ */ new Map()
				}
				get(sessionKey) {
					return this._sessionCache.get(sessionKey)
				}
				set(sessionKey, session) {
					if (this._maxCachedSessions === 0) {
						return
					}
					if (this._sessionCache.size >= this._maxCachedSessions) {
						const { value: oldestKey } = this._sessionCache.keys().next()
						this._sessionCache.delete(oldestKey)
					}
					this._sessionCache.set(sessionKey, session)
				}
			}
		}
		function buildConnector({ allowH2, maxCachedSessions, socketPath, timeout, ...opts }) {
			if (
				maxCachedSessions != undefined &&
				(!Number.isInteger(maxCachedSessions) || maxCachedSessions < 0)
			) {
				throw new InvalidArgumentError("maxCachedSessions must be a positive integer or zero")
			}
			const options = { path: socketPath, ...opts }
			const sessionCache = new SessionCache(
				maxCachedSessions == undefined ? 100 : maxCachedSessions
			)
			timeout = timeout == undefined ? 1e4 : timeout
			allowH2 = allowH2 != undefined ? allowH2 : false
			return function connect(
				{ hostname, host, protocol, port, servername, localAddress, httpSocket },
				callback
			) {
				let socket
				if (protocol === "https:") {
					if (!tls) {
						tls = require("node:tls")
					}
					servername = servername || options.servername || util.getServerName(host) || null
					const sessionKey = servername || hostname
					const session = sessionCache.get(sessionKey) || null
					assert(sessionKey)
					socket = tls.connect({
						highWaterMark: 16384,
						// TLS in node can't have bigger HWM anyway...
						...options,
						servername,
						session,
						localAddress,
						// TODO(HTTP/2): Add support for h2c
						ALPNProtocols: allowH2 ? ["http/1.1", "h2"] : ["http/1.1"],
						socket: httpSocket,
						// upgrade socket connection
						port: port || 443,
						host: hostname,
					})
					socket.on("session", function (session2) {
						sessionCache.set(sessionKey, session2)
					})
				} else {
					assert(!httpSocket, "httpSocket can only be sent on TLS update")
					socket = net.connect({
						highWaterMark: 64 * 1024,
						// Same as nodejs fs streams.
						...options,
						localAddress,
						port: port || 80,
						host: hostname,
					})
				}
				if (options.keepAlive == undefined || options.keepAlive) {
					const keepAliveInitialDelay =
						options.keepAliveInitialDelay === void 0 ? 6e4 : options.keepAliveInitialDelay
					socket.setKeepAlive(true, keepAliveInitialDelay)
				}
				const cancelTimeout = setupTimeout(() => onConnectTimeout(socket), timeout)
				socket
					.setNoDelay(true)
					.once(protocol === "https:" ? "secureConnect" : "connect", function () {
						cancelTimeout()
						if (callback) {
							const cb = callback
							callback = null
							cb(null, this)
						}
					})
					.on("error", function (err) {
						cancelTimeout()
						if (callback) {
							const cb = callback
							callback = null
							cb(err)
						}
					})
				return socket
			}
		}
		function setupTimeout(onConnectTimeout2, timeout) {
			if (!timeout) {
				return () => {}
			}
			let s1 = null
			let s2 = null
			const timeoutId = setTimeout(() => {
				s1 = setImmediate(() => {
					if (process.platform === "win32") {
						s2 = setImmediate(() => onConnectTimeout2())
					} else {
						onConnectTimeout2()
					}
				})
			}, timeout)
			return () => {
				clearTimeout(timeoutId)
				clearImmediate(s1)
				clearImmediate(s2)
			}
		}
		function onConnectTimeout(socket) {
			util.destroy(socket, new ConnectTimeoutError())
		}
		module2.exports = buildConnector
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/llhttp/utils.js
var require_utils2 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/llhttp/utils.js"(exports2) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.enumToMap = void 0
		function enumToMap(obj) {
			const res = {}
			for (const key of Object.keys(obj)) {
				const value = obj[key]
				if (typeof value === "number") {
					res[key] = value
				}
			}
			return res
		}
		exports2.enumToMap = enumToMap
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/llhttp/constants.js
var require_constants3 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/llhttp/constants.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.SPECIAL_HEADERS =
			exports2.HEADER_STATE =
			exports2.MINOR =
			exports2.MAJOR =
			exports2.CONNECTION_TOKEN_CHARS =
			exports2.HEADER_CHARS =
			exports2.TOKEN =
			exports2.STRICT_TOKEN =
			exports2.HEX =
			exports2.URL_CHAR =
			exports2.STRICT_URL_CHAR =
			exports2.USERINFO_CHARS =
			exports2.MARK =
			exports2.ALPHANUM =
			exports2.NUM =
			exports2.HEX_MAP =
			exports2.NUM_MAP =
			exports2.ALPHA =
			exports2.FINISH =
			exports2.H_METHOD_MAP =
			exports2.METHOD_MAP =
			exports2.METHODS_RTSP =
			exports2.METHODS_ICE =
			exports2.METHODS_HTTP =
			exports2.METHODS =
			exports2.LENIENT_FLAGS =
			exports2.FLAGS =
			exports2.TYPE =
			exports2.ERROR =
				void 0
		var utils_1 = require_utils2()
		var ERROR2
		;(function (ERROR3) {
			ERROR3[(ERROR3["OK"] = 0)] = "OK"
			ERROR3[(ERROR3["INTERNAL"] = 1)] = "INTERNAL"
			ERROR3[(ERROR3["STRICT"] = 2)] = "STRICT"
			ERROR3[(ERROR3["LF_EXPECTED"] = 3)] = "LF_EXPECTED"
			ERROR3[(ERROR3["UNEXPECTED_CONTENT_LENGTH"] = 4)] = "UNEXPECTED_CONTENT_LENGTH"
			ERROR3[(ERROR3["CLOSED_CONNECTION"] = 5)] = "CLOSED_CONNECTION"
			ERROR3[(ERROR3["INVALID_METHOD"] = 6)] = "INVALID_METHOD"
			ERROR3[(ERROR3["INVALID_URL"] = 7)] = "INVALID_URL"
			ERROR3[(ERROR3["INVALID_CONSTANT"] = 8)] = "INVALID_CONSTANT"
			ERROR3[(ERROR3["INVALID_VERSION"] = 9)] = "INVALID_VERSION"
			ERROR3[(ERROR3["INVALID_HEADER_TOKEN"] = 10)] = "INVALID_HEADER_TOKEN"
			ERROR3[(ERROR3["INVALID_CONTENT_LENGTH"] = 11)] = "INVALID_CONTENT_LENGTH"
			ERROR3[(ERROR3["INVALID_CHUNK_SIZE"] = 12)] = "INVALID_CHUNK_SIZE"
			ERROR3[(ERROR3["INVALID_STATUS"] = 13)] = "INVALID_STATUS"
			ERROR3[(ERROR3["INVALID_EOF_STATE"] = 14)] = "INVALID_EOF_STATE"
			ERROR3[(ERROR3["INVALID_TRANSFER_ENCODING"] = 15)] = "INVALID_TRANSFER_ENCODING"
			ERROR3[(ERROR3["CB_MESSAGE_BEGIN"] = 16)] = "CB_MESSAGE_BEGIN"
			ERROR3[(ERROR3["CB_HEADERS_COMPLETE"] = 17)] = "CB_HEADERS_COMPLETE"
			ERROR3[(ERROR3["CB_MESSAGE_COMPLETE"] = 18)] = "CB_MESSAGE_COMPLETE"
			ERROR3[(ERROR3["CB_CHUNK_HEADER"] = 19)] = "CB_CHUNK_HEADER"
			ERROR3[(ERROR3["CB_CHUNK_COMPLETE"] = 20)] = "CB_CHUNK_COMPLETE"
			ERROR3[(ERROR3["PAUSED"] = 21)] = "PAUSED"
			ERROR3[(ERROR3["PAUSED_UPGRADE"] = 22)] = "PAUSED_UPGRADE"
			ERROR3[(ERROR3["PAUSED_H2_UPGRADE"] = 23)] = "PAUSED_H2_UPGRADE"
			ERROR3[(ERROR3["USER"] = 24)] = "USER"
		})((ERROR2 = exports2.ERROR || (exports2.ERROR = {})))
		var TYPE
		;(function (TYPE2) {
			TYPE2[(TYPE2["BOTH"] = 0)] = "BOTH"
			TYPE2[(TYPE2["REQUEST"] = 1)] = "REQUEST"
			TYPE2[(TYPE2["RESPONSE"] = 2)] = "RESPONSE"
		})((TYPE = exports2.TYPE || (exports2.TYPE = {})))
		var FLAGS
		;(function (FLAGS2) {
			FLAGS2[(FLAGS2["CONNECTION_KEEP_ALIVE"] = 1)] = "CONNECTION_KEEP_ALIVE"
			FLAGS2[(FLAGS2["CONNECTION_CLOSE"] = 2)] = "CONNECTION_CLOSE"
			FLAGS2[(FLAGS2["CONNECTION_UPGRADE"] = 4)] = "CONNECTION_UPGRADE"
			FLAGS2[(FLAGS2["CHUNKED"] = 8)] = "CHUNKED"
			FLAGS2[(FLAGS2["UPGRADE"] = 16)] = "UPGRADE"
			FLAGS2[(FLAGS2["CONTENT_LENGTH"] = 32)] = "CONTENT_LENGTH"
			FLAGS2[(FLAGS2["SKIPBODY"] = 64)] = "SKIPBODY"
			FLAGS2[(FLAGS2["TRAILING"] = 128)] = "TRAILING"
			FLAGS2[(FLAGS2["TRANSFER_ENCODING"] = 512)] = "TRANSFER_ENCODING"
		})((FLAGS = exports2.FLAGS || (exports2.FLAGS = {})))
		var LENIENT_FLAGS
		;(function (LENIENT_FLAGS2) {
			LENIENT_FLAGS2[(LENIENT_FLAGS2["HEADERS"] = 1)] = "HEADERS"
			LENIENT_FLAGS2[(LENIENT_FLAGS2["CHUNKED_LENGTH"] = 2)] = "CHUNKED_LENGTH"
			LENIENT_FLAGS2[(LENIENT_FLAGS2["KEEP_ALIVE"] = 4)] = "KEEP_ALIVE"
		})((LENIENT_FLAGS = exports2.LENIENT_FLAGS || (exports2.LENIENT_FLAGS = {})))
		var METHODS
		;(function (METHODS2) {
			METHODS2[(METHODS2["DELETE"] = 0)] = "DELETE"
			METHODS2[(METHODS2["GET"] = 1)] = "GET"
			METHODS2[(METHODS2["HEAD"] = 2)] = "HEAD"
			METHODS2[(METHODS2["POST"] = 3)] = "POST"
			METHODS2[(METHODS2["PUT"] = 4)] = "PUT"
			METHODS2[(METHODS2["CONNECT"] = 5)] = "CONNECT"
			METHODS2[(METHODS2["OPTIONS"] = 6)] = "OPTIONS"
			METHODS2[(METHODS2["TRACE"] = 7)] = "TRACE"
			METHODS2[(METHODS2["COPY"] = 8)] = "COPY"
			METHODS2[(METHODS2["LOCK"] = 9)] = "LOCK"
			METHODS2[(METHODS2["MKCOL"] = 10)] = "MKCOL"
			METHODS2[(METHODS2["MOVE"] = 11)] = "MOVE"
			METHODS2[(METHODS2["PROPFIND"] = 12)] = "PROPFIND"
			METHODS2[(METHODS2["PROPPATCH"] = 13)] = "PROPPATCH"
			METHODS2[(METHODS2["SEARCH"] = 14)] = "SEARCH"
			METHODS2[(METHODS2["UNLOCK"] = 15)] = "UNLOCK"
			METHODS2[(METHODS2["BIND"] = 16)] = "BIND"
			METHODS2[(METHODS2["REBIND"] = 17)] = "REBIND"
			METHODS2[(METHODS2["UNBIND"] = 18)] = "UNBIND"
			METHODS2[(METHODS2["ACL"] = 19)] = "ACL"
			METHODS2[(METHODS2["REPORT"] = 20)] = "REPORT"
			METHODS2[(METHODS2["MKACTIVITY"] = 21)] = "MKACTIVITY"
			METHODS2[(METHODS2["CHECKOUT"] = 22)] = "CHECKOUT"
			METHODS2[(METHODS2["MERGE"] = 23)] = "MERGE"
			METHODS2[(METHODS2["M-SEARCH"] = 24)] = "M-SEARCH"
			METHODS2[(METHODS2["NOTIFY"] = 25)] = "NOTIFY"
			METHODS2[(METHODS2["SUBSCRIBE"] = 26)] = "SUBSCRIBE"
			METHODS2[(METHODS2["UNSUBSCRIBE"] = 27)] = "UNSUBSCRIBE"
			METHODS2[(METHODS2["PATCH"] = 28)] = "PATCH"
			METHODS2[(METHODS2["PURGE"] = 29)] = "PURGE"
			METHODS2[(METHODS2["MKCALENDAR"] = 30)] = "MKCALENDAR"
			METHODS2[(METHODS2["LINK"] = 31)] = "LINK"
			METHODS2[(METHODS2["UNLINK"] = 32)] = "UNLINK"
			METHODS2[(METHODS2["SOURCE"] = 33)] = "SOURCE"
			METHODS2[(METHODS2["PRI"] = 34)] = "PRI"
			METHODS2[(METHODS2["DESCRIBE"] = 35)] = "DESCRIBE"
			METHODS2[(METHODS2["ANNOUNCE"] = 36)] = "ANNOUNCE"
			METHODS2[(METHODS2["SETUP"] = 37)] = "SETUP"
			METHODS2[(METHODS2["PLAY"] = 38)] = "PLAY"
			METHODS2[(METHODS2["PAUSE"] = 39)] = "PAUSE"
			METHODS2[(METHODS2["TEARDOWN"] = 40)] = "TEARDOWN"
			METHODS2[(METHODS2["GET_PARAMETER"] = 41)] = "GET_PARAMETER"
			METHODS2[(METHODS2["SET_PARAMETER"] = 42)] = "SET_PARAMETER"
			METHODS2[(METHODS2["REDIRECT"] = 43)] = "REDIRECT"
			METHODS2[(METHODS2["RECORD"] = 44)] = "RECORD"
			METHODS2[(METHODS2["FLUSH"] = 45)] = "FLUSH"
		})((METHODS = exports2.METHODS || (exports2.METHODS = {})))
		exports2.METHODS_HTTP = [
			METHODS.DELETE,
			METHODS.GET,
			METHODS.HEAD,
			METHODS.POST,
			METHODS.PUT,
			METHODS.CONNECT,
			METHODS.OPTIONS,
			METHODS.TRACE,
			METHODS.COPY,
			METHODS.LOCK,
			METHODS.MKCOL,
			METHODS.MOVE,
			METHODS.PROPFIND,
			METHODS.PROPPATCH,
			METHODS.SEARCH,
			METHODS.UNLOCK,
			METHODS.BIND,
			METHODS.REBIND,
			METHODS.UNBIND,
			METHODS.ACL,
			METHODS.REPORT,
			METHODS.MKACTIVITY,
			METHODS.CHECKOUT,
			METHODS.MERGE,
			METHODS["M-SEARCH"],
			METHODS.NOTIFY,
			METHODS.SUBSCRIBE,
			METHODS.UNSUBSCRIBE,
			METHODS.PATCH,
			METHODS.PURGE,
			METHODS.MKCALENDAR,
			METHODS.LINK,
			METHODS.UNLINK,
			METHODS.PRI,
			// TODO(indutny): should we allow it with HTTP?
			METHODS.SOURCE,
		]
		exports2.METHODS_ICE = [METHODS.SOURCE]
		exports2.METHODS_RTSP = [
			METHODS.OPTIONS,
			METHODS.DESCRIBE,
			METHODS.ANNOUNCE,
			METHODS.SETUP,
			METHODS.PLAY,
			METHODS.PAUSE,
			METHODS.TEARDOWN,
			METHODS.GET_PARAMETER,
			METHODS.SET_PARAMETER,
			METHODS.REDIRECT,
			METHODS.RECORD,
			METHODS.FLUSH,
			// For AirPlay
			METHODS.GET,
			METHODS.POST,
		]
		exports2.METHOD_MAP = utils_1.enumToMap(METHODS)
		exports2.H_METHOD_MAP = {}
		for (const key of Object.keys(exports2.METHOD_MAP)) {
			if (/^H/.test(key)) {
				exports2.H_METHOD_MAP[key] = exports2.METHOD_MAP[key]
			}
		}
		var FINISH
		;(function (FINISH2) {
			FINISH2[(FINISH2["SAFE"] = 0)] = "SAFE"
			FINISH2[(FINISH2["SAFE_WITH_CB"] = 1)] = "SAFE_WITH_CB"
			FINISH2[(FINISH2["UNSAFE"] = 2)] = "UNSAFE"
		})((FINISH = exports2.FINISH || (exports2.FINISH = {})))
		exports2.ALPHA = []
		for (let i = "A".charCodeAt(0); i <= "Z".charCodeAt(0); i++) {
			exports2.ALPHA.push(String.fromCharCode(i))
			exports2.ALPHA.push(String.fromCharCode(i + 32))
		}
		exports2.NUM_MAP = {
			0: 0,
			1: 1,
			2: 2,
			3: 3,
			4: 4,
			5: 5,
			6: 6,
			7: 7,
			8: 8,
			9: 9,
		}
		exports2.HEX_MAP = {
			0: 0,
			1: 1,
			2: 2,
			3: 3,
			4: 4,
			5: 5,
			6: 6,
			7: 7,
			8: 8,
			9: 9,
			A: 10,
			B: 11,
			C: 12,
			D: 13,
			E: 14,
			F: 15,
			a: 10,
			b: 11,
			c: 12,
			d: 13,
			e: 14,
			f: 15,
		}
		exports2.NUM = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]
		exports2.ALPHANUM = exports2.ALPHA.concat(exports2.NUM)
		exports2.MARK = ["-", "_", ".", "!", "~", "*", "'", "(", ")"]
		exports2.USERINFO_CHARS = [
			...exports2.ALPHANUM.concat(exports2.MARK),
			"%",
			";",
			":",
			"&",
			"=",
			"+",
			"$",
			",",
		]
		exports2.STRICT_URL_CHAR = [
			"!",
			'"',
			"$",
			"%",
			"&",
			"'",
			"(",
			")",
			"*",
			"+",
			",",
			"-",
			".",
			"/",
			":",
			";",
			"<",
			"=",
			">",
			"@",
			"[",
			"\\",
			"]",
			"^",
			"_",
			"`",
			"{",
			"|",
			"}",
			"~",
		].concat(exports2.ALPHANUM)
		exports2.URL_CHAR = [...exports2.STRICT_URL_CHAR, "	", "\f"]
		for (let i = 128; i <= 255; i++) {
			exports2.URL_CHAR.push(i)
		}
		exports2.HEX = [...exports2.NUM, "a", "b", "c", "d", "e", "f", "A", "B", "C", "D", "E", "F"]
		exports2.STRICT_TOKEN = [
			"!",
			"#",
			"$",
			"%",
			"&",
			"'",
			"*",
			"+",
			"-",
			".",
			"^",
			"_",
			"`",
			"|",
			"~",
		].concat(exports2.ALPHANUM)
		exports2.TOKEN = [...exports2.STRICT_TOKEN, " "]
		exports2.HEADER_CHARS = ["	"]
		for (let i = 32; i <= 255; i++) {
			if (i !== 127) {
				exports2.HEADER_CHARS.push(i)
			}
		}
		exports2.CONNECTION_TOKEN_CHARS = exports2.HEADER_CHARS.filter((c) => c !== 44)
		exports2.MAJOR = exports2.NUM_MAP
		exports2.MINOR = exports2.MAJOR
		var HEADER_STATE
		;(function (HEADER_STATE2) {
			HEADER_STATE2[(HEADER_STATE2["GENERAL"] = 0)] = "GENERAL"
			HEADER_STATE2[(HEADER_STATE2["CONNECTION"] = 1)] = "CONNECTION"
			HEADER_STATE2[(HEADER_STATE2["CONTENT_LENGTH"] = 2)] = "CONTENT_LENGTH"
			HEADER_STATE2[(HEADER_STATE2["TRANSFER_ENCODING"] = 3)] = "TRANSFER_ENCODING"
			HEADER_STATE2[(HEADER_STATE2["UPGRADE"] = 4)] = "UPGRADE"
			HEADER_STATE2[(HEADER_STATE2["CONNECTION_KEEP_ALIVE"] = 5)] = "CONNECTION_KEEP_ALIVE"
			HEADER_STATE2[(HEADER_STATE2["CONNECTION_CLOSE"] = 6)] = "CONNECTION_CLOSE"
			HEADER_STATE2[(HEADER_STATE2["CONNECTION_UPGRADE"] = 7)] = "CONNECTION_UPGRADE"
			HEADER_STATE2[(HEADER_STATE2["TRANSFER_ENCODING_CHUNKED"] = 8)] = "TRANSFER_ENCODING_CHUNKED"
		})((HEADER_STATE = exports2.HEADER_STATE || (exports2.HEADER_STATE = {})))
		exports2.SPECIAL_HEADERS = {
			connection: HEADER_STATE.CONNECTION,
			"content-length": HEADER_STATE.CONTENT_LENGTH,
			"proxy-connection": HEADER_STATE.CONNECTION,
			"transfer-encoding": HEADER_STATE.TRANSFER_ENCODING,
			upgrade: HEADER_STATE.UPGRADE,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/handler/RedirectHandler.js
var require_RedirectHandler = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/handler/RedirectHandler.js"(
		exports2,
		module2
	) {
		"use strict"
		var util = require_util()
		var { kBodyUsed } = require_symbols()
		var assert = require("node:assert")
		var { InvalidArgumentError } = require_errors()
		var EE = require("node:events")
		var redirectableStatusCodes = [300, 301, 302, 303, 307, 308]
		var kBody = Symbol("body")
		var BodyAsyncIterable = class {
			constructor(body) {
				this[kBody] = body
				this[kBodyUsed] = false
			}
			async *[Symbol.asyncIterator]() {
				assert(!this[kBodyUsed], "disturbed")
				this[kBodyUsed] = true
				yield* this[kBody]
			}
		}
		var RedirectHandler = class {
			constructor(dispatch, maxRedirections, opts, handler) {
				if (
					maxRedirections != undefined &&
					(!Number.isInteger(maxRedirections) || maxRedirections < 0)
				) {
					throw new InvalidArgumentError("maxRedirections must be a positive number")
				}
				util.validateHandler(handler, opts.method, opts.upgrade)
				this.dispatch = dispatch
				this.location = null
				this.abort = null
				this.opts = { ...opts, maxRedirections: 0 }
				this.maxRedirections = maxRedirections
				this.handler = handler
				this.history = []
				if (util.isStream(this.opts.body)) {
					if (util.bodyLength(this.opts.body) === 0) {
						this.opts.body.on("data", function () {
							assert(false)
						})
					}
					if (typeof this.opts.body.readableDidRead !== "boolean") {
						this.opts.body[kBodyUsed] = false
						EE.prototype.on.call(this.opts.body, "data", function () {
							this[kBodyUsed] = true
						})
					}
				} else if (this.opts.body && typeof this.opts.body.pipeTo === "function") {
					this.opts.body = new BodyAsyncIterable(this.opts.body)
				} else if (
					this.opts.body &&
					typeof this.opts.body !== "string" &&
					!ArrayBuffer.isView(this.opts.body) &&
					util.isIterable(this.opts.body)
				) {
					this.opts.body = new BodyAsyncIterable(this.opts.body)
				}
			}
			onConnect(abort) {
				this.abort = abort
				this.handler.onConnect(abort, { history: this.history })
			}
			onUpgrade(statusCode, headers, socket) {
				this.handler.onUpgrade(statusCode, headers, socket)
			}
			onError(error) {
				this.handler.onError(error)
			}
			onHeaders(statusCode, headers, resume, statusText) {
				this.location =
					this.history.length >= this.maxRedirections || util.isDisturbed(this.opts.body)
						? null
						: parseLocation(statusCode, headers)
				if (this.opts.origin) {
					this.history.push(new URL(this.opts.path, this.opts.origin))
				}
				if (!this.location) {
					return this.handler.onHeaders(statusCode, headers, resume, statusText)
				}
				const { origin, pathname, search } = util.parseURL(
					new URL(this.location, this.opts.origin && new URL(this.opts.path, this.opts.origin))
				)
				const path = search ? `${pathname}${search}` : pathname
				this.opts.headers = cleanRequestHeaders(
					this.opts.headers,
					statusCode === 303,
					this.opts.origin !== origin
				)
				this.opts.path = path
				this.opts.origin = origin
				this.opts.maxRedirections = 0
				this.opts.query = null
				if (statusCode === 303 && this.opts.method !== "HEAD") {
					this.opts.method = "GET"
					this.opts.body = null
				}
			}
			onData(chunk) {
				if (this.location) {
				} else {
					return this.handler.onData(chunk)
				}
			}
			onComplete(trailers) {
				if (this.location) {
					this.location = null
					this.abort = null
					this.dispatch(this.opts, this)
				} else {
					this.handler.onComplete(trailers)
				}
			}
			onBodySent(chunk) {
				if (this.handler.onBodySent) {
					this.handler.onBodySent(chunk)
				}
			}
		}
		function parseLocation(statusCode, headers) {
			if (!redirectableStatusCodes.includes(statusCode)) {
				return null
			}
			for (let i = 0; i < headers.length; i += 2) {
				if (headers[i].toString().toLowerCase() === "location") {
					return headers[i + 1]
				}
			}
		}
		function shouldRemoveHeader(header, removeContent, unknownOrigin) {
			if (header.length === 4) {
				return util.headerNameToString(header) === "host"
			}
			if (removeContent && util.headerNameToString(header).startsWith("content-")) {
				return true
			}
			if (unknownOrigin && (header.length === 13 || header.length === 6 || header.length === 19)) {
				const name = util.headerNameToString(header)
				return name === "authorization" || name === "cookie" || name === "proxy-authorization"
			}
			return false
		}
		function cleanRequestHeaders(headers, removeContent, unknownOrigin) {
			const ret = []
			if (Array.isArray(headers)) {
				for (let i = 0; i < headers.length; i += 2) {
					if (!shouldRemoveHeader(headers[i], removeContent, unknownOrigin)) {
						ret.push(headers[i], headers[i + 1])
					}
				}
			} else if (headers && typeof headers === "object") {
				for (const key of Object.keys(headers)) {
					if (!shouldRemoveHeader(key, removeContent, unknownOrigin)) {
						ret.push(key, headers[key])
					}
				}
			} else {
				assert(headers == undefined, "headers must be an object or an array")
			}
			return ret
		}
		module2.exports = RedirectHandler
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/interceptor/redirectInterceptor.js
var require_redirectInterceptor = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/interceptor/redirectInterceptor.js"(
		exports2,
		module2
	) {
		"use strict"
		var RedirectHandler = require_RedirectHandler()
		function createRedirectInterceptor({ maxRedirections: defaultMaxRedirections }) {
			return (dispatch) => {
				return function Intercept(opts, handler) {
					const { maxRedirections = defaultMaxRedirections } = opts
					if (!maxRedirections) {
						return dispatch(opts, handler)
					}
					const redirectHandler = new RedirectHandler(dispatch, maxRedirections, opts, handler)
					opts = { ...opts, maxRedirections: 0 }
					return dispatch(opts, redirectHandler)
				}
			}
		}
		module2.exports = createRedirectInterceptor
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/llhttp/llhttp-wasm.js
var require_llhttp_wasm = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/llhttp/llhttp-wasm.js"(
		exports2,
		module2
	) {
		module2.exports =
			"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"
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/llhttp/llhttp_simd-wasm.js
var require_llhttp_simd_wasm = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/llhttp/llhttp_simd-wasm.js"(
		exports2,
		module2
	) {
		module2.exports =
			"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"
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/client.js
var require_client = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/client.js"(exports2, module2) {
		"use strict"
		var assert = require("node:assert")
		var net = require("node:net")
		var http = require("node:http")
		var { pipeline } = require("node:stream")
		var util = require_util()
		var timers = require_timers()
		var Request = require_request()
		var DispatcherBase = require_dispatcher_base()
		var {
			RequestContentLengthMismatchError,
			ResponseContentLengthMismatchError,
			InvalidArgumentError,
			RequestAbortedError,
			HeadersTimeoutError,
			HeadersOverflowError,
			SocketError,
			InformationalError,
			BodyTimeoutError,
			HTTPParserError,
			ResponseExceededMaxSizeError,
			ClientDestroyedError,
		} = require_errors()
		var buildConnector = require_connect()
		var {
			kUrl,
			kReset,
			kServerName,
			kClient,
			kBusy,
			kParser,
			kConnect,
			kBlocking,
			kResuming,
			kRunning,
			kPending,
			kSize,
			kWriting,
			kQueue,
			kConnected,
			kConnecting,
			kNeedDrain,
			kNoRef,
			kKeepAliveDefaultTimeout,
			kHostHeader,
			kPendingIdx,
			kRunningIdx,
			kError,
			kPipelining,
			kSocket,
			kKeepAliveTimeoutValue,
			kMaxHeadersSize,
			kKeepAliveMaxTimeout,
			kKeepAliveTimeoutThreshold,
			kHeadersTimeout,
			kBodyTimeout,
			kStrictContentLength,
			kConnector,
			kMaxRedirections,
			kMaxRequests,
			kCounter,
			kClose,
			kDestroy,
			kDispatch,
			kInterceptors,
			kLocalAddress,
			kMaxResponseSize,
			kHTTPConnVersion,
			// HTTP2
			kHost,
			kHTTP2Session,
			kHTTP2SessionState,
			kHTTP2BuildRequest,
			kHTTP2CopyHeaders,
			kHTTP1BuildRequest,
		} = require_symbols()
		var http2
		try {
			http2 = require("node:http2")
		} catch {
			http2 = { constants: {} }
		}
		var {
			constants: {
				HTTP2_HEADER_AUTHORITY,
				HTTP2_HEADER_METHOD,
				HTTP2_HEADER_PATH,
				HTTP2_HEADER_SCHEME,
				HTTP2_HEADER_CONTENT_LENGTH,
				HTTP2_HEADER_EXPECT,
				HTTP2_HEADER_STATUS,
			},
		} = http2
		var h2ExperimentalWarned = false
		var FastBuffer = Buffer[Symbol.species]
		var kClosedResolve = Symbol("kClosedResolve")
		var channels = {}
		try {
			const diagnosticsChannel = require("node:diagnostics_channel")
			channels.sendHeaders = diagnosticsChannel.channel("undici:client:sendHeaders")
			channels.beforeConnect = diagnosticsChannel.channel("undici:client:beforeConnect")
			channels.connectError = diagnosticsChannel.channel("undici:client:connectError")
			channels.connected = diagnosticsChannel.channel("undici:client:connected")
		} catch {
			channels.sendHeaders = { hasSubscribers: false }
			channels.beforeConnect = { hasSubscribers: false }
			channels.connectError = { hasSubscribers: false }
			channels.connected = { hasSubscribers: false }
		}
		var Client = class extends DispatcherBase {
			/**
			 *
			 * @param {string|URL} url
			 * @param {import('../types/client').Client.Options} options
			 */
			constructor(
				url,
				{
					interceptors,
					maxHeaderSize,
					headersTimeout,
					socketTimeout,
					requestTimeout,
					connectTimeout,
					bodyTimeout,
					idleTimeout,
					keepAlive,
					keepAliveTimeout,
					maxKeepAliveTimeout,
					keepAliveMaxTimeout,
					keepAliveTimeoutThreshold,
					socketPath,
					pipelining,
					tls,
					strictContentLength,
					maxCachedSessions,
					maxRedirections,
					connect: connect2,
					maxRequestsPerClient,
					localAddress,
					maxResponseSize,
					autoSelectFamily,
					autoSelectFamilyAttemptTimeout,
					// h2
					allowH2,
					maxConcurrentStreams,
				} = {}
			) {
				super()
				if (keepAlive !== void 0) {
					throw new InvalidArgumentError("unsupported keepAlive, use pipelining=0 instead")
				}
				if (socketTimeout !== void 0) {
					throw new InvalidArgumentError(
						"unsupported socketTimeout, use headersTimeout & bodyTimeout instead"
					)
				}
				if (requestTimeout !== void 0) {
					throw new InvalidArgumentError(
						"unsupported requestTimeout, use headersTimeout & bodyTimeout instead"
					)
				}
				if (idleTimeout !== void 0) {
					throw new InvalidArgumentError("unsupported idleTimeout, use keepAliveTimeout instead")
				}
				if (maxKeepAliveTimeout !== void 0) {
					throw new InvalidArgumentError(
						"unsupported maxKeepAliveTimeout, use keepAliveMaxTimeout instead"
					)
				}
				if (maxHeaderSize != undefined && !Number.isFinite(maxHeaderSize)) {
					throw new InvalidArgumentError("invalid maxHeaderSize")
				}
				if (socketPath != undefined && typeof socketPath !== "string") {
					throw new InvalidArgumentError("invalid socketPath")
				}
				if (
					connectTimeout != undefined &&
					(!Number.isFinite(connectTimeout) || connectTimeout < 0)
				) {
					throw new InvalidArgumentError("invalid connectTimeout")
				}
				if (
					keepAliveTimeout != undefined &&
					(!Number.isFinite(keepAliveTimeout) || keepAliveTimeout <= 0)
				) {
					throw new InvalidArgumentError("invalid keepAliveTimeout")
				}
				if (
					keepAliveMaxTimeout != undefined &&
					(!Number.isFinite(keepAliveMaxTimeout) || keepAliveMaxTimeout <= 0)
				) {
					throw new InvalidArgumentError("invalid keepAliveMaxTimeout")
				}
				if (keepAliveTimeoutThreshold != undefined && !Number.isFinite(keepAliveTimeoutThreshold)) {
					throw new InvalidArgumentError("invalid keepAliveTimeoutThreshold")
				}
				if (
					headersTimeout != undefined &&
					(!Number.isInteger(headersTimeout) || headersTimeout < 0)
				) {
					throw new InvalidArgumentError("headersTimeout must be a positive integer or zero")
				}
				if (bodyTimeout != undefined && (!Number.isInteger(bodyTimeout) || bodyTimeout < 0)) {
					throw new InvalidArgumentError("bodyTimeout must be a positive integer or zero")
				}
				if (
					connect2 != undefined &&
					typeof connect2 !== "function" &&
					typeof connect2 !== "object"
				) {
					throw new InvalidArgumentError("connect must be a function or an object")
				}
				if (
					maxRedirections != undefined &&
					(!Number.isInteger(maxRedirections) || maxRedirections < 0)
				) {
					throw new InvalidArgumentError("maxRedirections must be a positive number")
				}
				if (
					maxRequestsPerClient != undefined &&
					(!Number.isInteger(maxRequestsPerClient) || maxRequestsPerClient < 0)
				) {
					throw new InvalidArgumentError("maxRequestsPerClient must be a positive number")
				}
				if (
					localAddress != undefined &&
					(typeof localAddress !== "string" || net.isIP(localAddress) === 0)
				) {
					throw new InvalidArgumentError("localAddress must be valid string IP address")
				}
				if (
					maxResponseSize != undefined &&
					(!Number.isInteger(maxResponseSize) || maxResponseSize < -1)
				) {
					throw new InvalidArgumentError("maxResponseSize must be a positive number")
				}
				if (
					autoSelectFamilyAttemptTimeout != undefined &&
					(!Number.isInteger(autoSelectFamilyAttemptTimeout) || autoSelectFamilyAttemptTimeout < -1)
				) {
					throw new InvalidArgumentError("autoSelectFamilyAttemptTimeout must be a positive number")
				}
				if (allowH2 != undefined && typeof allowH2 !== "boolean") {
					throw new InvalidArgumentError("allowH2 must be a valid boolean value")
				}
				if (
					maxConcurrentStreams != undefined &&
					(typeof maxConcurrentStreams !== "number" || maxConcurrentStreams < 1)
				) {
					throw new InvalidArgumentError(
						"maxConcurrentStreams must be a possitive integer, greater than 0"
					)
				}
				if (typeof connect2 !== "function") {
					connect2 = buildConnector({
						...tls,
						maxCachedSessions,
						allowH2,
						socketPath,
						timeout: connectTimeout,
						...(util.nodeHasAutoSelectFamily && autoSelectFamily
							? { autoSelectFamily, autoSelectFamilyAttemptTimeout }
							: void 0),
						...connect2,
					})
				}
				this[kInterceptors] =
					interceptors && interceptors.Client && Array.isArray(interceptors.Client)
						? interceptors.Client
						: [createRedirectInterceptor({ maxRedirections })]
				this[kUrl] = util.parseOrigin(url)
				this[kConnector] = connect2
				this[kSocket] = null
				this[kPipelining] = pipelining != undefined ? pipelining : 1
				this[kMaxHeadersSize] = maxHeaderSize || http.maxHeaderSize
				this[kKeepAliveDefaultTimeout] = keepAliveTimeout == undefined ? 4e3 : keepAliveTimeout
				this[kKeepAliveMaxTimeout] = keepAliveMaxTimeout == undefined ? 6e5 : keepAliveMaxTimeout
				this[kKeepAliveTimeoutThreshold] =
					keepAliveTimeoutThreshold == undefined ? 1e3 : keepAliveTimeoutThreshold
				this[kKeepAliveTimeoutValue] = this[kKeepAliveDefaultTimeout]
				this[kServerName] = null
				this[kLocalAddress] = localAddress != undefined ? localAddress : null
				this[kResuming] = 0
				this[kNeedDrain] = 0
				this[kHostHeader] = `host: ${this[kUrl].hostname}${
					this[kUrl].port ? `:${this[kUrl].port}` : ""
				}\r
`
				this[kBodyTimeout] = bodyTimeout != undefined ? bodyTimeout : 3e5
				this[kHeadersTimeout] = headersTimeout != undefined ? headersTimeout : 3e5
				this[kStrictContentLength] = strictContentLength == undefined ? true : strictContentLength
				this[kMaxRedirections] = maxRedirections
				this[kMaxRequests] = maxRequestsPerClient
				this[kClosedResolve] = null
				this[kMaxResponseSize] = maxResponseSize > -1 ? maxResponseSize : -1
				this[kHTTPConnVersion] = "h1"
				this[kHTTP2Session] = null
				this[kHTTP2SessionState] = !allowH2
					? null
					: {
							// streams: null, // Fixed queue of streams - For future support of `push`
							openStreams: 0,
							// Keep track of them to decide wether or not unref the session
							maxConcurrentStreams: maxConcurrentStreams != undefined ? maxConcurrentStreams : 100,
							// Max peerConcurrentStreams for a Node h2 server
					  }
				this[kHost] = `${this[kUrl].hostname}${this[kUrl].port ? `:${this[kUrl].port}` : ""}`
				this[kQueue] = []
				this[kRunningIdx] = 0
				this[kPendingIdx] = 0
			}
			get pipelining() {
				return this[kPipelining]
			}
			set pipelining(value) {
				this[kPipelining] = value
				resume(this, true)
			}
			get [kPending]() {
				return this[kQueue].length - this[kPendingIdx]
			}
			get [kRunning]() {
				return this[kPendingIdx] - this[kRunningIdx]
			}
			get [kSize]() {
				return this[kQueue].length - this[kRunningIdx]
			}
			get [kConnected]() {
				return !!this[kSocket] && !this[kConnecting] && !this[kSocket].destroyed
			}
			get [kBusy]() {
				const socket = this[kSocket]
				return (
					(socket && (socket[kReset] || socket[kWriting] || socket[kBlocking])) ||
					this[kSize] >= (this[kPipelining] || 1) ||
					this[kPending] > 0
				)
			}
			/* istanbul ignore: only used for test */
			[kConnect](cb) {
				connect(this)
				this.once("connect", cb)
			}
			[kDispatch](opts, handler) {
				const origin = opts.origin || this[kUrl].origin
				const request =
					this[kHTTPConnVersion] === "h2"
						? Request[kHTTP2BuildRequest](origin, opts, handler)
						: Request[kHTTP1BuildRequest](origin, opts, handler)
				this[kQueue].push(request)
				if (this[kResuming]) {
				} else if (util.bodyLength(request.body) == undefined && util.isIterable(request.body)) {
					this[kResuming] = 1
					process.nextTick(resume, this)
				} else {
					resume(this, true)
				}
				if (this[kResuming] && this[kNeedDrain] !== 2 && this[kBusy]) {
					this[kNeedDrain] = 2
				}
				return this[kNeedDrain] < 2
			}
			async [kClose]() {
				return new Promise((resolve) => {
					if (!this[kSize]) {
						resolve(null)
					} else {
						this[kClosedResolve] = resolve
					}
				})
			}
			async [kDestroy](err) {
				return new Promise((resolve) => {
					const requests = this[kQueue].splice(this[kPendingIdx])
					for (const request of requests) {
						errorRequest(this, request, err)
					}
					const callback = () => {
						if (this[kClosedResolve]) {
							this[kClosedResolve]()
							this[kClosedResolve] = null
						}
						resolve()
					}
					if (this[kHTTP2Session] != undefined) {
						util.destroy(this[kHTTP2Session], err)
						this[kHTTP2Session] = null
						this[kHTTP2SessionState] = null
					}
					if (!this[kSocket]) {
						queueMicrotask(callback)
					} else {
						util.destroy(this[kSocket].on("close", callback), err)
					}
					resume(this)
				})
			}
		}
		function onHttp2SessionError(err) {
			assert(err.code !== "ERR_TLS_CERT_ALTNAME_INVALID")
			this[kSocket][kError] = err
			onError(this[kClient], err)
		}
		function onHttp2FrameError(type, code, id) {
			const err = new InformationalError(
				`HTTP/2: "frameError" received - type ${type}, code ${code}`
			)
			if (id === 0) {
				this[kSocket][kError] = err
				onError(this[kClient], err)
			}
		}
		function onHttp2SessionEnd() {
			util.destroy(this, new SocketError("other side closed"))
			util.destroy(this[kSocket], new SocketError("other side closed"))
		}
		function onHTTP2GoAway(code) {
			const client = this[kClient]
			const err = new InformationalError(`HTTP/2: "GOAWAY" frame received with code ${code}`)
			client[kSocket] = null
			client[kHTTP2Session] = null
			if (client.destroyed) {
				assert(this[kPending] === 0)
				const requests = client[kQueue].splice(client[kRunningIdx])
				for (const request of requests) {
					errorRequest(this, request, err)
				}
			} else if (client[kRunning] > 0) {
				const request = client[kQueue][client[kRunningIdx]]
				client[kQueue][client[kRunningIdx]++] = null
				errorRequest(client, request, err)
			}
			client[kPendingIdx] = client[kRunningIdx]
			assert(client[kRunning] === 0)
			client.emit("disconnect", client[kUrl], [client], err)
			resume(client)
		}
		var constants = require_constants3()
		var createRedirectInterceptor = require_redirectInterceptor()
		var EMPTY_BUF = Buffer.alloc(0)
		async function lazyllhttp() {
			const llhttpWasmData = process.env.JEST_WORKER_ID ? require_llhttp_wasm() : void 0
			let mod
			try {
				mod = await WebAssembly.compile(Buffer.from(require_llhttp_simd_wasm(), "base64"))
			} catch (e) {
				mod = await WebAssembly.compile(
					Buffer.from(llhttpWasmData || require_llhttp_wasm(), "base64")
				)
			}
			return await WebAssembly.instantiate(mod, {
				env: {
					/* eslint-disable camelcase */
					wasm_on_url: (p, at, len) => {
						return 0
					},
					wasm_on_status: (p, at, len) => {
						assert.strictEqual(currentParser.ptr, p)
						const start = at - currentBufferPtr + currentBufferRef.byteOffset
						return currentParser.onStatus(new FastBuffer(currentBufferRef.buffer, start, len)) || 0
					},
					wasm_on_message_begin: (p) => {
						assert.strictEqual(currentParser.ptr, p)
						return currentParser.onMessageBegin() || 0
					},
					wasm_on_header_field: (p, at, len) => {
						assert.strictEqual(currentParser.ptr, p)
						const start = at - currentBufferPtr + currentBufferRef.byteOffset
						return (
							currentParser.onHeaderField(new FastBuffer(currentBufferRef.buffer, start, len)) || 0
						)
					},
					wasm_on_header_value: (p, at, len) => {
						assert.strictEqual(currentParser.ptr, p)
						const start = at - currentBufferPtr + currentBufferRef.byteOffset
						return (
							currentParser.onHeaderValue(new FastBuffer(currentBufferRef.buffer, start, len)) || 0
						)
					},
					wasm_on_headers_complete: (p, statusCode, upgrade, shouldKeepAlive) => {
						assert.strictEqual(currentParser.ptr, p)
						return (
							currentParser.onHeadersComplete(
								statusCode,
								Boolean(upgrade),
								Boolean(shouldKeepAlive)
							) || 0
						)
					},
					wasm_on_body: (p, at, len) => {
						assert.strictEqual(currentParser.ptr, p)
						const start = at - currentBufferPtr + currentBufferRef.byteOffset
						return currentParser.onBody(new FastBuffer(currentBufferRef.buffer, start, len)) || 0
					},
					wasm_on_message_complete: (p) => {
						assert.strictEqual(currentParser.ptr, p)
						return currentParser.onMessageComplete() || 0
					},
					/* eslint-enable camelcase */
				},
			})
		}
		var llhttpInstance = null
		var llhttpPromise = lazyllhttp()
		llhttpPromise.catch()
		var currentParser = null
		var currentBufferRef = null
		var currentBufferSize = 0
		var currentBufferPtr = null
		var TIMEOUT_HEADERS = 1
		var TIMEOUT_BODY = 2
		var TIMEOUT_IDLE = 3
		var Parser = class {
			constructor(client, socket, { exports: exports3 }) {
				assert(Number.isFinite(client[kMaxHeadersSize]) && client[kMaxHeadersSize] > 0)
				this.llhttp = exports3
				this.ptr = this.llhttp.llhttp_alloc(constants.TYPE.RESPONSE)
				this.client = client
				this.socket = socket
				this.timeout = null
				this.timeoutValue = null
				this.timeoutType = null
				this.statusCode = null
				this.statusText = ""
				this.upgrade = false
				this.headers = []
				this.headersSize = 0
				this.headersMaxSize = client[kMaxHeadersSize]
				this.shouldKeepAlive = false
				this.paused = false
				this.resume = this.resume.bind(this)
				this.bytesRead = 0
				this.keepAlive = ""
				this.contentLength = ""
				this.connection = ""
				this.maxResponseSize = client[kMaxResponseSize]
			}
			setTimeout(value, type) {
				this.timeoutType = type
				if (value !== this.timeoutValue) {
					timers.clearTimeout(this.timeout)
					if (value) {
						this.timeout = timers.setTimeout(onParserTimeout, value, this)
						if (this.timeout.unref) {
							this.timeout.unref()
						}
					} else {
						this.timeout = null
					}
					this.timeoutValue = value
				} else if (this.timeout && this.timeout.refresh) {
					this.timeout.refresh()
				}
			}
			resume() {
				if (this.socket.destroyed || !this.paused) {
					return
				}
				assert(this.ptr != undefined)
				assert(currentParser == undefined)
				this.llhttp.llhttp_resume(this.ptr)
				assert(this.timeoutType === TIMEOUT_BODY)
				if (this.timeout && this.timeout.refresh) {
					this.timeout.refresh()
				}
				this.paused = false
				this.execute(this.socket.read() || EMPTY_BUF)
				this.readMore()
			}
			readMore() {
				while (!this.paused && this.ptr) {
					const chunk = this.socket.read()
					if (chunk === null) {
						break
					}
					this.execute(chunk)
				}
			}
			execute(data) {
				assert(this.ptr != undefined)
				assert(currentParser == undefined)
				assert(!this.paused)
				const { socket, llhttp } = this
				if (data.length > currentBufferSize) {
					if (currentBufferPtr) {
						llhttp.free(currentBufferPtr)
					}
					currentBufferSize = Math.ceil(data.length / 4096) * 4096
					currentBufferPtr = llhttp.malloc(currentBufferSize)
				}
				new Uint8Array(llhttp.memory.buffer, currentBufferPtr, currentBufferSize).set(data)
				try {
					let ret
					try {
						currentBufferRef = data
						currentParser = this
						ret = llhttp.llhttp_execute(this.ptr, currentBufferPtr, data.length)
					} catch (err) {
						throw err
					} finally {
						currentParser = null
						currentBufferRef = null
					}
					const offset = llhttp.llhttp_get_error_pos(this.ptr) - currentBufferPtr
					if (ret === constants.ERROR.PAUSED_UPGRADE) {
						this.onUpgrade(data.slice(offset))
					} else if (ret === constants.ERROR.PAUSED) {
						this.paused = true
						socket.unshift(data.slice(offset))
					} else if (ret !== constants.ERROR.OK) {
						const ptr = llhttp.llhttp_get_error_reason(this.ptr)
						let message = ""
						if (ptr) {
							const len = new Uint8Array(llhttp.memory.buffer, ptr).indexOf(0)
							message =
								"Response does not match the HTTP/1.1 protocol (" +
								Buffer.from(llhttp.memory.buffer, ptr, len).toString() +
								")"
						}
						throw new HTTPParserError(message, constants.ERROR[ret], data.slice(offset))
					}
				} catch (err) {
					util.destroy(socket, err)
				}
			}
			destroy() {
				assert(this.ptr != undefined)
				assert(currentParser == undefined)
				this.llhttp.llhttp_free(this.ptr)
				this.ptr = null
				timers.clearTimeout(this.timeout)
				this.timeout = null
				this.timeoutValue = null
				this.timeoutType = null
				this.paused = false
			}
			onStatus(buf) {
				this.statusText = buf.toString()
			}
			onMessageBegin() {
				const { socket, client } = this
				if (socket.destroyed) {
					return -1
				}
				const request = client[kQueue][client[kRunningIdx]]
				if (!request) {
					return -1
				}
			}
			onHeaderField(buf) {
				const len = this.headers.length
				if ((len & 1) === 0) {
					this.headers.push(buf)
				} else {
					this.headers[len - 1] = Buffer.concat([this.headers[len - 1], buf])
				}
				this.trackHeader(buf.length)
			}
			onHeaderValue(buf) {
				let len = this.headers.length
				if ((len & 1) === 1) {
					this.headers.push(buf)
					len += 1
				} else {
					this.headers[len - 1] = Buffer.concat([this.headers[len - 1], buf])
				}
				const key = this.headers[len - 2]
				if (key.length === 10 && key.toString().toLowerCase() === "keep-alive") {
					this.keepAlive += buf.toString()
				} else if (key.length === 10 && key.toString().toLowerCase() === "connection") {
					this.connection += buf.toString()
				} else if (key.length === 14 && key.toString().toLowerCase() === "content-length") {
					this.contentLength += buf.toString()
				}
				this.trackHeader(buf.length)
			}
			trackHeader(len) {
				this.headersSize += len
				if (this.headersSize >= this.headersMaxSize) {
					util.destroy(this.socket, new HeadersOverflowError())
				}
			}
			onUpgrade(head) {
				const { upgrade, client, socket, headers, statusCode } = this
				assert(upgrade)
				const request = client[kQueue][client[kRunningIdx]]
				assert(request)
				assert(!socket.destroyed)
				assert(socket === client[kSocket])
				assert(!this.paused)
				assert(request.upgrade || request.method === "CONNECT")
				this.statusCode = null
				this.statusText = ""
				this.shouldKeepAlive = null
				assert(this.headers.length % 2 === 0)
				this.headers = []
				this.headersSize = 0
				socket.unshift(head)
				socket[kParser].destroy()
				socket[kParser] = null
				socket[kClient] = null
				socket[kError] = null
				socket
					.removeListener("error", onSocketError)
					.removeListener("readable", onSocketReadable)
					.removeListener("end", onSocketEnd)
					.removeListener("close", onSocketClose)
				client[kSocket] = null
				client[kQueue][client[kRunningIdx]++] = null
				client.emit("disconnect", client[kUrl], [client], new InformationalError("upgrade"))
				try {
					request.onUpgrade(statusCode, headers, socket)
				} catch (err) {
					util.destroy(socket, err)
				}
				resume(client)
			}
			onHeadersComplete(statusCode, upgrade, shouldKeepAlive) {
				const { client, socket, headers, statusText } = this
				if (socket.destroyed) {
					return -1
				}
				const request = client[kQueue][client[kRunningIdx]]
				if (!request) {
					return -1
				}
				assert(!this.upgrade)
				assert(this.statusCode < 200)
				if (statusCode === 100) {
					util.destroy(socket, new SocketError("bad response", util.getSocketInfo(socket)))
					return -1
				}
				if (upgrade && !request.upgrade) {
					util.destroy(socket, new SocketError("bad upgrade", util.getSocketInfo(socket)))
					return -1
				}
				assert.strictEqual(this.timeoutType, TIMEOUT_HEADERS)
				this.statusCode = statusCode
				this.shouldKeepAlive =
					shouldKeepAlive || // Override llhttp value which does not allow keepAlive for HEAD.
					(request.method === "HEAD" &&
						!socket[kReset] &&
						this.connection.toLowerCase() === "keep-alive")
				if (this.statusCode >= 200) {
					const bodyTimeout =
						request.bodyTimeout != undefined ? request.bodyTimeout : client[kBodyTimeout]
					this.setTimeout(bodyTimeout, TIMEOUT_BODY)
				} else if (this.timeout && this.timeout.refresh) {
					this.timeout.refresh()
				}
				if (request.method === "CONNECT") {
					assert(client[kRunning] === 1)
					this.upgrade = true
					return 2
				}
				if (upgrade) {
					assert(client[kRunning] === 1)
					this.upgrade = true
					return 2
				}
				assert(this.headers.length % 2 === 0)
				this.headers = []
				this.headersSize = 0
				if (this.shouldKeepAlive && client[kPipelining]) {
					const keepAliveTimeout = this.keepAlive
						? util.parseKeepAliveTimeout(this.keepAlive)
						: null
					if (keepAliveTimeout != undefined) {
						const timeout = Math.min(
							keepAliveTimeout - client[kKeepAliveTimeoutThreshold],
							client[kKeepAliveMaxTimeout]
						)
						if (timeout <= 0) {
							socket[kReset] = true
						} else {
							client[kKeepAliveTimeoutValue] = timeout
						}
					} else {
						client[kKeepAliveTimeoutValue] = client[kKeepAliveDefaultTimeout]
					}
				} else {
					socket[kReset] = true
				}
				const pause = request.onHeaders(statusCode, headers, this.resume, statusText) === false
				if (request.aborted) {
					return -1
				}
				if (request.method === "HEAD") {
					return 1
				}
				if (statusCode < 200) {
					return 1
				}
				if (socket[kBlocking]) {
					socket[kBlocking] = false
					resume(client)
				}
				return pause ? constants.ERROR.PAUSED : 0
			}
			onBody(buf) {
				const { client, socket, statusCode, maxResponseSize } = this
				if (socket.destroyed) {
					return -1
				}
				const request = client[kQueue][client[kRunningIdx]]
				assert(request)
				assert.strictEqual(this.timeoutType, TIMEOUT_BODY)
				if (this.timeout && this.timeout.refresh) {
					this.timeout.refresh()
				}
				assert(statusCode >= 200)
				if (maxResponseSize > -1 && this.bytesRead + buf.length > maxResponseSize) {
					util.destroy(socket, new ResponseExceededMaxSizeError())
					return -1
				}
				this.bytesRead += buf.length
				if (request.onData(buf) === false) {
					return constants.ERROR.PAUSED
				}
			}
			onMessageComplete() {
				const {
					client,
					socket,
					statusCode,
					upgrade,
					headers,
					contentLength,
					bytesRead,
					shouldKeepAlive,
				} = this
				if (socket.destroyed && (!statusCode || shouldKeepAlive)) {
					return -1
				}
				if (upgrade) {
					return
				}
				const request = client[kQueue][client[kRunningIdx]]
				assert(request)
				assert(statusCode >= 100)
				this.statusCode = null
				this.statusText = ""
				this.bytesRead = 0
				this.contentLength = ""
				this.keepAlive = ""
				this.connection = ""
				assert(this.headers.length % 2 === 0)
				this.headers = []
				this.headersSize = 0
				if (statusCode < 200) {
					return
				}
				if (
					request.method !== "HEAD" &&
					contentLength &&
					bytesRead !== parseInt(contentLength, 10)
				) {
					util.destroy(socket, new ResponseContentLengthMismatchError())
					return -1
				}
				request.onComplete(headers)
				client[kQueue][client[kRunningIdx]++] = null
				if (socket[kWriting]) {
					assert.strictEqual(client[kRunning], 0)
					util.destroy(socket, new InformationalError("reset"))
					return constants.ERROR.PAUSED
				} else if (!shouldKeepAlive) {
					util.destroy(socket, new InformationalError("reset"))
					return constants.ERROR.PAUSED
				} else if (socket[kReset] && client[kRunning] === 0) {
					util.destroy(socket, new InformationalError("reset"))
					return constants.ERROR.PAUSED
				} else if (client[kPipelining] === 1) {
					setImmediate(resume, client)
				} else {
					resume(client)
				}
			}
		}
		function onParserTimeout(parser) {
			const { socket, timeoutType, client } = parser
			if (timeoutType === TIMEOUT_HEADERS) {
				if (!socket[kWriting] || socket.writableNeedDrain || client[kRunning] > 1) {
					assert(!parser.paused, "cannot be paused while waiting for headers")
					util.destroy(socket, new HeadersTimeoutError())
				}
			} else if (timeoutType === TIMEOUT_BODY) {
				if (!parser.paused) {
					util.destroy(socket, new BodyTimeoutError())
				}
			} else if (timeoutType === TIMEOUT_IDLE) {
				assert(client[kRunning] === 0 && client[kKeepAliveTimeoutValue])
				util.destroy(socket, new InformationalError("socket idle timeout"))
			}
		}
		function onSocketReadable() {
			const { [kParser]: parser } = this
			if (parser) {
				parser.readMore()
			}
		}
		function onSocketError(err) {
			const { [kClient]: client, [kParser]: parser } = this
			assert(err.code !== "ERR_TLS_CERT_ALTNAME_INVALID")
			if (
				client[kHTTPConnVersion] !== "h2" &&
				err.code === "ECONNRESET" &&
				parser.statusCode &&
				!parser.shouldKeepAlive
			) {
				parser.onMessageComplete()
				return
			}
			this[kError] = err
			onError(this[kClient], err)
		}
		function onError(client, err) {
			if (client[kRunning] === 0 && err.code !== "UND_ERR_INFO" && err.code !== "UND_ERR_SOCKET") {
				assert(client[kPendingIdx] === client[kRunningIdx])
				const requests = client[kQueue].splice(client[kRunningIdx])
				for (const request of requests) {
					errorRequest(client, request, err)
				}
				assert(client[kSize] === 0)
			}
		}
		function onSocketEnd() {
			const { [kParser]: parser, [kClient]: client } = this
			if (client[kHTTPConnVersion] !== "h2" && parser.statusCode && !parser.shouldKeepAlive) {
				parser.onMessageComplete()
				return
			}
			util.destroy(this, new SocketError("other side closed", util.getSocketInfo(this)))
		}
		function onSocketClose() {
			const { [kClient]: client, [kParser]: parser } = this
			if (client[kHTTPConnVersion] === "h1" && parser) {
				if (!this[kError] && parser.statusCode && !parser.shouldKeepAlive) {
					parser.onMessageComplete()
				}
				this[kParser].destroy()
				this[kParser] = null
			}
			const err = this[kError] || new SocketError("closed", util.getSocketInfo(this))
			client[kSocket] = null
			if (client.destroyed) {
				assert(client[kPending] === 0)
				const requests = client[kQueue].splice(client[kRunningIdx])
				for (const request of requests) {
					errorRequest(client, request, err)
				}
			} else if (client[kRunning] > 0 && err.code !== "UND_ERR_INFO") {
				const request = client[kQueue][client[kRunningIdx]]
				client[kQueue][client[kRunningIdx]++] = null
				errorRequest(client, request, err)
			}
			client[kPendingIdx] = client[kRunningIdx]
			assert(client[kRunning] === 0)
			client.emit("disconnect", client[kUrl], [client], err)
			resume(client)
		}
		async function connect(client) {
			assert(!client[kConnecting])
			assert(!client[kSocket])
			let { host, hostname, protocol, port } = client[kUrl]
			if (hostname[0] === "[") {
				const idx = hostname.indexOf("]")
				assert(idx !== -1)
				const ip = hostname.substring(1, idx)
				assert(net.isIP(ip))
				hostname = ip
			}
			client[kConnecting] = true
			if (channels.beforeConnect.hasSubscribers) {
				channels.beforeConnect.publish({
					connectParams: {
						host,
						hostname,
						protocol,
						port,
						servername: client[kServerName],
						localAddress: client[kLocalAddress],
					},
					connector: client[kConnector],
				})
			}
			try {
				const socket = await new Promise((resolve, reject) => {
					client[kConnector](
						{
							host,
							hostname,
							protocol,
							port,
							servername: client[kServerName],
							localAddress: client[kLocalAddress],
						},
						(err, socket2) => {
							if (err) {
								reject(err)
							} else {
								resolve(socket2)
							}
						}
					)
				})
				if (client.destroyed) {
					util.destroy(
						socket.on("error", () => {}),
						new ClientDestroyedError()
					)
					return
				}
				client[kConnecting] = false
				assert(socket)
				const isH2 = socket.alpnProtocol === "h2"
				if (isH2) {
					if (!h2ExperimentalWarned) {
						h2ExperimentalWarned = true
						process.emitWarning("H2 support is experimental, expect them to change at any time.", {
							code: "UNDICI-H2",
						})
					}
					const session = http2.connect(client[kUrl], {
						createConnection: () => socket,
						peerMaxConcurrentStreams: client[kHTTP2SessionState].maxConcurrentStreams,
					})
					client[kHTTPConnVersion] = "h2"
					session[kClient] = client
					session[kSocket] = socket
					session.on("error", onHttp2SessionError)
					session.on("frameError", onHttp2FrameError)
					session.on("end", onHttp2SessionEnd)
					session.on("goaway", onHTTP2GoAway)
					session.on("close", onSocketClose)
					session.unref()
					client[kHTTP2Session] = session
					socket[kHTTP2Session] = session
				} else {
					if (!llhttpInstance) {
						llhttpInstance = await llhttpPromise
						llhttpPromise = null
					}
					socket[kNoRef] = false
					socket[kWriting] = false
					socket[kReset] = false
					socket[kBlocking] = false
					socket[kParser] = new Parser(client, socket, llhttpInstance)
				}
				socket[kCounter] = 0
				socket[kMaxRequests] = client[kMaxRequests]
				socket[kClient] = client
				socket[kError] = null
				socket
					.on("error", onSocketError)
					.on("readable", onSocketReadable)
					.on("end", onSocketEnd)
					.on("close", onSocketClose)
				client[kSocket] = socket
				if (channels.connected.hasSubscribers) {
					channels.connected.publish({
						connectParams: {
							host,
							hostname,
							protocol,
							port,
							servername: client[kServerName],
							localAddress: client[kLocalAddress],
						},
						connector: client[kConnector],
						socket,
					})
				}
				client.emit("connect", client[kUrl], [client])
			} catch (err) {
				if (client.destroyed) {
					return
				}
				client[kConnecting] = false
				if (channels.connectError.hasSubscribers) {
					channels.connectError.publish({
						connectParams: {
							host,
							hostname,
							protocol,
							port,
							servername: client[kServerName],
							localAddress: client[kLocalAddress],
						},
						connector: client[kConnector],
						error: err,
					})
				}
				if (err.code === "ERR_TLS_CERT_ALTNAME_INVALID") {
					assert(client[kRunning] === 0)
					while (
						client[kPending] > 0 &&
						client[kQueue][client[kPendingIdx]].servername === client[kServerName]
					) {
						const request = client[kQueue][client[kPendingIdx]++]
						errorRequest(client, request, err)
					}
				} else {
					onError(client, err)
				}
				client.emit("connectionError", client[kUrl], [client], err)
			}
			resume(client)
		}
		function emitDrain(client) {
			client[kNeedDrain] = 0
			client.emit("drain", client[kUrl], [client])
		}
		function resume(client, sync) {
			if (client[kResuming] === 2) {
				return
			}
			client[kResuming] = 2
			_resume(client, sync)
			client[kResuming] = 0
			if (client[kRunningIdx] > 256) {
				client[kQueue].splice(0, client[kRunningIdx])
				client[kPendingIdx] -= client[kRunningIdx]
				client[kRunningIdx] = 0
			}
		}
		function _resume(client, sync) {
			while (true) {
				if (client.destroyed) {
					assert(client[kPending] === 0)
					return
				}
				if (client[kClosedResolve] && !client[kSize]) {
					client[kClosedResolve]()
					client[kClosedResolve] = null
					return
				}
				const socket = client[kSocket]
				if (socket && !socket.destroyed && socket.alpnProtocol !== "h2") {
					if (client[kSize] === 0) {
						if (!socket[kNoRef] && socket.unref) {
							socket.unref()
							socket[kNoRef] = true
						}
					} else if (socket[kNoRef] && socket.ref) {
						socket.ref()
						socket[kNoRef] = false
					}
					if (client[kSize] === 0) {
						if (socket[kParser].timeoutType !== TIMEOUT_IDLE) {
							socket[kParser].setTimeout(client[kKeepAliveTimeoutValue], TIMEOUT_IDLE)
						}
					} else if (
						client[kRunning] > 0 &&
						socket[kParser].statusCode < 200 &&
						socket[kParser].timeoutType !== TIMEOUT_HEADERS
					) {
						const request2 = client[kQueue][client[kRunningIdx]]
						const headersTimeout =
							request2.headersTimeout != undefined
								? request2.headersTimeout
								: client[kHeadersTimeout]
						socket[kParser].setTimeout(headersTimeout, TIMEOUT_HEADERS)
					}
				}
				if (client[kBusy]) {
					client[kNeedDrain] = 2
				} else if (client[kNeedDrain] === 2) {
					if (sync) {
						client[kNeedDrain] = 1
						process.nextTick(emitDrain, client)
					} else {
						emitDrain(client)
					}
					continue
				}
				if (client[kPending] === 0) {
					return
				}
				if (client[kRunning] >= (client[kPipelining] || 1)) {
					return
				}
				const request = client[kQueue][client[kPendingIdx]]
				if (client[kUrl].protocol === "https:" && client[kServerName] !== request.servername) {
					if (client[kRunning] > 0) {
						return
					}
					client[kServerName] = request.servername
					if (socket && socket.servername !== request.servername) {
						util.destroy(socket, new InformationalError("servername changed"))
						return
					}
				}
				if (client[kConnecting]) {
					return
				}
				if (!socket && !client[kHTTP2Session]) {
					connect(client)
					return
				}
				if (socket.destroyed || socket[kWriting] || socket[kReset] || socket[kBlocking]) {
					return
				}
				if (client[kRunning] > 0 && !request.idempotent) {
					return
				}
				if (client[kRunning] > 0 && (request.upgrade || request.method === "CONNECT")) {
					return
				}
				if (
					client[kRunning] > 0 &&
					util.bodyLength(request.body) !== 0 &&
					(util.isStream(request.body) || util.isAsyncIterable(request.body))
				) {
					return
				}
				if (!request.aborted && write(client, request)) {
					client[kPendingIdx]++
				} else {
					client[kQueue].splice(client[kPendingIdx], 1)
				}
			}
		}
		function shouldSendContentLength(method) {
			return (
				method !== "GET" &&
				method !== "HEAD" &&
				method !== "OPTIONS" &&
				method !== "TRACE" &&
				method !== "CONNECT"
			)
		}
		function write(client, request) {
			if (client[kHTTPConnVersion] === "h2") {
				writeH2(client, client[kHTTP2Session], request)
				return
			}
			const { body, method, path, host, upgrade, headers, blocking, reset: reset2 } = request
			const expectsPayload = method === "PUT" || method === "POST" || method === "PATCH"
			if (body && typeof body.read === "function") {
				body.read(0)
			}
			const bodyLength = util.bodyLength(body)
			let contentLength = bodyLength
			if (contentLength === null) {
				contentLength = request.contentLength
			}
			if (contentLength === 0 && !expectsPayload) {
				contentLength = null
			}
			if (
				shouldSendContentLength(method) &&
				contentLength > 0 &&
				request.contentLength !== null &&
				request.contentLength !== contentLength
			) {
				if (client[kStrictContentLength]) {
					errorRequest(client, request, new RequestContentLengthMismatchError())
					return false
				}
				process.emitWarning(new RequestContentLengthMismatchError())
			}
			const socket = client[kSocket]
			try {
				request.onConnect((err) => {
					if (request.aborted || request.completed) {
						return
					}
					errorRequest(client, request, err || new RequestAbortedError())
					util.destroy(socket, new InformationalError("aborted"))
				})
			} catch (err) {
				errorRequest(client, request, err)
			}
			if (request.aborted) {
				return false
			}
			if (method === "HEAD") {
				socket[kReset] = true
			}
			if (upgrade || method === "CONNECT") {
				socket[kReset] = true
			}
			if (reset2 != undefined) {
				socket[kReset] = reset2
			}
			if (client[kMaxRequests] && socket[kCounter]++ >= client[kMaxRequests]) {
				socket[kReset] = true
			}
			if (blocking) {
				socket[kBlocking] = true
			}
			let header = `${method} ${path} HTTP/1.1\r
`
			if (typeof host === "string") {
				header += `host: ${host}\r
`
			} else {
				header += client[kHostHeader]
			}
			if (upgrade) {
				header += `connection: upgrade\r
upgrade: ${upgrade}\r
`
			} else if (client[kPipelining] && !socket[kReset]) {
				header += "connection: keep-alive\r\n"
			} else {
				header += "connection: close\r\n"
			}
			if (headers) {
				header += headers
			}
			if (channels.sendHeaders.hasSubscribers) {
				channels.sendHeaders.publish({ request, headers: header, socket })
			}
			if (!body || bodyLength === 0) {
				if (contentLength === 0) {
					socket.write(
						`${header}content-length: 0\r
\r
`,
						"latin1"
					)
				} else {
					assert(contentLength === null, "no body must not have content length")
					socket.write(
						`${header}\r
`,
						"latin1"
					)
				}
				request.onRequestSent()
			} else if (util.isBuffer(body)) {
				assert(contentLength === body.byteLength, "buffer body must have content length")
				socket.cork()
				socket.write(
					`${header}content-length: ${contentLength}\r
\r
`,
					"latin1"
				)
				socket.write(body)
				socket.uncork()
				request.onBodySent(body)
				request.onRequestSent()
				if (!expectsPayload) {
					socket[kReset] = true
				}
			} else if (util.isBlobLike(body)) {
				if (typeof body.stream === "function") {
					writeIterable({
						body: body.stream(),
						client,
						request,
						socket,
						contentLength,
						header,
						expectsPayload,
					})
				} else {
					writeBlob2({ body, client, request, socket, contentLength, header, expectsPayload })
				}
			} else if (util.isStream(body)) {
				writeStream({ body, client, request, socket, contentLength, header, expectsPayload })
			} else if (util.isIterable(body)) {
				writeIterable({ body, client, request, socket, contentLength, header, expectsPayload })
			} else {
				assert(false)
			}
			return true
		}
		function writeH2(client, session, request) {
			const {
				body,
				method,
				path,
				host,
				upgrade,
				expectContinue,
				signal,
				headers: reqHeaders,
			} = request
			let headers
			if (typeof reqHeaders === "string") headers = Request[kHTTP2CopyHeaders](reqHeaders.trim())
			else headers = reqHeaders
			if (upgrade) {
				errorRequest(client, request, new Error("Upgrade not supported for H2"))
				return false
			}
			try {
				request.onConnect((err) => {
					if (request.aborted || request.completed) {
						return
					}
					errorRequest(client, request, err || new RequestAbortedError())
				})
			} catch (err) {
				errorRequest(client, request, err)
			}
			if (request.aborted) {
				return false
			}
			let stream
			const h2State = client[kHTTP2SessionState]
			headers[HTTP2_HEADER_AUTHORITY] = host || client[kHost]
			headers[HTTP2_HEADER_METHOD] = method
			if (method === "CONNECT") {
				session.ref()
				stream = session.request(headers, { endStream: false, signal })
				if (stream.id && !stream.pending) {
					request.onUpgrade(null, null, stream)
					++h2State.openStreams
				} else {
					stream.once("ready", () => {
						request.onUpgrade(null, null, stream)
						++h2State.openStreams
					})
				}
				stream.once("close", () => {
					h2State.openStreams -= 1
					if (h2State.openStreams === 0) session.unref()
				})
				return true
			}
			headers[HTTP2_HEADER_PATH] = path
			headers[HTTP2_HEADER_SCHEME] = "https"
			const expectsPayload = method === "PUT" || method === "POST" || method === "PATCH"
			if (body && typeof body.read === "function") {
				body.read(0)
			}
			let contentLength = util.bodyLength(body)
			if (contentLength == undefined) {
				contentLength = request.contentLength
			}
			if (contentLength === 0 || !expectsPayload) {
				contentLength = null
			}
			if (
				shouldSendContentLength(method) &&
				contentLength > 0 &&
				request.contentLength != undefined &&
				request.contentLength !== contentLength
			) {
				if (client[kStrictContentLength]) {
					errorRequest(client, request, new RequestContentLengthMismatchError())
					return false
				}
				process.emitWarning(new RequestContentLengthMismatchError())
			}
			if (contentLength != undefined) {
				assert(body, "no body must not have content length")
				headers[HTTP2_HEADER_CONTENT_LENGTH] = `${contentLength}`
			}
			session.ref()
			const shouldEndStream = method === "GET" || method === "HEAD"
			if (expectContinue) {
				headers[HTTP2_HEADER_EXPECT] = "100-continue"
				stream = session.request(headers, { endStream: shouldEndStream, signal })
				stream.once("continue", writeBodyH2)
			} else {
				stream = session.request(headers, {
					endStream: shouldEndStream,
					signal,
				})
				writeBodyH2()
			}
			++h2State.openStreams
			stream.once("response", (headers2) => {
				const { [HTTP2_HEADER_STATUS]: statusCode, ...realHeaders } = headers2
				if (
					request.onHeaders(Number(statusCode), realHeaders, stream.resume.bind(stream), "") ===
					false
				) {
					stream.pause()
				}
			})
			stream.once("end", () => {
				request.onComplete([])
			})
			stream.on("data", (chunk) => {
				if (request.onData(chunk) === false) {
					stream.pause()
				}
			})
			stream.once("close", () => {
				h2State.openStreams -= 1
				if (h2State.openStreams === 0) {
					session.unref()
				}
			})
			stream.once("error", function (err) {
				if (
					client[kHTTP2Session] &&
					!client[kHTTP2Session].destroyed &&
					!this.closed &&
					!this.destroyed
				) {
					h2State.streams -= 1
					util.destroy(stream, err)
				}
			})
			stream.once("frameError", (type, code) => {
				const err = new InformationalError(
					`HTTP/2: "frameError" received - type ${type}, code ${code}`
				)
				errorRequest(client, request, err)
				if (
					client[kHTTP2Session] &&
					!client[kHTTP2Session].destroyed &&
					!this.closed &&
					!this.destroyed
				) {
					h2State.streams -= 1
					util.destroy(stream, err)
				}
			})
			return true
			function writeBodyH2() {
				if (!body) {
					request.onRequestSent()
				} else if (util.isBuffer(body)) {
					assert(contentLength === body.byteLength, "buffer body must have content length")
					stream.cork()
					stream.write(body)
					stream.uncork()
					stream.end()
					request.onBodySent(body)
					request.onRequestSent()
				} else if (util.isBlobLike(body)) {
					if (typeof body.stream === "function") {
						writeIterable({
							client,
							request,
							contentLength,
							h2stream: stream,
							expectsPayload,
							body: body.stream(),
							socket: client[kSocket],
							header: "",
						})
					} else {
						writeBlob2({
							body,
							client,
							request,
							contentLength,
							expectsPayload,
							h2stream: stream,
							header: "",
							socket: client[kSocket],
						})
					}
				} else if (util.isStream(body)) {
					writeStream({
						body,
						client,
						request,
						contentLength,
						expectsPayload,
						socket: client[kSocket],
						h2stream: stream,
						header: "",
					})
				} else if (util.isIterable(body)) {
					writeIterable({
						body,
						client,
						request,
						contentLength,
						expectsPayload,
						header: "",
						h2stream: stream,
						socket: client[kSocket],
					})
				} else {
					assert(false)
				}
			}
		}
		function writeStream({
			h2stream,
			body,
			client,
			request,
			socket,
			contentLength,
			header,
			expectsPayload,
		}) {
			assert(contentLength !== 0 || client[kRunning] === 0, "stream body cannot be pipelined")
			if (client[kHTTPConnVersion] === "h2") {
				let onPipeData = function (chunk) {
					request.onBodySent(chunk)
				}
				const pipe = pipeline(body, h2stream, (err) => {
					if (err) {
						util.destroy(body, err)
						util.destroy(h2stream, err)
					} else {
						request.onRequestSent()
					}
				})
				pipe.on("data", onPipeData)
				pipe.once("end", () => {
					pipe.removeListener("data", onPipeData)
					util.destroy(pipe)
				})
				return
			}
			let finished = false
			const writer = new AsyncWriter({
				socket,
				request,
				contentLength,
				client,
				expectsPayload,
				header,
			})
			const onData = function (chunk) {
				if (finished) {
					return
				}
				try {
					if (!writer.write(chunk) && this.pause) {
						this.pause()
					}
				} catch (err) {
					util.destroy(this, err)
				}
			}
			const onDrain = function () {
				if (finished) {
					return
				}
				if (body.resume) {
					body.resume()
				}
			}
			const onAbort = function () {
				if (finished) {
					return
				}
				const err = new RequestAbortedError()
				queueMicrotask(() => onFinished(err))
			}
			const onFinished = function (err) {
				if (finished) {
					return
				}
				finished = true
				assert(socket.destroyed || (socket[kWriting] && client[kRunning] <= 1))
				socket.off("drain", onDrain).off("error", onFinished)
				body
					.removeListener("data", onData)
					.removeListener("end", onFinished)
					.removeListener("error", onFinished)
					.removeListener("close", onAbort)
				if (!err) {
					try {
						writer.end()
					} catch (er) {
						err = er
					}
				}
				writer.destroy(err)
				if (err && (err.code !== "UND_ERR_INFO" || err.message !== "reset")) {
					util.destroy(body, err)
				} else {
					util.destroy(body)
				}
			}
			body.on("data", onData).on("end", onFinished).on("error", onFinished).on("close", onAbort)
			if (body.resume) {
				body.resume()
			}
			socket.on("drain", onDrain).on("error", onFinished)
		}
		async function writeBlob2({
			h2stream,
			body,
			client,
			request,
			socket,
			contentLength,
			header,
			expectsPayload,
		}) {
			assert(contentLength === body.size, "blob body must have content length")
			const isH2 = client[kHTTPConnVersion] === "h2"
			try {
				if (contentLength != undefined && contentLength !== body.size) {
					throw new RequestContentLengthMismatchError()
				}
				const buffer = Buffer.from(await body.arrayBuffer())
				if (isH2) {
					h2stream.cork()
					h2stream.write(buffer)
					h2stream.uncork()
				} else {
					socket.cork()
					socket.write(
						`${header}content-length: ${contentLength}\r
\r
`,
						"latin1"
					)
					socket.write(buffer)
					socket.uncork()
				}
				request.onBodySent(buffer)
				request.onRequestSent()
				if (!expectsPayload) {
					socket[kReset] = true
				}
				resume(client)
			} catch (err) {
				util.destroy(isH2 ? h2stream : socket, err)
			}
		}
		async function writeIterable({
			h2stream,
			body,
			client,
			request,
			socket,
			contentLength,
			header,
			expectsPayload,
		}) {
			assert(contentLength !== 0 || client[kRunning] === 0, "iterator body cannot be pipelined")
			let callback = null
			function onDrain() {
				if (callback) {
					const cb = callback
					callback = null
					cb()
				}
			}
			const waitForDrain = () =>
				new Promise((resolve, reject) => {
					assert(callback === null)
					if (socket[kError]) {
						reject(socket[kError])
					} else {
						callback = resolve
					}
				})
			if (client[kHTTPConnVersion] === "h2") {
				h2stream.on("close", onDrain).on("drain", onDrain)
				try {
					for await (const chunk of body) {
						if (socket[kError]) {
							throw socket[kError]
						}
						const res = h2stream.write(chunk)
						request.onBodySent(chunk)
						if (!res) {
							await waitForDrain()
						}
					}
				} catch (err) {
					h2stream.destroy(err)
				} finally {
					request.onRequestSent()
					h2stream.end()
					h2stream.off("close", onDrain).off("drain", onDrain)
				}
				return
			}
			socket.on("close", onDrain).on("drain", onDrain)
			const writer = new AsyncWriter({
				socket,
				request,
				contentLength,
				client,
				expectsPayload,
				header,
			})
			try {
				for await (const chunk of body) {
					if (socket[kError]) {
						throw socket[kError]
					}
					if (!writer.write(chunk)) {
						await waitForDrain()
					}
				}
				writer.end()
			} catch (err) {
				writer.destroy(err)
			} finally {
				socket.off("close", onDrain).off("drain", onDrain)
			}
		}
		var AsyncWriter = class {
			constructor({ socket, request, contentLength, client, expectsPayload, header }) {
				this.socket = socket
				this.request = request
				this.contentLength = contentLength
				this.client = client
				this.bytesWritten = 0
				this.expectsPayload = expectsPayload
				this.header = header
				socket[kWriting] = true
			}
			write(chunk) {
				const { socket, request, contentLength, client, bytesWritten, expectsPayload, header } =
					this
				if (socket[kError]) {
					throw socket[kError]
				}
				if (socket.destroyed) {
					return false
				}
				const len = Buffer.byteLength(chunk)
				if (!len) {
					return true
				}
				if (contentLength !== null && bytesWritten + len > contentLength) {
					if (client[kStrictContentLength]) {
						throw new RequestContentLengthMismatchError()
					}
					process.emitWarning(new RequestContentLengthMismatchError())
				}
				socket.cork()
				if (bytesWritten === 0) {
					if (!expectsPayload) {
						socket[kReset] = true
					}
					if (contentLength === null) {
						socket.write(
							`${header}transfer-encoding: chunked\r
`,
							"latin1"
						)
					} else {
						socket.write(
							`${header}content-length: ${contentLength}\r
\r
`,
							"latin1"
						)
					}
				}
				if (contentLength === null) {
					socket.write(
						`\r
${len.toString(16)}\r
`,
						"latin1"
					)
				}
				this.bytesWritten += len
				const ret = socket.write(chunk)
				socket.uncork()
				request.onBodySent(chunk)
				if (
					!ret &&
					socket[kParser].timeout &&
					socket[kParser].timeoutType === TIMEOUT_HEADERS &&
					socket[kParser].timeout.refresh
				) {
					socket[kParser].timeout.refresh()
				}
				return ret
			}
			end() {
				const { socket, contentLength, client, bytesWritten, expectsPayload, header, request } =
					this
				request.onRequestSent()
				socket[kWriting] = false
				if (socket[kError]) {
					throw socket[kError]
				}
				if (socket.destroyed) {
					return
				}
				if (bytesWritten === 0) {
					if (expectsPayload) {
						socket.write(
							`${header}content-length: 0\r
\r
`,
							"latin1"
						)
					} else {
						socket.write(
							`${header}\r
`,
							"latin1"
						)
					}
				} else if (contentLength === null) {
					socket.write("\r\n0\r\n\r\n", "latin1")
				}
				if (contentLength !== null && bytesWritten !== contentLength) {
					if (client[kStrictContentLength]) {
						throw new RequestContentLengthMismatchError()
					} else {
						process.emitWarning(new RequestContentLengthMismatchError())
					}
				}
				if (
					socket[kParser].timeout &&
					socket[kParser].timeoutType === TIMEOUT_HEADERS &&
					socket[kParser].timeout.refresh
				) {
					socket[kParser].timeout.refresh()
				}
				resume(client)
			}
			destroy(err) {
				const { socket, client } = this
				socket[kWriting] = false
				if (err) {
					assert(client[kRunning] <= 1, "pipeline should only contain this request")
					util.destroy(socket, err)
				}
			}
		}
		function errorRequest(client, request, err) {
			try {
				request.onError(err)
				assert(request.aborted)
			} catch (err2) {
				client.emit("error", err2)
			}
		}
		module2.exports = Client
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/node/fixed-queue.js
var require_fixed_queue = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/node/fixed-queue.js"(
		exports2,
		module2
	) {
		"use strict"
		var kSize = 2048
		var kMask = kSize - 1
		var FixedCircularBuffer = class {
			constructor() {
				this.bottom = 0
				this.top = 0
				this.list = new Array(kSize)
				this.next = null
			}
			isEmpty() {
				return this.top === this.bottom
			}
			isFull() {
				return ((this.top + 1) & kMask) === this.bottom
			}
			push(data) {
				this.list[this.top] = data
				this.top = (this.top + 1) & kMask
			}
			shift() {
				const nextItem = this.list[this.bottom]
				if (nextItem === void 0) return null
				this.list[this.bottom] = void 0
				this.bottom = (this.bottom + 1) & kMask
				return nextItem
			}
		}
		module2.exports = class FixedQueue {
			constructor() {
				this.head = this.tail = new FixedCircularBuffer()
			}
			isEmpty() {
				return this.head.isEmpty()
			}
			push(data) {
				if (this.head.isFull()) {
					this.head = this.head.next = new FixedCircularBuffer()
				}
				this.head.push(data)
			}
			shift() {
				const tail = this.tail
				const next = tail.shift()
				if (tail.isEmpty() && tail.next !== null) {
					this.tail = tail.next
				}
				return next
			}
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/pool-stats.js
var require_pool_stats = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/pool-stats.js"(
		exports2,
		module2
	) {
		var { kFree, kConnected, kPending, kQueued, kRunning, kSize } = require_symbols()
		var kPool = Symbol("pool")
		var PoolStats = class {
			constructor(pool) {
				this[kPool] = pool
			}
			get connected() {
				return this[kPool][kConnected]
			}
			get free() {
				return this[kPool][kFree]
			}
			get pending() {
				return this[kPool][kPending]
			}
			get queued() {
				return this[kPool][kQueued]
			}
			get running() {
				return this[kPool][kRunning]
			}
			get size() {
				return this[kPool][kSize]
			}
		}
		module2.exports = PoolStats
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/pool-base.js
var require_pool_base = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/pool-base.js"(
		exports2,
		module2
	) {
		"use strict"
		var DispatcherBase = require_dispatcher_base()
		var FixedQueue = require_fixed_queue()
		var {
			kConnected,
			kSize,
			kRunning,
			kPending,
			kQueued,
			kBusy,
			kFree,
			kUrl,
			kClose,
			kDestroy,
			kDispatch,
		} = require_symbols()
		var PoolStats = require_pool_stats()
		var kClients = Symbol("clients")
		var kNeedDrain = Symbol("needDrain")
		var kQueue = Symbol("queue")
		var kClosedResolve = Symbol("closed resolve")
		var kOnDrain = Symbol("onDrain")
		var kOnConnect = Symbol("onConnect")
		var kOnDisconnect = Symbol("onDisconnect")
		var kOnConnectionError = Symbol("onConnectionError")
		var kGetDispatcher = Symbol("get dispatcher")
		var kAddClient = Symbol("add client")
		var kRemoveClient = Symbol("remove client")
		var kStats = Symbol("stats")
		var PoolBase = class extends DispatcherBase {
			constructor() {
				super()
				this[kQueue] = new FixedQueue()
				this[kClients] = []
				this[kQueued] = 0
				const pool = this
				this[kOnDrain] = function onDrain(origin, targets) {
					const queue = pool[kQueue]
					let needDrain = false
					while (!needDrain) {
						const item = queue.shift()
						if (!item) {
							break
						}
						pool[kQueued]--
						needDrain = !this.dispatch(item.opts, item.handler)
					}
					this[kNeedDrain] = needDrain
					if (!this[kNeedDrain] && pool[kNeedDrain]) {
						pool[kNeedDrain] = false
						pool.emit("drain", origin, [pool, ...targets])
					}
					if (pool[kClosedResolve] && queue.isEmpty()) {
						Promise.all(pool[kClients].map((c) => c.close())).then(pool[kClosedResolve])
					}
				}
				this[kOnConnect] = (origin, targets) => {
					pool.emit("connect", origin, [pool, ...targets])
				}
				this[kOnDisconnect] = (origin, targets, err) => {
					pool.emit("disconnect", origin, [pool, ...targets], err)
				}
				this[kOnConnectionError] = (origin, targets, err) => {
					pool.emit("connectionError", origin, [pool, ...targets], err)
				}
				this[kStats] = new PoolStats(this)
			}
			get [kBusy]() {
				return this[kNeedDrain]
			}
			get [kConnected]() {
				return this[kClients].filter((client) => client[kConnected]).length
			}
			get [kFree]() {
				return this[kClients].filter((client) => client[kConnected] && !client[kNeedDrain]).length
			}
			get [kPending]() {
				let ret = this[kQueued]
				for (const { [kPending]: pending } of this[kClients]) {
					ret += pending
				}
				return ret
			}
			get [kRunning]() {
				let ret = 0
				for (const { [kRunning]: running } of this[kClients]) {
					ret += running
				}
				return ret
			}
			get [kSize]() {
				let ret = this[kQueued]
				for (const { [kSize]: size } of this[kClients]) {
					ret += size
				}
				return ret
			}
			get stats() {
				return this[kStats]
			}
			async [kClose]() {
				if (this[kQueue].isEmpty()) {
					return Promise.all(this[kClients].map((c) => c.close()))
				} else {
					return new Promise((resolve) => {
						this[kClosedResolve] = resolve
					})
				}
			}
			async [kDestroy](err) {
				while (true) {
					const item = this[kQueue].shift()
					if (!item) {
						break
					}
					item.handler.onError(err)
				}
				return Promise.all(this[kClients].map((c) => c.destroy(err)))
			}
			[kDispatch](opts, handler) {
				const dispatcher = this[kGetDispatcher]()
				if (!dispatcher) {
					this[kNeedDrain] = true
					this[kQueue].push({ opts, handler })
					this[kQueued]++
				} else if (!dispatcher.dispatch(opts, handler)) {
					dispatcher[kNeedDrain] = true
					this[kNeedDrain] = !this[kGetDispatcher]()
				}
				return !this[kNeedDrain]
			}
			[kAddClient](client) {
				client
					.on("drain", this[kOnDrain])
					.on("connect", this[kOnConnect])
					.on("disconnect", this[kOnDisconnect])
					.on("connectionError", this[kOnConnectionError])
				this[kClients].push(client)
				if (this[kNeedDrain]) {
					process.nextTick(() => {
						if (this[kNeedDrain]) {
							this[kOnDrain](client[kUrl], [this, client])
						}
					})
				}
				return this
			}
			[kRemoveClient](client) {
				client.close(() => {
					const idx = this[kClients].indexOf(client)
					if (idx !== -1) {
						this[kClients].splice(idx, 1)
					}
				})
				this[kNeedDrain] = this[kClients].some(
					(dispatcher) =>
						!dispatcher[kNeedDrain] && dispatcher.closed !== true && dispatcher.destroyed !== true
				)
			}
		}
		module2.exports = {
			PoolBase,
			kClients,
			kNeedDrain,
			kAddClient,
			kRemoveClient,
			kGetDispatcher,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/pool.js
var require_pool = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/pool.js"(exports2, module2) {
		"use strict"
		var { PoolBase, kClients, kNeedDrain, kAddClient, kGetDispatcher } = require_pool_base()
		var Client = require_client()
		var { InvalidArgumentError } = require_errors()
		var util = require_util()
		var { kUrl, kInterceptors } = require_symbols()
		var buildConnector = require_connect()
		var kOptions = Symbol("options")
		var kConnections = Symbol("connections")
		var kFactory = Symbol("factory")
		function defaultFactory(origin, opts) {
			return new Client(origin, opts)
		}
		var Pool = class extends PoolBase {
			constructor(
				origin,
				{
					connections,
					factory = defaultFactory,
					connect,
					connectTimeout,
					tls,
					maxCachedSessions,
					socketPath,
					autoSelectFamily,
					autoSelectFamilyAttemptTimeout,
					allowH2,
					...options
				} = {}
			) {
				super()
				if (connections != undefined && (!Number.isFinite(connections) || connections < 0)) {
					throw new InvalidArgumentError("invalid connections")
				}
				if (typeof factory !== "function") {
					throw new InvalidArgumentError("factory must be a function.")
				}
				if (connect != undefined && typeof connect !== "function" && typeof connect !== "object") {
					throw new InvalidArgumentError("connect must be a function or an object")
				}
				if (typeof connect !== "function") {
					connect = buildConnector({
						...tls,
						maxCachedSessions,
						allowH2,
						socketPath,
						timeout: connectTimeout,
						...(util.nodeHasAutoSelectFamily && autoSelectFamily
							? { autoSelectFamily, autoSelectFamilyAttemptTimeout }
							: void 0),
						...connect,
					})
				}
				this[kInterceptors] =
					options.interceptors &&
					options.interceptors.Pool &&
					Array.isArray(options.interceptors.Pool)
						? options.interceptors.Pool
						: []
				this[kConnections] = connections || null
				this[kUrl] = util.parseOrigin(origin)
				this[kOptions] = { ...util.deepClone(options), connect, allowH2 }
				this[kOptions].interceptors = options.interceptors ? { ...options.interceptors } : void 0
				this[kFactory] = factory
			}
			[kGetDispatcher]() {
				let dispatcher = this[kClients].find((dispatcher2) => !dispatcher2[kNeedDrain])
				if (dispatcher) {
					return dispatcher
				}
				if (!this[kConnections] || this[kClients].length < this[kConnections]) {
					dispatcher = this[kFactory](this[kUrl], this[kOptions])
					this[kAddClient](dispatcher)
				}
				return dispatcher
			}
		}
		module2.exports = Pool
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/balanced-pool.js
var require_balanced_pool = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/balanced-pool.js"(
		exports2,
		module2
	) {
		"use strict"
		var { BalancedPoolMissingUpstreamError, InvalidArgumentError } = require_errors()
		var { PoolBase, kClients, kNeedDrain, kAddClient, kRemoveClient, kGetDispatcher } =
			require_pool_base()
		var Pool = require_pool()
		var { kUrl, kInterceptors } = require_symbols()
		var { parseOrigin: parseOrigin2 } = require_util()
		var kFactory = Symbol("factory")
		var kOptions = Symbol("options")
		var kGreatestCommonDivisor = Symbol("kGreatestCommonDivisor")
		var kCurrentWeight = Symbol("kCurrentWeight")
		var kIndex = Symbol("kIndex")
		var kWeight = Symbol("kWeight")
		var kMaxWeightPerServer = Symbol("kMaxWeightPerServer")
		var kErrorPenalty = Symbol("kErrorPenalty")
		function getGreatestCommonDivisor(a, b) {
			if (b === 0) return a
			return getGreatestCommonDivisor(b, a % b)
		}
		function defaultFactory(origin, opts) {
			return new Pool(origin, opts)
		}
		var BalancedPool = class extends PoolBase {
			constructor(upstreams = [], { factory = defaultFactory, ...opts } = {}) {
				super()
				this[kOptions] = opts
				this[kIndex] = -1
				this[kCurrentWeight] = 0
				this[kMaxWeightPerServer] = this[kOptions].maxWeightPerServer || 100
				this[kErrorPenalty] = this[kOptions].errorPenalty || 15
				if (!Array.isArray(upstreams)) {
					upstreams = [upstreams]
				}
				if (typeof factory !== "function") {
					throw new InvalidArgumentError("factory must be a function.")
				}
				this[kInterceptors] =
					opts.interceptors &&
					opts.interceptors.BalancedPool &&
					Array.isArray(opts.interceptors.BalancedPool)
						? opts.interceptors.BalancedPool
						: []
				this[kFactory] = factory
				for (const upstream of upstreams) {
					this.addUpstream(upstream)
				}
				this._updateBalancedPoolStats()
			}
			addUpstream(upstream) {
				const upstreamOrigin = parseOrigin2(upstream).origin
				if (
					this[kClients].find(
						(pool2) =>
							pool2[kUrl].origin === upstreamOrigin &&
							pool2.closed !== true &&
							pool2.destroyed !== true
					)
				) {
					return this
				}
				const pool = this[kFactory](upstreamOrigin, Object.assign({}, this[kOptions]))
				this[kAddClient](pool)
				pool.on("connect", () => {
					pool[kWeight] = Math.min(this[kMaxWeightPerServer], pool[kWeight] + this[kErrorPenalty])
				})
				pool.on("connectionError", () => {
					pool[kWeight] = Math.max(1, pool[kWeight] - this[kErrorPenalty])
					this._updateBalancedPoolStats()
				})
				pool.on("disconnect", (...args) => {
					const err = args[2]
					if (err && err.code === "UND_ERR_SOCKET") {
						pool[kWeight] = Math.max(1, pool[kWeight] - this[kErrorPenalty])
						this._updateBalancedPoolStats()
					}
				})
				for (const client of this[kClients]) {
					client[kWeight] = this[kMaxWeightPerServer]
				}
				this._updateBalancedPoolStats()
				return this
			}
			_updateBalancedPoolStats() {
				this[kGreatestCommonDivisor] = this[kClients]
					.map((p) => p[kWeight])
					.reduce(getGreatestCommonDivisor, 0)
			}
			removeUpstream(upstream) {
				const upstreamOrigin = parseOrigin2(upstream).origin
				const pool = this[kClients].find(
					(pool2) =>
						pool2[kUrl].origin === upstreamOrigin &&
						pool2.closed !== true &&
						pool2.destroyed !== true
				)
				if (pool) {
					this[kRemoveClient](pool)
				}
				return this
			}
			get upstreams() {
				return this[kClients]
					.filter((dispatcher) => dispatcher.closed !== true && dispatcher.destroyed !== true)
					.map((p) => p[kUrl].origin)
			}
			[kGetDispatcher]() {
				if (this[kClients].length === 0) {
					throw new BalancedPoolMissingUpstreamError()
				}
				const dispatcher = this[kClients].find(
					(dispatcher2) =>
						!dispatcher2[kNeedDrain] &&
						dispatcher2.closed !== true &&
						dispatcher2.destroyed !== true
				)
				if (!dispatcher) {
					return
				}
				const allClientsBusy = this[kClients]
					.map((pool) => pool[kNeedDrain])
					.reduce((a, b) => a && b, true)
				if (allClientsBusy) {
					return
				}
				let counter = 0
				let maxWeightIndex = this[kClients].findIndex((pool) => !pool[kNeedDrain])
				while (counter++ < this[kClients].length) {
					this[kIndex] = (this[kIndex] + 1) % this[kClients].length
					const pool = this[kClients][this[kIndex]]
					if (pool[kWeight] > this[kClients][maxWeightIndex][kWeight] && !pool[kNeedDrain]) {
						maxWeightIndex = this[kIndex]
					}
					if (this[kIndex] === 0) {
						this[kCurrentWeight] = this[kCurrentWeight] - this[kGreatestCommonDivisor]
						if (this[kCurrentWeight] <= 0) {
							this[kCurrentWeight] = this[kMaxWeightPerServer]
						}
					}
					if (pool[kWeight] >= this[kCurrentWeight] && !pool[kNeedDrain]) {
						return pool
					}
				}
				this[kCurrentWeight] = this[kClients][maxWeightIndex][kWeight]
				this[kIndex] = maxWeightIndex
				return this[kClients][maxWeightIndex]
			}
		}
		module2.exports = BalancedPool
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/compat/dispatcher-weakref.js
var require_dispatcher_weakref = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/compat/dispatcher-weakref.js"(
		exports2,
		module2
	) {
		"use strict"
		var { kConnected, kSize } = require_symbols()
		var CompatWeakRef = class {
			constructor(value) {
				this.value = value
			}
			deref() {
				return this.value[kConnected] === 0 && this.value[kSize] === 0 ? void 0 : this.value
			}
		}
		var CompatFinalizer = class {
			constructor(finalizer) {
				this.finalizer = finalizer
			}
			register(dispatcher, key) {
				if (dispatcher.on) {
					dispatcher.on("disconnect", () => {
						if (dispatcher[kConnected] === 0 && dispatcher[kSize] === 0) {
							this.finalizer(key)
						}
					})
				}
			}
		}
		module2.exports = function () {
			if (process.env.NODE_V8_COVERAGE) {
				return {
					WeakRef: CompatWeakRef,
					FinalizationRegistry: CompatFinalizer,
				}
			}
			return {
				WeakRef: global.WeakRef || CompatWeakRef,
				FinalizationRegistry: global.FinalizationRegistry || CompatFinalizer,
			}
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/agent.js
var require_agent = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/agent.js"(exports2, module2) {
		"use strict"
		var { InvalidArgumentError } = require_errors()
		var { kClients, kRunning, kClose, kDestroy, kDispatch, kInterceptors } = require_symbols()
		var DispatcherBase = require_dispatcher_base()
		var Pool = require_pool()
		var Client = require_client()
		var util = require_util()
		var createRedirectInterceptor = require_redirectInterceptor()
		var { WeakRef: WeakRef2, FinalizationRegistry } = require_dispatcher_weakref()()
		var kOnConnect = Symbol("onConnect")
		var kOnDisconnect = Symbol("onDisconnect")
		var kOnConnectionError = Symbol("onConnectionError")
		var kMaxRedirections = Symbol("maxRedirections")
		var kOnDrain = Symbol("onDrain")
		var kFactory = Symbol("factory")
		var kFinalizer = Symbol("finalizer")
		var kOptions = Symbol("options")
		function defaultFactory(origin, opts) {
			return opts && opts.connections === 1 ? new Client(origin, opts) : new Pool(origin, opts)
		}
		var Agent = class extends DispatcherBase {
			constructor({ factory = defaultFactory, maxRedirections = 0, connect, ...options } = {}) {
				super()
				if (typeof factory !== "function") {
					throw new InvalidArgumentError("factory must be a function.")
				}
				if (connect != undefined && typeof connect !== "function" && typeof connect !== "object") {
					throw new InvalidArgumentError("connect must be a function or an object")
				}
				if (!Number.isInteger(maxRedirections) || maxRedirections < 0) {
					throw new InvalidArgumentError("maxRedirections must be a positive number")
				}
				if (connect && typeof connect !== "function") {
					connect = { ...connect }
				}
				this[kInterceptors] =
					options.interceptors &&
					options.interceptors.Agent &&
					Array.isArray(options.interceptors.Agent)
						? options.interceptors.Agent
						: [createRedirectInterceptor({ maxRedirections })]
				this[kOptions] = { ...util.deepClone(options), connect }
				this[kOptions].interceptors = options.interceptors ? { ...options.interceptors } : void 0
				this[kMaxRedirections] = maxRedirections
				this[kFactory] = factory
				this[kClients] = /* @__PURE__ */ new Map()
				this[kFinalizer] = new FinalizationRegistry(
					/* istanbul ignore next: gc is undeterministic */
					(key) => {
						const ref = this[kClients].get(key)
						if (ref !== void 0 && ref.deref() === void 0) {
							this[kClients].delete(key)
						}
					}
				)
				const agent = this
				this[kOnDrain] = (origin, targets) => {
					agent.emit("drain", origin, [agent, ...targets])
				}
				this[kOnConnect] = (origin, targets) => {
					agent.emit("connect", origin, [agent, ...targets])
				}
				this[kOnDisconnect] = (origin, targets, err) => {
					agent.emit("disconnect", origin, [agent, ...targets], err)
				}
				this[kOnConnectionError] = (origin, targets, err) => {
					agent.emit("connectionError", origin, [agent, ...targets], err)
				}
			}
			get [kRunning]() {
				let ret = 0
				for (const ref of this[kClients].values()) {
					const client = ref.deref()
					if (client) {
						ret += client[kRunning]
					}
				}
				return ret
			}
			[kDispatch](opts, handler) {
				let key
				if (opts.origin && (typeof opts.origin === "string" || opts.origin instanceof URL)) {
					key = String(opts.origin)
				} else {
					throw new InvalidArgumentError("opts.origin must be a non-empty string or URL.")
				}
				const ref = this[kClients].get(key)
				let dispatcher = ref ? ref.deref() : null
				if (!dispatcher) {
					dispatcher = this[kFactory](opts.origin, this[kOptions])
						.on("drain", this[kOnDrain])
						.on("connect", this[kOnConnect])
						.on("disconnect", this[kOnDisconnect])
						.on("connectionError", this[kOnConnectionError])
					this[kClients].set(key, new WeakRef2(dispatcher))
					this[kFinalizer].register(dispatcher, key)
				}
				return dispatcher.dispatch(opts, handler)
			}
			async [kClose]() {
				const closePromises = []
				for (const ref of this[kClients].values()) {
					const client = ref.deref()
					if (client) {
						closePromises.push(client.close())
					}
				}
				await Promise.all(closePromises)
			}
			async [kDestroy](err) {
				const destroyPromises = []
				for (const ref of this[kClients].values()) {
					const client = ref.deref()
					if (client) {
						destroyPromises.push(client.destroy(err))
					}
				}
				await Promise.all(destroyPromises)
			}
		}
		module2.exports = Agent
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/readable.js
var require_readable = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/readable.js"(
		exports2,
		module2
	) {
		"use strict"
		var assert = require("node:assert")
		var { Readable } = require("node:stream")
		var { RequestAbortedError, NotSupportedError, InvalidArgumentError } = require_errors()
		var util = require_util()
		var { ReadableStreamFrom, toUSVString } = require_util()
		var Blob2
		var kConsume = Symbol("kConsume")
		var kReading = Symbol("kReading")
		var kBody = Symbol("kBody")
		var kAbort = Symbol("abort")
		var kContentType = Symbol("kContentType")
		var noop = () => {}
		module2.exports = class BodyReadable extends Readable {
			constructor({
				resume,
				abort,
				contentType = "",
				highWaterMark = 64 * 1024,
				// Same as nodejs fs streams.
			}) {
				super({
					autoDestroy: true,
					read: resume,
					highWaterMark,
				})
				this._readableState.dataEmitted = false
				this[kAbort] = abort
				this[kConsume] = null
				this[kBody] = null
				this[kContentType] = contentType
				this[kReading] = false
			}
			destroy(err) {
				if (this.destroyed) {
					return this
				}
				if (!err && !this._readableState.endEmitted) {
					err = new RequestAbortedError()
				}
				if (err) {
					this[kAbort]()
				}
				return super.destroy(err)
			}
			emit(ev, ...args) {
				if (ev === "data") {
					this._readableState.dataEmitted = true
				} else if (ev === "error") {
					this._readableState.errorEmitted = true
				}
				return super.emit(ev, ...args)
			}
			on(ev, ...args) {
				if (ev === "data" || ev === "readable") {
					this[kReading] = true
				}
				return super.on(ev, ...args)
			}
			addListener(ev, ...args) {
				return this.on(ev, ...args)
			}
			off(ev, ...args) {
				const ret = super.off(ev, ...args)
				if (ev === "data" || ev === "readable") {
					this[kReading] = this.listenerCount("data") > 0 || this.listenerCount("readable") > 0
				}
				return ret
			}
			removeListener(ev, ...args) {
				return this.off(ev, ...args)
			}
			push(chunk) {
				if (this[kConsume] && chunk !== null && this.readableLength === 0) {
					consumePush(this[kConsume], chunk)
					return this[kReading] ? super.push(chunk) : true
				}
				return super.push(chunk)
			}
			// https://fetch.spec.whatwg.org/#dom-body-text
			async text() {
				return consume(this, "text")
			}
			// https://fetch.spec.whatwg.org/#dom-body-json
			async json() {
				return consume(this, "json")
			}
			// https://fetch.spec.whatwg.org/#dom-body-blob
			async blob() {
				return consume(this, "blob")
			}
			// https://fetch.spec.whatwg.org/#dom-body-arraybuffer
			async arrayBuffer() {
				return consume(this, "arrayBuffer")
			}
			// https://fetch.spec.whatwg.org/#dom-body-formdata
			async formData() {
				throw new NotSupportedError()
			}
			// https://fetch.spec.whatwg.org/#dom-body-bodyused
			get bodyUsed() {
				return util.isDisturbed(this)
			}
			// https://fetch.spec.whatwg.org/#dom-body-body
			get body() {
				if (!this[kBody]) {
					this[kBody] = ReadableStreamFrom(this)
					if (this[kConsume]) {
						this[kBody].getReader()
						assert(this[kBody].locked)
					}
				}
				return this[kBody]
			}
			dump(opts) {
				let limit = opts && Number.isFinite(opts.limit) ? opts.limit : 262144
				const signal = opts && opts.signal
				if (signal) {
					try {
						if (typeof signal !== "object" || !("aborted" in signal)) {
							throw new InvalidArgumentError("signal must be an AbortSignal")
						}
						util.throwIfAborted(signal)
					} catch (err) {
						return Promise.reject(err)
					}
				}
				if (this.closed) {
					return Promise.resolve(null)
				}
				return new Promise((resolve, reject) => {
					const signalListenerCleanup = signal
						? util.addAbortListener(signal, () => {
								this.destroy()
						  })
						: noop
					this.on("close", function () {
						signalListenerCleanup()
						if (signal && signal.aborted) {
							reject(
								signal.reason ||
									Object.assign(new Error("The operation was aborted"), { name: "AbortError" })
							)
						} else {
							resolve(null)
						}
					})
						.on("error", noop)
						.on("data", function (chunk) {
							limit -= chunk.length
							if (limit <= 0) {
								this.destroy()
							}
						})
						.resume()
				})
			}
		}
		function isLocked(self2) {
			return (self2[kBody] && self2[kBody].locked === true) || self2[kConsume]
		}
		function isUnusable(self2) {
			return util.isDisturbed(self2) || isLocked(self2)
		}
		async function consume(stream, type) {
			if (isUnusable(stream)) {
				throw new TypeError("unusable")
			}
			assert(!stream[kConsume])
			return new Promise((resolve, reject) => {
				stream[kConsume] = {
					type,
					stream,
					resolve,
					reject,
					length: 0,
					body: [],
				}
				stream
					.on("error", function (err) {
						consumeFinish(this[kConsume], err)
					})
					.on("close", function () {
						if (this[kConsume].body !== null) {
							consumeFinish(this[kConsume], new RequestAbortedError())
						}
					})
				process.nextTick(consumeStart, stream[kConsume])
			})
		}
		function consumeStart(consume2) {
			if (consume2.body === null) {
				return
			}
			const { _readableState: state } = consume2.stream
			for (const chunk of state.buffer) {
				consumePush(consume2, chunk)
			}
			if (state.endEmitted) {
				consumeEnd(this[kConsume])
			} else {
				consume2.stream.on("end", function () {
					consumeEnd(this[kConsume])
				})
			}
			consume2.stream.resume()
			while (consume2.stream.read() != undefined) {}
		}
		function consumeEnd(consume2) {
			const { type, body, resolve, stream, length } = consume2
			try {
				if (type === "text") {
					resolve(toUSVString(Buffer.concat(body)))
				} else if (type === "json") {
					resolve(JSON.parse(Buffer.concat(body)))
				} else if (type === "arrayBuffer") {
					const dst = new Uint8Array(length)
					let pos = 0
					for (const buf of body) {
						dst.set(buf, pos)
						pos += buf.byteLength
					}
					resolve(dst.buffer)
				} else if (type === "blob") {
					if (!Blob2) {
						Blob2 = require("node:buffer").Blob
					}
					resolve(new Blob2(body, { type: stream[kContentType] }))
				}
				consumeFinish(consume2)
			} catch (err) {
				stream.destroy(err)
			}
		}
		function consumePush(consume2, chunk) {
			consume2.length += chunk.length
			consume2.body.push(chunk)
		}
		function consumeFinish(consume2, err) {
			if (consume2.body === null) {
				return
			}
			if (err) {
				consume2.reject(err)
			} else {
				consume2.resolve()
			}
			consume2.type = null
			consume2.stream = null
			consume2.resolve = null
			consume2.reject = null
			consume2.length = 0
			consume2.body = null
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/util.js
var require_util3 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/util.js"(
		exports2,
		module2
	) {
		var assert = require("node:assert")
		var { ResponseStatusCodeError } = require_errors()
		var { toUSVString } = require_util()
		async function getResolveErrorBodyCallback({
			callback,
			body,
			contentType,
			statusCode,
			statusMessage,
			headers,
		}) {
			assert(body)
			let chunks = []
			let limit = 0
			for await (const chunk of body) {
				chunks.push(chunk)
				limit += chunk.length
				if (limit > 128 * 1024) {
					chunks = null
					break
				}
			}
			if (statusCode === 204 || !contentType || !chunks) {
				process.nextTick(
					callback,
					new ResponseStatusCodeError(
						`Response status code ${statusCode}${statusMessage ? `: ${statusMessage}` : ""}`,
						statusCode,
						headers
					)
				)
				return
			}
			try {
				if (contentType.startsWith("application/json")) {
					const payload = JSON.parse(toUSVString(Buffer.concat(chunks)))
					process.nextTick(
						callback,
						new ResponseStatusCodeError(
							`Response status code ${statusCode}${statusMessage ? `: ${statusMessage}` : ""}`,
							statusCode,
							headers,
							payload
						)
					)
					return
				}
				if (contentType.startsWith("text/")) {
					const payload = toUSVString(Buffer.concat(chunks))
					process.nextTick(
						callback,
						new ResponseStatusCodeError(
							`Response status code ${statusCode}${statusMessage ? `: ${statusMessage}` : ""}`,
							statusCode,
							headers,
							payload
						)
					)
					return
				}
			} catch (err) {}
			process.nextTick(
				callback,
				new ResponseStatusCodeError(
					`Response status code ${statusCode}${statusMessage ? `: ${statusMessage}` : ""}`,
					statusCode,
					headers
				)
			)
		}
		module2.exports = { getResolveErrorBodyCallback }
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/abort-signal.js
var require_abort_signal = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/abort-signal.js"(
		exports2,
		module2
	) {
		var { addAbortListener } = require_util()
		var { RequestAbortedError } = require_errors()
		var kListener = Symbol("kListener")
		var kSignal = Symbol("kSignal")
		function abort(self2) {
			if (self2.abort) {
				self2.abort()
			} else {
				self2.onError(new RequestAbortedError())
			}
		}
		function addSignal(self2, signal) {
			self2[kSignal] = null
			self2[kListener] = null
			if (!signal) {
				return
			}
			if (signal.aborted) {
				abort(self2)
				return
			}
			self2[kSignal] = signal
			self2[kListener] = () => {
				abort(self2)
			}
			addAbortListener(self2[kSignal], self2[kListener])
		}
		function removeSignal(self2) {
			if (!self2[kSignal]) {
				return
			}
			if ("removeEventListener" in self2[kSignal]) {
				self2[kSignal].removeEventListener("abort", self2[kListener])
			} else {
				self2[kSignal].removeListener("abort", self2[kListener])
			}
			self2[kSignal] = null
			self2[kListener] = null
		}
		module2.exports = {
			addSignal,
			removeSignal,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/api-request.js
var require_api_request = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/api-request.js"(
		exports2,
		module2
	) {
		"use strict"
		var Readable = require_readable()
		var { InvalidArgumentError, RequestAbortedError } = require_errors()
		var util = require_util()
		var { getResolveErrorBodyCallback } = require_util3()
		var { AsyncResource } = require("node:async_hooks")
		var { addSignal, removeSignal } = require_abort_signal()
		var RequestHandler = class extends AsyncResource {
			constructor(opts, callback) {
				if (!opts || typeof opts !== "object") {
					throw new InvalidArgumentError("invalid opts")
				}
				const {
					signal,
					method,
					opaque,
					body,
					onInfo,
					responseHeaders,
					throwOnError,
					highWaterMark,
				} = opts
				try {
					if (typeof callback !== "function") {
						throw new InvalidArgumentError("invalid callback")
					}
					if (highWaterMark && (typeof highWaterMark !== "number" || highWaterMark < 0)) {
						throw new InvalidArgumentError("invalid highWaterMark")
					}
					if (
						signal &&
						typeof signal.on !== "function" &&
						typeof signal.addEventListener !== "function"
					) {
						throw new InvalidArgumentError("signal must be an EventEmitter or EventTarget")
					}
					if (method === "CONNECT") {
						throw new InvalidArgumentError("invalid method")
					}
					if (onInfo && typeof onInfo !== "function") {
						throw new InvalidArgumentError("invalid onInfo callback")
					}
					super("UNDICI_REQUEST")
				} catch (err) {
					if (util.isStream(body)) {
						util.destroy(body.on("error", util.nop), err)
					}
					throw err
				}
				this.responseHeaders = responseHeaders || null
				this.opaque = opaque || null
				this.callback = callback
				this.res = null
				this.abort = null
				this.body = body
				this.trailers = {}
				this.context = null
				this.onInfo = onInfo || null
				this.throwOnError = throwOnError
				this.highWaterMark = highWaterMark
				if (util.isStream(body)) {
					body.on("error", (err) => {
						this.onError(err)
					})
				}
				addSignal(this, signal)
			}
			onConnect(abort, context2) {
				if (!this.callback) {
					throw new RequestAbortedError()
				}
				this.abort = abort
				this.context = context2
			}
			onHeaders(statusCode, rawHeaders, resume, statusMessage) {
				const { callback, opaque, abort, context: context2, responseHeaders, highWaterMark } = this
				const headers =
					responseHeaders === "raw"
						? util.parseRawHeaders(rawHeaders)
						: util.parseHeaders(rawHeaders)
				if (statusCode < 200) {
					if (this.onInfo) {
						this.onInfo({ statusCode, headers })
					}
					return
				}
				const parsedHeaders = responseHeaders === "raw" ? util.parseHeaders(rawHeaders) : headers
				const contentType = parsedHeaders["content-type"]
				const body = new Readable({ resume, abort, contentType, highWaterMark })
				this.callback = null
				this.res = body
				if (callback !== null) {
					if (this.throwOnError && statusCode >= 400) {
						this.runInAsyncScope(getResolveErrorBodyCallback, null, {
							callback,
							body,
							contentType,
							statusCode,
							statusMessage,
							headers,
						})
					} else {
						this.runInAsyncScope(callback, null, null, {
							statusCode,
							headers,
							trailers: this.trailers,
							opaque,
							body,
							context: context2,
						})
					}
				}
			}
			onData(chunk) {
				const { res } = this
				return res.push(chunk)
			}
			onComplete(trailers) {
				const { res } = this
				removeSignal(this)
				util.parseHeaders(trailers, this.trailers)
				res.push(null)
			}
			onError(err) {
				const { res, callback, body, opaque } = this
				removeSignal(this)
				if (callback) {
					this.callback = null
					queueMicrotask(() => {
						this.runInAsyncScope(callback, null, err, { opaque })
					})
				}
				if (res) {
					this.res = null
					queueMicrotask(() => {
						util.destroy(res, err)
					})
				}
				if (body) {
					this.body = null
					util.destroy(body, err)
				}
			}
		}
		function request(opts, callback) {
			if (callback === void 0) {
				return new Promise((resolve, reject) => {
					request.call(this, opts, (err, data) => {
						return err ? reject(err) : resolve(data)
					})
				})
			}
			try {
				this.dispatch(opts, new RequestHandler(opts, callback))
			} catch (err) {
				if (typeof callback !== "function") {
					throw err
				}
				const opaque = opts && opts.opaque
				queueMicrotask(() => callback(err, { opaque }))
			}
		}
		module2.exports = request
		module2.exports.RequestHandler = RequestHandler
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/api-stream.js
var require_api_stream = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/api-stream.js"(
		exports2,
		module2
	) {
		"use strict"
		var { finished, PassThrough } = require("node:stream")
		var { InvalidArgumentError, InvalidReturnValueError, RequestAbortedError } = require_errors()
		var util = require_util()
		var { getResolveErrorBodyCallback } = require_util3()
		var { AsyncResource } = require("node:async_hooks")
		var { addSignal, removeSignal } = require_abort_signal()
		var StreamHandler = class extends AsyncResource {
			constructor(opts, factory, callback) {
				if (!opts || typeof opts !== "object") {
					throw new InvalidArgumentError("invalid opts")
				}
				const { signal, method, opaque, body, onInfo, responseHeaders, throwOnError } = opts
				try {
					if (typeof callback !== "function") {
						throw new InvalidArgumentError("invalid callback")
					}
					if (typeof factory !== "function") {
						throw new InvalidArgumentError("invalid factory")
					}
					if (
						signal &&
						typeof signal.on !== "function" &&
						typeof signal.addEventListener !== "function"
					) {
						throw new InvalidArgumentError("signal must be an EventEmitter or EventTarget")
					}
					if (method === "CONNECT") {
						throw new InvalidArgumentError("invalid method")
					}
					if (onInfo && typeof onInfo !== "function") {
						throw new InvalidArgumentError("invalid onInfo callback")
					}
					super("UNDICI_STREAM")
				} catch (err) {
					if (util.isStream(body)) {
						util.destroy(body.on("error", util.nop), err)
					}
					throw err
				}
				this.responseHeaders = responseHeaders || null
				this.opaque = opaque || null
				this.factory = factory
				this.callback = callback
				this.res = null
				this.abort = null
				this.context = null
				this.trailers = null
				this.body = body
				this.onInfo = onInfo || null
				this.throwOnError = throwOnError || false
				if (util.isStream(body)) {
					body.on("error", (err) => {
						this.onError(err)
					})
				}
				addSignal(this, signal)
			}
			onConnect(abort, context2) {
				if (!this.callback) {
					throw new RequestAbortedError()
				}
				this.abort = abort
				this.context = context2
			}
			onHeaders(statusCode, rawHeaders, resume, statusMessage) {
				const { factory, opaque, context: context2, callback, responseHeaders } = this
				const headers =
					responseHeaders === "raw"
						? util.parseRawHeaders(rawHeaders)
						: util.parseHeaders(rawHeaders)
				if (statusCode < 200) {
					if (this.onInfo) {
						this.onInfo({ statusCode, headers })
					}
					return
				}
				this.factory = null
				let res
				if (this.throwOnError && statusCode >= 400) {
					const parsedHeaders = responseHeaders === "raw" ? util.parseHeaders(rawHeaders) : headers
					const contentType = parsedHeaders["content-type"]
					res = new PassThrough()
					this.callback = null
					this.runInAsyncScope(getResolveErrorBodyCallback, null, {
						callback,
						body: res,
						contentType,
						statusCode,
						statusMessage,
						headers,
					})
				} else {
					if (factory === null) {
						return
					}
					res = this.runInAsyncScope(factory, null, {
						statusCode,
						headers,
						opaque,
						context: context2,
					})
					if (
						!res ||
						typeof res.write !== "function" ||
						typeof res.end !== "function" ||
						typeof res.on !== "function"
					) {
						throw new InvalidReturnValueError("expected Writable")
					}
					finished(res, { readable: false }, (err) => {
						const { callback: callback2, res: res2, opaque: opaque2, trailers, abort } = this
						this.res = null
						if (err || !res2.readable) {
							util.destroy(res2, err)
						}
						this.callback = null
						this.runInAsyncScope(callback2, null, err || null, { opaque: opaque2, trailers })
						if (err) {
							abort()
						}
					})
				}
				res.on("drain", resume)
				this.res = res
				const needDrain =
					res.writableNeedDrain !== void 0
						? res.writableNeedDrain
						: res._writableState && res._writableState.needDrain
				return needDrain !== true
			}
			onData(chunk) {
				const { res } = this
				return res ? res.write(chunk) : true
			}
			onComplete(trailers) {
				const { res } = this
				removeSignal(this)
				if (!res) {
					return
				}
				this.trailers = util.parseHeaders(trailers)
				res.end()
			}
			onError(err) {
				const { res, callback, opaque, body } = this
				removeSignal(this)
				this.factory = null
				if (res) {
					this.res = null
					util.destroy(res, err)
				} else if (callback) {
					this.callback = null
					queueMicrotask(() => {
						this.runInAsyncScope(callback, null, err, { opaque })
					})
				}
				if (body) {
					this.body = null
					util.destroy(body, err)
				}
			}
		}
		function stream(opts, factory, callback) {
			if (callback === void 0) {
				return new Promise((resolve, reject) => {
					stream.call(this, opts, factory, (err, data) => {
						return err ? reject(err) : resolve(data)
					})
				})
			}
			try {
				this.dispatch(opts, new StreamHandler(opts, factory, callback))
			} catch (err) {
				if (typeof callback !== "function") {
					throw err
				}
				const opaque = opts && opts.opaque
				queueMicrotask(() => callback(err, { opaque }))
			}
		}
		module2.exports = stream
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/api-pipeline.js
var require_api_pipeline = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/api-pipeline.js"(
		exports2,
		module2
	) {
		"use strict"
		var { Readable, Duplex, PassThrough } = require("node:stream")
		var { InvalidArgumentError, InvalidReturnValueError, RequestAbortedError } = require_errors()
		var util = require_util()
		var { AsyncResource } = require("node:async_hooks")
		var { addSignal, removeSignal } = require_abort_signal()
		var assert = require("node:assert")
		var kResume = Symbol("resume")
		var PipelineRequest = class extends Readable {
			constructor() {
				super({ autoDestroy: true })
				this[kResume] = null
			}
			_read() {
				const { [kResume]: resume } = this
				if (resume) {
					this[kResume] = null
					resume()
				}
			}
			_destroy(err, callback) {
				this._read()
				callback(err)
			}
		}
		var PipelineResponse = class extends Readable {
			constructor(resume) {
				super({ autoDestroy: true })
				this[kResume] = resume
			}
			_read() {
				this[kResume]()
			}
			_destroy(err, callback) {
				if (!err && !this._readableState.endEmitted) {
					err = new RequestAbortedError()
				}
				callback(err)
			}
		}
		var PipelineHandler = class extends AsyncResource {
			constructor(opts, handler) {
				if (!opts || typeof opts !== "object") {
					throw new InvalidArgumentError("invalid opts")
				}
				if (typeof handler !== "function") {
					throw new InvalidArgumentError("invalid handler")
				}
				const { signal, method, opaque, onInfo, responseHeaders } = opts
				if (
					signal &&
					typeof signal.on !== "function" &&
					typeof signal.addEventListener !== "function"
				) {
					throw new InvalidArgumentError("signal must be an EventEmitter or EventTarget")
				}
				if (method === "CONNECT") {
					throw new InvalidArgumentError("invalid method")
				}
				if (onInfo && typeof onInfo !== "function") {
					throw new InvalidArgumentError("invalid onInfo callback")
				}
				super("UNDICI_PIPELINE")
				this.opaque = opaque || null
				this.responseHeaders = responseHeaders || null
				this.handler = handler
				this.abort = null
				this.context = null
				this.onInfo = onInfo || null
				this.req = new PipelineRequest().on("error", util.nop)
				this.ret = new Duplex({
					readableObjectMode: opts.objectMode,
					autoDestroy: true,
					read: () => {
						const { body } = this
						if (body && body.resume) {
							body.resume()
						}
					},
					write: (chunk, encoding, callback) => {
						const { req } = this
						if (req.push(chunk, encoding) || req._readableState.destroyed) {
							callback()
						} else {
							req[kResume] = callback
						}
					},
					destroy: (err, callback) => {
						const { body, req, res, ret, abort } = this
						if (!err && !ret._readableState.endEmitted) {
							err = new RequestAbortedError()
						}
						if (abort && err) {
							abort()
						}
						util.destroy(body, err)
						util.destroy(req, err)
						util.destroy(res, err)
						removeSignal(this)
						callback(err)
					},
				}).on("prefinish", () => {
					const { req } = this
					req.push(null)
				})
				this.res = null
				addSignal(this, signal)
			}
			onConnect(abort, context2) {
				const { ret, res } = this
				assert(!res, "pipeline cannot be retried")
				if (ret.destroyed) {
					throw new RequestAbortedError()
				}
				this.abort = abort
				this.context = context2
			}
			onHeaders(statusCode, rawHeaders, resume) {
				const { opaque, handler, context: context2 } = this
				if (statusCode < 200) {
					if (this.onInfo) {
						const headers =
							this.responseHeaders === "raw"
								? util.parseRawHeaders(rawHeaders)
								: util.parseHeaders(rawHeaders)
						this.onInfo({ statusCode, headers })
					}
					return
				}
				this.res = new PipelineResponse(resume)
				let body
				try {
					this.handler = null
					const headers =
						this.responseHeaders === "raw"
							? util.parseRawHeaders(rawHeaders)
							: util.parseHeaders(rawHeaders)
					body = this.runInAsyncScope(handler, null, {
						statusCode,
						headers,
						opaque,
						body: this.res,
						context: context2,
					})
				} catch (err) {
					this.res.on("error", util.nop)
					throw err
				}
				if (!body || typeof body.on !== "function") {
					throw new InvalidReturnValueError("expected Readable")
				}
				body
					.on("data", (chunk) => {
						const { ret, body: body2 } = this
						if (!ret.push(chunk) && body2.pause) {
							body2.pause()
						}
					})
					.on("error", (err) => {
						const { ret } = this
						util.destroy(ret, err)
					})
					.on("end", () => {
						const { ret } = this
						ret.push(null)
					})
					.on("close", () => {
						const { ret } = this
						if (!ret._readableState.ended) {
							util.destroy(ret, new RequestAbortedError())
						}
					})
				this.body = body
			}
			onData(chunk) {
				const { res } = this
				return res.push(chunk)
			}
			onComplete(trailers) {
				const { res } = this
				res.push(null)
			}
			onError(err) {
				const { ret } = this
				this.handler = null
				util.destroy(ret, err)
			}
		}
		function pipeline(opts, handler) {
			try {
				const pipelineHandler = new PipelineHandler(opts, handler)
				this.dispatch({ ...opts, body: pipelineHandler.req }, pipelineHandler)
				return pipelineHandler.ret
			} catch (err) {
				return new PassThrough().destroy(err)
			}
		}
		module2.exports = pipeline
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/api-upgrade.js
var require_api_upgrade = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/api-upgrade.js"(
		exports2,
		module2
	) {
		"use strict"
		var { InvalidArgumentError, RequestAbortedError, SocketError } = require_errors()
		var { AsyncResource } = require("node:async_hooks")
		var util = require_util()
		var { addSignal, removeSignal } = require_abort_signal()
		var assert = require("node:assert")
		var UpgradeHandler = class extends AsyncResource {
			constructor(opts, callback) {
				if (!opts || typeof opts !== "object") {
					throw new InvalidArgumentError("invalid opts")
				}
				if (typeof callback !== "function") {
					throw new InvalidArgumentError("invalid callback")
				}
				const { signal, opaque, responseHeaders } = opts
				if (
					signal &&
					typeof signal.on !== "function" &&
					typeof signal.addEventListener !== "function"
				) {
					throw new InvalidArgumentError("signal must be an EventEmitter or EventTarget")
				}
				super("UNDICI_UPGRADE")
				this.responseHeaders = responseHeaders || null
				this.opaque = opaque || null
				this.callback = callback
				this.abort = null
				this.context = null
				addSignal(this, signal)
			}
			onConnect(abort, context2) {
				if (!this.callback) {
					throw new RequestAbortedError()
				}
				this.abort = abort
				this.context = null
			}
			onHeaders() {
				throw new SocketError("bad upgrade", null)
			}
			onUpgrade(statusCode, rawHeaders, socket) {
				const { callback, opaque, context: context2 } = this
				assert.strictEqual(statusCode, 101)
				removeSignal(this)
				this.callback = null
				const headers =
					this.responseHeaders === "raw"
						? util.parseRawHeaders(rawHeaders)
						: util.parseHeaders(rawHeaders)
				this.runInAsyncScope(callback, null, null, {
					headers,
					socket,
					opaque,
					context: context2,
				})
			}
			onError(err) {
				const { callback, opaque } = this
				removeSignal(this)
				if (callback) {
					this.callback = null
					queueMicrotask(() => {
						this.runInAsyncScope(callback, null, err, { opaque })
					})
				}
			}
		}
		function upgrade(opts, callback) {
			if (callback === void 0) {
				return new Promise((resolve, reject) => {
					upgrade.call(this, opts, (err, data) => {
						return err ? reject(err) : resolve(data)
					})
				})
			}
			try {
				const upgradeHandler = new UpgradeHandler(opts, callback)
				this.dispatch(
					{
						...opts,
						method: opts.method || "GET",
						upgrade: opts.protocol || "Websocket",
					},
					upgradeHandler
				)
			} catch (err) {
				if (typeof callback !== "function") {
					throw err
				}
				const opaque = opts && opts.opaque
				queueMicrotask(() => callback(err, { opaque }))
			}
		}
		module2.exports = upgrade
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/api-connect.js
var require_api_connect = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/api-connect.js"(
		exports2,
		module2
	) {
		"use strict"
		var { AsyncResource } = require("node:async_hooks")
		var { InvalidArgumentError, RequestAbortedError, SocketError } = require_errors()
		var util = require_util()
		var { addSignal, removeSignal } = require_abort_signal()
		var ConnectHandler = class extends AsyncResource {
			constructor(opts, callback) {
				if (!opts || typeof opts !== "object") {
					throw new InvalidArgumentError("invalid opts")
				}
				if (typeof callback !== "function") {
					throw new InvalidArgumentError("invalid callback")
				}
				const { signal, opaque, responseHeaders } = opts
				if (
					signal &&
					typeof signal.on !== "function" &&
					typeof signal.addEventListener !== "function"
				) {
					throw new InvalidArgumentError("signal must be an EventEmitter or EventTarget")
				}
				super("UNDICI_CONNECT")
				this.opaque = opaque || null
				this.responseHeaders = responseHeaders || null
				this.callback = callback
				this.abort = null
				addSignal(this, signal)
			}
			onConnect(abort, context2) {
				if (!this.callback) {
					throw new RequestAbortedError()
				}
				this.abort = abort
				this.context = context2
			}
			onHeaders() {
				throw new SocketError("bad connect", null)
			}
			onUpgrade(statusCode, rawHeaders, socket) {
				const { callback, opaque, context: context2 } = this
				removeSignal(this)
				this.callback = null
				let headers = rawHeaders
				if (headers != undefined) {
					headers =
						this.responseHeaders === "raw"
							? util.parseRawHeaders(rawHeaders)
							: util.parseHeaders(rawHeaders)
				}
				this.runInAsyncScope(callback, null, null, {
					statusCode,
					headers,
					socket,
					opaque,
					context: context2,
				})
			}
			onError(err) {
				const { callback, opaque } = this
				removeSignal(this)
				if (callback) {
					this.callback = null
					queueMicrotask(() => {
						this.runInAsyncScope(callback, null, err, { opaque })
					})
				}
			}
		}
		function connect(opts, callback) {
			if (callback === void 0) {
				return new Promise((resolve, reject) => {
					connect.call(this, opts, (err, data) => {
						return err ? reject(err) : resolve(data)
					})
				})
			}
			try {
				const connectHandler = new ConnectHandler(opts, callback)
				this.dispatch({ ...opts, method: "CONNECT" }, connectHandler)
			} catch (err) {
				if (typeof callback !== "function") {
					throw err
				}
				const opaque = opts && opts.opaque
				queueMicrotask(() => callback(err, { opaque }))
			}
		}
		module2.exports = connect
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/index.js
var require_api = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/api/index.js"(
		exports2,
		module2
	) {
		"use strict"
		module2.exports.request = require_api_request()
		module2.exports.stream = require_api_stream()
		module2.exports.pipeline = require_api_pipeline()
		module2.exports.upgrade = require_api_upgrade()
		module2.exports.connect = require_api_connect()
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/mock-errors.js
var require_mock_errors = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/mock-errors.js"(
		exports2,
		module2
	) {
		"use strict"
		var { UndiciError } = require_errors()
		var MockNotMatchedError = class _MockNotMatchedError extends UndiciError {
			constructor(message) {
				super(message)
				Error.captureStackTrace(this, _MockNotMatchedError)
				this.name = "MockNotMatchedError"
				this.message = message || "The request does not match any registered mock dispatches"
				this.code = "UND_MOCK_ERR_MOCK_NOT_MATCHED"
			}
		}
		module2.exports = {
			MockNotMatchedError,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/mock-symbols.js
var require_mock_symbols = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/mock-symbols.js"(
		exports2,
		module2
	) {
		"use strict"
		module2.exports = {
			kAgent: Symbol("agent"),
			kOptions: Symbol("options"),
			kFactory: Symbol("factory"),
			kDispatches: Symbol("dispatches"),
			kDispatchKey: Symbol("dispatch key"),
			kDefaultHeaders: Symbol("default headers"),
			kDefaultTrailers: Symbol("default trailers"),
			kContentLength: Symbol("content length"),
			kMockAgent: Symbol("mock agent"),
			kMockAgentSet: Symbol("mock agent set"),
			kMockAgentGet: Symbol("mock agent get"),
			kMockDispatch: Symbol("mock dispatch"),
			kClose: Symbol("close"),
			kOriginalClose: Symbol("original agent close"),
			kOrigin: Symbol("origin"),
			kIsMockActive: Symbol("is mock active"),
			kNetConnect: Symbol("net connect"),
			kGetNetConnect: Symbol("get net connect"),
			kConnected: Symbol("connected"),
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/mock-utils.js
var require_mock_utils = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/mock-utils.js"(
		exports2,
		module2
	) {
		"use strict"
		var { MockNotMatchedError } = require_mock_errors()
		var { kDispatches, kMockAgent, kOriginalDispatch, kOrigin, kGetNetConnect } =
			require_mock_symbols()
		var { buildURL, nop } = require_util()
		var { STATUS_CODES } = require("node:http")
		var {
			types: { isPromise },
		} = require("node:util")
		function matchValue(match, value) {
			if (typeof match === "string") {
				return match === value
			}
			if (match instanceof RegExp) {
				return match.test(value)
			}
			if (typeof match === "function") {
				return match(value) === true
			}
			return false
		}
		function lowerCaseEntries(headers) {
			return Object.fromEntries(
				Object.entries(headers).map(([headerName, headerValue]) => {
					return [headerName.toLocaleLowerCase(), headerValue]
				})
			)
		}
		function getHeaderByName(headers, key) {
			if (Array.isArray(headers)) {
				for (let i = 0; i < headers.length; i += 2) {
					if (headers[i].toLocaleLowerCase() === key.toLocaleLowerCase()) {
						return headers[i + 1]
					}
				}
				return void 0
			} else if (typeof headers.get === "function") {
				return headers.get(key)
			} else {
				return lowerCaseEntries(headers)[key.toLocaleLowerCase()]
			}
		}
		function buildHeadersFromArray(headers) {
			const clone2 = [...headers]
			const entries = []
			for (let index2 = 0; index2 < clone2.length; index2 += 2) {
				entries.push([clone2[index2], clone2[index2 + 1]])
			}
			return Object.fromEntries(entries)
		}
		function matchHeaders(mockDispatch2, headers) {
			if (typeof mockDispatch2.headers === "function") {
				if (Array.isArray(headers)) {
					headers = buildHeadersFromArray(headers)
				}
				return mockDispatch2.headers(headers ? lowerCaseEntries(headers) : {})
			}
			if (typeof mockDispatch2.headers === "undefined") {
				return true
			}
			if (typeof headers !== "object" || typeof mockDispatch2.headers !== "object") {
				return false
			}
			for (const [matchHeaderName, matchHeaderValue] of Object.entries(mockDispatch2.headers)) {
				const headerValue = getHeaderByName(headers, matchHeaderName)
				if (!matchValue(matchHeaderValue, headerValue)) {
					return false
				}
			}
			return true
		}
		function safeUrl(path) {
			if (typeof path !== "string") {
				return path
			}
			const pathSegments = path.split("?")
			if (pathSegments.length !== 2) {
				return path
			}
			const qp = new URLSearchParams(pathSegments.pop())
			qp.sort()
			return [...pathSegments, qp.toString()].join("?")
		}
		function matchKey(mockDispatch2, { path, method, body, headers }) {
			const pathMatch = matchValue(mockDispatch2.path, path)
			const methodMatch = matchValue(mockDispatch2.method, method)
			const bodyMatch =
				typeof mockDispatch2.body !== "undefined" ? matchValue(mockDispatch2.body, body) : true
			const headersMatch = matchHeaders(mockDispatch2, headers)
			return pathMatch && methodMatch && bodyMatch && headersMatch
		}
		function getResponseData(data) {
			if (Buffer.isBuffer(data)) {
				return data
			} else if (typeof data === "object") {
				return JSON.stringify(data)
			} else {
				return data.toString()
			}
		}
		function getMockDispatch(mockDispatches, key) {
			const basePath = key.query ? buildURL(key.path, key.query) : key.path
			const resolvedPath = typeof basePath === "string" ? safeUrl(basePath) : basePath
			let matchedMockDispatches = mockDispatches
				.filter(({ consumed }) => !consumed)
				.filter(({ path }) => matchValue(safeUrl(path), resolvedPath))
			if (matchedMockDispatches.length === 0) {
				throw new MockNotMatchedError(`Mock dispatch not matched for path '${resolvedPath}'`)
			}
			matchedMockDispatches = matchedMockDispatches.filter(({ method }) =>
				matchValue(method, key.method)
			)
			if (matchedMockDispatches.length === 0) {
				throw new MockNotMatchedError(`Mock dispatch not matched for method '${key.method}'`)
			}
			matchedMockDispatches = matchedMockDispatches.filter(({ body }) =>
				typeof body !== "undefined" ? matchValue(body, key.body) : true
			)
			if (matchedMockDispatches.length === 0) {
				throw new MockNotMatchedError(`Mock dispatch not matched for body '${key.body}'`)
			}
			matchedMockDispatches = matchedMockDispatches.filter((mockDispatch2) =>
				matchHeaders(mockDispatch2, key.headers)
			)
			if (matchedMockDispatches.length === 0) {
				throw new MockNotMatchedError(
					`Mock dispatch not matched for headers '${
						typeof key.headers === "object" ? JSON.stringify(key.headers) : key.headers
					}'`
				)
			}
			return matchedMockDispatches[0]
		}
		function addMockDispatch(mockDispatches, key, data) {
			const baseData = { timesInvoked: 0, times: 1, persist: false, consumed: false }
			const replyData = typeof data === "function" ? { callback: data } : { ...data }
			const newMockDispatch = {
				...baseData,
				...key,
				pending: true,
				data: { error: null, ...replyData },
			}
			mockDispatches.push(newMockDispatch)
			return newMockDispatch
		}
		function deleteMockDispatch(mockDispatches, key) {
			const index2 = mockDispatches.findIndex((dispatch) => {
				if (!dispatch.consumed) {
					return false
				}
				return matchKey(dispatch, key)
			})
			if (index2 !== -1) {
				mockDispatches.splice(index2, 1)
			}
		}
		function buildKey(opts) {
			const { path, method, body, headers, query } = opts
			return {
				path,
				method,
				body,
				headers,
				query,
			}
		}
		function generateKeyValues(data) {
			return Object.entries(data).reduce(
				(keyValuePairs, [key, value]) => [
					...keyValuePairs,
					Buffer.from(`${key}`),
					Array.isArray(value) ? value.map((x) => Buffer.from(`${x}`)) : Buffer.from(`${value}`),
				],
				[]
			)
		}
		function getStatusText(statusCode) {
			return STATUS_CODES[statusCode] || "unknown"
		}
		async function getResponse(body) {
			const buffers = []
			for await (const data of body) {
				buffers.push(data)
			}
			return Buffer.concat(buffers).toString("utf8")
		}
		function mockDispatch(opts, handler) {
			const key = buildKey(opts)
			const mockDispatch2 = getMockDispatch(this[kDispatches], key)
			mockDispatch2.timesInvoked++
			if (mockDispatch2.data.callback) {
				mockDispatch2.data = { ...mockDispatch2.data, ...mockDispatch2.data.callback(opts) }
			}
			const {
				data: { statusCode, data, headers, trailers, error },
				delay,
				persist,
			} = mockDispatch2
			const { timesInvoked, times } = mockDispatch2
			mockDispatch2.consumed = !persist && timesInvoked >= times
			mockDispatch2.pending = timesInvoked < times
			if (error !== null) {
				deleteMockDispatch(this[kDispatches], key)
				handler.onError(error)
				return true
			}
			if (typeof delay === "number" && delay > 0) {
				setTimeout(() => {
					handleReply(this[kDispatches])
				}, delay)
			} else {
				handleReply(this[kDispatches])
			}
			function handleReply(mockDispatches, _data = data) {
				const optsHeaders = Array.isArray(opts.headers)
					? buildHeadersFromArray(opts.headers)
					: opts.headers
				const body = typeof _data === "function" ? _data({ ...opts, headers: optsHeaders }) : _data
				if (isPromise(body)) {
					body.then((newData) => handleReply(mockDispatches, newData))
					return
				}
				const responseData = getResponseData(body)
				const responseHeaders = generateKeyValues(headers)
				const responseTrailers = generateKeyValues(trailers)
				handler.abort = nop
				handler.onHeaders(statusCode, responseHeaders, resume, getStatusText(statusCode))
				handler.onData(Buffer.from(responseData))
				handler.onComplete(responseTrailers)
				deleteMockDispatch(mockDispatches, key)
			}
			function resume() {}
			return true
		}
		function buildMockDispatch() {
			const agent = this[kMockAgent]
			const origin = this[kOrigin]
			const originalDispatch = this[kOriginalDispatch]
			return function dispatch(opts, handler) {
				if (agent.isMockActive) {
					try {
						mockDispatch.call(this, opts, handler)
					} catch (error) {
						if (error instanceof MockNotMatchedError) {
							const netConnect = agent[kGetNetConnect]()
							if (netConnect === false) {
								throw new MockNotMatchedError(
									`${error.message}: subsequent request to origin ${origin} was not allowed (net.connect disabled)`
								)
							}
							if (checkNetConnect(netConnect, origin)) {
								originalDispatch.call(this, opts, handler)
							} else {
								throw new MockNotMatchedError(
									`${error.message}: subsequent request to origin ${origin} was not allowed (net.connect is not enabled for this origin)`
								)
							}
						} else {
							throw error
						}
					}
				} else {
					originalDispatch.call(this, opts, handler)
				}
			}
		}
		function checkNetConnect(netConnect, origin) {
			const url = new URL(origin)
			if (netConnect === true) {
				return true
			} else if (
				Array.isArray(netConnect) &&
				netConnect.some((matcher) => matchValue(matcher, url.host))
			) {
				return true
			}
			return false
		}
		function buildMockOptions(opts) {
			if (opts) {
				const { agent, ...mockOptions } = opts
				return mockOptions
			}
		}
		module2.exports = {
			getResponseData,
			getMockDispatch,
			addMockDispatch,
			deleteMockDispatch,
			buildKey,
			generateKeyValues,
			matchValue,
			getResponse,
			getStatusText,
			mockDispatch,
			buildMockDispatch,
			checkNetConnect,
			buildMockOptions,
			getHeaderByName,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/mock-interceptor.js
var require_mock_interceptor = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/mock-interceptor.js"(
		exports2,
		module2
	) {
		"use strict"
		var { getResponseData, buildKey, addMockDispatch } = require_mock_utils()
		var {
			kDispatches,
			kDispatchKey,
			kDefaultHeaders,
			kDefaultTrailers,
			kContentLength,
			kMockDispatch,
		} = require_mock_symbols()
		var { InvalidArgumentError } = require_errors()
		var { buildURL } = require_util()
		var MockScope = class {
			constructor(mockDispatch) {
				this[kMockDispatch] = mockDispatch
			}
			/**
			 * Delay a reply by a set amount in ms.
			 */
			delay(waitInMs) {
				if (typeof waitInMs !== "number" || !Number.isInteger(waitInMs) || waitInMs <= 0) {
					throw new InvalidArgumentError("waitInMs must be a valid integer > 0")
				}
				this[kMockDispatch].delay = waitInMs
				return this
			}
			/**
			 * For a defined reply, never mark as consumed.
			 */
			persist() {
				this[kMockDispatch].persist = true
				return this
			}
			/**
			 * Allow one to define a reply for a set amount of matching requests.
			 */
			times(repeatTimes) {
				if (typeof repeatTimes !== "number" || !Number.isInteger(repeatTimes) || repeatTimes <= 0) {
					throw new InvalidArgumentError("repeatTimes must be a valid integer > 0")
				}
				this[kMockDispatch].times = repeatTimes
				return this
			}
		}
		var MockInterceptor = class {
			constructor(opts, mockDispatches) {
				if (typeof opts !== "object") {
					throw new InvalidArgumentError("opts must be an object")
				}
				if (typeof opts.path === "undefined") {
					throw new InvalidArgumentError("opts.path must be defined")
				}
				if (typeof opts.method === "undefined") {
					opts.method = "GET"
				}
				if (typeof opts.path === "string") {
					if (opts.query) {
						opts.path = buildURL(opts.path, opts.query)
					} else {
						const parsedURL = new URL(opts.path, "data://")
						opts.path = parsedURL.pathname + parsedURL.search
					}
				}
				if (typeof opts.method === "string") {
					opts.method = opts.method.toUpperCase()
				}
				this[kDispatchKey] = buildKey(opts)
				this[kDispatches] = mockDispatches
				this[kDefaultHeaders] = {}
				this[kDefaultTrailers] = {}
				this[kContentLength] = false
			}
			createMockScopeDispatchData(statusCode, data, responseOptions = {}) {
				const responseData = getResponseData(data)
				const contentLength = this[kContentLength] ? { "content-length": responseData.length } : {}
				const headers = { ...this[kDefaultHeaders], ...contentLength, ...responseOptions.headers }
				const trailers = { ...this[kDefaultTrailers], ...responseOptions.trailers }
				return { statusCode, data, headers, trailers }
			}
			validateReplyParameters(statusCode, data, responseOptions) {
				if (typeof statusCode === "undefined") {
					throw new InvalidArgumentError("statusCode must be defined")
				}
				if (typeof data === "undefined") {
					throw new InvalidArgumentError("data must be defined")
				}
				if (typeof responseOptions !== "object") {
					throw new InvalidArgumentError("responseOptions must be an object")
				}
			}
			/**
			 * Mock an undici request with a defined reply.
			 */
			reply(replyData) {
				if (typeof replyData === "function") {
					const wrappedDefaultsCallback = (opts) => {
						const resolvedData = replyData(opts)
						if (typeof resolvedData !== "object") {
							throw new InvalidArgumentError("reply options callback must return an object")
						}
						const {
							statusCode: statusCode2,
							data: data2 = "",
							responseOptions: responseOptions2 = {},
						} = resolvedData
						this.validateReplyParameters(statusCode2, data2, responseOptions2)
						return {
							...this.createMockScopeDispatchData(statusCode2, data2, responseOptions2),
						}
					}
					const newMockDispatch2 = addMockDispatch(
						this[kDispatches],
						this[kDispatchKey],
						wrappedDefaultsCallback
					)
					return new MockScope(newMockDispatch2)
				}
				const [statusCode, data = "", responseOptions = {}] = [...arguments]
				this.validateReplyParameters(statusCode, data, responseOptions)
				const dispatchData = this.createMockScopeDispatchData(statusCode, data, responseOptions)
				const newMockDispatch = addMockDispatch(this[kDispatches], this[kDispatchKey], dispatchData)
				return new MockScope(newMockDispatch)
			}
			/**
			 * Mock an undici request with a defined error.
			 */
			replyWithError(error) {
				if (typeof error === "undefined") {
					throw new InvalidArgumentError("error must be defined")
				}
				const newMockDispatch = addMockDispatch(this[kDispatches], this[kDispatchKey], { error })
				return new MockScope(newMockDispatch)
			}
			/**
			 * Set default reply headers on the interceptor for subsequent replies
			 */
			defaultReplyHeaders(headers) {
				if (typeof headers === "undefined") {
					throw new InvalidArgumentError("headers must be defined")
				}
				this[kDefaultHeaders] = headers
				return this
			}
			/**
			 * Set default reply trailers on the interceptor for subsequent replies
			 */
			defaultReplyTrailers(trailers) {
				if (typeof trailers === "undefined") {
					throw new InvalidArgumentError("trailers must be defined")
				}
				this[kDefaultTrailers] = trailers
				return this
			}
			/**
			 * Set reply content length header for replies on the interceptor
			 */
			replyContentLength() {
				this[kContentLength] = true
				return this
			}
		}
		module2.exports.MockInterceptor = MockInterceptor
		module2.exports.MockScope = MockScope
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/mock-client.js
var require_mock_client = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/mock-client.js"(
		exports2,
		module2
	) {
		"use strict"
		var { promisify } = require("node:util")
		var Client = require_client()
		var { buildMockDispatch } = require_mock_utils()
		var {
			kDispatches,
			kMockAgent,
			kClose,
			kOriginalClose,
			kOrigin,
			kOriginalDispatch,
			kConnected,
		} = require_mock_symbols()
		var { MockInterceptor } = require_mock_interceptor()
		var Symbols = require_symbols()
		var { InvalidArgumentError } = require_errors()
		var MockClient = class extends Client {
			constructor(origin, opts) {
				super(origin, opts)
				if (!opts || !opts.agent || typeof opts.agent.dispatch !== "function") {
					throw new InvalidArgumentError("Argument opts.agent must implement Agent")
				}
				this[kMockAgent] = opts.agent
				this[kOrigin] = origin
				this[kDispatches] = []
				this[kConnected] = 1
				this[kOriginalDispatch] = this.dispatch
				this[kOriginalClose] = this.close.bind(this)
				this.dispatch = buildMockDispatch.call(this)
				this.close = this[kClose]
			}
			get [Symbols.kConnected]() {
				return this[kConnected]
			}
			/**
			 * Sets up the base interceptor for mocking replies from undici.
			 */
			intercept(opts) {
				return new MockInterceptor(opts, this[kDispatches])
			}
			async [kClose]() {
				await promisify(this[kOriginalClose])()
				this[kConnected] = 0
				this[kMockAgent][Symbols.kClients].delete(this[kOrigin])
			}
		}
		module2.exports = MockClient
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/mock-pool.js
var require_mock_pool = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/mock-pool.js"(
		exports2,
		module2
	) {
		"use strict"
		var { promisify } = require("node:util")
		var Pool = require_pool()
		var { buildMockDispatch } = require_mock_utils()
		var {
			kDispatches,
			kMockAgent,
			kClose,
			kOriginalClose,
			kOrigin,
			kOriginalDispatch,
			kConnected,
		} = require_mock_symbols()
		var { MockInterceptor } = require_mock_interceptor()
		var Symbols = require_symbols()
		var { InvalidArgumentError } = require_errors()
		var MockPool = class extends Pool {
			constructor(origin, opts) {
				super(origin, opts)
				if (!opts || !opts.agent || typeof opts.agent.dispatch !== "function") {
					throw new InvalidArgumentError("Argument opts.agent must implement Agent")
				}
				this[kMockAgent] = opts.agent
				this[kOrigin] = origin
				this[kDispatches] = []
				this[kConnected] = 1
				this[kOriginalDispatch] = this.dispatch
				this[kOriginalClose] = this.close.bind(this)
				this.dispatch = buildMockDispatch.call(this)
				this.close = this[kClose]
			}
			get [Symbols.kConnected]() {
				return this[kConnected]
			}
			/**
			 * Sets up the base interceptor for mocking replies from undici.
			 */
			intercept(opts) {
				return new MockInterceptor(opts, this[kDispatches])
			}
			async [kClose]() {
				await promisify(this[kOriginalClose])()
				this[kConnected] = 0
				this[kMockAgent][Symbols.kClients].delete(this[kOrigin])
			}
		}
		module2.exports = MockPool
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/pluralizer.js
var require_pluralizer = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/pluralizer.js"(
		exports2,
		module2
	) {
		"use strict"
		var singulars = {
			pronoun: "it",
			is: "is",
			was: "was",
			this: "this",
		}
		var plurals = {
			pronoun: "they",
			is: "are",
			was: "were",
			this: "these",
		}
		module2.exports = class Pluralizer {
			constructor(singular, plural) {
				this.singular = singular
				this.plural = plural
			}
			pluralize(count) {
				const one = count === 1
				const keys = one ? singulars : plurals
				const noun = one ? this.singular : this.plural
				return { ...keys, count, noun }
			}
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/pending-interceptors-formatter.js
var require_pending_interceptors_formatter = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/pending-interceptors-formatter.js"(
		exports2,
		module2
	) {
		"use strict"
		var { Transform } = require("node:stream")
		var { Console } = require("node:console")
		module2.exports = class PendingInterceptorsFormatter {
			constructor({ disableColors } = {}) {
				this.transform = new Transform({
					transform(chunk, _enc, cb) {
						cb(null, chunk)
					},
				})
				this.logger = new Console({
					stdout: this.transform,
					inspectOptions: {
						colors: !disableColors && !process.env.CI,
					},
				})
			}
			format(pendingInterceptors) {
				const withPrettyHeaders = pendingInterceptors.map(
					({ method, path, data: { statusCode }, persist, times, timesInvoked, origin }) => ({
						Method: method,
						Origin: origin,
						Path: path,
						"Status code": statusCode,
						Persistent: persist ? "\u2705" : "\u274C",
						Invocations: timesInvoked,
						Remaining: persist ? Infinity : times - timesInvoked,
					})
				)
				this.logger.table(withPrettyHeaders)
				return this.transform.read().toString()
			}
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/mock-agent.js
var require_mock_agent = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/mock/mock-agent.js"(
		exports2,
		module2
	) {
		"use strict"
		var { kClients } = require_symbols()
		var Agent = require_agent()
		var {
			kAgent,
			kMockAgentSet,
			kMockAgentGet,
			kDispatches,
			kIsMockActive,
			kNetConnect,
			kGetNetConnect,
			kOptions,
			kFactory,
		} = require_mock_symbols()
		var MockClient = require_mock_client()
		var MockPool = require_mock_pool()
		var { matchValue, buildMockOptions } = require_mock_utils()
		var { InvalidArgumentError, UndiciError } = require_errors()
		var Dispatcher = require_dispatcher()
		var Pluralizer = require_pluralizer()
		var PendingInterceptorsFormatter = require_pending_interceptors_formatter()
		var FakeWeakRef = class {
			constructor(value) {
				this.value = value
			}
			deref() {
				return this.value
			}
		}
		var MockAgent = class extends Dispatcher {
			constructor(opts) {
				super(opts)
				this[kNetConnect] = true
				this[kIsMockActive] = true
				if (opts && opts.agent && typeof opts.agent.dispatch !== "function") {
					throw new InvalidArgumentError("Argument opts.agent must implement Agent")
				}
				const agent = opts && opts.agent ? opts.agent : new Agent(opts)
				this[kAgent] = agent
				this[kClients] = agent[kClients]
				this[kOptions] = buildMockOptions(opts)
			}
			get(origin) {
				let dispatcher = this[kMockAgentGet](origin)
				if (!dispatcher) {
					dispatcher = this[kFactory](origin)
					this[kMockAgentSet](origin, dispatcher)
				}
				return dispatcher
			}
			dispatch(opts, handler) {
				this.get(opts.origin)
				return this[kAgent].dispatch(opts, handler)
			}
			async close() {
				await this[kAgent].close()
				this[kClients].clear()
			}
			deactivate() {
				this[kIsMockActive] = false
			}
			activate() {
				this[kIsMockActive] = true
			}
			enableNetConnect(matcher) {
				if (
					typeof matcher === "string" ||
					typeof matcher === "function" ||
					matcher instanceof RegExp
				) {
					if (Array.isArray(this[kNetConnect])) {
						this[kNetConnect].push(matcher)
					} else {
						this[kNetConnect] = [matcher]
					}
				} else if (typeof matcher === "undefined") {
					this[kNetConnect] = true
				} else {
					throw new InvalidArgumentError(
						"Unsupported matcher. Must be one of String|Function|RegExp."
					)
				}
			}
			disableNetConnect() {
				this[kNetConnect] = false
			}
			// This is required to bypass issues caused by using global symbols - see:
			// https://github.com/nodejs/undici/issues/1447
			get isMockActive() {
				return this[kIsMockActive]
			}
			[kMockAgentSet](origin, dispatcher) {
				this[kClients].set(origin, new FakeWeakRef(dispatcher))
			}
			[kFactory](origin) {
				const mockOptions = Object.assign({ agent: this }, this[kOptions])
				return this[kOptions] && this[kOptions].connections === 1
					? new MockClient(origin, mockOptions)
					: new MockPool(origin, mockOptions)
			}
			[kMockAgentGet](origin) {
				const ref = this[kClients].get(origin)
				if (ref) {
					return ref.deref()
				}
				if (typeof origin !== "string") {
					const dispatcher = this[kFactory]("http://localhost:9999")
					this[kMockAgentSet](origin, dispatcher)
					return dispatcher
				}
				for (const [keyMatcher, nonExplicitRef] of [...this[kClients]]) {
					const nonExplicitDispatcher = nonExplicitRef.deref()
					if (
						nonExplicitDispatcher &&
						typeof keyMatcher !== "string" &&
						matchValue(keyMatcher, origin)
					) {
						const dispatcher = this[kFactory](origin)
						this[kMockAgentSet](origin, dispatcher)
						dispatcher[kDispatches] = nonExplicitDispatcher[kDispatches]
						return dispatcher
					}
				}
			}
			[kGetNetConnect]() {
				return this[kNetConnect]
			}
			pendingInterceptors() {
				const mockAgentClients = this[kClients]
				return [...mockAgentClients.entries()]
					.flatMap(([origin, scope]) =>
						scope.deref()[kDispatches].map((dispatch) => ({ ...dispatch, origin }))
					)
					.filter(({ pending }) => pending)
			}
			assertNoPendingInterceptors({
				pendingInterceptorsFormatter = new PendingInterceptorsFormatter(),
			} = {}) {
				const pending = this.pendingInterceptors()
				if (pending.length === 0) {
					return
				}
				const pluralizer = new Pluralizer("interceptor", "interceptors").pluralize(pending.length)
				throw new UndiciError(
					`
${pluralizer.count} ${pluralizer.noun} ${pluralizer.is} pending:

${pendingInterceptorsFormatter.format(pending)}
`.trim()
				)
			}
		}
		module2.exports = MockAgent
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/proxy-agent.js
var require_proxy_agent = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/proxy-agent.js"(
		exports2,
		module2
	) {
		"use strict"
		var { kProxy, kClose, kDestroy, kInterceptors } = require_symbols()
		var { URL: URL3 } = require("node:url")
		var Agent = require_agent()
		var Pool = require_pool()
		var DispatcherBase = require_dispatcher_base()
		var { InvalidArgumentError, RequestAbortedError } = require_errors()
		var buildConnector = require_connect()
		var kAgent = Symbol("proxy agent")
		var kClient = Symbol("proxy client")
		var kProxyHeaders = Symbol("proxy headers")
		var kRequestTls = Symbol("request tls settings")
		var kProxyTls = Symbol("proxy tls settings")
		var kConnectEndpoint = Symbol("connect endpoint function")
		function defaultProtocolPort(protocol) {
			return protocol === "https:" ? 443 : 80
		}
		function buildProxyOptions(opts) {
			if (typeof opts === "string") {
				opts = { uri: opts }
			}
			if (!opts || !opts.uri) {
				throw new InvalidArgumentError("Proxy opts.uri is mandatory")
			}
			return {
				uri: opts.uri,
				protocol: opts.protocol || "https",
			}
		}
		function defaultFactory(origin, opts) {
			return new Pool(origin, opts)
		}
		var ProxyAgent = class extends DispatcherBase {
			constructor(opts) {
				super(opts)
				this[kProxy] = buildProxyOptions(opts)
				this[kAgent] = new Agent(opts)
				this[kInterceptors] =
					opts.interceptors &&
					opts.interceptors.ProxyAgent &&
					Array.isArray(opts.interceptors.ProxyAgent)
						? opts.interceptors.ProxyAgent
						: []
				if (typeof opts === "string") {
					opts = { uri: opts }
				}
				if (!opts || !opts.uri) {
					throw new InvalidArgumentError("Proxy opts.uri is mandatory")
				}
				const { clientFactory = defaultFactory } = opts
				if (typeof clientFactory !== "function") {
					throw new InvalidArgumentError("Proxy opts.clientFactory must be a function.")
				}
				this[kRequestTls] = opts.requestTls
				this[kProxyTls] = opts.proxyTls
				this[kProxyHeaders] = opts.headers || {}
				const resolvedUrl = new URL3(opts.uri)
				const { origin, port, host, username, password } = resolvedUrl
				if (opts.auth && opts.token) {
					throw new InvalidArgumentError("opts.auth cannot be used in combination with opts.token")
				} else if (opts.auth) {
					this[kProxyHeaders]["proxy-authorization"] = `Basic ${opts.auth}`
				} else if (opts.token) {
					this[kProxyHeaders]["proxy-authorization"] = opts.token
				} else if (username && password) {
					this[kProxyHeaders]["proxy-authorization"] = `Basic ${Buffer.from(
						`${decodeURIComponent(username)}:${decodeURIComponent(password)}`
					).toString("base64")}`
				}
				const connect = buildConnector({ ...opts.proxyTls })
				this[kConnectEndpoint] = buildConnector({ ...opts.requestTls })
				this[kClient] = clientFactory(resolvedUrl, { connect })
				this[kAgent] = new Agent({
					...opts,
					connect: async (opts2, callback) => {
						let requestedHost = opts2.host
						if (!opts2.port) {
							requestedHost += `:${defaultProtocolPort(opts2.protocol)}`
						}
						try {
							const { socket, statusCode } = await this[kClient].connect({
								origin,
								port,
								path: requestedHost,
								signal: opts2.signal,
								headers: {
									...this[kProxyHeaders],
									host,
								},
							})
							if (statusCode !== 200) {
								socket.on("error", () => {}).destroy()
								callback(
									new RequestAbortedError(
										`Proxy response (${statusCode}) !== 200 when HTTP Tunneling`
									)
								)
							}
							if (opts2.protocol !== "https:") {
								callback(null, socket)
								return
							}
							let servername
							if (this[kRequestTls]) {
								servername = this[kRequestTls].servername
							} else {
								servername = opts2.servername
							}
							this[kConnectEndpoint]({ ...opts2, servername, httpSocket: socket }, callback)
						} catch (err) {
							callback(err)
						}
					},
				})
			}
			dispatch(opts, handler) {
				const { host } = new URL3(opts.origin)
				const headers = buildHeaders(opts.headers)
				throwIfProxyAuthIsSent(headers)
				return this[kAgent].dispatch(
					{
						...opts,
						headers: {
							...headers,
							host,
						},
					},
					handler
				)
			}
			async [kClose]() {
				await this[kAgent].close()
				await this[kClient].close()
			}
			async [kDestroy]() {
				await this[kAgent].destroy()
				await this[kClient].destroy()
			}
		}
		function buildHeaders(headers) {
			if (Array.isArray(headers)) {
				const headersPair = {}
				for (let i = 0; i < headers.length; i += 2) {
					headersPair[headers[i]] = headers[i + 1]
				}
				return headersPair
			}
			return headers
		}
		function throwIfProxyAuthIsSent(headers) {
			const existProxyAuth =
				headers && Object.keys(headers).find((key) => key.toLowerCase() === "proxy-authorization")
			if (existProxyAuth) {
				throw new InvalidArgumentError(
					"Proxy-Authorization should be sent in ProxyAgent constructor"
				)
			}
		}
		module2.exports = ProxyAgent
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/handler/RetryHandler.js
var require_RetryHandler = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/handler/RetryHandler.js"(
		exports2,
		module2
	) {
		var assert = require("node:assert")
		var { kRetryHandlerDefaultRetry } = require_symbols()
		var { RequestRetryError } = require_errors()
		var { isDisturbed, parseHeaders, parseRangeHeader } = require_util()
		function calculateRetryAfterHeader(retryAfter) {
			const current = Date.now()
			const diff = new Date(retryAfter).getTime() - current
			return diff
		}
		var RetryHandler = class _RetryHandler {
			constructor(opts, handlers) {
				const { retryOptions, ...dispatchOpts } = opts
				const {
					// Retry scoped
					retry: retryFn,
					maxRetries: maxRetries2,
					maxTimeout,
					minTimeout,
					timeoutFactor,
					// Response scoped
					methods,
					errorCodes,
					retryAfter,
					statusCodes,
				} = retryOptions ?? {}
				this.dispatch = handlers.dispatch
				this.handler = handlers.handler
				this.opts = dispatchOpts
				this.abort = null
				this.aborted = false
				this.retryOpts = {
					retry: retryFn ?? _RetryHandler[kRetryHandlerDefaultRetry],
					retryAfter: retryAfter ?? true,
					maxTimeout: maxTimeout ?? 30 * 1e3,
					// 30s,
					timeout: minTimeout ?? 500,
					// .5s
					timeoutFactor: timeoutFactor ?? 2,
					maxRetries: maxRetries2 ?? 5,
					// What errors we should retry
					methods: methods ?? ["GET", "HEAD", "OPTIONS", "PUT", "DELETE", "TRACE"],
					// Indicates which errors to retry
					statusCodes: statusCodes ?? [500, 502, 503, 504, 429],
					// List of errors to retry
					errorCodes: errorCodes ?? [
						"ECONNRESET",
						"ECONNREFUSED",
						"ENOTFOUND",
						"ENETDOWN",
						"ENETUNREACH",
						"EHOSTDOWN",
						"EHOSTUNREACH",
						"EPIPE",
					],
				}
				this.retryCount = 0
				this.start = 0
				this.end = null
				this.etag = null
				this.resume = null
				this.handler.onConnect((reason) => {
					this.aborted = true
					if (this.abort) {
						this.abort(reason)
					} else {
						this.reason = reason
					}
				})
			}
			onRequestSent() {
				if (this.handler.onRequestSent) {
					this.handler.onRequestSent()
				}
			}
			onUpgrade(statusCode, headers, socket) {
				if (this.handler.onUpgrade) {
					this.handler.onUpgrade(statusCode, headers, socket)
				}
			}
			onConnect(abort) {
				if (this.aborted) {
					abort(this.reason)
				} else {
					this.abort = abort
				}
			}
			onBodySent(chunk) {
				if (this.handler.onBodySent) return this.handler.onBodySent(chunk)
			}
			static [kRetryHandlerDefaultRetry](err, { state, opts }, cb) {
				const { statusCode, code, headers } = err
				const { method, retryOptions } = opts
				const {
					maxRetries: maxRetries2,
					timeout,
					maxTimeout,
					timeoutFactor,
					statusCodes,
					errorCodes,
					methods,
				} = retryOptions
				let { counter, currentTimeout } = state
				currentTimeout =
					currentTimeout != undefined && currentTimeout > 0 ? currentTimeout : timeout
				if (
					code &&
					code !== "UND_ERR_REQ_RETRY" &&
					code !== "UND_ERR_SOCKET" &&
					!errorCodes.includes(code)
				) {
					cb(err)
					return
				}
				if (Array.isArray(methods) && !methods.includes(method)) {
					cb(err)
					return
				}
				if (
					statusCode != undefined &&
					Array.isArray(statusCodes) &&
					!statusCodes.includes(statusCode)
				) {
					cb(err)
					return
				}
				if (counter > maxRetries2) {
					cb(err)
					return
				}
				let retryAfterHeader = headers != undefined && headers["retry-after"]
				if (retryAfterHeader) {
					retryAfterHeader = Number(retryAfterHeader)
					retryAfterHeader = isNaN(retryAfterHeader)
						? calculateRetryAfterHeader(retryAfterHeader)
						: retryAfterHeader * 1e3
				}
				const retryTimeout =
					retryAfterHeader > 0
						? Math.min(retryAfterHeader, maxTimeout)
						: Math.min(currentTimeout * timeoutFactor ** counter, maxTimeout)
				state.currentTimeout = retryTimeout
				setTimeout(() => cb(null), retryTimeout)
			}
			onHeaders(statusCode, rawHeaders, resume, statusMessage) {
				const headers = parseHeaders(rawHeaders)
				this.retryCount += 1
				if (statusCode >= 300) {
					this.abort(
						new RequestRetryError("Request failed", statusCode, {
							headers,
							count: this.retryCount,
						})
					)
					return false
				}
				if (this.resume != undefined) {
					this.resume = null
					if (statusCode !== 206) {
						return true
					}
					const contentRange = parseRangeHeader(headers["content-range"])
					if (!contentRange) {
						this.abort(
							new RequestRetryError("Content-Range mismatch", statusCode, {
								headers,
								count: this.retryCount,
							})
						)
						return false
					}
					if (this.etag != undefined && this.etag !== headers.etag) {
						this.abort(
							new RequestRetryError("ETag mismatch", statusCode, {
								headers,
								count: this.retryCount,
							})
						)
						return false
					}
					const { start, size, end = size } = contentRange
					assert(this.start === start, "content-range mismatch")
					assert(this.end == undefined || this.end === end, "content-range mismatch")
					this.resume = resume
					return true
				}
				if (this.end == undefined) {
					if (statusCode === 206) {
						const range = parseRangeHeader(headers["content-range"])
						if (range == undefined) {
							return this.handler.onHeaders(statusCode, rawHeaders, resume, statusMessage)
						}
						const { start, size, end = size } = range
						assert(
							start != undefined && Number.isFinite(start) && this.start !== start,
							"content-range mismatch"
						)
						assert(Number.isFinite(start))
						assert(
							end != undefined && Number.isFinite(end) && this.end !== end,
							"invalid content-length"
						)
						this.start = start
						this.end = end
					}
					if (this.end == undefined) {
						const contentLength = headers["content-length"]
						this.end = contentLength != undefined ? Number(contentLength) : null
					}
					assert(Number.isFinite(this.start))
					assert(this.end == undefined || Number.isFinite(this.end), "invalid content-length")
					this.resume = resume
					this.etag = headers.etag != undefined ? headers.etag : null
					return this.handler.onHeaders(statusCode, rawHeaders, resume, statusMessage)
				}
				const err = new RequestRetryError("Request failed", statusCode, {
					headers,
					count: this.retryCount,
				})
				this.abort(err)
				return false
			}
			onData(chunk) {
				this.start += chunk.length
				return this.handler.onData(chunk)
			}
			onComplete(rawTrailers) {
				this.retryCount = 0
				return this.handler.onComplete(rawTrailers)
			}
			onError(err) {
				if (this.aborted || isDisturbed(this.opts.body)) {
					return this.handler.onError(err)
				}
				this.retryOpts.retry(
					err,
					{
						state: { counter: this.retryCount++, currentTimeout: this.retryAfter },
						opts: { retryOptions: this.retryOpts, ...this.opts },
					},
					onRetry.bind(this)
				)
				function onRetry(err2) {
					if (err2 != undefined || this.aborted || isDisturbed(this.opts.body)) {
						return this.handler.onError(err2)
					}
					if (this.start !== 0) {
						this.opts = {
							...this.opts,
							headers: {
								...this.opts.headers,
								range: `bytes=${this.start}-${this.end ?? ""}`,
							},
						}
					}
					try {
						this.dispatch(this.opts, this)
					} catch (err3) {
						this.handler.onError(err3)
					}
				}
			}
		}
		module2.exports = RetryHandler
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/global.js
var require_global2 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/global.js"(exports2, module2) {
		"use strict"
		var globalDispatcher = Symbol.for("undici.globalDispatcher.1")
		var { InvalidArgumentError } = require_errors()
		var Agent = require_agent()
		if (getGlobalDispatcher() === void 0) {
			setGlobalDispatcher(new Agent())
		}
		function setGlobalDispatcher(agent) {
			if (!agent || typeof agent.dispatch !== "function") {
				throw new InvalidArgumentError("Argument agent must implement Agent")
			}
			Object.defineProperty(globalThis, globalDispatcher, {
				value: agent,
				writable: true,
				enumerable: false,
				configurable: false,
			})
		}
		function getGlobalDispatcher() {
			return globalThis[globalDispatcher]
		}
		module2.exports = {
			setGlobalDispatcher,
			getGlobalDispatcher,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/handler/DecoratorHandler.js
var require_DecoratorHandler = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/handler/DecoratorHandler.js"(
		exports2,
		module2
	) {
		"use strict"
		module2.exports = class DecoratorHandler {
			constructor(handler) {
				this.handler = handler
			}
			onConnect(...args) {
				return this.handler.onConnect(...args)
			}
			onError(...args) {
				return this.handler.onError(...args)
			}
			onUpgrade(...args) {
				return this.handler.onUpgrade(...args)
			}
			onHeaders(...args) {
				return this.handler.onHeaders(...args)
			}
			onData(...args) {
				return this.handler.onData(...args)
			}
			onComplete(...args) {
				return this.handler.onComplete(...args)
			}
			onBodySent(...args) {
				return this.handler.onBodySent(...args)
			}
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/headers.js
var require_headers = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/headers.js"(
		exports2,
		module2
	) {
		"use strict"
		var { kHeadersList, kConstruct } = require_symbols()
		var { kGuard } = require_symbols2()
		var { kEnumerableProperty } = require_util()
		var { makeIterator, isValidHeaderName, isValidHeaderValue } = require_util2()
		var { webidl } = require_webidl()
		var assert = require("node:assert")
		var kHeadersMap = Symbol("headers map")
		var kHeadersSortedMap = Symbol("headers map sorted")
		function isHTTPWhiteSpaceCharCode(code) {
			return code === 10 || code === 13 || code === 9 || code === 32
		}
		function headerValueNormalize(potentialValue) {
			let i = 0
			let j = potentialValue.length
			while (j > i && isHTTPWhiteSpaceCharCode(potentialValue.charCodeAt(j - 1))) --j
			while (j > i && isHTTPWhiteSpaceCharCode(potentialValue.charCodeAt(i))) ++i
			return i === 0 && j === potentialValue.length
				? potentialValue
				: potentialValue.substring(i, j)
		}
		function fill(headers, object) {
			if (Array.isArray(object)) {
				for (const header of object) {
					if (header.length !== 2) {
						throw webidl.errors.exception({
							header: "Headers constructor",
							message: `expected name/value pair to be length 2, found ${header.length}.`,
						})
					}
					appendHeader(headers, header[0], header[1])
				}
			} else if (typeof object === "object" && object !== null) {
				const keys = Object.keys(object)
				for (const key of keys) {
					appendHeader(headers, key, object[key])
				}
			} else {
				throw webidl.errors.conversionFailed({
					prefix: "Headers constructor",
					argument: "Argument 1",
					types: ["sequence<sequence<ByteString>>", "record<ByteString, ByteString>"],
				})
			}
		}
		function appendHeader(headers, name, value) {
			value = headerValueNormalize(value)
			if (!isValidHeaderName(name)) {
				throw webidl.errors.invalidArgument({
					prefix: "Headers.append",
					value: name,
					type: "header name",
				})
			} else if (!isValidHeaderValue(value)) {
				throw webidl.errors.invalidArgument({
					prefix: "Headers.append",
					value,
					type: "header value",
				})
			}
			if (headers[kGuard] === "immutable") {
				throw new TypeError("immutable")
			} else if (headers[kGuard] === "request-no-cors") {
			}
			return headers[kHeadersList].append(name, value)
		}
		var HeadersList = class _HeadersList {
			/** @type {[string, string][]|null} */
			cookies = null
			constructor(init2) {
				if (init2 instanceof _HeadersList) {
					this[kHeadersMap] = new Map(init2[kHeadersMap])
					this[kHeadersSortedMap] = init2[kHeadersSortedMap]
					this.cookies = init2.cookies === null ? null : [...init2.cookies]
				} else {
					this[kHeadersMap] = new Map(init2)
					this[kHeadersSortedMap] = null
				}
			}
			// https://fetch.spec.whatwg.org/#header-list-contains
			contains(name) {
				name = name.toLowerCase()
				return this[kHeadersMap].has(name)
			}
			clear() {
				this[kHeadersMap].clear()
				this[kHeadersSortedMap] = null
				this.cookies = null
			}
			// https://fetch.spec.whatwg.org/#concept-header-list-append
			append(name, value) {
				this[kHeadersSortedMap] = null
				const lowercaseName = name.toLowerCase()
				const exists = this[kHeadersMap].get(lowercaseName)
				if (exists) {
					const delimiter = lowercaseName === "cookie" ? "; " : ", "
					this[kHeadersMap].set(lowercaseName, {
						name: exists.name,
						value: `${exists.value}${delimiter}${value}`,
					})
				} else {
					this[kHeadersMap].set(lowercaseName, { name, value })
				}
				if (lowercaseName === "set-cookie") {
					this.cookies ??= []
					this.cookies.push(value)
				}
			}
			// https://fetch.spec.whatwg.org/#concept-header-list-set
			set(name, value) {
				this[kHeadersSortedMap] = null
				const lowercaseName = name.toLowerCase()
				if (lowercaseName === "set-cookie") {
					this.cookies = [value]
				}
				this[kHeadersMap].set(lowercaseName, { name, value })
			}
			// https://fetch.spec.whatwg.org/#concept-header-list-delete
			delete(name) {
				this[kHeadersSortedMap] = null
				name = name.toLowerCase()
				if (name === "set-cookie") {
					this.cookies = null
				}
				this[kHeadersMap].delete(name)
			}
			// https://fetch.spec.whatwg.org/#concept-header-list-get
			get(name) {
				const value = this[kHeadersMap].get(name.toLowerCase())
				return value === void 0 ? null : value.value
			}
			*[Symbol.iterator]() {
				for (const [name, { value }] of this[kHeadersMap]) {
					yield [name, value]
				}
			}
			get entries() {
				const headers = {}
				if (this[kHeadersMap].size) {
					for (const { name, value } of this[kHeadersMap].values()) {
						headers[name] = value
					}
				}
				return headers
			}
		}
		var Headers = class _Headers {
			constructor(init2 = void 0) {
				if (init2 === kConstruct) {
					return
				}
				this[kHeadersList] = new HeadersList()
				this[kGuard] = "none"
				if (init2 !== void 0) {
					init2 = webidl.converters.HeadersInit(init2)
					fill(this, init2)
				}
			}
			// https://fetch.spec.whatwg.org/#dom-headers-append
			append(name, value) {
				webidl.brandCheck(this, _Headers)
				webidl.argumentLengthCheck(arguments, 2, { header: "Headers.append" })
				name = webidl.converters.ByteString(name)
				value = webidl.converters.ByteString(value)
				return appendHeader(this, name, value)
			}
			// https://fetch.spec.whatwg.org/#dom-headers-delete
			delete(name) {
				webidl.brandCheck(this, _Headers)
				webidl.argumentLengthCheck(arguments, 1, { header: "Headers.delete" })
				name = webidl.converters.ByteString(name)
				if (!isValidHeaderName(name)) {
					throw webidl.errors.invalidArgument({
						prefix: "Headers.delete",
						value: name,
						type: "header name",
					})
				}
				if (this[kGuard] === "immutable") {
					throw new TypeError("immutable")
				} else if (this[kGuard] === "request-no-cors") {
				}
				if (!this[kHeadersList].contains(name)) {
					return
				}
				this[kHeadersList].delete(name)
			}
			// https://fetch.spec.whatwg.org/#dom-headers-get
			get(name) {
				webidl.brandCheck(this, _Headers)
				webidl.argumentLengthCheck(arguments, 1, { header: "Headers.get" })
				name = webidl.converters.ByteString(name)
				if (!isValidHeaderName(name)) {
					throw webidl.errors.invalidArgument({
						prefix: "Headers.get",
						value: name,
						type: "header name",
					})
				}
				return this[kHeadersList].get(name)
			}
			// https://fetch.spec.whatwg.org/#dom-headers-has
			has(name) {
				webidl.brandCheck(this, _Headers)
				webidl.argumentLengthCheck(arguments, 1, { header: "Headers.has" })
				name = webidl.converters.ByteString(name)
				if (!isValidHeaderName(name)) {
					throw webidl.errors.invalidArgument({
						prefix: "Headers.has",
						value: name,
						type: "header name",
					})
				}
				return this[kHeadersList].contains(name)
			}
			// https://fetch.spec.whatwg.org/#dom-headers-set
			set(name, value) {
				webidl.brandCheck(this, _Headers)
				webidl.argumentLengthCheck(arguments, 2, { header: "Headers.set" })
				name = webidl.converters.ByteString(name)
				value = webidl.converters.ByteString(value)
				value = headerValueNormalize(value)
				if (!isValidHeaderName(name)) {
					throw webidl.errors.invalidArgument({
						prefix: "Headers.set",
						value: name,
						type: "header name",
					})
				} else if (!isValidHeaderValue(value)) {
					throw webidl.errors.invalidArgument({
						prefix: "Headers.set",
						value,
						type: "header value",
					})
				}
				if (this[kGuard] === "immutable") {
					throw new TypeError("immutable")
				} else if (this[kGuard] === "request-no-cors") {
				}
				this[kHeadersList].set(name, value)
			}
			// https://fetch.spec.whatwg.org/#dom-headers-getsetcookie
			getSetCookie() {
				webidl.brandCheck(this, _Headers)
				const list = this[kHeadersList].cookies
				if (list) {
					return [...list]
				}
				return []
			}
			// https://fetch.spec.whatwg.org/#concept-header-list-sort-and-combine
			get [kHeadersSortedMap]() {
				if (this[kHeadersList][kHeadersSortedMap]) {
					return this[kHeadersList][kHeadersSortedMap]
				}
				const headers = []
				const names = [...this[kHeadersList]].sort((a, b) => (a[0] < b[0] ? -1 : 1))
				const cookies = this[kHeadersList].cookies
				for (const [name, value] of names) {
					if (name === "set-cookie") {
						for (const cooky of cookies) {
							headers.push([name, cooky])
						}
					} else {
						assert(value !== null)
						headers.push([name, value])
					}
				}
				this[kHeadersList][kHeadersSortedMap] = headers
				return headers
			}
			keys() {
				webidl.brandCheck(this, _Headers)
				if (this[kGuard] === "immutable") {
					const value = this[kHeadersSortedMap]
					return makeIterator(() => value, "Headers", "key")
				}
				return makeIterator(() => [...this[kHeadersSortedMap].values()], "Headers", "key")
			}
			values() {
				webidl.brandCheck(this, _Headers)
				if (this[kGuard] === "immutable") {
					const value = this[kHeadersSortedMap]
					return makeIterator(() => value, "Headers", "value")
				}
				return makeIterator(() => [...this[kHeadersSortedMap].values()], "Headers", "value")
			}
			entries() {
				webidl.brandCheck(this, _Headers)
				if (this[kGuard] === "immutable") {
					const value = this[kHeadersSortedMap]
					return makeIterator(() => value, "Headers", "key+value")
				}
				return makeIterator(() => [...this[kHeadersSortedMap].values()], "Headers", "key+value")
			}
			/**
			 * @param {(value: string, key: string, self: Headers) => void} callbackFn
			 * @param {unknown} thisArg
			 */
			forEach(callbackFn, thisArg = globalThis) {
				webidl.brandCheck(this, _Headers)
				webidl.argumentLengthCheck(arguments, 1, { header: "Headers.forEach" })
				if (typeof callbackFn !== "function") {
					throw new TypeError(
						"Failed to execute 'forEach' on 'Headers': parameter 1 is not of type 'Function'."
					)
				}
				for (const [key, value] of this) {
					callbackFn.apply(thisArg, [value, key, this])
				}
			}
			[Symbol.for("nodejs.util.inspect.custom")]() {
				webidl.brandCheck(this, _Headers)
				return this[kHeadersList]
			}
		}
		Headers.prototype[Symbol.iterator] = Headers.prototype.entries
		Object.defineProperties(Headers.prototype, {
			append: kEnumerableProperty,
			delete: kEnumerableProperty,
			get: kEnumerableProperty,
			has: kEnumerableProperty,
			set: kEnumerableProperty,
			getSetCookie: kEnumerableProperty,
			keys: kEnumerableProperty,
			values: kEnumerableProperty,
			entries: kEnumerableProperty,
			forEach: kEnumerableProperty,
			[Symbol.iterator]: { enumerable: false },
			[Symbol.toStringTag]: {
				value: "Headers",
				configurable: true,
			},
		})
		webidl.converters.HeadersInit = function (V) {
			if (webidl.util.Type(V) === "Object") {
				if (V[Symbol.iterator]) {
					return webidl.converters["sequence<sequence<ByteString>>"](V)
				}
				return webidl.converters["record<ByteString, ByteString>"](V)
			}
			throw webidl.errors.conversionFailed({
				prefix: "Headers constructor",
				argument: "Argument 1",
				types: ["sequence<sequence<ByteString>>", "record<ByteString, ByteString>"],
			})
		}
		module2.exports = {
			fill,
			Headers,
			HeadersList,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/response.js
var require_response = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/response.js"(
		exports2,
		module2
	) {
		"use strict"
		var { Headers, HeadersList, fill } = require_headers()
		var { extractBody, cloneBody, mixinBody } = require_body()
		var util = require_util()
		var { kEnumerableProperty } = util
		var {
			isValidReasonPhrase,
			isCancelled,
			isAborted,
			isBlobLike,
			serializeJavascriptValueToJSONString,
			isErrorLike,
			isomorphicEncode,
		} = require_util2()
		var { redirectStatusSet, nullBodyStatus, DOMException: DOMException2 } = require_constants2()
		var { kState, kHeaders, kGuard, kRealm } = require_symbols2()
		var { webidl } = require_webidl()
		var { FormData } = require_formdata()
		var { getGlobalOrigin } = require_global()
		var { URLSerializer } = require_dataURL()
		var { kHeadersList, kConstruct } = require_symbols()
		var assert = require("node:assert")
		var { types: types2 } = require("node:util")
		var ReadableStream = globalThis.ReadableStream || require("node:stream/web").ReadableStream
		var textEncoder = new TextEncoder("utf-8")
		var Response2 = class _Response {
			// Creates network error Response.
			static error() {
				const relevantRealm = { settingsObject: {} }
				const responseObject = new _Response()
				responseObject[kState] = makeNetworkError()
				responseObject[kRealm] = relevantRealm
				responseObject[kHeaders][kHeadersList] = responseObject[kState].headersList
				responseObject[kHeaders][kGuard] = "immutable"
				responseObject[kHeaders][kRealm] = relevantRealm
				return responseObject
			}
			// https://fetch.spec.whatwg.org/#dom-response-json
			static json(data, init2 = {}) {
				webidl.argumentLengthCheck(arguments, 1, { header: "Response.json" })
				if (init2 !== null) {
					init2 = webidl.converters.ResponseInit(init2)
				}
				const bytes = textEncoder.encode(serializeJavascriptValueToJSONString(data))
				const body = extractBody(bytes)
				const relevantRealm = { settingsObject: {} }
				const responseObject = new _Response()
				responseObject[kRealm] = relevantRealm
				responseObject[kHeaders][kGuard] = "response"
				responseObject[kHeaders][kRealm] = relevantRealm
				initializeResponse(responseObject, init2, { body: body[0], type: "application/json" })
				return responseObject
			}
			// Creates a redirect Response that redirects to url with status status.
			static redirect(url, status3 = 302) {
				const relevantRealm = { settingsObject: {} }
				webidl.argumentLengthCheck(arguments, 1, { header: "Response.redirect" })
				url = webidl.converters.USVString(url)
				status3 = webidl.converters["unsigned short"](status3)
				let parsedURL
				try {
					parsedURL = new URL(url, getGlobalOrigin())
				} catch (err) {
					throw Object.assign(new TypeError("Failed to parse URL from " + url), {
						cause: err,
					})
				}
				if (!redirectStatusSet.has(status3)) {
					throw new RangeError("Invalid status code " + status3)
				}
				const responseObject = new _Response()
				responseObject[kRealm] = relevantRealm
				responseObject[kHeaders][kGuard] = "immutable"
				responseObject[kHeaders][kRealm] = relevantRealm
				responseObject[kState].status = status3
				const value = isomorphicEncode(URLSerializer(parsedURL))
				responseObject[kState].headersList.append("location", value)
				return responseObject
			}
			// https://fetch.spec.whatwg.org/#dom-response
			constructor(body = null, init2 = {}) {
				if (body !== null) {
					body = webidl.converters.BodyInit(body)
				}
				init2 = webidl.converters.ResponseInit(init2)
				this[kRealm] = { settingsObject: {} }
				this[kState] = makeResponse({})
				this[kHeaders] = new Headers(kConstruct)
				this[kHeaders][kGuard] = "response"
				this[kHeaders][kHeadersList] = this[kState].headersList
				this[kHeaders][kRealm] = this[kRealm]
				let bodyWithType = null
				if (body != undefined) {
					const [extractedBody, type] = extractBody(body)
					bodyWithType = { body: extractedBody, type }
				}
				initializeResponse(this, init2, bodyWithType)
			}
			// Returns responses type, e.g., "cors".
			get type() {
				webidl.brandCheck(this, _Response)
				return this[kState].type
			}
			// Returns responses URL, if it has one; otherwise the empty string.
			get url() {
				webidl.brandCheck(this, _Response)
				const urlList = this[kState].urlList
				const url = urlList.at(-1) ?? null
				if (url === null) {
					return ""
				}
				return URLSerializer(url, true)
			}
			// Returns whether response was obtained through a redirect.
			get redirected() {
				webidl.brandCheck(this, _Response)
				return this[kState].urlList.length > 1
			}
			// Returns responses status.
			get status() {
				webidl.brandCheck(this, _Response)
				return this[kState].status
			}
			// Returns whether responses status is an ok status.
			get ok() {
				webidl.brandCheck(this, _Response)
				return this[kState].status >= 200 && this[kState].status <= 299
			}
			// Returns responses status message.
			get statusText() {
				webidl.brandCheck(this, _Response)
				return this[kState].statusText
			}
			// Returns responses headers as Headers.
			get headers() {
				webidl.brandCheck(this, _Response)
				return this[kHeaders]
			}
			get body() {
				webidl.brandCheck(this, _Response)
				return this[kState].body ? this[kState].body.stream : null
			}
			get bodyUsed() {
				webidl.brandCheck(this, _Response)
				return !!this[kState].body && util.isDisturbed(this[kState].body.stream)
			}
			// Returns a clone of response.
			clone() {
				webidl.brandCheck(this, _Response)
				if (this.bodyUsed || (this.body && this.body.locked)) {
					throw webidl.errors.exception({
						header: "Response.clone",
						message: "Body has already been consumed.",
					})
				}
				const clonedResponse = cloneResponse(this[kState])
				const clonedResponseObject = new _Response()
				clonedResponseObject[kState] = clonedResponse
				clonedResponseObject[kRealm] = this[kRealm]
				clonedResponseObject[kHeaders][kHeadersList] = clonedResponse.headersList
				clonedResponseObject[kHeaders][kGuard] = this[kHeaders][kGuard]
				clonedResponseObject[kHeaders][kRealm] = this[kHeaders][kRealm]
				return clonedResponseObject
			}
		}
		mixinBody(Response2)
		Object.defineProperties(Response2.prototype, {
			type: kEnumerableProperty,
			url: kEnumerableProperty,
			status: kEnumerableProperty,
			ok: kEnumerableProperty,
			redirected: kEnumerableProperty,
			statusText: kEnumerableProperty,
			headers: kEnumerableProperty,
			clone: kEnumerableProperty,
			body: kEnumerableProperty,
			bodyUsed: kEnumerableProperty,
			[Symbol.toStringTag]: {
				value: "Response",
				configurable: true,
			},
		})
		Object.defineProperties(Response2, {
			json: kEnumerableProperty,
			redirect: kEnumerableProperty,
			error: kEnumerableProperty,
		})
		function cloneResponse(response) {
			if (response.internalResponse) {
				return filterResponse(cloneResponse(response.internalResponse), response.type)
			}
			const newResponse = makeResponse({ ...response, body: null })
			if (response.body != undefined) {
				newResponse.body = cloneBody(response.body)
			}
			return newResponse
		}
		function makeResponse(init2) {
			return {
				aborted: false,
				rangeRequested: false,
				timingAllowPassed: false,
				requestIncludesCredentials: false,
				type: "default",
				status: 200,
				timingInfo: null,
				cacheState: "",
				statusText: "",
				...init2,
				headersList: init2.headersList ? new HeadersList(init2.headersList) : new HeadersList(),
				urlList: init2.urlList ? [...init2.urlList] : [],
			}
		}
		function makeNetworkError(reason) {
			const isError = isErrorLike(reason)
			return makeResponse({
				type: "error",
				status: 0,
				error: isError ? reason : new Error(reason ? String(reason) : reason),
				aborted: reason && reason.name === "AbortError",
			})
		}
		function makeFilteredResponse(response, state) {
			state = {
				internalResponse: response,
				...state,
			}
			return new Proxy(response, {
				get(target, p) {
					return p in state ? state[p] : target[p]
				},
				set(target, p, value) {
					assert(!(p in state))
					target[p] = value
					return true
				},
			})
		}
		function filterResponse(response, type) {
			if (type === "basic") {
				return makeFilteredResponse(response, {
					type: "basic",
					headersList: response.headersList,
				})
			} else if (type === "cors") {
				return makeFilteredResponse(response, {
					type: "cors",
					headersList: response.headersList,
				})
			} else if (type === "opaque") {
				return makeFilteredResponse(response, {
					type: "opaque",
					urlList: Object.freeze([]),
					status: 0,
					statusText: "",
					body: null,
				})
			} else if (type === "opaqueredirect") {
				return makeFilteredResponse(response, {
					type: "opaqueredirect",
					status: 0,
					statusText: "",
					headersList: [],
					body: null,
				})
			} else {
				assert(false)
			}
		}
		function makeAppropriateNetworkError(fetchParams, err = null) {
			assert(isCancelled(fetchParams))
			return isAborted(fetchParams)
				? makeNetworkError(
						Object.assign(new DOMException2("The operation was aborted.", "AbortError"), {
							cause: err,
						})
				  )
				: makeNetworkError(
						Object.assign(new DOMException2("Request was cancelled."), { cause: err })
				  )
		}
		function initializeResponse(response, init2, body) {
			if (init2.status !== null && (init2.status < 200 || init2.status > 599)) {
				throw new RangeError('init["status"] must be in the range of 200 to 599, inclusive.')
			}
			if (
				"statusText" in init2 &&
				init2.statusText != undefined &&
				!isValidReasonPhrase(String(init2.statusText))
			) {
				throw new TypeError("Invalid statusText")
			}
			if ("status" in init2 && init2.status != undefined) {
				response[kState].status = init2.status
			}
			if ("statusText" in init2 && init2.statusText != undefined) {
				response[kState].statusText = init2.statusText
			}
			if ("headers" in init2 && init2.headers != undefined) {
				fill(response[kHeaders], init2.headers)
			}
			if (body) {
				if (nullBodyStatus.includes(response.status)) {
					throw webidl.errors.exception({
						header: "Response constructor",
						message: "Invalid response status code " + response.status,
					})
				}
				response[kState].body = body.body
				if (body.type != undefined && !response[kState].headersList.contains("Content-Type")) {
					response[kState].headersList.append("content-type", body.type)
				}
			}
		}
		webidl.converters.ReadableStream = webidl.interfaceConverter(ReadableStream)
		webidl.converters.FormData = webidl.interfaceConverter(FormData)
		webidl.converters.URLSearchParams = webidl.interfaceConverter(URLSearchParams)
		webidl.converters.XMLHttpRequestBodyInit = function (V) {
			if (typeof V === "string") {
				return webidl.converters.USVString(V)
			}
			if (isBlobLike(V)) {
				return webidl.converters.Blob(V, { strict: false })
			}
			if (types2.isArrayBuffer(V) || types2.isTypedArray(V) || types2.isDataView(V)) {
				return webidl.converters.BufferSource(V)
			}
			if (util.isFormDataLike(V)) {
				return webidl.converters.FormData(V, { strict: false })
			}
			if (V instanceof URLSearchParams) {
				return webidl.converters.URLSearchParams(V)
			}
			return webidl.converters.DOMString(V)
		}
		webidl.converters.BodyInit = function (V) {
			if (V instanceof ReadableStream) {
				return webidl.converters.ReadableStream(V)
			}
			if (V?.[Symbol.asyncIterator]) {
				return V
			}
			return webidl.converters.XMLHttpRequestBodyInit(V)
		}
		webidl.converters.ResponseInit = webidl.dictionaryConverter([
			{
				key: "status",
				converter: webidl.converters["unsigned short"],
				defaultValue: 200,
			},
			{
				key: "statusText",
				converter: webidl.converters.ByteString,
				defaultValue: "",
			},
			{
				key: "headers",
				converter: webidl.converters.HeadersInit,
			},
		])
		module2.exports = {
			makeNetworkError,
			makeResponse,
			makeAppropriateNetworkError,
			filterResponse,
			Response: Response2,
			cloneResponse,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/request.js
var require_request2 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/request.js"(
		exports2,
		module2
	) {
		"use strict"
		var { extractBody, mixinBody, cloneBody } = require_body()
		var { Headers, fill: fillHeaders, HeadersList } = require_headers()
		var { FinalizationRegistry } = require_dispatcher_weakref()()
		var util = require_util()
		var {
			isValidHTTPToken,
			sameOrigin,
			normalizeMethod,
			makePolicyContainer,
			normalizeMethodRecord,
		} = require_util2()
		var {
			forbiddenMethodsSet,
			corsSafeListedMethodsSet,
			referrerPolicy,
			requestRedirect,
			requestMode,
			requestCredentials,
			requestCache,
			requestDuplex,
		} = require_constants2()
		var { kEnumerableProperty } = util
		var { kHeaders, kSignal, kState, kGuard, kRealm } = require_symbols2()
		var { webidl } = require_webidl()
		var { getGlobalOrigin } = require_global()
		var { URLSerializer } = require_dataURL()
		var { kHeadersList, kConstruct } = require_symbols()
		var assert = require("node:assert")
		var {
			getMaxListeners,
			setMaxListeners,
			getEventListeners,
			defaultMaxListeners,
		} = require("node:events")
		var TransformStream = globalThis.TransformStream
		var kAbortController = Symbol("abortController")
		var requestFinalizer = new FinalizationRegistry(({ signal, abort }) => {
			signal.removeEventListener("abort", abort)
		})
		var Request = class _Request {
			// https://fetch.spec.whatwg.org/#dom-request
			constructor(input, init2 = {}) {
				if (input === kConstruct) {
					return
				}
				webidl.argumentLengthCheck(arguments, 1, { header: "Request constructor" })
				input = webidl.converters.RequestInfo(input)
				init2 = webidl.converters.RequestInit(init2)
				this[kRealm] = {
					settingsObject: {
						baseUrl: getGlobalOrigin(),
						get origin() {
							return this.baseUrl?.origin
						},
						policyContainer: makePolicyContainer(),
					},
				}
				let request = null
				let fallbackMode = null
				const baseUrl = this[kRealm].settingsObject.baseUrl
				let signal = null
				if (typeof input === "string") {
					let parsedURL
					try {
						parsedURL = new URL(input, baseUrl)
					} catch (err) {
						throw new TypeError("Failed to parse URL from " + input, { cause: err })
					}
					if (parsedURL.username || parsedURL.password) {
						throw new TypeError(
							"Request cannot be constructed from a URL that includes credentials: " + input
						)
					}
					request = makeRequest({ urlList: [parsedURL] })
					fallbackMode = "cors"
				} else {
					assert(input instanceof _Request)
					request = input[kState]
					signal = input[kSignal]
				}
				const origin = this[kRealm].settingsObject.origin
				let window2 = "client"
				if (
					request.window?.constructor?.name === "EnvironmentSettingsObject" &&
					sameOrigin(request.window, origin)
				) {
					window2 = request.window
				}
				if (init2.window != undefined) {
					throw new TypeError(`'window' option '${window2}' must be null`)
				}
				if ("window" in init2) {
					window2 = "no-window"
				}
				request = makeRequest({
					// URL requests URL.
					// undici implementation note: this is set as the first item in request's urlList in makeRequest
					// method requests method.
					method: request.method,
					// header list A copy of requests header list.
					// undici implementation note: headersList is cloned in makeRequest
					headersList: request.headersList,
					// unsafe-request flag Set.
					unsafeRequest: request.unsafeRequest,
					// client Thiss relevant settings object.
					client: this[kRealm].settingsObject,
					// window window.
					window: window2,
					// priority requests priority.
					priority: request.priority,
					// origin requests origin. The propagation of the origin is only significant for navigation requests
					// being handled by a service worker. In this scenario a request can have an origin that is different
					// from the current client.
					origin: request.origin,
					// referrer requests referrer.
					referrer: request.referrer,
					// referrer policy requests referrer policy.
					referrerPolicy: request.referrerPolicy,
					// mode requests mode.
					mode: request.mode,
					// credentials mode requests credentials mode.
					credentials: request.credentials,
					// cache mode requests cache mode.
					cache: request.cache,
					// redirect mode requests redirect mode.
					redirect: request.redirect,
					// integrity metadata requests integrity metadata.
					integrity: request.integrity,
					// keepalive requests keepalive.
					keepalive: request.keepalive,
					// reload-navigation flag requests reload-navigation flag.
					reloadNavigation: request.reloadNavigation,
					// history-navigation flag requests history-navigation flag.
					historyNavigation: request.historyNavigation,
					// URL list A clone of requests URL list.
					urlList: [...request.urlList],
				})
				const initHasKey = Object.keys(init2).length !== 0
				if (initHasKey) {
					if (request.mode === "navigate") {
						request.mode = "same-origin"
					}
					request.reloadNavigation = false
					request.historyNavigation = false
					request.origin = "client"
					request.referrer = "client"
					request.referrerPolicy = ""
					request.url = request.urlList.at(-1)
					request.urlList = [request.url]
				}
				if (init2.referrer !== void 0) {
					const referrer = init2.referrer
					if (referrer === "") {
						request.referrer = "no-referrer"
					} else {
						let parsedReferrer
						try {
							parsedReferrer = new URL(referrer, baseUrl)
						} catch (err) {
							throw new TypeError(`Referrer "${referrer}" is not a valid URL.`, { cause: err })
						}
						if (
							(parsedReferrer.protocol === "about:" && parsedReferrer.hostname === "client") ||
							(origin && !sameOrigin(parsedReferrer, this[kRealm].settingsObject.baseUrl))
						) {
							request.referrer = "client"
						} else {
							request.referrer = parsedReferrer
						}
					}
				}
				if (init2.referrerPolicy !== void 0) {
					request.referrerPolicy = init2.referrerPolicy
				}
				let mode
				if (init2.mode !== void 0) {
					mode = init2.mode
				} else {
					mode = fallbackMode
				}
				if (mode === "navigate") {
					throw webidl.errors.exception({
						header: "Request constructor",
						message: "invalid request mode navigate.",
					})
				}
				if (mode != undefined) {
					request.mode = mode
				}
				if (init2.credentials !== void 0) {
					request.credentials = init2.credentials
				}
				if (init2.cache !== void 0) {
					request.cache = init2.cache
				}
				if (request.cache === "only-if-cached" && request.mode !== "same-origin") {
					throw new TypeError("'only-if-cached' can be set only with 'same-origin' mode")
				}
				if (init2.redirect !== void 0) {
					request.redirect = init2.redirect
				}
				if (init2.integrity != undefined) {
					request.integrity = String(init2.integrity)
				}
				if (init2.keepalive !== void 0) {
					request.keepalive = Boolean(init2.keepalive)
				}
				if (init2.method !== void 0) {
					let method = init2.method
					if (!isValidHTTPToken(method)) {
						throw new TypeError(`'${method}' is not a valid HTTP method.`)
					}
					if (forbiddenMethodsSet.has(method.toUpperCase())) {
						throw new TypeError(`'${method}' HTTP method is unsupported.`)
					}
					method = normalizeMethodRecord[method] ?? normalizeMethod(method)
					request.method = method
				}
				if (init2.signal !== void 0) {
					signal = init2.signal
				}
				this[kState] = request
				const ac = new AbortController()
				this[kSignal] = ac.signal
				this[kSignal][kRealm] = this[kRealm]
				if (signal != undefined) {
					if (
						!signal ||
						typeof signal.aborted !== "boolean" ||
						typeof signal.addEventListener !== "function"
					) {
						throw new TypeError(
							"Failed to construct 'Request': member signal is not of type AbortSignal."
						)
					}
					if (signal.aborted) {
						ac.abort(signal.reason)
					} else {
						this[kAbortController] = ac
						const acRef = new WeakRef(ac)
						const abort = function () {
							const ac2 = acRef.deref()
							if (ac2 !== void 0) {
								ac2.abort(this.reason)
							}
						}
						try {
							if (
								typeof getMaxListeners === "function" &&
								getMaxListeners(signal) === defaultMaxListeners
							) {
								setMaxListeners(100, signal)
							} else if (getEventListeners(signal, "abort").length >= defaultMaxListeners) {
								setMaxListeners(100, signal)
							}
						} catch {}
						util.addAbortListener(signal, abort)
						requestFinalizer.register(ac, { signal, abort })
					}
				}
				this[kHeaders] = new Headers(kConstruct)
				this[kHeaders][kHeadersList] = request.headersList
				this[kHeaders][kGuard] = "request"
				this[kHeaders][kRealm] = this[kRealm]
				if (mode === "no-cors") {
					if (!corsSafeListedMethodsSet.has(request.method)) {
						throw new TypeError(`'${request.method} is unsupported in no-cors mode.`)
					}
					this[kHeaders][kGuard] = "request-no-cors"
				}
				if (initHasKey) {
					const headersList = this[kHeaders][kHeadersList]
					const headers = init2.headers !== void 0 ? init2.headers : new HeadersList(headersList)
					headersList.clear()
					if (headers instanceof HeadersList) {
						for (const [key, val] of headers) {
							headersList.append(key, val)
						}
						headersList.cookies = headers.cookies
					} else {
						fillHeaders(this[kHeaders], headers)
					}
				}
				const inputBody = input instanceof _Request ? input[kState].body : null
				if (
					(init2.body != undefined || inputBody != undefined) &&
					(request.method === "GET" || request.method === "HEAD")
				) {
					throw new TypeError("Request with GET/HEAD method cannot have body.")
				}
				let initBody = null
				if (init2.body != undefined) {
					const [extractedBody, contentType] = extractBody(init2.body, request.keepalive)
					initBody = extractedBody
					if (contentType && !this[kHeaders][kHeadersList].contains("content-type")) {
						this[kHeaders].append("content-type", contentType)
					}
				}
				const inputOrInitBody = initBody ?? inputBody
				if (inputOrInitBody != undefined && inputOrInitBody.source == undefined) {
					if (initBody != undefined && init2.duplex == undefined) {
						throw new TypeError("RequestInit: duplex option is required when sending a body.")
					}
					if (request.mode !== "same-origin" && request.mode !== "cors") {
						throw new TypeError(
							'If request is made from ReadableStream, mode should be "same-origin" or "cors"'
						)
					}
					request.useCORSPreflightFlag = true
				}
				let finalBody = inputOrInitBody
				if (initBody == undefined && inputBody != undefined) {
					if (util.isDisturbed(inputBody.stream) || inputBody.stream.locked) {
						throw new TypeError(
							"Cannot construct a Request with a Request object that has already been used."
						)
					}
					if (!TransformStream) {
						TransformStream = require("node:stream/web").TransformStream
					}
					const identityTransform = new TransformStream()
					inputBody.stream.pipeThrough(identityTransform)
					finalBody = {
						source: inputBody.source,
						length: inputBody.length,
						stream: identityTransform.readable,
					}
				}
				this[kState].body = finalBody
			}
			// Returns requests HTTP method, which is "GET" by default.
			get method() {
				webidl.brandCheck(this, _Request)
				return this[kState].method
			}
			// Returns the URL of request as a string.
			get url() {
				webidl.brandCheck(this, _Request)
				return URLSerializer(this[kState].url)
			}
			// Returns a Headers object consisting of the headers associated with request.
			// Note that headers added in the network layer by the user agent will not
			// be accounted for in this object, e.g., the "Host" header.
			get headers() {
				webidl.brandCheck(this, _Request)
				return this[kHeaders]
			}
			// Returns the kind of resource requested by request, e.g., "document"
			// or "script".
			get destination() {
				webidl.brandCheck(this, _Request)
				return this[kState].destination
			}
			// Returns the referrer of request. Its value can be a same-origin URL if
			// explicitly set in init, the empty string to indicate no referrer, and
			// "about:client" when defaulting to the globals default. This is used
			// during fetching to determine the value of the `Referer` header of the
			// request being made.
			get referrer() {
				webidl.brandCheck(this, _Request)
				if (this[kState].referrer === "no-referrer") {
					return ""
				}
				if (this[kState].referrer === "client") {
					return "about:client"
				}
				return this[kState].referrer.toString()
			}
			// Returns the referrer policy associated with request.
			// This is used during fetching to compute the value of the requests
			// referrer.
			get referrerPolicy() {
				webidl.brandCheck(this, _Request)
				return this[kState].referrerPolicy
			}
			// Returns the mode associated with request, which is a string indicating
			// whether the request will use CORS, or will be restricted to same-origin
			// URLs.
			get mode() {
				webidl.brandCheck(this, _Request)
				return this[kState].mode
			}
			// Returns the credentials mode associated with request,
			// which is a string indicating whether credentials will be sent with the
			// request always, never, or only when sent to a same-origin URL.
			get credentials() {
				return this[kState].credentials
			}
			// Returns the cache mode associated with request,
			// which is a string indicating how the request will
			// interact with the browsers cache when fetching.
			get cache() {
				webidl.brandCheck(this, _Request)
				return this[kState].cache
			}
			// Returns the redirect mode associated with request,
			// which is a string indicating how redirects for the
			// request will be handled during fetching. A request
			// will follow redirects by default.
			get redirect() {
				webidl.brandCheck(this, _Request)
				return this[kState].redirect
			}
			// Returns requests subresource integrity metadata, which is a
			// cryptographic hash of the resource being fetched. Its value
			// consists of multiple hashes separated by whitespace. [SRI]
			get integrity() {
				webidl.brandCheck(this, _Request)
				return this[kState].integrity
			}
			// Returns a boolean indicating whether or not request can outlive the
			// global in which it was created.
			get keepalive() {
				webidl.brandCheck(this, _Request)
				return this[kState].keepalive
			}
			// Returns a boolean indicating whether or not request is for a reload
			// navigation.
			get isReloadNavigation() {
				webidl.brandCheck(this, _Request)
				return this[kState].reloadNavigation
			}
			// Returns a boolean indicating whether or not request is for a history
			// navigation (a.k.a. back-foward navigation).
			get isHistoryNavigation() {
				webidl.brandCheck(this, _Request)
				return this[kState].historyNavigation
			}
			// Returns the signal associated with request, which is an AbortSignal
			// object indicating whether or not request has been aborted, and its
			// abort event handler.
			get signal() {
				webidl.brandCheck(this, _Request)
				return this[kSignal]
			}
			get body() {
				webidl.brandCheck(this, _Request)
				return this[kState].body ? this[kState].body.stream : null
			}
			get bodyUsed() {
				webidl.brandCheck(this, _Request)
				return !!this[kState].body && util.isDisturbed(this[kState].body.stream)
			}
			get duplex() {
				webidl.brandCheck(this, _Request)
				return "half"
			}
			// Returns a clone of request.
			clone() {
				webidl.brandCheck(this, _Request)
				if (this.bodyUsed || this.body?.locked) {
					throw new TypeError("unusable")
				}
				const clonedRequest = cloneRequest(this[kState])
				const clonedRequestObject = new _Request(kConstruct)
				clonedRequestObject[kState] = clonedRequest
				clonedRequestObject[kRealm] = this[kRealm]
				clonedRequestObject[kHeaders] = new Headers(kConstruct)
				clonedRequestObject[kHeaders][kHeadersList] = clonedRequest.headersList
				clonedRequestObject[kHeaders][kGuard] = this[kHeaders][kGuard]
				clonedRequestObject[kHeaders][kRealm] = this[kHeaders][kRealm]
				const ac = new AbortController()
				if (this.signal.aborted) {
					ac.abort(this.signal.reason)
				} else {
					util.addAbortListener(this.signal, () => {
						ac.abort(this.signal.reason)
					})
				}
				clonedRequestObject[kSignal] = ac.signal
				return clonedRequestObject
			}
		}
		mixinBody(Request)
		function makeRequest(init2) {
			const request = {
				method: "GET",
				localURLsOnly: false,
				unsafeRequest: false,
				body: null,
				client: null,
				reservedClient: null,
				replacesClientId: "",
				window: "client",
				keepalive: false,
				serviceWorkers: "all",
				initiator: "",
				destination: "",
				priority: null,
				origin: "client",
				policyContainer: "client",
				referrer: "client",
				referrerPolicy: "",
				mode: "no-cors",
				useCORSPreflightFlag: false,
				credentials: "same-origin",
				useCredentials: false,
				cache: "default",
				redirect: "follow",
				integrity: "",
				cryptoGraphicsNonceMetadata: "",
				parserMetadata: "",
				reloadNavigation: false,
				historyNavigation: false,
				userActivation: false,
				taintedOrigin: false,
				redirectCount: 0,
				responseTainting: "basic",
				preventNoCacheCacheControlHeaderModification: false,
				done: false,
				timingAllowFailed: false,
				...init2,
				headersList: init2.headersList ? new HeadersList(init2.headersList) : new HeadersList(),
			}
			request.url = request.urlList[0]
			return request
		}
		function cloneRequest(request) {
			const newRequest = makeRequest({ ...request, body: null })
			if (request.body != undefined) {
				newRequest.body = cloneBody(request.body)
			}
			return newRequest
		}
		Object.defineProperties(Request.prototype, {
			method: kEnumerableProperty,
			url: kEnumerableProperty,
			headers: kEnumerableProperty,
			redirect: kEnumerableProperty,
			clone: kEnumerableProperty,
			signal: kEnumerableProperty,
			duplex: kEnumerableProperty,
			destination: kEnumerableProperty,
			body: kEnumerableProperty,
			bodyUsed: kEnumerableProperty,
			isHistoryNavigation: kEnumerableProperty,
			isReloadNavigation: kEnumerableProperty,
			keepalive: kEnumerableProperty,
			integrity: kEnumerableProperty,
			cache: kEnumerableProperty,
			credentials: kEnumerableProperty,
			attribute: kEnumerableProperty,
			referrerPolicy: kEnumerableProperty,
			referrer: kEnumerableProperty,
			mode: kEnumerableProperty,
			[Symbol.toStringTag]: {
				value: "Request",
				configurable: true,
			},
		})
		webidl.converters.Request = webidl.interfaceConverter(Request)
		webidl.converters.RequestInfo = function (V) {
			if (typeof V === "string") {
				return webidl.converters.USVString(V)
			}
			if (V instanceof Request) {
				return webidl.converters.Request(V)
			}
			return webidl.converters.USVString(V)
		}
		webidl.converters.AbortSignal = webidl.interfaceConverter(AbortSignal)
		webidl.converters.RequestInit = webidl.dictionaryConverter([
			{
				key: "method",
				converter: webidl.converters.ByteString,
			},
			{
				key: "headers",
				converter: webidl.converters.HeadersInit,
			},
			{
				key: "body",
				converter: webidl.nullableConverter(webidl.converters.BodyInit),
			},
			{
				key: "referrer",
				converter: webidl.converters.USVString,
			},
			{
				key: "referrerPolicy",
				converter: webidl.converters.DOMString,
				// https://w3c.github.io/webappsec-referrer-policy/#referrer-policy
				allowedValues: referrerPolicy,
			},
			{
				key: "mode",
				converter: webidl.converters.DOMString,
				// https://fetch.spec.whatwg.org/#concept-request-mode
				allowedValues: requestMode,
			},
			{
				key: "credentials",
				converter: webidl.converters.DOMString,
				// https://fetch.spec.whatwg.org/#requestcredentials
				allowedValues: requestCredentials,
			},
			{
				key: "cache",
				converter: webidl.converters.DOMString,
				// https://fetch.spec.whatwg.org/#requestcache
				allowedValues: requestCache,
			},
			{
				key: "redirect",
				converter: webidl.converters.DOMString,
				// https://fetch.spec.whatwg.org/#requestredirect
				allowedValues: requestRedirect,
			},
			{
				key: "integrity",
				converter: webidl.converters.DOMString,
			},
			{
				key: "keepalive",
				converter: webidl.converters.boolean,
			},
			{
				key: "signal",
				converter: webidl.nullableConverter((signal) =>
					webidl.converters.AbortSignal(signal, { strict: false })
				),
			},
			{
				key: "window",
				converter: webidl.converters.any,
			},
			{
				key: "duplex",
				converter: webidl.converters.DOMString,
				allowedValues: requestDuplex,
			},
		])
		module2.exports = { Request, makeRequest }
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/index.js
var require_fetch = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fetch/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var {
			Response: Response2,
			makeNetworkError,
			makeAppropriateNetworkError,
			filterResponse,
			makeResponse,
		} = require_response()
		var { Headers } = require_headers()
		var { Request, makeRequest } = require_request2()
		var zlib = require("node:zlib")
		var {
			bytesMatch,
			makePolicyContainer,
			clonePolicyContainer,
			requestBadPort,
			TAOCheck,
			appendRequestOriginHeader,
			responseLocationURL,
			requestCurrentURL,
			setRequestReferrerPolicyOnRedirect,
			tryUpgradeRequestToAPotentiallyTrustworthyURL,
			createOpaqueTimingInfo,
			appendFetchMetadata,
			corsCheck,
			crossOriginResourcePolicyCheck,
			determineRequestsReferrer,
			coarsenedSharedCurrentTime,
			createDeferredPromise,
			isBlobLike,
			sameOrigin,
			isCancelled,
			isAborted,
			isErrorLike,
			fullyReadBody,
			readableStreamClose,
			isomorphicEncode,
			urlIsLocal,
			urlIsHttpHttpsScheme,
			urlHasHttpsScheme,
		} = require_util2()
		var { kState, kHeaders, kGuard, kRealm } = require_symbols2()
		var assert = require("node:assert")
		var { safelyExtractBody } = require_body()
		var {
			redirectStatusSet,
			nullBodyStatus,
			safeMethodsSet,
			requestBodyHeader,
			subresourceSet,
			DOMException: DOMException2,
		} = require_constants2()
		var { kHeadersList } = require_symbols()
		var EE = require("node:events")
		var { Readable, pipeline } = require("node:stream")
		var { addAbortListener, isErrored, isReadable, nodeMajor, nodeMinor } = require_util()
		var { dataURLProcessor, serializeAMimeType } = require_dataURL()
		var { TransformStream } = require("node:stream/web")
		var { getGlobalDispatcher } = require_global2()
		var { webidl } = require_webidl()
		var { STATUS_CODES } = require("node:http")
		var GET_OR_HEAD = ["GET", "HEAD"]
		var resolveObjectURL
		var ReadableStream = globalThis.ReadableStream
		var Fetch = class extends EE {
			constructor(dispatcher) {
				super()
				this.dispatcher = dispatcher
				this.connection = null
				this.dump = false
				this.state = "ongoing"
				this.setMaxListeners(21)
			}
			terminate(reason) {
				if (this.state !== "ongoing") {
					return
				}
				this.state = "terminated"
				this.connection?.destroy(reason)
				this.emit("terminated", reason)
			}
			// https://fetch.spec.whatwg.org/#fetch-controller-abort
			abort(error) {
				if (this.state !== "ongoing") {
					return
				}
				this.state = "aborted"
				if (!error) {
					error = new DOMException2("The operation was aborted.", "AbortError")
				}
				this.serializedAbortReason = error
				this.connection?.destroy(error)
				this.emit("terminated", error)
			}
		}
		function fetch3(input, init2 = {}) {
			webidl.argumentLengthCheck(arguments, 1, { header: "globalThis.fetch" })
			const p = createDeferredPromise()
			let requestObject
			try {
				requestObject = new Request(input, init2)
			} catch (e) {
				p.reject(e)
				return p.promise
			}
			const request = requestObject[kState]
			if (requestObject.signal.aborted) {
				abortFetch(p, request, null, requestObject.signal.reason)
				return p.promise
			}
			const globalObject = request.client.globalObject
			if (globalObject?.constructor?.name === "ServiceWorkerGlobalScope") {
				request.serviceWorkers = "none"
			}
			let responseObject = null
			const relevantRealm = null
			let locallyAborted = false
			let controller = null
			addAbortListener(requestObject.signal, () => {
				locallyAborted = true
				assert(controller != undefined)
				controller.abort(requestObject.signal.reason)
				abortFetch(p, request, responseObject, requestObject.signal.reason)
			})
			const handleFetchDone = (response) => finalizeAndReportTiming(response, "fetch")
			const processResponse = (response) => {
				if (locallyAborted) {
					return Promise.resolve()
				}
				if (response.aborted) {
					abortFetch(p, request, responseObject, controller.serializedAbortReason)
					return Promise.resolve()
				}
				if (response.type === "error") {
					p.reject(Object.assign(new TypeError("fetch failed"), { cause: response.error }))
					return Promise.resolve()
				}
				responseObject = new Response2()
				responseObject[kState] = response
				responseObject[kRealm] = relevantRealm
				responseObject[kHeaders][kHeadersList] = response.headersList
				responseObject[kHeaders][kGuard] = "immutable"
				responseObject[kHeaders][kRealm] = relevantRealm
				p.resolve(responseObject)
			}
			controller = fetching({
				request,
				processResponseEndOfBody: handleFetchDone,
				processResponse,
				dispatcher: init2.dispatcher ?? getGlobalDispatcher(),
				// undici
			})
			return p.promise
		}
		function finalizeAndReportTiming(response, initiatorType = "other") {
			if (response.type === "error" && response.aborted) {
				return
			}
			if (!response.urlList?.length) {
				return
			}
			const originalURL = response.urlList[0]
			let timingInfo = response.timingInfo
			let cacheState = response.cacheState
			if (!urlIsHttpHttpsScheme(originalURL)) {
				return
			}
			if (timingInfo === null) {
				return
			}
			if (!response.timingAllowPassed) {
				timingInfo = createOpaqueTimingInfo({
					startTime: timingInfo.startTime,
				})
				cacheState = ""
			}
			timingInfo.endTime = coarsenedSharedCurrentTime()
			response.timingInfo = timingInfo
			markResourceTiming(timingInfo, originalURL, initiatorType, globalThis, cacheState)
		}
		function markResourceTiming(timingInfo, originalURL, initiatorType, globalThis2, cacheState) {
			if (nodeMajor > 18 || (nodeMajor === 18 && nodeMinor >= 2)) {
				performance.markResourceTiming(
					timingInfo,
					originalURL.href,
					initiatorType,
					globalThis2,
					cacheState
				)
			}
		}
		function abortFetch(p, request, responseObject, error) {
			if (!error) {
				error = new DOMException2("The operation was aborted.", "AbortError")
			}
			p.reject(error)
			if (request.body != undefined && isReadable(request.body?.stream)) {
				request.body.stream.cancel(error).catch((err) => {
					if (err.code === "ERR_INVALID_STATE") {
						return
					}
					throw err
				})
			}
			if (responseObject == undefined) {
				return
			}
			const response = responseObject[kState]
			if (response.body != undefined && isReadable(response.body?.stream)) {
				response.body.stream.cancel(error).catch((err) => {
					if (err.code === "ERR_INVALID_STATE") {
						return
					}
					throw err
				})
			}
		}
		function fetching({
			request,
			processRequestBodyChunkLength,
			processRequestEndOfBody,
			processResponse,
			processResponseEndOfBody,
			processResponseConsumeBody,
			useParallelQueue = false,
			dispatcher,
			// undici
		}) {
			let taskDestination = null
			let crossOriginIsolatedCapability = false
			if (request.client != undefined) {
				taskDestination = request.client.globalObject
				crossOriginIsolatedCapability = request.client.crossOriginIsolatedCapability
			}
			const currenTime = coarsenedSharedCurrentTime(crossOriginIsolatedCapability)
			const timingInfo = createOpaqueTimingInfo({
				startTime: currenTime,
			})
			const fetchParams = {
				controller: new Fetch(dispatcher),
				request,
				timingInfo,
				processRequestBodyChunkLength,
				processRequestEndOfBody,
				processResponse,
				processResponseConsumeBody,
				processResponseEndOfBody,
				taskDestination,
				crossOriginIsolatedCapability,
			}
			assert(!request.body || request.body.stream)
			if (request.window === "client") {
				request.window =
					request.client?.globalObject?.constructor?.name === "Window"
						? request.client
						: "no-window"
			}
			if (request.origin === "client") {
				request.origin = request.client?.origin
			}
			if (request.policyContainer === "client") {
				if (request.client != undefined) {
					request.policyContainer = clonePolicyContainer(request.client.policyContainer)
				} else {
					request.policyContainer = makePolicyContainer()
				}
			}
			if (!request.headersList.contains("accept")) {
				const value = "*/*"
				request.headersList.append("accept", value)
			}
			if (!request.headersList.contains("accept-language")) {
				request.headersList.append("accept-language", "*")
			}
			if (request.priority === null) {
			}
			if (subresourceSet.has(request.destination)) {
			}
			mainFetch(fetchParams).catch((err) => {
				fetchParams.controller.terminate(err)
			})
			return fetchParams.controller
		}
		async function mainFetch(fetchParams, recursive = false) {
			const request = fetchParams.request
			let response = null
			if (request.localURLsOnly && !urlIsLocal(requestCurrentURL(request))) {
				response = makeNetworkError("local URLs only")
			}
			tryUpgradeRequestToAPotentiallyTrustworthyURL(request)
			if (requestBadPort(request) === "blocked") {
				response = makeNetworkError("bad port")
			}
			if (request.referrerPolicy === "") {
				request.referrerPolicy = request.policyContainer.referrerPolicy
			}
			if (request.referrer !== "no-referrer") {
				request.referrer = determineRequestsReferrer(request)
			}
			if (response === null) {
				response = await (async () => {
					const currentURL = requestCurrentURL(request)
					if (
						// - requests current URLs origin is same origin with requests origin,
						//   and requests response tainting is "basic"
						(sameOrigin(currentURL, request.url) && request.responseTainting === "basic") || // requests current URLs scheme is "data"
						currentURL.protocol === "data:" || // - requests mode is "navigate" or "websocket"
						request.mode === "navigate" ||
						request.mode === "websocket"
					) {
						request.responseTainting = "basic"
						return await schemeFetch(fetchParams)
					}
					if (request.mode === "same-origin") {
						return makeNetworkError('request mode cannot be "same-origin"')
					}
					if (request.mode === "no-cors") {
						if (request.redirect !== "follow") {
							return makeNetworkError('redirect mode cannot be "follow" for "no-cors" request')
						}
						request.responseTainting = "opaque"
						return await schemeFetch(fetchParams)
					}
					if (!urlIsHttpHttpsScheme(requestCurrentURL(request))) {
						return makeNetworkError("URL scheme must be a HTTP(S) scheme")
					}
					request.responseTainting = "cors"
					return await httpFetch(fetchParams)
				})()
			}
			if (recursive) {
				return response
			}
			if (response.status !== 0 && !response.internalResponse) {
				if (request.responseTainting === "cors") {
				}
				if (request.responseTainting === "basic") {
					response = filterResponse(response, "basic")
				} else if (request.responseTainting === "cors") {
					response = filterResponse(response, "cors")
				} else if (request.responseTainting === "opaque") {
					response = filterResponse(response, "opaque")
				} else {
					assert(false)
				}
			}
			let internalResponse = response.status === 0 ? response : response.internalResponse
			if (internalResponse.urlList.length === 0) {
				internalResponse.urlList.push(...request.urlList)
			}
			if (!request.timingAllowFailed) {
				response.timingAllowPassed = true
			}
			if (
				response.type === "opaque" &&
				internalResponse.status === 206 &&
				internalResponse.rangeRequested &&
				!request.headers.contains("range")
			) {
				response = internalResponse = makeNetworkError()
			}
			if (
				response.status !== 0 &&
				(request.method === "HEAD" ||
					request.method === "CONNECT" ||
					nullBodyStatus.includes(internalResponse.status))
			) {
				internalResponse.body = null
				fetchParams.controller.dump = true
			}
			if (request.integrity) {
				const processBodyError = (reason) => fetchFinale(fetchParams, makeNetworkError(reason))
				if (request.responseTainting === "opaque" || response.body == undefined) {
					processBodyError(response.error)
					return
				}
				const processBody = (bytes) => {
					if (!bytesMatch(bytes, request.integrity)) {
						processBodyError("integrity mismatch")
						return
					}
					response.body = safelyExtractBody(bytes)[0]
					fetchFinale(fetchParams, response)
				}
				await fullyReadBody(response.body, processBody, processBodyError)
			} else {
				fetchFinale(fetchParams, response)
			}
		}
		function schemeFetch(fetchParams) {
			if (isCancelled(fetchParams) && fetchParams.request.redirectCount === 0) {
				return Promise.resolve(makeAppropriateNetworkError(fetchParams))
			}
			const { request } = fetchParams
			const { protocol: scheme } = requestCurrentURL(request)
			switch (scheme) {
				case "about:": {
					return Promise.resolve(makeNetworkError("about scheme is not supported"))
				}
				case "blob:": {
					if (!resolveObjectURL) {
						resolveObjectURL = require("node:buffer").resolveObjectURL
					}
					const blobURLEntry = requestCurrentURL(request)
					if (blobURLEntry.search.length !== 0) {
						return Promise.resolve(
							makeNetworkError("NetworkError when attempting to fetch resource.")
						)
					}
					const blobURLEntryObject = resolveObjectURL(blobURLEntry.toString())
					if (request.method !== "GET" || !isBlobLike(blobURLEntryObject)) {
						return Promise.resolve(makeNetworkError("invalid method"))
					}
					const bodyWithType = safelyExtractBody(blobURLEntryObject)
					const body = bodyWithType[0]
					const length = isomorphicEncode(`${body.length}`)
					const type = bodyWithType[1] ?? ""
					const response = makeResponse({
						statusText: "OK",
						headersList: [
							["content-length", { name: "Content-Length", value: length }],
							["content-type", { name: "Content-Type", value: type }],
						],
					})
					response.body = body
					return Promise.resolve(response)
				}
				case "data:": {
					const currentURL = requestCurrentURL(request)
					const dataURLStruct = dataURLProcessor(currentURL)
					if (dataURLStruct === "failure") {
						return Promise.resolve(makeNetworkError("failed to fetch the data URL"))
					}
					const mimeType = serializeAMimeType(dataURLStruct.mimeType)
					return Promise.resolve(
						makeResponse({
							statusText: "OK",
							headersList: [["content-type", { name: "Content-Type", value: mimeType }]],
							body: safelyExtractBody(dataURLStruct.body)[0],
						})
					)
				}
				case "file:": {
					return Promise.resolve(makeNetworkError("not implemented... yet..."))
				}
				case "http:":
				case "https:": {
					return httpFetch(fetchParams).catch((err) => makeNetworkError(err))
				}
				default: {
					return Promise.resolve(makeNetworkError("unknown scheme"))
				}
			}
		}
		function finalizeResponse(fetchParams, response) {
			fetchParams.request.done = true
			if (fetchParams.processResponseDone != undefined) {
				queueMicrotask(() => fetchParams.processResponseDone(response))
			}
		}
		function fetchFinale(fetchParams, response) {
			if (response.type === "error") {
				response.urlList = [fetchParams.request.urlList[0]]
				response.timingInfo = createOpaqueTimingInfo({
					startTime: fetchParams.timingInfo.startTime,
				})
			}
			const processResponseEndOfBody = () => {
				fetchParams.request.done = true
				if (fetchParams.processResponseEndOfBody != undefined) {
					queueMicrotask(() => fetchParams.processResponseEndOfBody(response))
				}
			}
			if (fetchParams.processResponse != undefined) {
				queueMicrotask(() => fetchParams.processResponse(response))
			}
			if (response.body == undefined) {
				processResponseEndOfBody()
			} else {
				const identityTransformAlgorithm = (chunk, controller) => {
					controller.enqueue(chunk)
				}
				const transformStream = new TransformStream(
					{
						start() {},
						transform: identityTransformAlgorithm,
						flush: processResponseEndOfBody,
					},
					{
						size() {
							return 1
						},
					},
					{
						size() {
							return 1
						},
					}
				)
				response.body = { stream: response.body.stream.pipeThrough(transformStream) }
			}
			if (fetchParams.processResponseConsumeBody != undefined) {
				const processBody = (nullOrBytes) =>
					fetchParams.processResponseConsumeBody(response, nullOrBytes)
				const processBodyError = (failure) =>
					fetchParams.processResponseConsumeBody(response, failure)
				if (response.body == undefined) {
					queueMicrotask(() => processBody(null))
				} else {
					return fullyReadBody(response.body, processBody, processBodyError)
				}
				return Promise.resolve()
			}
		}
		async function httpFetch(fetchParams) {
			const request = fetchParams.request
			let response = null
			let actualResponse = null
			const timingInfo = fetchParams.timingInfo
			if (request.serviceWorkers === "all") {
			}
			if (response === null) {
				if (request.redirect === "follow") {
					request.serviceWorkers = "none"
				}
				actualResponse = response = await httpNetworkOrCacheFetch(fetchParams)
				if (request.responseTainting === "cors" && corsCheck(request, response) === "failure") {
					return makeNetworkError("cors failure")
				}
				if (TAOCheck(request, response) === "failure") {
					request.timingAllowFailed = true
				}
			}
			if (
				(request.responseTainting === "opaque" || response.type === "opaque") &&
				crossOriginResourcePolicyCheck(
					request.origin,
					request.client,
					request.destination,
					actualResponse
				) === "blocked"
			) {
				return makeNetworkError("blocked")
			}
			if (redirectStatusSet.has(actualResponse.status)) {
				if (request.redirect !== "manual") {
					fetchParams.controller.connection.destroy()
				}
				if (request.redirect === "error") {
					response = makeNetworkError("unexpected redirect")
				} else if (request.redirect === "manual") {
					response = actualResponse
				} else if (request.redirect === "follow") {
					response = await httpRedirectFetch(fetchParams, response)
				} else {
					assert(false)
				}
			}
			response.timingInfo = timingInfo
			return response
		}
		function httpRedirectFetch(fetchParams, response) {
			const request = fetchParams.request
			const actualResponse = response.internalResponse ? response.internalResponse : response
			let locationURL
			try {
				locationURL = responseLocationURL(actualResponse, requestCurrentURL(request).hash)
				if (locationURL == undefined) {
					return response
				}
			} catch (err) {
				return Promise.resolve(makeNetworkError(err))
			}
			if (!urlIsHttpHttpsScheme(locationURL)) {
				return Promise.resolve(makeNetworkError("URL scheme must be a HTTP(S) scheme"))
			}
			if (request.redirectCount === 20) {
				return Promise.resolve(makeNetworkError("redirect count exceeded"))
			}
			request.redirectCount += 1
			if (
				request.mode === "cors" &&
				(locationURL.username || locationURL.password) &&
				!sameOrigin(request, locationURL)
			) {
				return Promise.resolve(makeNetworkError('cross origin not allowed for request mode "cors"'))
			}
			if (request.responseTainting === "cors" && (locationURL.username || locationURL.password)) {
				return Promise.resolve(
					makeNetworkError('URL cannot contain credentials for request mode "cors"')
				)
			}
			if (
				actualResponse.status !== 303 &&
				request.body != undefined &&
				request.body.source == undefined
			) {
				return Promise.resolve(makeNetworkError())
			}
			if (
				([301, 302].includes(actualResponse.status) && request.method === "POST") ||
				(actualResponse.status === 303 && !GET_OR_HEAD.includes(request.method))
			) {
				request.method = "GET"
				request.body = null
				for (const headerName of requestBodyHeader) {
					request.headersList.delete(headerName)
				}
			}
			if (!sameOrigin(requestCurrentURL(request), locationURL)) {
				request.headersList.delete("authorization")
				request.headersList.delete("proxy-authorization", true)
				request.headersList.delete("cookie")
				request.headersList.delete("host")
			}
			if (request.body != undefined) {
				assert(request.body.source != undefined)
				request.body = safelyExtractBody(request.body.source)[0]
			}
			const timingInfo = fetchParams.timingInfo
			timingInfo.redirectEndTime = timingInfo.postRedirectStartTime = coarsenedSharedCurrentTime(
				fetchParams.crossOriginIsolatedCapability
			)
			if (timingInfo.redirectStartTime === 0) {
				timingInfo.redirectStartTime = timingInfo.startTime
			}
			request.urlList.push(locationURL)
			setRequestReferrerPolicyOnRedirect(request, actualResponse)
			return mainFetch(fetchParams, true)
		}
		async function httpNetworkOrCacheFetch(
			fetchParams,
			isAuthenticationFetch = false,
			isNewConnectionFetch = false
		) {
			const request = fetchParams.request
			let httpFetchParams = null
			let httpRequest = null
			let response = null
			const httpCache = null
			const revalidatingFlag = false
			if (request.window === "no-window" && request.redirect === "error") {
				httpFetchParams = fetchParams
				httpRequest = request
			} else {
				httpRequest = makeRequest(request)
				httpFetchParams = { ...fetchParams }
				httpFetchParams.request = httpRequest
			}
			const includeCredentials =
				request.credentials === "include" ||
				(request.credentials === "same-origin" && request.responseTainting === "basic")
			const contentLength = httpRequest.body ? httpRequest.body.length : null
			let contentLengthHeaderValue = null
			if (httpRequest.body == undefined && ["POST", "PUT"].includes(httpRequest.method)) {
				contentLengthHeaderValue = "0"
			}
			if (contentLength != undefined) {
				contentLengthHeaderValue = isomorphicEncode(`${contentLength}`)
			}
			if (contentLengthHeaderValue != undefined) {
				httpRequest.headersList.append("content-length", contentLengthHeaderValue)
			}
			if (contentLength != undefined && httpRequest.keepalive) {
			}
			if (httpRequest.referrer instanceof URL) {
				httpRequest.headersList.append("referer", isomorphicEncode(httpRequest.referrer.href))
			}
			appendRequestOriginHeader(httpRequest)
			appendFetchMetadata(httpRequest)
			if (!httpRequest.headersList.contains("user-agent")) {
				httpRequest.headersList.append(
					"user-agent",
					typeof esbuildDetection === "undefined" ? "undici" : "node"
				)
			}
			if (
				httpRequest.cache === "default" &&
				(httpRequest.headersList.contains("if-modified-since") ||
					httpRequest.headersList.contains("if-none-match") ||
					httpRequest.headersList.contains("if-unmodified-since") ||
					httpRequest.headersList.contains("if-match") ||
					httpRequest.headersList.contains("if-range"))
			) {
				httpRequest.cache = "no-store"
			}
			if (
				httpRequest.cache === "no-cache" &&
				!httpRequest.preventNoCacheCacheControlHeaderModification &&
				!httpRequest.headersList.contains("cache-control")
			) {
				httpRequest.headersList.append("cache-control", "max-age=0")
			}
			if (httpRequest.cache === "no-store" || httpRequest.cache === "reload") {
				if (!httpRequest.headersList.contains("pragma")) {
					httpRequest.headersList.append("pragma", "no-cache")
				}
				if (!httpRequest.headersList.contains("cache-control")) {
					httpRequest.headersList.append("cache-control", "no-cache")
				}
			}
			if (httpRequest.headersList.contains("range")) {
				httpRequest.headersList.append("accept-encoding", "identity")
			}
			if (!httpRequest.headersList.contains("accept-encoding")) {
				if (urlHasHttpsScheme(requestCurrentURL(httpRequest))) {
					httpRequest.headersList.append("accept-encoding", "br, gzip, deflate")
				} else {
					httpRequest.headersList.append("accept-encoding", "gzip, deflate")
				}
			}
			httpRequest.headersList.delete("host")
			if (includeCredentials) {
			}
			if (httpCache == undefined) {
				httpRequest.cache = "no-store"
			}
			if (httpRequest.mode !== "no-store" && httpRequest.mode !== "reload") {
			}
			if (response == undefined) {
				if (httpRequest.mode === "only-if-cached") {
					return makeNetworkError("only if cached")
				}
				const forwardResponse = await httpNetworkFetch(
					httpFetchParams,
					includeCredentials,
					isNewConnectionFetch
				)
				if (
					!safeMethodsSet.has(httpRequest.method) &&
					forwardResponse.status >= 200 &&
					forwardResponse.status <= 399
				) {
				}
				if (revalidatingFlag && forwardResponse.status === 304) {
				}
				if (response == undefined) {
					response = forwardResponse
				}
			}
			response.urlList = [...httpRequest.urlList]
			if (httpRequest.headersList.contains("range")) {
				response.rangeRequested = true
			}
			response.requestIncludesCredentials = includeCredentials
			if (response.status === 407) {
				if (request.window === "no-window") {
					return makeNetworkError()
				}
				if (isCancelled(fetchParams)) {
					return makeAppropriateNetworkError(fetchParams)
				}
				return makeNetworkError("proxy authentication required")
			}
			if (
				// responses status is 421
				response.status === 421 && // isNewConnectionFetch is false
				!isNewConnectionFetch && // requests body is null, or requests body is non-null and requests bodys source is non-null
				(request.body == undefined || request.body.source != undefined)
			) {
				if (isCancelled(fetchParams)) {
					return makeAppropriateNetworkError(fetchParams)
				}
				fetchParams.controller.connection.destroy()
				response = await httpNetworkOrCacheFetch(fetchParams, isAuthenticationFetch, true)
			}
			if (isAuthenticationFetch) {
			}
			return response
		}
		async function httpNetworkFetch(
			fetchParams,
			includeCredentials = false,
			forceNewConnection = false
		) {
			assert(!fetchParams.controller.connection || fetchParams.controller.connection.destroyed)
			fetchParams.controller.connection = {
				abort: null,
				destroyed: false,
				destroy(err) {
					if (!this.destroyed) {
						this.destroyed = true
						this.abort?.(err ?? new DOMException2("The operation was aborted.", "AbortError"))
					}
				},
			}
			const request = fetchParams.request
			let response = null
			const timingInfo = fetchParams.timingInfo
			const httpCache = null
			if (httpCache == undefined) {
				request.cache = "no-store"
			}
			const newConnection = forceNewConnection ? "yes" : "no"
			if (request.mode === "websocket") {
			} else {
			}
			let requestBody = null
			if (request.body == undefined && fetchParams.processRequestEndOfBody) {
				queueMicrotask(() => fetchParams.processRequestEndOfBody())
			} else if (request.body != undefined) {
				const processBodyChunk = async function* (bytes) {
					if (isCancelled(fetchParams)) {
						return
					}
					yield bytes
					fetchParams.processRequestBodyChunkLength?.(bytes.byteLength)
				}
				const processEndOfBody = () => {
					if (isCancelled(fetchParams)) {
						return
					}
					if (fetchParams.processRequestEndOfBody) {
						fetchParams.processRequestEndOfBody()
					}
				}
				const processBodyError = (e) => {
					if (isCancelled(fetchParams)) {
						return
					}
					if (e.name === "AbortError") {
						fetchParams.controller.abort()
					} else {
						fetchParams.controller.terminate(e)
					}
				}
				requestBody = (async function* () {
					try {
						for await (const bytes of request.body.stream) {
							yield* processBodyChunk(bytes)
						}
						processEndOfBody()
					} catch (err) {
						processBodyError(err)
					}
				})()
			}
			try {
				const {
					body,
					status: status3,
					statusText,
					headersList,
					socket,
				} = await dispatch({ body: requestBody })
				if (socket) {
					response = makeResponse({ status: status3, statusText, headersList, socket })
				} else {
					const iterator = body[Symbol.asyncIterator]()
					fetchParams.controller.next = () => iterator.next()
					response = makeResponse({ status: status3, statusText, headersList })
				}
			} catch (err) {
				if (err.name === "AbortError") {
					fetchParams.controller.connection.destroy()
					return makeAppropriateNetworkError(fetchParams, err)
				}
				return makeNetworkError(err)
			}
			const pullAlgorithm = () => {
				fetchParams.controller.resume()
			}
			const cancelAlgorithm = (reason) => {
				fetchParams.controller.abort(reason)
			}
			if (!ReadableStream) {
				ReadableStream = require("node:stream/web").ReadableStream
			}
			const stream = new ReadableStream(
				{
					async start(controller) {
						fetchParams.controller.controller = controller
					},
					async pull(controller) {
						await pullAlgorithm(controller)
					},
					async cancel(reason) {
						await cancelAlgorithm(reason)
					},
				},
				{
					highWaterMark: 0,
					size() {
						return 1
					},
				}
			)
			response.body = { stream }
			fetchParams.controller.on("terminated", onAborted)
			fetchParams.controller.resume = async () => {
				while (true) {
					let bytes
					let isFailure
					try {
						const { done, value } = await fetchParams.controller.next()
						if (isAborted(fetchParams)) {
							break
						}
						bytes = done ? void 0 : value
					} catch (err) {
						if (fetchParams.controller.ended && !timingInfo.encodedBodySize) {
							bytes = void 0
						} else {
							bytes = err
							isFailure = true
						}
					}
					if (bytes === void 0) {
						readableStreamClose(fetchParams.controller.controller)
						finalizeResponse(fetchParams, response)
						return
					}
					timingInfo.decodedBodySize += bytes?.byteLength ?? 0
					if (isFailure) {
						fetchParams.controller.terminate(bytes)
						return
					}
					fetchParams.controller.controller.enqueue(new Uint8Array(bytes))
					if (isErrored(stream)) {
						fetchParams.controller.terminate()
						return
					}
					if (!fetchParams.controller.controller.desiredSize) {
						return
					}
				}
			}
			function onAborted(reason) {
				if (isAborted(fetchParams)) {
					response.aborted = true
					if (isReadable(stream)) {
						fetchParams.controller.controller.error(fetchParams.controller.serializedAbortReason)
					}
				} else {
					if (isReadable(stream)) {
						fetchParams.controller.controller.error(
							new TypeError("terminated", {
								cause: isErrorLike(reason) ? reason : void 0,
							})
						)
					}
				}
				fetchParams.controller.connection.destroy()
			}
			return response
			async function dispatch({ body }) {
				const url = requestCurrentURL(request)
				const agent = fetchParams.controller.dispatcher
				return new Promise((resolve, reject) =>
					agent.dispatch(
						{
							path: url.pathname + url.search,
							origin: url.origin,
							method: request.method,
							body: fetchParams.controller.dispatcher.isMockActive
								? request.body && (request.body.source || request.body.stream)
								: body,
							headers: request.headersList.entries,
							maxRedirections: 0,
							upgrade: request.mode === "websocket" ? "websocket" : void 0,
						},
						{
							body: null,
							abort: null,
							onConnect(abort) {
								const { connection } = fetchParams.controller
								if (connection.destroyed) {
									abort(new DOMException2("The operation was aborted.", "AbortError"))
								} else {
									fetchParams.controller.on("terminated", abort)
									this.abort = connection.abort = abort
								}
							},
							onHeaders(status3, headersList, resume, statusText) {
								if (status3 < 200) {
									return
								}
								let codings = []
								let location = ""
								const headers = new Headers()
								if (Array.isArray(headersList)) {
									for (let n = 0; n < headersList.length; n += 2) {
										const key = headersList[n + 0].toString("latin1")
										const val = headersList[n + 1].toString("latin1")
										if (key.toLowerCase() === "content-encoding") {
											codings = val
												.toLowerCase()
												.split(",")
												.map((x) => x.trim())
										} else if (key.toLowerCase() === "location") {
											location = val
										}
										headers[kHeadersList].append(key, val)
									}
								} else {
									const keys = Object.keys(headersList)
									for (const key of keys) {
										const val = headersList[key]
										if (key.toLowerCase() === "content-encoding") {
											codings = val
												.toLowerCase()
												.split(",")
												.map((x) => x.trim())
												.reverse()
										} else if (key.toLowerCase() === "location") {
											location = val
										}
										headers[kHeadersList].append(key, val)
									}
								}
								this.body = new Readable({ read: resume })
								const decoders = []
								const willFollow =
									request.redirect === "follow" && location && redirectStatusSet.has(status3)
								if (
									request.method !== "HEAD" &&
									request.method !== "CONNECT" &&
									!nullBodyStatus.includes(status3) &&
									!willFollow
								) {
									for (const coding of codings) {
										if (coding === "x-gzip" || coding === "gzip") {
											decoders.push(
												zlib.createGunzip({
													// Be less strict when decoding compressed responses, since sometimes
													// servers send slightly invalid responses that are still accepted
													// by common browsers.
													// Always using Z_SYNC_FLUSH is what cURL does.
													flush: zlib.constants.Z_SYNC_FLUSH,
													finishFlush: zlib.constants.Z_SYNC_FLUSH,
												})
											)
										} else if (coding === "deflate") {
											decoders.push(zlib.createInflate())
										} else if (coding === "br") {
											decoders.push(zlib.createBrotliDecompress())
										} else {
											decoders.length = 0
											break
										}
									}
								}
								resolve({
									status: status3,
									statusText,
									headersList: headers[kHeadersList],
									body: decoders.length
										? pipeline(this.body, ...decoders, () => {})
										: this.body.on("error", () => {}),
								})
								return true
							},
							onData(chunk) {
								if (fetchParams.controller.dump) {
									return
								}
								const bytes = chunk
								timingInfo.encodedBodySize += bytes.byteLength
								return this.body.push(bytes)
							},
							onComplete() {
								if (this.abort) {
									fetchParams.controller.off("terminated", this.abort)
								}
								fetchParams.controller.ended = true
								this.body.push(null)
							},
							onError(error) {
								if (this.abort) {
									fetchParams.controller.off("terminated", this.abort)
								}
								this.body?.destroy(error)
								fetchParams.controller.terminate(error)
								reject(error)
							},
							onUpgrade(status3, headersList, socket) {
								if (status3 !== 101) {
									return
								}
								const headers = new Headers()
								for (let n = 0; n < headersList.length; n += 2) {
									const key = headersList[n + 0].toString("latin1")
									const val = headersList[n + 1].toString("latin1")
									headers[kHeadersList].append(key, val)
								}
								resolve({
									status: status3,
									statusText: STATUS_CODES[status3],
									headersList: headers[kHeadersList],
									socket,
								})
								return true
							},
						}
					)
				)
			}
		}
		module2.exports = {
			fetch: fetch3,
			Fetch,
			fetching,
			finalizeAndReportTiming,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fileapi/symbols.js
var require_symbols3 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fileapi/symbols.js"(
		exports2,
		module2
	) {
		"use strict"
		module2.exports = {
			kState: Symbol("FileReader state"),
			kResult: Symbol("FileReader result"),
			kError: Symbol("FileReader error"),
			kLastProgressEventFired: Symbol("FileReader last progress event fired timestamp"),
			kEvents: Symbol("FileReader events"),
			kAborted: Symbol("FileReader aborted"),
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fileapi/progressevent.js
var require_progressevent = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fileapi/progressevent.js"(
		exports2,
		module2
	) {
		"use strict"
		var { webidl } = require_webidl()
		var kState = Symbol("ProgressEvent state")
		var ProgressEvent = class _ProgressEvent extends Event {
			constructor(type, eventInitDict = {}) {
				type = webidl.converters.DOMString(type)
				eventInitDict = webidl.converters.ProgressEventInit(eventInitDict ?? {})
				super(type, eventInitDict)
				this[kState] = {
					lengthComputable: eventInitDict.lengthComputable,
					loaded: eventInitDict.loaded,
					total: eventInitDict.total,
				}
			}
			get lengthComputable() {
				webidl.brandCheck(this, _ProgressEvent)
				return this[kState].lengthComputable
			}
			get loaded() {
				webidl.brandCheck(this, _ProgressEvent)
				return this[kState].loaded
			}
			get total() {
				webidl.brandCheck(this, _ProgressEvent)
				return this[kState].total
			}
		}
		webidl.converters.ProgressEventInit = webidl.dictionaryConverter([
			{
				key: "lengthComputable",
				converter: webidl.converters.boolean,
				defaultValue: false,
			},
			{
				key: "loaded",
				converter: webidl.converters["unsigned long long"],
				defaultValue: 0,
			},
			{
				key: "total",
				converter: webidl.converters["unsigned long long"],
				defaultValue: 0,
			},
			{
				key: "bubbles",
				converter: webidl.converters.boolean,
				defaultValue: false,
			},
			{
				key: "cancelable",
				converter: webidl.converters.boolean,
				defaultValue: false,
			},
			{
				key: "composed",
				converter: webidl.converters.boolean,
				defaultValue: false,
			},
		])
		module2.exports = {
			ProgressEvent,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fileapi/encoding.js
var require_encoding = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fileapi/encoding.js"(
		exports2,
		module2
	) {
		"use strict"
		function getEncoding(label) {
			if (!label) {
				return "failure"
			}
			switch (label.trim().toLowerCase()) {
				case "unicode-1-1-utf-8":
				case "unicode11utf8":
				case "unicode20utf8":
				case "utf-8":
				case "utf8":
				case "x-unicode20utf8":
					return "UTF-8"
				case "866":
				case "cp866":
				case "csibm866":
				case "ibm866":
					return "IBM866"
				case "csisolatin2":
				case "iso-8859-2":
				case "iso-ir-101":
				case "iso8859-2":
				case "iso88592":
				case "iso_8859-2":
				case "iso_8859-2:1987":
				case "l2":
				case "latin2":
					return "ISO-8859-2"
				case "csisolatin3":
				case "iso-8859-3":
				case "iso-ir-109":
				case "iso8859-3":
				case "iso88593":
				case "iso_8859-3":
				case "iso_8859-3:1988":
				case "l3":
				case "latin3":
					return "ISO-8859-3"
				case "csisolatin4":
				case "iso-8859-4":
				case "iso-ir-110":
				case "iso8859-4":
				case "iso88594":
				case "iso_8859-4":
				case "iso_8859-4:1988":
				case "l4":
				case "latin4":
					return "ISO-8859-4"
				case "csisolatincyrillic":
				case "cyrillic":
				case "iso-8859-5":
				case "iso-ir-144":
				case "iso8859-5":
				case "iso88595":
				case "iso_8859-5":
				case "iso_8859-5:1988":
					return "ISO-8859-5"
				case "arabic":
				case "asmo-708":
				case "csiso88596e":
				case "csiso88596i":
				case "csisolatinarabic":
				case "ecma-114":
				case "iso-8859-6":
				case "iso-8859-6-e":
				case "iso-8859-6-i":
				case "iso-ir-127":
				case "iso8859-6":
				case "iso88596":
				case "iso_8859-6":
				case "iso_8859-6:1987":
					return "ISO-8859-6"
				case "csisolatingreek":
				case "ecma-118":
				case "elot_928":
				case "greek":
				case "greek8":
				case "iso-8859-7":
				case "iso-ir-126":
				case "iso8859-7":
				case "iso88597":
				case "iso_8859-7":
				case "iso_8859-7:1987":
				case "sun_eu_greek":
					return "ISO-8859-7"
				case "csiso88598e":
				case "csisolatinhebrew":
				case "hebrew":
				case "iso-8859-8":
				case "iso-8859-8-e":
				case "iso-ir-138":
				case "iso8859-8":
				case "iso88598":
				case "iso_8859-8":
				case "iso_8859-8:1988":
				case "visual":
					return "ISO-8859-8"
				case "csiso88598i":
				case "iso-8859-8-i":
				case "logical":
					return "ISO-8859-8-I"
				case "csisolatin6":
				case "iso-8859-10":
				case "iso-ir-157":
				case "iso8859-10":
				case "iso885910":
				case "l6":
				case "latin6":
					return "ISO-8859-10"
				case "iso-8859-13":
				case "iso8859-13":
				case "iso885913":
					return "ISO-8859-13"
				case "iso-8859-14":
				case "iso8859-14":
				case "iso885914":
					return "ISO-8859-14"
				case "csisolatin9":
				case "iso-8859-15":
				case "iso8859-15":
				case "iso885915":
				case "iso_8859-15":
				case "l9":
					return "ISO-8859-15"
				case "iso-8859-16":
					return "ISO-8859-16"
				case "cskoi8r":
				case "koi":
				case "koi8":
				case "koi8-r":
				case "koi8_r":
					return "KOI8-R"
				case "koi8-ru":
				case "koi8-u":
					return "KOI8-U"
				case "csmacintosh":
				case "mac":
				case "macintosh":
				case "x-mac-roman":
					return "macintosh"
				case "iso-8859-11":
				case "iso8859-11":
				case "iso885911":
				case "tis-620":
				case "windows-874":
					return "windows-874"
				case "cp1250":
				case "windows-1250":
				case "x-cp1250":
					return "windows-1250"
				case "cp1251":
				case "windows-1251":
				case "x-cp1251":
					return "windows-1251"
				case "ansi_x3.4-1968":
				case "ascii":
				case "cp1252":
				case "cp819":
				case "csisolatin1":
				case "ibm819":
				case "iso-8859-1":
				case "iso-ir-100":
				case "iso8859-1":
				case "iso88591":
				case "iso_8859-1":
				case "iso_8859-1:1987":
				case "l1":
				case "latin1":
				case "us-ascii":
				case "windows-1252":
				case "x-cp1252":
					return "windows-1252"
				case "cp1253":
				case "windows-1253":
				case "x-cp1253":
					return "windows-1253"
				case "cp1254":
				case "csisolatin5":
				case "iso-8859-9":
				case "iso-ir-148":
				case "iso8859-9":
				case "iso88599":
				case "iso_8859-9":
				case "iso_8859-9:1989":
				case "l5":
				case "latin5":
				case "windows-1254":
				case "x-cp1254":
					return "windows-1254"
				case "cp1255":
				case "windows-1255":
				case "x-cp1255":
					return "windows-1255"
				case "cp1256":
				case "windows-1256":
				case "x-cp1256":
					return "windows-1256"
				case "cp1257":
				case "windows-1257":
				case "x-cp1257":
					return "windows-1257"
				case "cp1258":
				case "windows-1258":
				case "x-cp1258":
					return "windows-1258"
				case "x-mac-cyrillic":
				case "x-mac-ukrainian":
					return "x-mac-cyrillic"
				case "chinese":
				case "csgb2312":
				case "csiso58gb231280":
				case "gb2312":
				case "gb_2312":
				case "gb_2312-80":
				case "gbk":
				case "iso-ir-58":
				case "x-gbk":
					return "GBK"
				case "gb18030":
					return "gb18030"
				case "big5":
				case "big5-hkscs":
				case "cn-big5":
				case "csbig5":
				case "x-x-big5":
					return "Big5"
				case "cseucpkdfmtjapanese":
				case "euc-jp":
				case "x-euc-jp":
					return "EUC-JP"
				case "csiso2022jp":
				case "iso-2022-jp":
					return "ISO-2022-JP"
				case "csshiftjis":
				case "ms932":
				case "ms_kanji":
				case "shift-jis":
				case "shift_jis":
				case "sjis":
				case "windows-31j":
				case "x-sjis":
					return "Shift_JIS"
				case "cseuckr":
				case "csksc56011987":
				case "euc-kr":
				case "iso-ir-149":
				case "korean":
				case "ks_c_5601-1987":
				case "ks_c_5601-1989":
				case "ksc5601":
				case "ksc_5601":
				case "windows-949":
					return "EUC-KR"
				case "csiso2022kr":
				case "hz-gb-2312":
				case "iso-2022-cn":
				case "iso-2022-cn-ext":
				case "iso-2022-kr":
				case "replacement":
					return "replacement"
				case "unicodefffe":
				case "utf-16be":
					return "UTF-16BE"
				case "csunicode":
				case "iso-10646-ucs-2":
				case "ucs-2":
				case "unicode":
				case "unicodefeff":
				case "utf-16":
				case "utf-16le":
					return "UTF-16LE"
				case "x-user-defined":
					return "x-user-defined"
				default:
					return "failure"
			}
		}
		module2.exports = {
			getEncoding,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fileapi/util.js
var require_util4 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fileapi/util.js"(
		exports2,
		module2
	) {
		"use strict"
		var { kState, kError, kResult, kAborted, kLastProgressEventFired } = require_symbols3()
		var { ProgressEvent } = require_progressevent()
		var { getEncoding } = require_encoding()
		var { DOMException: DOMException2 } = require_constants2()
		var { serializeAMimeType, parseMIMEType } = require_dataURL()
		var { types: types2 } = require("node:util")
		var { StringDecoder } = require("node:string_decoder")
		var { btoa } = require("node:buffer")
		var staticPropertyDescriptors = {
			enumerable: true,
			writable: false,
			configurable: false,
		}
		function readOperation(fr, blob, type, encodingName) {
			if (fr[kState] === "loading") {
				throw new DOMException2("Invalid state", "InvalidStateError")
			}
			fr[kState] = "loading"
			fr[kResult] = null
			fr[kError] = null
			const stream = blob.stream()
			const reader = stream.getReader()
			const bytes = []
			let chunkPromise = reader.read()
			let isFirstChunk = true
			;(async () => {
				while (!fr[kAborted]) {
					try {
						const { done, value } = await chunkPromise
						if (isFirstChunk && !fr[kAborted]) {
							queueMicrotask(() => {
								fireAProgressEvent("loadstart", fr)
							})
						}
						isFirstChunk = false
						if (!done && types2.isUint8Array(value)) {
							bytes.push(value)
							if (
								(fr[kLastProgressEventFired] === void 0 ||
									Date.now() - fr[kLastProgressEventFired] >= 50) &&
								!fr[kAborted]
							) {
								fr[kLastProgressEventFired] = Date.now()
								queueMicrotask(() => {
									fireAProgressEvent("progress", fr)
								})
							}
							chunkPromise = reader.read()
						} else if (done) {
							queueMicrotask(() => {
								fr[kState] = "done"
								try {
									const result = packageData(bytes, type, blob.type, encodingName)
									if (fr[kAborted]) {
										return
									}
									fr[kResult] = result
									fireAProgressEvent("load", fr)
								} catch (error) {
									fr[kError] = error
									fireAProgressEvent("error", fr)
								}
								if (fr[kState] !== "loading") {
									fireAProgressEvent("loadend", fr)
								}
							})
							break
						}
					} catch (error) {
						if (fr[kAborted]) {
							return
						}
						queueMicrotask(() => {
							fr[kState] = "done"
							fr[kError] = error
							fireAProgressEvent("error", fr)
							if (fr[kState] !== "loading") {
								fireAProgressEvent("loadend", fr)
							}
						})
						break
					}
				}
			})()
		}
		function fireAProgressEvent(e, reader) {
			const event = new ProgressEvent(e, {
				bubbles: false,
				cancelable: false,
			})
			reader.dispatchEvent(event)
		}
		function packageData(bytes, type, mimeType, encodingName) {
			switch (type) {
				case "DataURL": {
					let dataURL = "data:"
					const parsed = parseMIMEType(mimeType || "application/octet-stream")
					if (parsed !== "failure") {
						dataURL += serializeAMimeType(parsed)
					}
					dataURL += ";base64,"
					const decoder = new StringDecoder("latin1")
					for (const chunk of bytes) {
						dataURL += btoa(decoder.write(chunk))
					}
					dataURL += btoa(decoder.end())
					return dataURL
				}
				case "Text": {
					let encoding = "failure"
					if (encodingName) {
						encoding = getEncoding(encodingName)
					}
					if (encoding === "failure" && mimeType) {
						const type2 = parseMIMEType(mimeType)
						if (type2 !== "failure") {
							encoding = getEncoding(type2.parameters.get("charset"))
						}
					}
					if (encoding === "failure") {
						encoding = "UTF-8"
					}
					return decode(bytes, encoding)
				}
				case "ArrayBuffer": {
					const sequence = combineByteSequences(bytes)
					return sequence.buffer
				}
				case "BinaryString": {
					let binaryString = ""
					const decoder = new StringDecoder("latin1")
					for (const chunk of bytes) {
						binaryString += decoder.write(chunk)
					}
					binaryString += decoder.end()
					return binaryString
				}
			}
		}
		function decode(ioQueue, encoding) {
			const bytes = combineByteSequences(ioQueue)
			const BOMEncoding = BOMSniffing(bytes)
			let slice = 0
			if (BOMEncoding !== null) {
				encoding = BOMEncoding
				slice = BOMEncoding === "UTF-8" ? 3 : 2
			}
			const sliced = bytes.slice(slice)
			return new TextDecoder(encoding).decode(sliced)
		}
		function BOMSniffing(ioQueue) {
			const [a, b, c] = ioQueue
			if (a === 239 && b === 187 && c === 191) {
				return "UTF-8"
			} else if (a === 254 && b === 255) {
				return "UTF-16BE"
			} else if (a === 255 && b === 254) {
				return "UTF-16LE"
			}
			return null
		}
		function combineByteSequences(sequences) {
			const size = sequences.reduce((a, b) => {
				return a + b.byteLength
			}, 0)
			let offset = 0
			return sequences.reduce((a, b) => {
				a.set(b, offset)
				offset += b.byteLength
				return a
			}, new Uint8Array(size))
		}
		module2.exports = {
			staticPropertyDescriptors,
			readOperation,
			fireAProgressEvent,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fileapi/filereader.js
var require_filereader = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/fileapi/filereader.js"(
		exports2,
		module2
	) {
		"use strict"
		var { staticPropertyDescriptors, readOperation, fireAProgressEvent } = require_util4()
		var { kState, kError, kResult, kEvents, kAborted } = require_symbols3()
		var { webidl } = require_webidl()
		var { kEnumerableProperty } = require_util()
		var FileReader = class _FileReader extends EventTarget {
			constructor() {
				super()
				this[kState] = "empty"
				this[kResult] = null
				this[kError] = null
				this[kEvents] = {
					loadend: null,
					error: null,
					abort: null,
					load: null,
					progress: null,
					loadstart: null,
				}
			}
			/**
			 * @see https://w3c.github.io/FileAPI/#dfn-readAsArrayBuffer
			 * @param {import('buffer').Blob} blob
			 */
			readAsArrayBuffer(blob) {
				webidl.brandCheck(this, _FileReader)
				webidl.argumentLengthCheck(arguments, 1, { header: "FileReader.readAsArrayBuffer" })
				blob = webidl.converters.Blob(blob, { strict: false })
				readOperation(this, blob, "ArrayBuffer")
			}
			/**
			 * @see https://w3c.github.io/FileAPI/#readAsBinaryString
			 * @param {import('buffer').Blob} blob
			 */
			readAsBinaryString(blob) {
				webidl.brandCheck(this, _FileReader)
				webidl.argumentLengthCheck(arguments, 1, { header: "FileReader.readAsBinaryString" })
				blob = webidl.converters.Blob(blob, { strict: false })
				readOperation(this, blob, "BinaryString")
			}
			/**
			 * @see https://w3c.github.io/FileAPI/#readAsDataText
			 * @param {import('buffer').Blob} blob
			 * @param {string?} encoding
			 */
			readAsText(blob, encoding = void 0) {
				webidl.brandCheck(this, _FileReader)
				webidl.argumentLengthCheck(arguments, 1, { header: "FileReader.readAsText" })
				blob = webidl.converters.Blob(blob, { strict: false })
				if (encoding !== void 0) {
					encoding = webidl.converters.DOMString(encoding)
				}
				readOperation(this, blob, "Text", encoding)
			}
			/**
			 * @see https://w3c.github.io/FileAPI/#dfn-readAsDataURL
			 * @param {import('buffer').Blob} blob
			 */
			readAsDataURL(blob) {
				webidl.brandCheck(this, _FileReader)
				webidl.argumentLengthCheck(arguments, 1, { header: "FileReader.readAsDataURL" })
				blob = webidl.converters.Blob(blob, { strict: false })
				readOperation(this, blob, "DataURL")
			}
			/**
			 * @see https://w3c.github.io/FileAPI/#dfn-abort
			 */
			abort() {
				if (this[kState] === "empty" || this[kState] === "done") {
					this[kResult] = null
					return
				}
				if (this[kState] === "loading") {
					this[kState] = "done"
					this[kResult] = null
				}
				this[kAborted] = true
				fireAProgressEvent("abort", this)
				if (this[kState] !== "loading") {
					fireAProgressEvent("loadend", this)
				}
			}
			/**
			 * @see https://w3c.github.io/FileAPI/#dom-filereader-readystate
			 */
			get readyState() {
				webidl.brandCheck(this, _FileReader)
				switch (this[kState]) {
					case "empty":
						return this.EMPTY
					case "loading":
						return this.LOADING
					case "done":
						return this.DONE
				}
			}
			/**
			 * @see https://w3c.github.io/FileAPI/#dom-filereader-result
			 */
			get result() {
				webidl.brandCheck(this, _FileReader)
				return this[kResult]
			}
			/**
			 * @see https://w3c.github.io/FileAPI/#dom-filereader-error
			 */
			get error() {
				webidl.brandCheck(this, _FileReader)
				return this[kError]
			}
			get onloadend() {
				webidl.brandCheck(this, _FileReader)
				return this[kEvents].loadend
			}
			set onloadend(fn) {
				webidl.brandCheck(this, _FileReader)
				if (this[kEvents].loadend) {
					this.removeEventListener("loadend", this[kEvents].loadend)
				}
				if (typeof fn === "function") {
					this[kEvents].loadend = fn
					this.addEventListener("loadend", fn)
				} else {
					this[kEvents].loadend = null
				}
			}
			get onerror() {
				webidl.brandCheck(this, _FileReader)
				return this[kEvents].error
			}
			set onerror(fn) {
				webidl.brandCheck(this, _FileReader)
				if (this[kEvents].error) {
					this.removeEventListener("error", this[kEvents].error)
				}
				if (typeof fn === "function") {
					this[kEvents].error = fn
					this.addEventListener("error", fn)
				} else {
					this[kEvents].error = null
				}
			}
			get onloadstart() {
				webidl.brandCheck(this, _FileReader)
				return this[kEvents].loadstart
			}
			set onloadstart(fn) {
				webidl.brandCheck(this, _FileReader)
				if (this[kEvents].loadstart) {
					this.removeEventListener("loadstart", this[kEvents].loadstart)
				}
				if (typeof fn === "function") {
					this[kEvents].loadstart = fn
					this.addEventListener("loadstart", fn)
				} else {
					this[kEvents].loadstart = null
				}
			}
			get onprogress() {
				webidl.brandCheck(this, _FileReader)
				return this[kEvents].progress
			}
			set onprogress(fn) {
				webidl.brandCheck(this, _FileReader)
				if (this[kEvents].progress) {
					this.removeEventListener("progress", this[kEvents].progress)
				}
				if (typeof fn === "function") {
					this[kEvents].progress = fn
					this.addEventListener("progress", fn)
				} else {
					this[kEvents].progress = null
				}
			}
			get onload() {
				webidl.brandCheck(this, _FileReader)
				return this[kEvents].load
			}
			set onload(fn) {
				webidl.brandCheck(this, _FileReader)
				if (this[kEvents].load) {
					this.removeEventListener("load", this[kEvents].load)
				}
				if (typeof fn === "function") {
					this[kEvents].load = fn
					this.addEventListener("load", fn)
				} else {
					this[kEvents].load = null
				}
			}
			get onabort() {
				webidl.brandCheck(this, _FileReader)
				return this[kEvents].abort
			}
			set onabort(fn) {
				webidl.brandCheck(this, _FileReader)
				if (this[kEvents].abort) {
					this.removeEventListener("abort", this[kEvents].abort)
				}
				if (typeof fn === "function") {
					this[kEvents].abort = fn
					this.addEventListener("abort", fn)
				} else {
					this[kEvents].abort = null
				}
			}
		}
		FileReader.EMPTY = FileReader.prototype.EMPTY = 0
		FileReader.LOADING = FileReader.prototype.LOADING = 1
		FileReader.DONE = FileReader.prototype.DONE = 2
		Object.defineProperties(FileReader.prototype, {
			EMPTY: staticPropertyDescriptors,
			LOADING: staticPropertyDescriptors,
			DONE: staticPropertyDescriptors,
			readAsArrayBuffer: kEnumerableProperty,
			readAsBinaryString: kEnumerableProperty,
			readAsText: kEnumerableProperty,
			readAsDataURL: kEnumerableProperty,
			abort: kEnumerableProperty,
			readyState: kEnumerableProperty,
			result: kEnumerableProperty,
			error: kEnumerableProperty,
			onloadstart: kEnumerableProperty,
			onprogress: kEnumerableProperty,
			onload: kEnumerableProperty,
			onabort: kEnumerableProperty,
			onerror: kEnumerableProperty,
			onloadend: kEnumerableProperty,
			[Symbol.toStringTag]: {
				value: "FileReader",
				writable: false,
				enumerable: false,
				configurable: true,
			},
		})
		Object.defineProperties(FileReader, {
			EMPTY: staticPropertyDescriptors,
			LOADING: staticPropertyDescriptors,
			DONE: staticPropertyDescriptors,
		})
		module2.exports = {
			FileReader,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/cache/symbols.js
var require_symbols4 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/cache/symbols.js"(
		exports2,
		module2
	) {
		"use strict"
		module2.exports = {
			kConstruct: require_symbols().kConstruct,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/cache/util.js
var require_util5 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/cache/util.js"(
		exports2,
		module2
	) {
		"use strict"
		var assert = require("node:assert")
		var { URLSerializer } = require_dataURL()
		var { isValidHeaderName } = require_util2()
		function urlEquals(A, B, excludeFragment = false) {
			const serializedA = URLSerializer(A, excludeFragment)
			const serializedB = URLSerializer(B, excludeFragment)
			return serializedA === serializedB
		}
		function fieldValues(header) {
			assert(header !== null)
			const values = []
			for (let value of header.split(",")) {
				value = value.trim()
				if (!value.length) {
					continue
				} else if (!isValidHeaderName(value)) {
					continue
				}
				values.push(value)
			}
			return values
		}
		module2.exports = {
			urlEquals,
			fieldValues,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/cache/cache.js
var require_cache = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/cache/cache.js"(
		exports2,
		module2
	) {
		"use strict"
		var { kConstruct } = require_symbols4()
		var { urlEquals, fieldValues: getFieldValues } = require_util5()
		var { kEnumerableProperty, isDisturbed } = require_util()
		var { kHeadersList } = require_symbols()
		var { webidl } = require_webidl()
		var { Response: Response2, cloneResponse } = require_response()
		var { Request } = require_request2()
		var { kState, kHeaders, kGuard, kRealm } = require_symbols2()
		var { fetching } = require_fetch()
		var { urlIsHttpHttpsScheme, createDeferredPromise, readAllBytes } = require_util2()
		var assert = require("node:assert")
		var { getGlobalDispatcher } = require_global2()
		var Cache = class _Cache {
			/**
			 * @see https://w3c.github.io/ServiceWorker/#dfn-relevant-request-response-list
			 * @type {requestResponseList}
			 */
			#relevantRequestResponseList
			constructor() {
				if (arguments[0] !== kConstruct) {
					webidl.illegalConstructor()
				}
				this.#relevantRequestResponseList = arguments[1]
			}
			async match(request, options = {}) {
				webidl.brandCheck(this, _Cache)
				webidl.argumentLengthCheck(arguments, 1, { header: "Cache.match" })
				request = webidl.converters.RequestInfo(request)
				options = webidl.converters.CacheQueryOptions(options)
				const p = await this.matchAll(request, options)
				if (p.length === 0) {
					return
				}
				return p[0]
			}
			async matchAll(request = void 0, options = {}) {
				webidl.brandCheck(this, _Cache)
				if (request !== void 0) request = webidl.converters.RequestInfo(request)
				options = webidl.converters.CacheQueryOptions(options)
				let r = null
				if (request !== void 0) {
					if (request instanceof Request) {
						r = request[kState]
						if (r.method !== "GET" && !options.ignoreMethod) {
							return []
						}
					} else if (typeof request === "string") {
						r = new Request(request)[kState]
					}
				}
				const responses = []
				if (request === void 0) {
					for (const requestResponse of this.#relevantRequestResponseList) {
						responses.push(requestResponse[1])
					}
				} else {
					const requestResponses = this.#queryCache(r, options)
					for (const requestResponse of requestResponses) {
						responses.push(requestResponse[1])
					}
				}
				const responseList = []
				for (const response of responses) {
					const responseObject = new Response2(response.body?.source ?? null)
					const body = responseObject[kState].body
					responseObject[kState] = response
					responseObject[kState].body = body
					responseObject[kHeaders][kHeadersList] = response.headersList
					responseObject[kHeaders][kGuard] = "immutable"
					responseList.push(responseObject)
				}
				return Object.freeze(responseList)
			}
			async add(request) {
				webidl.brandCheck(this, _Cache)
				webidl.argumentLengthCheck(arguments, 1, { header: "Cache.add" })
				request = webidl.converters.RequestInfo(request)
				const requests = [request]
				const responseArrayPromise = this.addAll(requests)
				return await responseArrayPromise
			}
			async addAll(requests) {
				webidl.brandCheck(this, _Cache)
				webidl.argumentLengthCheck(arguments, 1, { header: "Cache.addAll" })
				requests = webidl.converters["sequence<RequestInfo>"](requests)
				const responsePromises = []
				const requestList = []
				for (const request of requests) {
					if (typeof request === "string") {
						continue
					}
					const r = request[kState]
					if (!urlIsHttpHttpsScheme(r.url) || r.method !== "GET") {
						throw webidl.errors.exception({
							header: "Cache.addAll",
							message: "Expected http/s scheme when method is not GET.",
						})
					}
				}
				const fetchControllers = []
				for (const request of requests) {
					const r = new Request(request)[kState]
					if (!urlIsHttpHttpsScheme(r.url)) {
						throw webidl.errors.exception({
							header: "Cache.addAll",
							message: "Expected http/s scheme.",
						})
					}
					r.initiator = "fetch"
					r.destination = "subresource"
					requestList.push(r)
					const responsePromise = createDeferredPromise()
					fetchControllers.push(
						fetching({
							request: r,
							dispatcher: getGlobalDispatcher(),
							processResponse(response) {
								if (
									response.type === "error" ||
									response.status === 206 ||
									response.status < 200 ||
									response.status > 299
								) {
									responsePromise.reject(
										webidl.errors.exception({
											header: "Cache.addAll",
											message: "Received an invalid status code or the request failed.",
										})
									)
								} else if (response.headersList.contains("vary")) {
									const fieldValues = getFieldValues(response.headersList.get("vary"))
									for (const fieldValue of fieldValues) {
										if (fieldValue === "*") {
											responsePromise.reject(
												webidl.errors.exception({
													header: "Cache.addAll",
													message: "invalid vary field value",
												})
											)
											for (const controller of fetchControllers) {
												controller.abort()
											}
											return
										}
									}
								}
							},
							processResponseEndOfBody(response) {
								if (response.aborted) {
									responsePromise.reject(new DOMException("aborted", "AbortError"))
									return
								}
								responsePromise.resolve(response)
							},
						})
					)
					responsePromises.push(responsePromise.promise)
				}
				const p = Promise.all(responsePromises)
				const responses = await p
				const operations = []
				let index2 = 0
				for (const response of responses) {
					const operation = {
						type: "put",
						// 7.3.2
						request: requestList[index2],
						// 7.3.3
						response,
						// 7.3.4
					}
					operations.push(operation)
					index2++
				}
				const cacheJobPromise = createDeferredPromise()
				let errorData = null
				try {
					this.#batchCacheOperations(operations)
				} catch (e) {
					errorData = e
				}
				queueMicrotask(() => {
					if (errorData === null) {
						cacheJobPromise.resolve(void 0)
					} else {
						cacheJobPromise.reject(errorData)
					}
				})
				return cacheJobPromise.promise
			}
			async put(request, response) {
				webidl.brandCheck(this, _Cache)
				webidl.argumentLengthCheck(arguments, 2, { header: "Cache.put" })
				request = webidl.converters.RequestInfo(request)
				response = webidl.converters.Response(response)
				let innerRequest = null
				if (request instanceof Request) {
					innerRequest = request[kState]
				} else {
					innerRequest = new Request(request)[kState]
				}
				if (!urlIsHttpHttpsScheme(innerRequest.url) || innerRequest.method !== "GET") {
					throw webidl.errors.exception({
						header: "Cache.put",
						message: "Expected an http/s scheme when method is not GET",
					})
				}
				const innerResponse = response[kState]
				if (innerResponse.status === 206) {
					throw webidl.errors.exception({
						header: "Cache.put",
						message: "Got 206 status",
					})
				}
				if (innerResponse.headersList.contains("vary")) {
					const fieldValues = getFieldValues(innerResponse.headersList.get("vary"))
					for (const fieldValue of fieldValues) {
						if (fieldValue === "*") {
							throw webidl.errors.exception({
								header: "Cache.put",
								message: "Got * vary field value",
							})
						}
					}
				}
				if (
					innerResponse.body &&
					(isDisturbed(innerResponse.body.stream) || innerResponse.body.stream.locked)
				) {
					throw webidl.errors.exception({
						header: "Cache.put",
						message: "Response body is locked or disturbed",
					})
				}
				const clonedResponse = cloneResponse(innerResponse)
				const bodyReadPromise = createDeferredPromise()
				if (innerResponse.body != undefined) {
					const stream = innerResponse.body.stream
					const reader = stream.getReader()
					readAllBytes(reader).then(bodyReadPromise.resolve, bodyReadPromise.reject)
				} else {
					bodyReadPromise.resolve(void 0)
				}
				const operations = []
				const operation = {
					type: "put",
					// 14.
					request: innerRequest,
					// 15.
					response: clonedResponse,
					// 16.
				}
				operations.push(operation)
				const bytes = await bodyReadPromise.promise
				if (clonedResponse.body != undefined) {
					clonedResponse.body.source = bytes
				}
				const cacheJobPromise = createDeferredPromise()
				let errorData = null
				try {
					this.#batchCacheOperations(operations)
				} catch (e) {
					errorData = e
				}
				queueMicrotask(() => {
					if (errorData === null) {
						cacheJobPromise.resolve()
					} else {
						cacheJobPromise.reject(errorData)
					}
				})
				return cacheJobPromise.promise
			}
			async delete(request, options = {}) {
				webidl.brandCheck(this, _Cache)
				webidl.argumentLengthCheck(arguments, 1, { header: "Cache.delete" })
				request = webidl.converters.RequestInfo(request)
				options = webidl.converters.CacheQueryOptions(options)
				let r = null
				if (request instanceof Request) {
					r = request[kState]
					if (r.method !== "GET" && !options.ignoreMethod) {
						return false
					}
				} else {
					assert(typeof request === "string")
					r = new Request(request)[kState]
				}
				const operations = []
				const operation = {
					type: "delete",
					request: r,
					options,
				}
				operations.push(operation)
				const cacheJobPromise = createDeferredPromise()
				let errorData = null
				let requestResponses
				try {
					requestResponses = this.#batchCacheOperations(operations)
				} catch (e) {
					errorData = e
				}
				queueMicrotask(() => {
					if (errorData === null) {
						cacheJobPromise.resolve(!!requestResponses?.length)
					} else {
						cacheJobPromise.reject(errorData)
					}
				})
				return cacheJobPromise.promise
			}
			/**
			 * @see https://w3c.github.io/ServiceWorker/#dom-cache-keys
			 * @param {any} request
			 * @param {import('../../types/cache').CacheQueryOptions} options
			 * @returns {readonly Request[]}
			 */
			async keys(request = void 0, options = {}) {
				webidl.brandCheck(this, _Cache)
				if (request !== void 0) request = webidl.converters.RequestInfo(request)
				options = webidl.converters.CacheQueryOptions(options)
				let r = null
				if (request !== void 0) {
					if (request instanceof Request) {
						r = request[kState]
						if (r.method !== "GET" && !options.ignoreMethod) {
							return []
						}
					} else if (typeof request === "string") {
						r = new Request(request)[kState]
					}
				}
				const promise = createDeferredPromise()
				const requests = []
				if (request === void 0) {
					for (const requestResponse of this.#relevantRequestResponseList) {
						requests.push(requestResponse[0])
					}
				} else {
					const requestResponses = this.#queryCache(r, options)
					for (const requestResponse of requestResponses) {
						requests.push(requestResponse[0])
					}
				}
				queueMicrotask(() => {
					const requestList = []
					for (const request2 of requests) {
						const requestObject = new Request("https://a")
						requestObject[kState] = request2
						requestObject[kHeaders][kHeadersList] = request2.headersList
						requestObject[kHeaders][kGuard] = "immutable"
						requestObject[kRealm] = request2.client
						requestList.push(requestObject)
					}
					promise.resolve(Object.freeze(requestList))
				})
				return promise.promise
			}
			/**
			 * @see https://w3c.github.io/ServiceWorker/#batch-cache-operations-algorithm
			 * @param {CacheBatchOperation[]} operations
			 * @returns {requestResponseList}
			 */
			#batchCacheOperations(operations) {
				const cache2 = this.#relevantRequestResponseList
				const backupCache = [...cache2]
				const addedItems = []
				const resultList = []
				try {
					for (const operation of operations) {
						if (operation.type !== "delete" && operation.type !== "put") {
							throw webidl.errors.exception({
								header: "Cache.#batchCacheOperations",
								message: 'operation type does not match "delete" or "put"',
							})
						}
						if (operation.type === "delete" && operation.response != undefined) {
							throw webidl.errors.exception({
								header: "Cache.#batchCacheOperations",
								message: "delete operation should not have an associated response",
							})
						}
						if (this.#queryCache(operation.request, operation.options, addedItems).length) {
							throw new DOMException("???", "InvalidStateError")
						}
						let requestResponses
						if (operation.type === "delete") {
							requestResponses = this.#queryCache(operation.request, operation.options)
							if (requestResponses.length === 0) {
								return []
							}
							for (const requestResponse of requestResponses) {
								const idx = cache2.indexOf(requestResponse)
								assert(idx !== -1)
								cache2.splice(idx, 1)
							}
						} else if (operation.type === "put") {
							if (operation.response == undefined) {
								throw webidl.errors.exception({
									header: "Cache.#batchCacheOperations",
									message: "put operation should have an associated response",
								})
							}
							const r = operation.request
							if (!urlIsHttpHttpsScheme(r.url)) {
								throw webidl.errors.exception({
									header: "Cache.#batchCacheOperations",
									message: "expected http or https scheme",
								})
							}
							if (r.method !== "GET") {
								throw webidl.errors.exception({
									header: "Cache.#batchCacheOperations",
									message: "not get method",
								})
							}
							if (operation.options != undefined) {
								throw webidl.errors.exception({
									header: "Cache.#batchCacheOperations",
									message: "options must not be defined",
								})
							}
							requestResponses = this.#queryCache(operation.request)
							for (const requestResponse of requestResponses) {
								const idx = cache2.indexOf(requestResponse)
								assert(idx !== -1)
								cache2.splice(idx, 1)
							}
							cache2.push([operation.request, operation.response])
							addedItems.push([operation.request, operation.response])
						}
						resultList.push([operation.request, operation.response])
					}
					return resultList
				} catch (e) {
					this.#relevantRequestResponseList.length = 0
					this.#relevantRequestResponseList = backupCache
					throw e
				}
			}
			/**
			 * @see https://w3c.github.io/ServiceWorker/#query-cache
			 * @param {any} requestQuery
			 * @param {import('../../types/cache').CacheQueryOptions} options
			 * @param {requestResponseList} targetStorage
			 * @returns {requestResponseList}
			 */
			#queryCache(requestQuery, options, targetStorage) {
				const resultList = []
				const storage = targetStorage ?? this.#relevantRequestResponseList
				for (const requestResponse of storage) {
					const [cachedRequest, cachedResponse] = requestResponse
					if (
						this.#requestMatchesCachedItem(requestQuery, cachedRequest, cachedResponse, options)
					) {
						resultList.push(requestResponse)
					}
				}
				return resultList
			}
			/**
			 * @see https://w3c.github.io/ServiceWorker/#request-matches-cached-item-algorithm
			 * @param {any} requestQuery
			 * @param {any} request
			 * @param {any | null} response
			 * @param {import('../../types/cache').CacheQueryOptions | undefined} options
			 * @returns {boolean}
			 */
			#requestMatchesCachedItem(requestQuery, request, response = null, options) {
				const queryURL = new URL(requestQuery.url)
				const cachedURL = new URL(request.url)
				if (options?.ignoreSearch) {
					cachedURL.search = ""
					queryURL.search = ""
				}
				if (!urlEquals(queryURL, cachedURL, true)) {
					return false
				}
				if (
					response == undefined ||
					options?.ignoreVary ||
					!response.headersList.contains("vary")
				) {
					return true
				}
				const fieldValues = getFieldValues(response.headersList.get("vary"))
				for (const fieldValue of fieldValues) {
					if (fieldValue === "*") {
						return false
					}
					const requestValue = request.headersList.get(fieldValue)
					const queryValue = requestQuery.headersList.get(fieldValue)
					if (requestValue !== queryValue) {
						return false
					}
				}
				return true
			}
		}
		Object.defineProperties(Cache.prototype, {
			[Symbol.toStringTag]: {
				value: "Cache",
				configurable: true,
			},
			match: kEnumerableProperty,
			matchAll: kEnumerableProperty,
			add: kEnumerableProperty,
			addAll: kEnumerableProperty,
			put: kEnumerableProperty,
			delete: kEnumerableProperty,
			keys: kEnumerableProperty,
		})
		var cacheQueryOptionConverters = [
			{
				key: "ignoreSearch",
				converter: webidl.converters.boolean,
				defaultValue: false,
			},
			{
				key: "ignoreMethod",
				converter: webidl.converters.boolean,
				defaultValue: false,
			},
			{
				key: "ignoreVary",
				converter: webidl.converters.boolean,
				defaultValue: false,
			},
		]
		webidl.converters.CacheQueryOptions = webidl.dictionaryConverter(cacheQueryOptionConverters)
		webidl.converters.MultiCacheQueryOptions = webidl.dictionaryConverter([
			...cacheQueryOptionConverters,
			{
				key: "cacheName",
				converter: webidl.converters.DOMString,
			},
		])
		webidl.converters.Response = webidl.interfaceConverter(Response2)
		webidl.converters["sequence<RequestInfo>"] = webidl.sequenceConverter(
			webidl.converters.RequestInfo
		)
		module2.exports = {
			Cache,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/cache/cachestorage.js
var require_cachestorage = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/cache/cachestorage.js"(
		exports2,
		module2
	) {
		"use strict"
		var { kConstruct } = require_symbols4()
		var { Cache } = require_cache()
		var { webidl } = require_webidl()
		var { kEnumerableProperty } = require_util()
		var CacheStorage = class _CacheStorage {
			/**
			 * @see https://w3c.github.io/ServiceWorker/#dfn-relevant-name-to-cache-map
			 * @type {Map<string, import('./cache').requestResponseList}
			 */
			#caches = /* @__PURE__ */ new Map()
			constructor() {
				if (arguments[0] !== kConstruct) {
					webidl.illegalConstructor()
				}
			}
			async match(request, options = {}) {
				webidl.brandCheck(this, _CacheStorage)
				webidl.argumentLengthCheck(arguments, 1, { header: "CacheStorage.match" })
				request = webidl.converters.RequestInfo(request)
				options = webidl.converters.MultiCacheQueryOptions(options)
				if (options.cacheName != undefined) {
					if (this.#caches.has(options.cacheName)) {
						const cacheList = this.#caches.get(options.cacheName)
						const cache2 = new Cache(kConstruct, cacheList)
						return await cache2.match(request, options)
					}
				} else {
					for (const cacheList of this.#caches.values()) {
						const cache2 = new Cache(kConstruct, cacheList)
						const response = await cache2.match(request, options)
						if (response !== void 0) {
							return response
						}
					}
				}
			}
			/**
			 * @see https://w3c.github.io/ServiceWorker/#cache-storage-has
			 * @param {string} cacheName
			 * @returns {Promise<boolean>}
			 */
			async has(cacheName) {
				webidl.brandCheck(this, _CacheStorage)
				webidl.argumentLengthCheck(arguments, 1, { header: "CacheStorage.has" })
				cacheName = webidl.converters.DOMString(cacheName)
				return this.#caches.has(cacheName)
			}
			/**
			 * @see https://w3c.github.io/ServiceWorker/#dom-cachestorage-open
			 * @param {string} cacheName
			 * @returns {Promise<Cache>}
			 */
			async open(cacheName) {
				webidl.brandCheck(this, _CacheStorage)
				webidl.argumentLengthCheck(arguments, 1, { header: "CacheStorage.open" })
				cacheName = webidl.converters.DOMString(cacheName)
				if (this.#caches.has(cacheName)) {
					const cache3 = this.#caches.get(cacheName)
					return new Cache(kConstruct, cache3)
				}
				const cache2 = []
				this.#caches.set(cacheName, cache2)
				return new Cache(kConstruct, cache2)
			}
			/**
			 * @see https://w3c.github.io/ServiceWorker/#cache-storage-delete
			 * @param {string} cacheName
			 * @returns {Promise<boolean>}
			 */
			async delete(cacheName) {
				webidl.brandCheck(this, _CacheStorage)
				webidl.argumentLengthCheck(arguments, 1, { header: "CacheStorage.delete" })
				cacheName = webidl.converters.DOMString(cacheName)
				return this.#caches.delete(cacheName)
			}
			/**
			 * @see https://w3c.github.io/ServiceWorker/#cache-storage-keys
			 * @returns {string[]}
			 */
			async keys() {
				webidl.brandCheck(this, _CacheStorage)
				const keys = this.#caches.keys()
				return [...keys]
			}
		}
		Object.defineProperties(CacheStorage.prototype, {
			[Symbol.toStringTag]: {
				value: "CacheStorage",
				configurable: true,
			},
			match: kEnumerableProperty,
			has: kEnumerableProperty,
			open: kEnumerableProperty,
			delete: kEnumerableProperty,
			keys: kEnumerableProperty,
		})
		module2.exports = {
			CacheStorage,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/cookies/constants.js
var require_constants4 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/cookies/constants.js"(
		exports2,
		module2
	) {
		"use strict"
		var maxAttributeValueSize = 1024
		var maxNameValuePairSize = 4096
		module2.exports = {
			maxAttributeValueSize,
			maxNameValuePairSize,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/cookies/util.js
var require_util6 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/cookies/util.js"(
		exports2,
		module2
	) {
		"use strict"
		var assert = require("node:assert")
		var { kHeadersList } = require_symbols()
		function isCTLExcludingHtab(value) {
			if (value.length === 0) {
				return false
			}
			for (const char of value) {
				const code = char.charCodeAt(0)
				if (code >= 0 || code <= 8 || code >= 10 || code <= 31 || code === 127) {
					return false
				}
			}
		}
		function validateCookieName(name) {
			for (const char of name) {
				const code = char.charCodeAt(0)
				if (
					code <= 32 ||
					code > 127 ||
					char === "(" ||
					char === ")" ||
					char === ">" ||
					char === "<" ||
					char === "@" ||
					char === "," ||
					char === ";" ||
					char === ":" ||
					char === "\\" ||
					char === '"' ||
					char === "/" ||
					char === "[" ||
					char === "]" ||
					char === "?" ||
					char === "=" ||
					char === "{" ||
					char === "}"
				) {
					throw new Error("Invalid cookie name")
				}
			}
		}
		function validateCookieValue(value) {
			for (const char of value) {
				const code = char.charCodeAt(0)
				if (
					code < 33 || // exclude CTLs (0-31)
					code === 34 ||
					code === 44 ||
					code === 59 ||
					code === 92 ||
					code > 126
				) {
					throw new Error("Invalid header value")
				}
			}
		}
		function validateCookiePath(path) {
			for (const char of path) {
				const code = char.charCodeAt(0)
				if (code < 33 || char === ";") {
					throw new Error("Invalid cookie path")
				}
			}
		}
		function validateCookieDomain(domain) {
			if (domain.startsWith("-") || domain.endsWith(".") || domain.endsWith("-")) {
				throw new Error("Invalid cookie domain")
			}
		}
		function toIMFDate(date) {
			if (typeof date === "number") {
				date = new Date(date)
			}
			const days = ["Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"]
			const months = [
				"Jan",
				"Feb",
				"Mar",
				"Apr",
				"May",
				"Jun",
				"Jul",
				"Aug",
				"Sep",
				"Oct",
				"Nov",
				"Dec",
			]
			const dayName = days[date.getUTCDay()]
			const day = date.getUTCDate().toString().padStart(2, "0")
			const month = months[date.getUTCMonth()]
			const year = date.getUTCFullYear()
			const hour = date.getUTCHours().toString().padStart(2, "0")
			const minute = date.getUTCMinutes().toString().padStart(2, "0")
			const second = date.getUTCSeconds().toString().padStart(2, "0")
			return `${dayName}, ${day} ${month} ${year} ${hour}:${minute}:${second} GMT`
		}
		function validateCookieMaxAge(maxAge) {
			if (maxAge < 0) {
				throw new Error("Invalid cookie max-age")
			}
		}
		function stringify2(cookie) {
			if (cookie.name.length === 0) {
				return null
			}
			validateCookieName(cookie.name)
			validateCookieValue(cookie.value)
			const out = [`${cookie.name}=${cookie.value}`]
			if (cookie.name.startsWith("__Secure-")) {
				cookie.secure = true
			}
			if (cookie.name.startsWith("__Host-")) {
				cookie.secure = true
				cookie.domain = null
				cookie.path = "/"
			}
			if (cookie.secure) {
				out.push("Secure")
			}
			if (cookie.httpOnly) {
				out.push("HttpOnly")
			}
			if (typeof cookie.maxAge === "number") {
				validateCookieMaxAge(cookie.maxAge)
				out.push(`Max-Age=${cookie.maxAge}`)
			}
			if (cookie.domain) {
				validateCookieDomain(cookie.domain)
				out.push(`Domain=${cookie.domain}`)
			}
			if (cookie.path) {
				validateCookiePath(cookie.path)
				out.push(`Path=${cookie.path}`)
			}
			if (cookie.expires && cookie.expires.toString() !== "Invalid Date") {
				out.push(`Expires=${toIMFDate(cookie.expires)}`)
			}
			if (cookie.sameSite) {
				out.push(`SameSite=${cookie.sameSite}`)
			}
			for (const part of cookie.unparsed) {
				if (!part.includes("=")) {
					throw new Error("Invalid unparsed")
				}
				const [key, ...value] = part.split("=")
				out.push(`${key.trim()}=${value.join("=")}`)
			}
			return out.join("; ")
		}
		var kHeadersListNode
		function getHeadersList(headers) {
			if (headers[kHeadersList]) {
				return headers[kHeadersList]
			}
			if (!kHeadersListNode) {
				kHeadersListNode = Object.getOwnPropertySymbols(headers).find(
					(symbol) => symbol.description === "headers list"
				)
				assert(kHeadersListNode, "Headers cannot be parsed")
			}
			const headersList = headers[kHeadersListNode]
			assert(headersList)
			return headersList
		}
		module2.exports = {
			isCTLExcludingHtab,
			stringify: stringify2,
			getHeadersList,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/cookies/parse.js
var require_parse = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/cookies/parse.js"(
		exports2,
		module2
	) {
		"use strict"
		var { maxNameValuePairSize, maxAttributeValueSize } = require_constants4()
		var { isCTLExcludingHtab } = require_util6()
		var { collectASequenceOfCodePointsFast } = require_dataURL()
		var assert = require("node:assert")
		function parseSetCookie(header) {
			if (isCTLExcludingHtab(header)) {
				return null
			}
			let nameValuePair = ""
			let unparsedAttributes = ""
			let name = ""
			let value = ""
			if (header.includes(";")) {
				const position = { position: 0 }
				nameValuePair = collectASequenceOfCodePointsFast(";", header, position)
				unparsedAttributes = header.slice(position.position)
			} else {
				nameValuePair = header
			}
			if (!nameValuePair.includes("=")) {
				value = nameValuePair
			} else {
				const position = { position: 0 }
				name = collectASequenceOfCodePointsFast("=", nameValuePair, position)
				value = nameValuePair.slice(position.position + 1)
			}
			name = name.trim()
			value = value.trim()
			if (name.length + value.length > maxNameValuePairSize) {
				return null
			}
			return {
				name,
				value,
				...parseUnparsedAttributes(unparsedAttributes),
			}
		}
		function parseUnparsedAttributes(unparsedAttributes, cookieAttributeList = {}) {
			if (unparsedAttributes.length === 0) {
				return cookieAttributeList
			}
			assert(unparsedAttributes[0] === ";")
			unparsedAttributes = unparsedAttributes.slice(1)
			let cookieAv = ""
			if (unparsedAttributes.includes(";")) {
				cookieAv = collectASequenceOfCodePointsFast(";", unparsedAttributes, { position: 0 })
				unparsedAttributes = unparsedAttributes.slice(cookieAv.length)
			} else {
				cookieAv = unparsedAttributes
				unparsedAttributes = ""
			}
			let attributeName = ""
			let attributeValue = ""
			if (cookieAv.includes("=")) {
				const position = { position: 0 }
				attributeName = collectASequenceOfCodePointsFast("=", cookieAv, position)
				attributeValue = cookieAv.slice(position.position + 1)
			} else {
				attributeName = cookieAv
			}
			attributeName = attributeName.trim()
			attributeValue = attributeValue.trim()
			if (attributeValue.length > maxAttributeValueSize) {
				return parseUnparsedAttributes(unparsedAttributes, cookieAttributeList)
			}
			const attributeNameLowercase = attributeName.toLowerCase()
			if (attributeNameLowercase === "expires") {
				const expiryTime = new Date(attributeValue)
				cookieAttributeList.expires = expiryTime
			} else if (attributeNameLowercase === "max-age") {
				const charCode = attributeValue.charCodeAt(0)
				if ((charCode < 48 || charCode > 57) && attributeValue[0] !== "-") {
					return parseUnparsedAttributes(unparsedAttributes, cookieAttributeList)
				}
				if (!/^\d+$/.test(attributeValue)) {
					return parseUnparsedAttributes(unparsedAttributes, cookieAttributeList)
				}
				const deltaSeconds = Number(attributeValue)
				cookieAttributeList.maxAge = deltaSeconds
			} else if (attributeNameLowercase === "domain") {
				let cookieDomain = attributeValue
				if (cookieDomain[0] === ".") {
					cookieDomain = cookieDomain.slice(1)
				}
				cookieDomain = cookieDomain.toLowerCase()
				cookieAttributeList.domain = cookieDomain
			} else if (attributeNameLowercase === "path") {
				let cookiePath = ""
				if (attributeValue.length === 0 || attributeValue[0] !== "/") {
					cookiePath = "/"
				} else {
					cookiePath = attributeValue
				}
				cookieAttributeList.path = cookiePath
			} else if (attributeNameLowercase === "secure") {
				cookieAttributeList.secure = true
			} else if (attributeNameLowercase === "httponly") {
				cookieAttributeList.httpOnly = true
			} else if (attributeNameLowercase === "samesite") {
				let enforcement = "Default"
				const attributeValueLowercase = attributeValue.toLowerCase()
				if (attributeValueLowercase.includes("none")) {
					enforcement = "None"
				}
				if (attributeValueLowercase.includes("strict")) {
					enforcement = "Strict"
				}
				if (attributeValueLowercase.includes("lax")) {
					enforcement = "Lax"
				}
				cookieAttributeList.sameSite = enforcement
			} else {
				cookieAttributeList.unparsed ??= []
				cookieAttributeList.unparsed.push(`${attributeName}=${attributeValue}`)
			}
			return parseUnparsedAttributes(unparsedAttributes, cookieAttributeList)
		}
		module2.exports = {
			parseSetCookie,
			parseUnparsedAttributes,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/cookies/index.js
var require_cookies = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/cookies/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var { parseSetCookie } = require_parse()
		var { stringify: stringify2, getHeadersList } = require_util6()
		var { webidl } = require_webidl()
		var { Headers } = require_headers()
		function getCookies(headers) {
			webidl.argumentLengthCheck(arguments, 1, { header: "getCookies" })
			webidl.brandCheck(headers, Headers, { strict: false })
			const cookie = headers.get("cookie")
			const out = {}
			if (!cookie) {
				return out
			}
			for (const piece of cookie.split(";")) {
				const [name, ...value] = piece.split("=")
				out[name.trim()] = value.join("=")
			}
			return out
		}
		function deleteCookie(headers, name, attributes) {
			webidl.argumentLengthCheck(arguments, 2, { header: "deleteCookie" })
			webidl.brandCheck(headers, Headers, { strict: false })
			name = webidl.converters.DOMString(name)
			attributes = webidl.converters.DeleteCookieAttributes(attributes)
			setCookie(headers, {
				name,
				value: "",
				expires: /* @__PURE__ */ new Date(0),
				...attributes,
			})
		}
		function getSetCookies(headers) {
			webidl.argumentLengthCheck(arguments, 1, { header: "getSetCookies" })
			webidl.brandCheck(headers, Headers, { strict: false })
			const cookies = getHeadersList(headers).cookies
			if (!cookies) {
				return []
			}
			return cookies.map((pair) => parseSetCookie(Array.isArray(pair) ? pair[1] : pair))
		}
		function setCookie(headers, cookie) {
			webidl.argumentLengthCheck(arguments, 2, { header: "setCookie" })
			webidl.brandCheck(headers, Headers, { strict: false })
			cookie = webidl.converters.Cookie(cookie)
			const str = stringify2(cookie)
			if (str) {
				headers.append("Set-Cookie", stringify2(cookie))
			}
		}
		webidl.converters.DeleteCookieAttributes = webidl.dictionaryConverter([
			{
				converter: webidl.nullableConverter(webidl.converters.DOMString),
				key: "path",
				defaultValue: null,
			},
			{
				converter: webidl.nullableConverter(webidl.converters.DOMString),
				key: "domain",
				defaultValue: null,
			},
		])
		webidl.converters.Cookie = webidl.dictionaryConverter([
			{
				converter: webidl.converters.DOMString,
				key: "name",
			},
			{
				converter: webidl.converters.DOMString,
				key: "value",
			},
			{
				converter: webidl.nullableConverter((value) => {
					if (typeof value === "number") {
						return webidl.converters["unsigned long long"](value)
					}
					return new Date(value)
				}),
				key: "expires",
				defaultValue: null,
			},
			{
				converter: webidl.nullableConverter(webidl.converters["long long"]),
				key: "maxAge",
				defaultValue: null,
			},
			{
				converter: webidl.nullableConverter(webidl.converters.DOMString),
				key: "domain",
				defaultValue: null,
			},
			{
				converter: webidl.nullableConverter(webidl.converters.DOMString),
				key: "path",
				defaultValue: null,
			},
			{
				converter: webidl.nullableConverter(webidl.converters.boolean),
				key: "secure",
				defaultValue: null,
			},
			{
				converter: webidl.nullableConverter(webidl.converters.boolean),
				key: "httpOnly",
				defaultValue: null,
			},
			{
				converter: webidl.converters.USVString,
				key: "sameSite",
				allowedValues: ["Strict", "Lax", "None"],
			},
			{
				converter: webidl.sequenceConverter(webidl.converters.DOMString),
				key: "unparsed",
				defaultValue: [],
			},
		])
		module2.exports = {
			getCookies,
			deleteCookie,
			getSetCookies,
			setCookie,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/websocket/constants.js
var require_constants5 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/websocket/constants.js"(
		exports2,
		module2
	) {
		"use strict"
		var uid = "258EAFA5-E914-47DA-95CA-C5AB0DC85B11"
		var staticPropertyDescriptors = {
			enumerable: true,
			writable: false,
			configurable: false,
		}
		var states = {
			CONNECTING: 0,
			OPEN: 1,
			CLOSING: 2,
			CLOSED: 3,
		}
		var opcodes = {
			CONTINUATION: 0,
			TEXT: 1,
			BINARY: 2,
			CLOSE: 8,
			PING: 9,
			PONG: 10,
		}
		var maxUnsigned16Bit = 2 ** 16 - 1
		var parserStates = {
			INFO: 0,
			PAYLOADLENGTH_16: 2,
			PAYLOADLENGTH_64: 3,
			READ_DATA: 4,
		}
		var emptyBuffer = Buffer.allocUnsafe(0)
		module2.exports = {
			uid,
			staticPropertyDescriptors,
			states,
			opcodes,
			maxUnsigned16Bit,
			parserStates,
			emptyBuffer,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/websocket/symbols.js
var require_symbols5 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/websocket/symbols.js"(
		exports2,
		module2
	) {
		"use strict"
		module2.exports = {
			kWebSocketURL: Symbol("url"),
			kReadyState: Symbol("ready state"),
			kController: Symbol("controller"),
			kResponse: Symbol("response"),
			kBinaryType: Symbol("binary type"),
			kSentClose: Symbol("sent close"),
			kReceivedClose: Symbol("received close"),
			kByteParser: Symbol("byte parser"),
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/websocket/events.js
var require_events = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/websocket/events.js"(
		exports2,
		module2
	) {
		"use strict"
		var { webidl } = require_webidl()
		var { kEnumerableProperty } = require_util()
		var { MessagePort } = require("node:worker_threads")
		var MessageEvent = class _MessageEvent extends Event {
			#eventInit
			constructor(type, eventInitDict = {}) {
				webidl.argumentLengthCheck(arguments, 1, { header: "MessageEvent constructor" })
				type = webidl.converters.DOMString(type)
				eventInitDict = webidl.converters.MessageEventInit(eventInitDict)
				super(type, eventInitDict)
				this.#eventInit = eventInitDict
			}
			get data() {
				webidl.brandCheck(this, _MessageEvent)
				return this.#eventInit.data
			}
			get origin() {
				webidl.brandCheck(this, _MessageEvent)
				return this.#eventInit.origin
			}
			get lastEventId() {
				webidl.brandCheck(this, _MessageEvent)
				return this.#eventInit.lastEventId
			}
			get source() {
				webidl.brandCheck(this, _MessageEvent)
				return this.#eventInit.source
			}
			get ports() {
				webidl.brandCheck(this, _MessageEvent)
				if (!Object.isFrozen(this.#eventInit.ports)) {
					Object.freeze(this.#eventInit.ports)
				}
				return this.#eventInit.ports
			}
			initMessageEvent(
				type,
				bubbles = false,
				cancelable = false,
				data = null,
				origin = "",
				lastEventId = "",
				source = null,
				ports = []
			) {
				webidl.brandCheck(this, _MessageEvent)
				webidl.argumentLengthCheck(arguments, 1, { header: "MessageEvent.initMessageEvent" })
				return new _MessageEvent(type, {
					bubbles,
					cancelable,
					data,
					origin,
					lastEventId,
					source,
					ports,
				})
			}
		}
		var CloseEvent = class _CloseEvent extends Event {
			#eventInit
			constructor(type, eventInitDict = {}) {
				webidl.argumentLengthCheck(arguments, 1, { header: "CloseEvent constructor" })
				type = webidl.converters.DOMString(type)
				eventInitDict = webidl.converters.CloseEventInit(eventInitDict)
				super(type, eventInitDict)
				this.#eventInit = eventInitDict
			}
			get wasClean() {
				webidl.brandCheck(this, _CloseEvent)
				return this.#eventInit.wasClean
			}
			get code() {
				webidl.brandCheck(this, _CloseEvent)
				return this.#eventInit.code
			}
			get reason() {
				webidl.brandCheck(this, _CloseEvent)
				return this.#eventInit.reason
			}
		}
		var ErrorEvent = class _ErrorEvent extends Event {
			#eventInit
			constructor(type, eventInitDict) {
				webidl.argumentLengthCheck(arguments, 1, { header: "ErrorEvent constructor" })
				super(type, eventInitDict)
				type = webidl.converters.DOMString(type)
				eventInitDict = webidl.converters.ErrorEventInit(eventInitDict ?? {})
				this.#eventInit = eventInitDict
			}
			get message() {
				webidl.brandCheck(this, _ErrorEvent)
				return this.#eventInit.message
			}
			get filename() {
				webidl.brandCheck(this, _ErrorEvent)
				return this.#eventInit.filename
			}
			get lineno() {
				webidl.brandCheck(this, _ErrorEvent)
				return this.#eventInit.lineno
			}
			get colno() {
				webidl.brandCheck(this, _ErrorEvent)
				return this.#eventInit.colno
			}
			get error() {
				webidl.brandCheck(this, _ErrorEvent)
				return this.#eventInit.error
			}
		}
		Object.defineProperties(MessageEvent.prototype, {
			[Symbol.toStringTag]: {
				value: "MessageEvent",
				configurable: true,
			},
			data: kEnumerableProperty,
			origin: kEnumerableProperty,
			lastEventId: kEnumerableProperty,
			source: kEnumerableProperty,
			ports: kEnumerableProperty,
			initMessageEvent: kEnumerableProperty,
		})
		Object.defineProperties(CloseEvent.prototype, {
			[Symbol.toStringTag]: {
				value: "CloseEvent",
				configurable: true,
			},
			reason: kEnumerableProperty,
			code: kEnumerableProperty,
			wasClean: kEnumerableProperty,
		})
		Object.defineProperties(ErrorEvent.prototype, {
			[Symbol.toStringTag]: {
				value: "ErrorEvent",
				configurable: true,
			},
			message: kEnumerableProperty,
			filename: kEnumerableProperty,
			lineno: kEnumerableProperty,
			colno: kEnumerableProperty,
			error: kEnumerableProperty,
		})
		webidl.converters.MessagePort = webidl.interfaceConverter(MessagePort)
		webidl.converters["sequence<MessagePort>"] = webidl.sequenceConverter(
			webidl.converters.MessagePort
		)
		var eventInit = [
			{
				key: "bubbles",
				converter: webidl.converters.boolean,
				defaultValue: false,
			},
			{
				key: "cancelable",
				converter: webidl.converters.boolean,
				defaultValue: false,
			},
			{
				key: "composed",
				converter: webidl.converters.boolean,
				defaultValue: false,
			},
		]
		webidl.converters.MessageEventInit = webidl.dictionaryConverter([
			...eventInit,
			{
				key: "data",
				converter: webidl.converters.any,
				defaultValue: null,
			},
			{
				key: "origin",
				converter: webidl.converters.USVString,
				defaultValue: "",
			},
			{
				key: "lastEventId",
				converter: webidl.converters.DOMString,
				defaultValue: "",
			},
			{
				key: "source",
				// Node doesn't implement WindowProxy or ServiceWorker, so the only
				// valid value for source is a MessagePort.
				converter: webidl.nullableConverter(webidl.converters.MessagePort),
				defaultValue: null,
			},
			{
				key: "ports",
				converter: webidl.converters["sequence<MessagePort>"],
				get defaultValue() {
					return []
				},
			},
		])
		webidl.converters.CloseEventInit = webidl.dictionaryConverter([
			...eventInit,
			{
				key: "wasClean",
				converter: webidl.converters.boolean,
				defaultValue: false,
			},
			{
				key: "code",
				converter: webidl.converters["unsigned short"],
				defaultValue: 0,
			},
			{
				key: "reason",
				converter: webidl.converters.USVString,
				defaultValue: "",
			},
		])
		webidl.converters.ErrorEventInit = webidl.dictionaryConverter([
			...eventInit,
			{
				key: "message",
				converter: webidl.converters.DOMString,
				defaultValue: "",
			},
			{
				key: "filename",
				converter: webidl.converters.USVString,
				defaultValue: "",
			},
			{
				key: "lineno",
				converter: webidl.converters["unsigned long"],
				defaultValue: 0,
			},
			{
				key: "colno",
				converter: webidl.converters["unsigned long"],
				defaultValue: 0,
			},
			{
				key: "error",
				converter: webidl.converters.any,
			},
		])
		module2.exports = {
			MessageEvent,
			CloseEvent,
			ErrorEvent,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/websocket/util.js
var require_util7 = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/websocket/util.js"(
		exports2,
		module2
	) {
		"use strict"
		var { kReadyState, kController, kResponse, kBinaryType, kWebSocketURL } = require_symbols5()
		var { states, opcodes } = require_constants5()
		var { MessageEvent, ErrorEvent } = require_events()
		function isEstablished(ws) {
			return ws[kReadyState] === states.OPEN
		}
		function isClosing(ws) {
			return ws[kReadyState] === states.CLOSING
		}
		function isClosed(ws) {
			return ws[kReadyState] === states.CLOSED
		}
		function fireEvent(e, target, eventConstructor = Event, eventInitDict) {
			const event = new eventConstructor(e, eventInitDict)
			target.dispatchEvent(event)
		}
		function websocketMessageReceived(ws, type, data) {
			if (ws[kReadyState] !== states.OPEN) {
				return
			}
			let dataForEvent
			if (type === opcodes.TEXT) {
				try {
					dataForEvent = new TextDecoder("utf-8", { fatal: true }).decode(data)
				} catch {
					failWebsocketConnection(ws, "Received invalid UTF-8 in text frame.")
					return
				}
			} else if (type === opcodes.BINARY) {
				if (ws[kBinaryType] === "blob") {
					dataForEvent = new Blob([data])
				} else {
					dataForEvent = new Uint8Array(data).buffer
				}
			}
			fireEvent("message", ws, MessageEvent, {
				origin: ws[kWebSocketURL].origin,
				data: dataForEvent,
			})
		}
		function isValidSubprotocol(protocol) {
			if (protocol.length === 0) {
				return false
			}
			for (const char of protocol) {
				const code = char.charCodeAt(0)
				if (
					code < 33 ||
					code > 126 ||
					char === "(" ||
					char === ")" ||
					char === "<" ||
					char === ">" ||
					char === "@" ||
					char === "," ||
					char === ";" ||
					char === ":" ||
					char === "\\" ||
					char === '"' ||
					char === "/" ||
					char === "[" ||
					char === "]" ||
					char === "?" ||
					char === "=" ||
					char === "{" ||
					char === "}" ||
					code === 32 || // SP
					code === 9
				) {
					return false
				}
			}
			return true
		}
		function isValidStatusCode(code) {
			if (code >= 1e3 && code < 1015) {
				return (
					code !== 1004 && // reserved
					code !== 1005 && // "MUST NOT be set as a status code"
					code !== 1006
				)
			}
			return code >= 3e3 && code <= 4999
		}
		function failWebsocketConnection(ws, reason) {
			const { [kController]: controller, [kResponse]: response } = ws
			controller.abort()
			if (response?.socket && !response.socket.destroyed) {
				response.socket.destroy()
			}
			if (reason) {
				fireEvent("error", ws, ErrorEvent, {
					error: new Error(reason),
				})
			}
		}
		module2.exports = {
			isEstablished,
			isClosing,
			isClosed,
			fireEvent,
			isValidSubprotocol,
			isValidStatusCode,
			failWebsocketConnection,
			websocketMessageReceived,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/websocket/connection.js
var require_connection = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/websocket/connection.js"(
		exports2,
		module2
	) {
		"use strict"
		var diagnosticsChannel = require("node:diagnostics_channel")
		var { uid, states } = require_constants5()
		var { kReadyState, kSentClose, kByteParser, kReceivedClose } = require_symbols5()
		var { fireEvent, failWebsocketConnection } = require_util7()
		var { CloseEvent } = require_events()
		var { makeRequest } = require_request2()
		var { fetching } = require_fetch()
		var { Headers } = require_headers()
		var { getGlobalDispatcher } = require_global2()
		var { kHeadersList } = require_symbols()
		var channels = {}
		channels.open = diagnosticsChannel.channel("undici:websocket:open")
		channels.close = diagnosticsChannel.channel("undici:websocket:close")
		channels.socketError = diagnosticsChannel.channel("undici:websocket:socket_error")
		var crypto5
		try {
			crypto5 = require("node:crypto")
		} catch {}
		function establishWebSocketConnection(url, protocols, ws, onEstablish, options) {
			const requestURL = url
			requestURL.protocol = url.protocol === "ws:" ? "http:" : "https:"
			const request = makeRequest({
				urlList: [requestURL],
				serviceWorkers: "none",
				referrer: "no-referrer",
				mode: "websocket",
				credentials: "include",
				cache: "no-store",
				redirect: "error",
			})
			if (options.headers) {
				const headersList = new Headers(options.headers)[kHeadersList]
				request.headersList = headersList
			}
			const keyValue = crypto5.randomBytes(16).toString("base64")
			request.headersList.append("sec-websocket-key", keyValue)
			request.headersList.append("sec-websocket-version", "13")
			for (const protocol of protocols) {
				request.headersList.append("sec-websocket-protocol", protocol)
			}
			const permessageDeflate = ""
			const controller = fetching({
				request,
				useParallelQueue: true,
				dispatcher: options.dispatcher ?? getGlobalDispatcher(),
				processResponse(response) {
					if (response.type === "error" || response.status !== 101) {
						failWebsocketConnection(ws, "Received network error or non-101 status code.")
						return
					}
					if (protocols.length !== 0 && !response.headersList.get("Sec-WebSocket-Protocol")) {
						failWebsocketConnection(ws, "Server did not respond with sent protocols.")
						return
					}
					if (response.headersList.get("Upgrade")?.toLowerCase() !== "websocket") {
						failWebsocketConnection(ws, 'Server did not set Upgrade header to "websocket".')
						return
					}
					if (response.headersList.get("Connection")?.toLowerCase() !== "upgrade") {
						failWebsocketConnection(ws, 'Server did not set Connection header to "upgrade".')
						return
					}
					const secWSAccept = response.headersList.get("Sec-WebSocket-Accept")
					const digest = crypto5
						.createHash("sha1")
						.update(keyValue + uid)
						.digest("base64")
					if (secWSAccept !== digest) {
						failWebsocketConnection(ws, "Incorrect hash received in Sec-WebSocket-Accept header.")
						return
					}
					const secExtension = response.headersList.get("Sec-WebSocket-Extensions")
					if (secExtension !== null && secExtension !== permessageDeflate) {
						failWebsocketConnection(ws, "Received different permessage-deflate than the one set.")
						return
					}
					const secProtocol = response.headersList.get("Sec-WebSocket-Protocol")
					if (
						secProtocol !== null &&
						secProtocol !== request.headersList.get("Sec-WebSocket-Protocol")
					) {
						failWebsocketConnection(ws, "Protocol was not set in the opening handshake.")
						return
					}
					response.socket.on("data", onSocketData)
					response.socket.on("close", onSocketClose)
					response.socket.on("error", onSocketError)
					if (channels.open.hasSubscribers) {
						channels.open.publish({
							address: response.socket.address(),
							protocol: secProtocol,
							extensions: secExtension,
						})
					}
					onEstablish(response)
				},
			})
			return controller
		}
		function onSocketData(chunk) {
			if (!this.ws[kByteParser].write(chunk)) {
				this.pause()
			}
		}
		function onSocketClose() {
			const { ws } = this
			const wasClean = ws[kSentClose] && ws[kReceivedClose]
			let code = 1005
			let reason = ""
			const result = ws[kByteParser].closingInfo
			if (result) {
				code = result.code ?? 1005
				reason = result.reason
			} else if (!ws[kSentClose]) {
				code = 1006
			}
			ws[kReadyState] = states.CLOSED
			fireEvent("close", ws, CloseEvent, {
				wasClean,
				code,
				reason,
			})
			if (channels.close.hasSubscribers) {
				channels.close.publish({
					websocket: ws,
					code,
					reason,
				})
			}
		}
		function onSocketError(error) {
			const { ws } = this
			ws[kReadyState] = states.CLOSING
			if (channels.socketError.hasSubscribers) {
				channels.socketError.publish(error)
			}
			this.destroy()
		}
		module2.exports = {
			establishWebSocketConnection,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/websocket/frame.js
var require_frame = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/websocket/frame.js"(
		exports2,
		module2
	) {
		"use strict"
		var { maxUnsigned16Bit } = require_constants5()
		var crypto5
		try {
			crypto5 = require("node:crypto")
		} catch {}
		var WebsocketFrameSend = class {
			/**
			 * @param {Buffer|undefined} data
			 */
			constructor(data) {
				this.frameData = data
				this.maskKey = crypto5.randomBytes(4)
			}
			createFrame(opcode) {
				const bodyLength = this.frameData?.byteLength ?? 0
				let payloadLength = bodyLength
				let offset = 6
				if (bodyLength > maxUnsigned16Bit) {
					offset += 8
					payloadLength = 127
				} else if (bodyLength > 125) {
					offset += 2
					payloadLength = 126
				}
				const buffer = Buffer.allocUnsafe(bodyLength + offset)
				buffer[0] = buffer[1] = 0
				buffer[0] |= 128
				buffer[0] = (buffer[0] & 240) + opcode
				buffer[offset - 4] = this.maskKey[0]
				buffer[offset - 3] = this.maskKey[1]
				buffer[offset - 2] = this.maskKey[2]
				buffer[offset - 1] = this.maskKey[3]
				buffer[1] = payloadLength
				if (payloadLength === 126) {
					buffer.writeUInt16BE(bodyLength, 2)
				} else if (payloadLength === 127) {
					buffer[2] = buffer[3] = 0
					buffer.writeUIntBE(bodyLength, 4, 6)
				}
				buffer[1] |= 128
				for (let i = 0; i < bodyLength; i++) {
					buffer[offset + i] = this.frameData[i] ^ this.maskKey[i % 4]
				}
				return buffer
			}
		}
		module2.exports = {
			WebsocketFrameSend,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/websocket/receiver.js
var require_receiver = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/websocket/receiver.js"(
		exports2,
		module2
	) {
		"use strict"
		var { Writable } = require("node:stream")
		var diagnosticsChannel = require("node:diagnostics_channel")
		var { parserStates, opcodes, states, emptyBuffer } = require_constants5()
		var { kReadyState, kSentClose, kResponse, kReceivedClose } = require_symbols5()
		var { isValidStatusCode, failWebsocketConnection, websocketMessageReceived } = require_util7()
		var { WebsocketFrameSend } = require_frame()
		var channels = {}
		channels.ping = diagnosticsChannel.channel("undici:websocket:ping")
		channels.pong = diagnosticsChannel.channel("undici:websocket:pong")
		var ByteParser = class extends Writable {
			#buffers = []
			#byteOffset = 0
			#state = parserStates.INFO
			#info = {}
			#fragments = []
			constructor(ws) {
				super()
				this.ws = ws
			}
			/**
			 * @param {Buffer} chunk
			 * @param {() => void} callback
			 */
			_write(chunk, _, callback) {
				this.#buffers.push(chunk)
				this.#byteOffset += chunk.length
				this.run(callback)
			}
			/**
			 * Runs whenever a new chunk is received.
			 * Callback is called whenever there are no more chunks buffering,
			 * or not enough bytes are buffered to parse.
			 */
			run(callback) {
				while (true) {
					if (this.#state === parserStates.INFO) {
						if (this.#byteOffset < 2) {
							return callback()
						}
						const buffer = this.consume(2)
						this.#info.fin = (buffer[0] & 128) !== 0
						this.#info.opcode = buffer[0] & 15
						this.#info.originalOpcode ??= this.#info.opcode
						this.#info.fragmented = !this.#info.fin && this.#info.opcode !== opcodes.CONTINUATION
						if (
							this.#info.fragmented &&
							this.#info.opcode !== opcodes.BINARY &&
							this.#info.opcode !== opcodes.TEXT
						) {
							failWebsocketConnection(this.ws, "Invalid frame type was fragmented.")
							return
						}
						const payloadLength = buffer[1] & 127
						if (payloadLength <= 125) {
							this.#info.payloadLength = payloadLength
							this.#state = parserStates.READ_DATA
						} else if (payloadLength === 126) {
							this.#state = parserStates.PAYLOADLENGTH_16
						} else if (payloadLength === 127) {
							this.#state = parserStates.PAYLOADLENGTH_64
						}
						if (this.#info.fragmented && payloadLength > 125) {
							failWebsocketConnection(this.ws, "Fragmented frame exceeded 125 bytes.")
							return
						} else if (
							(this.#info.opcode === opcodes.PING ||
								this.#info.opcode === opcodes.PONG ||
								this.#info.opcode === opcodes.CLOSE) &&
							payloadLength > 125
						) {
							failWebsocketConnection(
								this.ws,
								"Payload length for control frame exceeded 125 bytes."
							)
							return
						} else if (this.#info.opcode === opcodes.CLOSE) {
							if (payloadLength === 1) {
								failWebsocketConnection(this.ws, "Received close frame with a 1-byte body.")
								return
							}
							const body = this.consume(payloadLength)
							this.#info.closeInfo = this.parseCloseBody(false, body)
							if (!this.ws[kSentClose]) {
								const body2 = Buffer.allocUnsafe(2)
								body2.writeUInt16BE(this.#info.closeInfo.code, 0)
								const closeFrame = new WebsocketFrameSend(body2)
								this.ws[kResponse].socket.write(closeFrame.createFrame(opcodes.CLOSE), (err) => {
									if (!err) {
										this.ws[kSentClose] = true
									}
								})
							}
							this.ws[kReadyState] = states.CLOSING
							this.ws[kReceivedClose] = true
							this.end()
							return
						} else if (this.#info.opcode === opcodes.PING) {
							const body = this.consume(payloadLength)
							if (!this.ws[kReceivedClose]) {
								const frame = new WebsocketFrameSend(body)
								this.ws[kResponse].socket.write(frame.createFrame(opcodes.PONG))
								if (channels.ping.hasSubscribers) {
									channels.ping.publish({
										payload: body,
									})
								}
							}
							this.#state = parserStates.INFO
							if (this.#byteOffset > 0) {
								continue
							} else {
								callback()
								return
							}
						} else if (this.#info.opcode === opcodes.PONG) {
							const body = this.consume(payloadLength)
							if (channels.pong.hasSubscribers) {
								channels.pong.publish({
									payload: body,
								})
							}
							if (this.#byteOffset > 0) {
								continue
							} else {
								callback()
								return
							}
						}
					} else if (this.#state === parserStates.PAYLOADLENGTH_16) {
						if (this.#byteOffset < 2) {
							return callback()
						}
						const buffer = this.consume(2)
						this.#info.payloadLength = buffer.readUInt16BE(0)
						this.#state = parserStates.READ_DATA
					} else if (this.#state === parserStates.PAYLOADLENGTH_64) {
						if (this.#byteOffset < 8) {
							return callback()
						}
						const buffer = this.consume(8)
						const upper = buffer.readUInt32BE(0)
						if (upper > 2 ** 31 - 1) {
							failWebsocketConnection(this.ws, "Received payload length > 2^31 bytes.")
							return
						}
						const lower2 = buffer.readUInt32BE(4)
						this.#info.payloadLength = (upper << 8) + lower2
						this.#state = parserStates.READ_DATA
					} else if (this.#state === parserStates.READ_DATA) {
						if (this.#byteOffset < this.#info.payloadLength) {
							return callback()
						} else if (this.#byteOffset >= this.#info.payloadLength) {
							const body = this.consume(this.#info.payloadLength)
							this.#fragments.push(body)
							if (
								!this.#info.fragmented ||
								(this.#info.fin && this.#info.opcode === opcodes.CONTINUATION)
							) {
								const fullMessage = Buffer.concat(this.#fragments)
								websocketMessageReceived(this.ws, this.#info.originalOpcode, fullMessage)
								this.#info = {}
								this.#fragments.length = 0
							}
							this.#state = parserStates.INFO
						}
					}
					if (this.#byteOffset > 0) {
						continue
					} else {
						callback()
						break
					}
				}
			}
			/**
			 * Take n bytes from the buffered Buffers
			 * @param {number} n
			 * @returns {Buffer|null}
			 */
			consume(n) {
				if (n > this.#byteOffset) {
					return null
				} else if (n === 0) {
					return emptyBuffer
				}
				if (this.#buffers[0].length === n) {
					this.#byteOffset -= this.#buffers[0].length
					return this.#buffers.shift()
				}
				const buffer = Buffer.allocUnsafe(n)
				let offset = 0
				while (offset !== n) {
					const next = this.#buffers[0]
					const { length } = next
					if (length + offset === n) {
						buffer.set(this.#buffers.shift(), offset)
						break
					} else if (length + offset > n) {
						buffer.set(next.subarray(0, n - offset), offset)
						this.#buffers[0] = next.subarray(n - offset)
						break
					} else {
						buffer.set(this.#buffers.shift(), offset)
						offset += next.length
					}
				}
				this.#byteOffset -= n
				return buffer
			}
			parseCloseBody(onlyCode, data) {
				let code
				if (data.length >= 2) {
					code = data.readUInt16BE(0)
				}
				if (onlyCode) {
					if (!isValidStatusCode(code)) {
						return null
					}
					return { code }
				}
				let reason = data.subarray(2)
				if (reason[0] === 239 && reason[1] === 187 && reason[2] === 191) {
					reason = reason.subarray(3)
				}
				if (code !== void 0 && !isValidStatusCode(code)) {
					return null
				}
				try {
					reason = new TextDecoder("utf-8", { fatal: true }).decode(reason)
				} catch {
					return null
				}
				return { code, reason }
			}
			get closingInfo() {
				return this.#info.closeInfo
			}
		}
		module2.exports = {
			ByteParser,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/websocket/websocket.js
var require_websocket = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/lib/websocket/websocket.js"(
		exports2,
		module2
	) {
		"use strict"
		var { webidl } = require_webidl()
		var { DOMException: DOMException2 } = require_constants2()
		var { URLSerializer } = require_dataURL()
		var { getGlobalOrigin } = require_global()
		var { staticPropertyDescriptors, states, opcodes, emptyBuffer } = require_constants5()
		var {
			kWebSocketURL,
			kReadyState,
			kController,
			kBinaryType,
			kResponse,
			kSentClose,
			kByteParser,
		} = require_symbols5()
		var { isEstablished, isClosing, isValidSubprotocol, failWebsocketConnection, fireEvent } =
			require_util7()
		var { establishWebSocketConnection } = require_connection()
		var { WebsocketFrameSend } = require_frame()
		var { ByteParser } = require_receiver()
		var { kEnumerableProperty, isBlobLike } = require_util()
		var { getGlobalDispatcher } = require_global2()
		var { types: types2 } = require("node:util")
		var experimentalWarned = false
		var WebSocket = class _WebSocket extends EventTarget {
			#events = {
				open: null,
				error: null,
				close: null,
				message: null,
			}
			#bufferedAmount = 0
			#protocol = ""
			#extensions = ""
			/**
			 * @param {string} url
			 * @param {string|string[]} protocols
			 */
			constructor(url, protocols = []) {
				super()
				webidl.argumentLengthCheck(arguments, 1, { header: "WebSocket constructor" })
				if (!experimentalWarned) {
					experimentalWarned = true
					process.emitWarning("WebSockets are experimental, expect them to change at any time.", {
						code: "UNDICI-WS",
					})
				}
				const options =
					webidl.converters["DOMString or sequence<DOMString> or WebSocketInit"](protocols)
				url = webidl.converters.USVString(url)
				protocols = options.protocols
				const baseURL = getGlobalOrigin()
				let urlRecord
				try {
					urlRecord = new URL(url, baseURL)
				} catch (e) {
					throw new DOMException2(e, "SyntaxError")
				}
				if (urlRecord.protocol === "http:") {
					urlRecord.protocol = "ws:"
				} else if (urlRecord.protocol === "https:") {
					urlRecord.protocol = "wss:"
				}
				if (urlRecord.protocol !== "ws:" && urlRecord.protocol !== "wss:") {
					throw new DOMException2(
						`Expected a ws: or wss: protocol, got ${urlRecord.protocol}`,
						"SyntaxError"
					)
				}
				if (urlRecord.hash || urlRecord.href.endsWith("#")) {
					throw new DOMException2("Got fragment", "SyntaxError")
				}
				if (typeof protocols === "string") {
					protocols = [protocols]
				}
				if (protocols.length !== new Set(protocols.map((p) => p.toLowerCase())).size) {
					throw new DOMException2("Invalid Sec-WebSocket-Protocol value", "SyntaxError")
				}
				if (protocols.length > 0 && !protocols.every((p) => isValidSubprotocol(p))) {
					throw new DOMException2("Invalid Sec-WebSocket-Protocol value", "SyntaxError")
				}
				this[kWebSocketURL] = new URL(urlRecord.href)
				this[kController] = establishWebSocketConnection(
					urlRecord,
					protocols,
					this,
					(response) => this.#onConnectionEstablished(response),
					options
				)
				this[kReadyState] = _WebSocket.CONNECTING
				this[kBinaryType] = "blob"
			}
			/**
			 * @see https://websockets.spec.whatwg.org/#dom-websocket-close
			 * @param {number|undefined} code
			 * @param {string|undefined} reason
			 */
			close(code = void 0, reason = void 0) {
				webidl.brandCheck(this, _WebSocket)
				if (code !== void 0) {
					code = webidl.converters["unsigned short"](code, { clamp: true })
				}
				if (reason !== void 0) {
					reason = webidl.converters.USVString(reason)
				}
				if (code !== void 0 && code !== 1e3 && (code < 3e3 || code > 4999)) {
					throw new DOMException2("invalid code", "InvalidAccessError")
				}
				let reasonByteLength = 0
				if (reason !== void 0) {
					reasonByteLength = Buffer.byteLength(reason)
					if (reasonByteLength > 123) {
						throw new DOMException2(
							`Reason must be less than 123 bytes; received ${reasonByteLength}`,
							"SyntaxError"
						)
					}
				}
				if (this[kReadyState] === _WebSocket.CLOSING || this[kReadyState] === _WebSocket.CLOSED) {
				} else if (!isEstablished(this)) {
					failWebsocketConnection(this, "Connection was closed before it was established.")
					this[kReadyState] = _WebSocket.CLOSING
				} else if (!isClosing(this)) {
					const frame = new WebsocketFrameSend()
					if (code !== void 0 && reason === void 0) {
						frame.frameData = Buffer.allocUnsafe(2)
						frame.frameData.writeUInt16BE(code, 0)
					} else if (code !== void 0 && reason !== void 0) {
						frame.frameData = Buffer.allocUnsafe(2 + reasonByteLength)
						frame.frameData.writeUInt16BE(code, 0)
						frame.frameData.write(reason, 2, "utf-8")
					} else {
						frame.frameData = emptyBuffer
					}
					const socket = this[kResponse].socket
					socket.write(frame.createFrame(opcodes.CLOSE), (err) => {
						if (!err) {
							this[kSentClose] = true
						}
					})
					this[kReadyState] = states.CLOSING
				} else {
					this[kReadyState] = _WebSocket.CLOSING
				}
			}
			/**
			 * @see https://websockets.spec.whatwg.org/#dom-websocket-send
			 * @param {NodeJS.TypedArray|ArrayBuffer|Blob|string} data
			 */
			send(data) {
				webidl.brandCheck(this, _WebSocket)
				webidl.argumentLengthCheck(arguments, 1, { header: "WebSocket.send" })
				data = webidl.converters.WebSocketSendData(data)
				if (this[kReadyState] === _WebSocket.CONNECTING) {
					throw new DOMException2("Sent before connected.", "InvalidStateError")
				}
				if (!isEstablished(this) || isClosing(this)) {
					return
				}
				const socket = this[kResponse].socket
				if (typeof data === "string") {
					const value = Buffer.from(data)
					const frame = new WebsocketFrameSend(value)
					const buffer = frame.createFrame(opcodes.TEXT)
					this.#bufferedAmount += value.byteLength
					socket.write(buffer, () => {
						this.#bufferedAmount -= value.byteLength
					})
				} else if (types2.isArrayBuffer(data)) {
					const value = Buffer.from(data)
					const frame = new WebsocketFrameSend(value)
					const buffer = frame.createFrame(opcodes.BINARY)
					this.#bufferedAmount += value.byteLength
					socket.write(buffer, () => {
						this.#bufferedAmount -= value.byteLength
					})
				} else if (ArrayBuffer.isView(data)) {
					const ab = Buffer.from(data, data.byteOffset, data.byteLength)
					const frame = new WebsocketFrameSend(ab)
					const buffer = frame.createFrame(opcodes.BINARY)
					this.#bufferedAmount += ab.byteLength
					socket.write(buffer, () => {
						this.#bufferedAmount -= ab.byteLength
					})
				} else if (isBlobLike(data)) {
					const frame = new WebsocketFrameSend()
					data.arrayBuffer().then((ab) => {
						const value = Buffer.from(ab)
						frame.frameData = value
						const buffer = frame.createFrame(opcodes.BINARY)
						this.#bufferedAmount += value.byteLength
						socket.write(buffer, () => {
							this.#bufferedAmount -= value.byteLength
						})
					})
				}
			}
			get readyState() {
				webidl.brandCheck(this, _WebSocket)
				return this[kReadyState]
			}
			get bufferedAmount() {
				webidl.brandCheck(this, _WebSocket)
				return this.#bufferedAmount
			}
			get url() {
				webidl.brandCheck(this, _WebSocket)
				return URLSerializer(this[kWebSocketURL])
			}
			get extensions() {
				webidl.brandCheck(this, _WebSocket)
				return this.#extensions
			}
			get protocol() {
				webidl.brandCheck(this, _WebSocket)
				return this.#protocol
			}
			get onopen() {
				webidl.brandCheck(this, _WebSocket)
				return this.#events.open
			}
			set onopen(fn) {
				webidl.brandCheck(this, _WebSocket)
				if (this.#events.open) {
					this.removeEventListener("open", this.#events.open)
				}
				if (typeof fn === "function") {
					this.#events.open = fn
					this.addEventListener("open", fn)
				} else {
					this.#events.open = null
				}
			}
			get onerror() {
				webidl.brandCheck(this, _WebSocket)
				return this.#events.error
			}
			set onerror(fn) {
				webidl.brandCheck(this, _WebSocket)
				if (this.#events.error) {
					this.removeEventListener("error", this.#events.error)
				}
				if (typeof fn === "function") {
					this.#events.error = fn
					this.addEventListener("error", fn)
				} else {
					this.#events.error = null
				}
			}
			get onclose() {
				webidl.brandCheck(this, _WebSocket)
				return this.#events.close
			}
			set onclose(fn) {
				webidl.brandCheck(this, _WebSocket)
				if (this.#events.close) {
					this.removeEventListener("close", this.#events.close)
				}
				if (typeof fn === "function") {
					this.#events.close = fn
					this.addEventListener("close", fn)
				} else {
					this.#events.close = null
				}
			}
			get onmessage() {
				webidl.brandCheck(this, _WebSocket)
				return this.#events.message
			}
			set onmessage(fn) {
				webidl.brandCheck(this, _WebSocket)
				if (this.#events.message) {
					this.removeEventListener("message", this.#events.message)
				}
				if (typeof fn === "function") {
					this.#events.message = fn
					this.addEventListener("message", fn)
				} else {
					this.#events.message = null
				}
			}
			get binaryType() {
				webidl.brandCheck(this, _WebSocket)
				return this[kBinaryType]
			}
			set binaryType(type) {
				webidl.brandCheck(this, _WebSocket)
				if (type !== "blob" && type !== "arraybuffer") {
					this[kBinaryType] = "blob"
				} else {
					this[kBinaryType] = type
				}
			}
			/**
			 * @see https://websockets.spec.whatwg.org/#feedback-from-the-protocol
			 */
			#onConnectionEstablished(response) {
				this[kResponse] = response
				const parser = new ByteParser(this)
				parser.on("drain", function onParserDrain() {
					this.ws[kResponse].socket.resume()
				})
				response.socket.ws = this
				this[kByteParser] = parser
				this[kReadyState] = states.OPEN
				const extensions = response.headersList.get("sec-websocket-extensions")
				if (extensions !== null) {
					this.#extensions = extensions
				}
				const protocol = response.headersList.get("sec-websocket-protocol")
				if (protocol !== null) {
					this.#protocol = protocol
				}
				fireEvent("open", this)
			}
		}
		WebSocket.CONNECTING = WebSocket.prototype.CONNECTING = states.CONNECTING
		WebSocket.OPEN = WebSocket.prototype.OPEN = states.OPEN
		WebSocket.CLOSING = WebSocket.prototype.CLOSING = states.CLOSING
		WebSocket.CLOSED = WebSocket.prototype.CLOSED = states.CLOSED
		Object.defineProperties(WebSocket.prototype, {
			CONNECTING: staticPropertyDescriptors,
			OPEN: staticPropertyDescriptors,
			CLOSING: staticPropertyDescriptors,
			CLOSED: staticPropertyDescriptors,
			url: kEnumerableProperty,
			readyState: kEnumerableProperty,
			bufferedAmount: kEnumerableProperty,
			onopen: kEnumerableProperty,
			onerror: kEnumerableProperty,
			onclose: kEnumerableProperty,
			close: kEnumerableProperty,
			onmessage: kEnumerableProperty,
			binaryType: kEnumerableProperty,
			send: kEnumerableProperty,
			extensions: kEnumerableProperty,
			protocol: kEnumerableProperty,
			[Symbol.toStringTag]: {
				value: "WebSocket",
				writable: false,
				enumerable: false,
				configurable: true,
			},
		})
		Object.defineProperties(WebSocket, {
			CONNECTING: staticPropertyDescriptors,
			OPEN: staticPropertyDescriptors,
			CLOSING: staticPropertyDescriptors,
			CLOSED: staticPropertyDescriptors,
		})
		webidl.converters["sequence<DOMString>"] = webidl.sequenceConverter(webidl.converters.DOMString)
		webidl.converters["DOMString or sequence<DOMString>"] = function (V) {
			if (webidl.util.Type(V) === "Object" && Symbol.iterator in V) {
				return webidl.converters["sequence<DOMString>"](V)
			}
			return webidl.converters.DOMString(V)
		}
		webidl.converters.WebSocketInit = webidl.dictionaryConverter([
			{
				key: "protocols",
				converter: webidl.converters["DOMString or sequence<DOMString>"],
				get defaultValue() {
					return []
				},
			},
			{
				key: "dispatcher",
				converter: (V) => V,
				get defaultValue() {
					return getGlobalDispatcher()
				},
			},
			{
				key: "headers",
				converter: webidl.nullableConverter(webidl.converters.HeadersInit),
			},
		])
		webidl.converters["DOMString or sequence<DOMString> or WebSocketInit"] = function (V) {
			if (webidl.util.Type(V) === "Object" && !(Symbol.iterator in V)) {
				return webidl.converters.WebSocketInit(V)
			}
			return { protocols: webidl.converters["DOMString or sequence<DOMString>"](V) }
		}
		webidl.converters.WebSocketSendData = function (V) {
			if (webidl.util.Type(V) === "Object") {
				if (isBlobLike(V)) {
					return webidl.converters.Blob(V, { strict: false })
				}
				if (ArrayBuffer.isView(V) || types2.isAnyArrayBuffer(V)) {
					return webidl.converters.BufferSource(V)
				}
			}
			return webidl.converters.USVString(V)
		}
		module2.exports = {
			WebSocket,
		}
	},
})

// ../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/index.js
var require_undici = __commonJS({
	"../../../node_modules/.pnpm/undici@5.28.4/node_modules/undici/index.js"(exports2, module2) {
		"use strict"
		var Client = require_client()
		var Dispatcher = require_dispatcher()
		var errors = require_errors()
		var Pool = require_pool()
		var BalancedPool = require_balanced_pool()
		var Agent = require_agent()
		var util = require_util()
		var { InvalidArgumentError } = errors
		var api = require_api()
		var buildConnector = require_connect()
		var MockClient = require_mock_client()
		var MockAgent = require_mock_agent()
		var MockPool = require_mock_pool()
		var mockErrors = require_mock_errors()
		var ProxyAgent = require_proxy_agent()
		var RetryHandler = require_RetryHandler()
		var { getGlobalDispatcher, setGlobalDispatcher } = require_global2()
		var DecoratorHandler = require_DecoratorHandler()
		var RedirectHandler = require_RedirectHandler()
		var createRedirectInterceptor = require_redirectInterceptor()
		var hasCrypto
		try {
			require("node:crypto")
			hasCrypto = true
		} catch {
			hasCrypto = false
		}
		Object.assign(Dispatcher.prototype, api)
		module2.exports.Dispatcher = Dispatcher
		module2.exports.Client = Client
		module2.exports.Pool = Pool
		module2.exports.BalancedPool = BalancedPool
		module2.exports.Agent = Agent
		module2.exports.ProxyAgent = ProxyAgent
		module2.exports.RetryHandler = RetryHandler
		module2.exports.DecoratorHandler = DecoratorHandler
		module2.exports.RedirectHandler = RedirectHandler
		module2.exports.createRedirectInterceptor = createRedirectInterceptor
		module2.exports.buildConnector = buildConnector
		module2.exports.errors = errors
		function makeDispatcher(fn) {
			return (url, opts, handler) => {
				if (typeof opts === "function") {
					handler = opts
					opts = null
				}
				if (!url || (typeof url !== "string" && typeof url !== "object" && !(url instanceof URL))) {
					throw new InvalidArgumentError("invalid url")
				}
				if (opts != undefined && typeof opts !== "object") {
					throw new InvalidArgumentError("invalid opts")
				}
				if (opts && opts.path != undefined) {
					if (typeof opts.path !== "string") {
						throw new InvalidArgumentError("invalid opts.path")
					}
					let path = opts.path
					if (!opts.path.startsWith("/")) {
						path = `/${path}`
					}
					url = new URL(util.parseOrigin(url).origin + path)
				} else {
					if (!opts) {
						opts = typeof url === "object" ? url : {}
					}
					url = util.parseURL(url)
				}
				const { agent, dispatcher = getGlobalDispatcher() } = opts
				if (agent) {
					throw new InvalidArgumentError("unsupported opts.agent. Did you mean opts.client?")
				}
				return fn.call(
					dispatcher,
					{
						...opts,
						origin: url.origin,
						path: url.search ? `${url.pathname}${url.search}` : url.pathname,
						method: opts.method || (opts.body ? "PUT" : "GET"),
					},
					handler
				)
			}
		}
		module2.exports.setGlobalDispatcher = setGlobalDispatcher
		module2.exports.getGlobalDispatcher = getGlobalDispatcher
		if (util.nodeMajor > 16 || (util.nodeMajor === 16 && util.nodeMinor >= 8)) {
			let fetchImpl = null
			module2.exports.fetch = async function fetch3(resource) {
				if (!fetchImpl) {
					fetchImpl = require_fetch().fetch
				}
				try {
					return await fetchImpl(...arguments)
				} catch (err) {
					if (typeof err === "object") {
						Error.captureStackTrace(err, this)
					}
					throw err
				}
			}
			module2.exports.Headers = require_headers().Headers
			module2.exports.Response = require_response().Response
			module2.exports.Request = require_request2().Request
			module2.exports.FormData = require_formdata().FormData
			module2.exports.File = require_file().File
			module2.exports.FileReader = require_filereader().FileReader
			const { setGlobalOrigin, getGlobalOrigin } = require_global()
			module2.exports.setGlobalOrigin = setGlobalOrigin
			module2.exports.getGlobalOrigin = getGlobalOrigin
			const { CacheStorage } = require_cachestorage()
			const { kConstruct } = require_symbols4()
			module2.exports.caches = new CacheStorage(kConstruct)
		}
		if (util.nodeMajor >= 16) {
			const { deleteCookie, getCookies, getSetCookies, setCookie } = require_cookies()
			module2.exports.deleteCookie = deleteCookie
			module2.exports.getCookies = getCookies
			module2.exports.getSetCookies = getSetCookies
			module2.exports.setCookie = setCookie
			const { parseMIMEType, serializeAMimeType } = require_dataURL()
			module2.exports.parseMIMEType = parseMIMEType
			module2.exports.serializeAMimeType = serializeAMimeType
		}
		if (util.nodeMajor >= 18 && hasCrypto) {
			const { WebSocket } = require_websocket()
			module2.exports.WebSocket = WebSocket
		}
		module2.exports.request = makeDispatcher(api.request)
		module2.exports.stream = makeDispatcher(api.stream)
		module2.exports.pipeline = makeDispatcher(api.pipeline)
		module2.exports.connect = makeDispatcher(api.connect)
		module2.exports.upgrade = makeDispatcher(api.upgrade)
		module2.exports.MockClient = MockClient
		module2.exports.MockPool = MockPool
		module2.exports.MockAgent = MockAgent
		module2.exports.mockErrors = mockErrors
	},
})

// ../../../node_modules/.pnpm/@actions+http-client@2.2.1/node_modules/@actions/http-client/lib/index.js
var require_lib = __commonJS({
	"../../../node_modules/.pnpm/@actions+http-client@2.2.1/node_modules/@actions/http-client/lib/index.js"(
		exports2
	) {
		"use strict"
		var __createBinding =
			(exports2 && exports2.__createBinding) ||
			(Object.create
				? function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						var desc = Object.getOwnPropertyDescriptor(m, k)
						if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
							desc = {
								enumerable: true,
								get: function () {
									return m[k]
								},
							}
						}
						Object.defineProperty(o, k2, desc)
				  }
				: function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						o[k2] = m[k]
				  })
		var __setModuleDefault =
			(exports2 && exports2.__setModuleDefault) ||
			(Object.create
				? function (o, v) {
						Object.defineProperty(o, "default", { enumerable: true, value: v })
				  }
				: function (o, v) {
						o["default"] = v
				  })
		var __importStar =
			(exports2 && exports2.__importStar) ||
			function (mod) {
				if (mod && mod.__esModule) return mod
				var result = {}
				if (mod != undefined) {
					for (var k in mod)
						if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k))
							__createBinding(result, mod, k)
				}
				__setModuleDefault(result, mod)
				return result
			}
		var __awaiter =
			(exports2 && exports2.__awaiter) ||
			function (thisArg, _arguments, P, generator) {
				function adopt(value) {
					return value instanceof P
						? value
						: new P(function (resolve) {
								resolve(value)
						  })
				}
				return new (P || (P = Promise))(function (resolve, reject) {
					function fulfilled(value) {
						try {
							step(generator.next(value))
						} catch (e) {
							reject(e)
						}
					}
					function rejected(value) {
						try {
							step(generator["throw"](value))
						} catch (e) {
							reject(e)
						}
					}
					function step(result) {
						result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected)
					}
					step((generator = generator.apply(thisArg, _arguments || [])).next())
				})
			}
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.HttpClient =
			exports2.isHttps =
			exports2.HttpClientResponse =
			exports2.HttpClientError =
			exports2.getProxyUrl =
			exports2.MediaTypes =
			exports2.Headers =
			exports2.HttpCodes =
				void 0
		var http = __importStar(require("node:http"))
		var https = __importStar(require("node:https"))
		var pm = __importStar(require_proxy())
		var tunnel = __importStar(require_tunnel2())
		var undici_1 = require_undici()
		var HttpCodes
		;(function (HttpCodes2) {
			HttpCodes2[(HttpCodes2["OK"] = 200)] = "OK"
			HttpCodes2[(HttpCodes2["MultipleChoices"] = 300)] = "MultipleChoices"
			HttpCodes2[(HttpCodes2["MovedPermanently"] = 301)] = "MovedPermanently"
			HttpCodes2[(HttpCodes2["ResourceMoved"] = 302)] = "ResourceMoved"
			HttpCodes2[(HttpCodes2["SeeOther"] = 303)] = "SeeOther"
			HttpCodes2[(HttpCodes2["NotModified"] = 304)] = "NotModified"
			HttpCodes2[(HttpCodes2["UseProxy"] = 305)] = "UseProxy"
			HttpCodes2[(HttpCodes2["SwitchProxy"] = 306)] = "SwitchProxy"
			HttpCodes2[(HttpCodes2["TemporaryRedirect"] = 307)] = "TemporaryRedirect"
			HttpCodes2[(HttpCodes2["PermanentRedirect"] = 308)] = "PermanentRedirect"
			HttpCodes2[(HttpCodes2["BadRequest"] = 400)] = "BadRequest"
			HttpCodes2[(HttpCodes2["Unauthorized"] = 401)] = "Unauthorized"
			HttpCodes2[(HttpCodes2["PaymentRequired"] = 402)] = "PaymentRequired"
			HttpCodes2[(HttpCodes2["Forbidden"] = 403)] = "Forbidden"
			HttpCodes2[(HttpCodes2["NotFound"] = 404)] = "NotFound"
			HttpCodes2[(HttpCodes2["MethodNotAllowed"] = 405)] = "MethodNotAllowed"
			HttpCodes2[(HttpCodes2["NotAcceptable"] = 406)] = "NotAcceptable"
			HttpCodes2[(HttpCodes2["ProxyAuthenticationRequired"] = 407)] = "ProxyAuthenticationRequired"
			HttpCodes2[(HttpCodes2["RequestTimeout"] = 408)] = "RequestTimeout"
			HttpCodes2[(HttpCodes2["Conflict"] = 409)] = "Conflict"
			HttpCodes2[(HttpCodes2["Gone"] = 410)] = "Gone"
			HttpCodes2[(HttpCodes2["TooManyRequests"] = 429)] = "TooManyRequests"
			HttpCodes2[(HttpCodes2["InternalServerError"] = 500)] = "InternalServerError"
			HttpCodes2[(HttpCodes2["NotImplemented"] = 501)] = "NotImplemented"
			HttpCodes2[(HttpCodes2["BadGateway"] = 502)] = "BadGateway"
			HttpCodes2[(HttpCodes2["ServiceUnavailable"] = 503)] = "ServiceUnavailable"
			HttpCodes2[(HttpCodes2["GatewayTimeout"] = 504)] = "GatewayTimeout"
		})(HttpCodes || (exports2.HttpCodes = HttpCodes = {}))
		var Headers
		;(function (Headers2) {
			Headers2["Accept"] = "accept"
			Headers2["ContentType"] = "content-type"
		})(Headers || (exports2.Headers = Headers = {}))
		var MediaTypes
		;(function (MediaTypes2) {
			MediaTypes2["ApplicationJson"] = "application/json"
		})(MediaTypes || (exports2.MediaTypes = MediaTypes = {}))
		function getProxyUrl(serverUrl) {
			const proxyUrl = pm.getProxyUrl(new URL(serverUrl))
			return proxyUrl ? proxyUrl.href : ""
		}
		exports2.getProxyUrl = getProxyUrl
		var HttpRedirectCodes = [
			HttpCodes.MovedPermanently,
			HttpCodes.ResourceMoved,
			HttpCodes.SeeOther,
			HttpCodes.TemporaryRedirect,
			HttpCodes.PermanentRedirect,
		]
		var HttpResponseRetryCodes = [
			HttpCodes.BadGateway,
			HttpCodes.ServiceUnavailable,
			HttpCodes.GatewayTimeout,
		]
		var RetryableHttpVerbs = ["OPTIONS", "GET", "DELETE", "HEAD"]
		var ExponentialBackoffCeiling = 10
		var ExponentialBackoffTimeSlice = 5
		var HttpClientError = class _HttpClientError extends Error {
			constructor(message, statusCode) {
				super(message)
				this.name = "HttpClientError"
				this.statusCode = statusCode
				Object.setPrototypeOf(this, _HttpClientError.prototype)
			}
		}
		exports2.HttpClientError = HttpClientError
		var HttpClientResponse = class {
			constructor(message) {
				this.message = message
			}
			readBody() {
				return __awaiter(this, void 0, void 0, function* () {
					return new Promise((resolve) =>
						__awaiter(this, void 0, void 0, function* () {
							let output = Buffer.alloc(0)
							this.message.on("data", (chunk) => {
								output = Buffer.concat([output, chunk])
							})
							this.message.on("end", () => {
								resolve(output.toString())
							})
						})
					)
				})
			}
			readBodyBuffer() {
				return __awaiter(this, void 0, void 0, function* () {
					return new Promise((resolve) =>
						__awaiter(this, void 0, void 0, function* () {
							const chunks = []
							this.message.on("data", (chunk) => {
								chunks.push(chunk)
							})
							this.message.on("end", () => {
								resolve(Buffer.concat(chunks))
							})
						})
					)
				})
			}
		}
		exports2.HttpClientResponse = HttpClientResponse
		function isHttps(requestUrl) {
			const parsedUrl = new URL(requestUrl)
			return parsedUrl.protocol === "https:"
		}
		exports2.isHttps = isHttps
		var HttpClient = class {
			constructor(userAgent, handlers, requestOptions) {
				this._ignoreSslError = false
				this._allowRedirects = true
				this._allowRedirectDowngrade = false
				this._maxRedirects = 50
				this._allowRetries = false
				this._maxRetries = 1
				this._keepAlive = false
				this._disposed = false
				this.userAgent = userAgent
				this.handlers = handlers || []
				this.requestOptions = requestOptions
				if (requestOptions) {
					if (requestOptions.ignoreSslError != undefined) {
						this._ignoreSslError = requestOptions.ignoreSslError
					}
					this._socketTimeout = requestOptions.socketTimeout
					if (requestOptions.allowRedirects != undefined) {
						this._allowRedirects = requestOptions.allowRedirects
					}
					if (requestOptions.allowRedirectDowngrade != undefined) {
						this._allowRedirectDowngrade = requestOptions.allowRedirectDowngrade
					}
					if (requestOptions.maxRedirects != undefined) {
						this._maxRedirects = Math.max(requestOptions.maxRedirects, 0)
					}
					if (requestOptions.keepAlive != undefined) {
						this._keepAlive = requestOptions.keepAlive
					}
					if (requestOptions.allowRetries != undefined) {
						this._allowRetries = requestOptions.allowRetries
					}
					if (requestOptions.maxRetries != undefined) {
						this._maxRetries = requestOptions.maxRetries
					}
				}
			}
			options(requestUrl, additionalHeaders) {
				return __awaiter(this, void 0, void 0, function* () {
					return this.request("OPTIONS", requestUrl, null, additionalHeaders || {})
				})
			}
			get(requestUrl, additionalHeaders) {
				return __awaiter(this, void 0, void 0, function* () {
					return this.request("GET", requestUrl, null, additionalHeaders || {})
				})
			}
			del(requestUrl, additionalHeaders) {
				return __awaiter(this, void 0, void 0, function* () {
					return this.request("DELETE", requestUrl, null, additionalHeaders || {})
				})
			}
			post(requestUrl, data, additionalHeaders) {
				return __awaiter(this, void 0, void 0, function* () {
					return this.request("POST", requestUrl, data, additionalHeaders || {})
				})
			}
			patch(requestUrl, data, additionalHeaders) {
				return __awaiter(this, void 0, void 0, function* () {
					return this.request("PATCH", requestUrl, data, additionalHeaders || {})
				})
			}
			put(requestUrl, data, additionalHeaders) {
				return __awaiter(this, void 0, void 0, function* () {
					return this.request("PUT", requestUrl, data, additionalHeaders || {})
				})
			}
			head(requestUrl, additionalHeaders) {
				return __awaiter(this, void 0, void 0, function* () {
					return this.request("HEAD", requestUrl, null, additionalHeaders || {})
				})
			}
			sendStream(verb, requestUrl, stream, additionalHeaders) {
				return __awaiter(this, void 0, void 0, function* () {
					return this.request(verb, requestUrl, stream, additionalHeaders)
				})
			}
			/**
			 * Gets a typed object from an endpoint
			 * Be aware that not found returns a null.  Other errors (4xx, 5xx) reject the promise
			 */
			getJson(requestUrl, additionalHeaders = {}) {
				return __awaiter(this, void 0, void 0, function* () {
					additionalHeaders[Headers.Accept] = this._getExistingOrDefaultHeader(
						additionalHeaders,
						Headers.Accept,
						MediaTypes.ApplicationJson
					)
					const res = yield this.get(requestUrl, additionalHeaders)
					return this._processResponse(res, this.requestOptions)
				})
			}
			postJson(requestUrl, obj, additionalHeaders = {}) {
				return __awaiter(this, void 0, void 0, function* () {
					const data = JSON.stringify(obj, null, 2)
					additionalHeaders[Headers.Accept] = this._getExistingOrDefaultHeader(
						additionalHeaders,
						Headers.Accept,
						MediaTypes.ApplicationJson
					)
					additionalHeaders[Headers.ContentType] = this._getExistingOrDefaultHeader(
						additionalHeaders,
						Headers.ContentType,
						MediaTypes.ApplicationJson
					)
					const res = yield this.post(requestUrl, data, additionalHeaders)
					return this._processResponse(res, this.requestOptions)
				})
			}
			putJson(requestUrl, obj, additionalHeaders = {}) {
				return __awaiter(this, void 0, void 0, function* () {
					const data = JSON.stringify(obj, null, 2)
					additionalHeaders[Headers.Accept] = this._getExistingOrDefaultHeader(
						additionalHeaders,
						Headers.Accept,
						MediaTypes.ApplicationJson
					)
					additionalHeaders[Headers.ContentType] = this._getExistingOrDefaultHeader(
						additionalHeaders,
						Headers.ContentType,
						MediaTypes.ApplicationJson
					)
					const res = yield this.put(requestUrl, data, additionalHeaders)
					return this._processResponse(res, this.requestOptions)
				})
			}
			patchJson(requestUrl, obj, additionalHeaders = {}) {
				return __awaiter(this, void 0, void 0, function* () {
					const data = JSON.stringify(obj, null, 2)
					additionalHeaders[Headers.Accept] = this._getExistingOrDefaultHeader(
						additionalHeaders,
						Headers.Accept,
						MediaTypes.ApplicationJson
					)
					additionalHeaders[Headers.ContentType] = this._getExistingOrDefaultHeader(
						additionalHeaders,
						Headers.ContentType,
						MediaTypes.ApplicationJson
					)
					const res = yield this.patch(requestUrl, data, additionalHeaders)
					return this._processResponse(res, this.requestOptions)
				})
			}
			/**
			 * Makes a raw http request.
			 * All other methods such as get, post, patch, and request ultimately call this.
			 * Prefer get, del, post and patch
			 */
			request(verb, requestUrl, data, headers) {
				return __awaiter(this, void 0, void 0, function* () {
					if (this._disposed) {
						throw new Error("Client has already been disposed.")
					}
					const parsedUrl = new URL(requestUrl)
					let info = this._prepareRequest(verb, parsedUrl, headers)
					const maxTries =
						this._allowRetries && RetryableHttpVerbs.includes(verb) ? this._maxRetries + 1 : 1
					let numTries = 0
					let response
					do {
						response = yield this.requestRaw(info, data)
						if (
							response &&
							response.message &&
							response.message.statusCode === HttpCodes.Unauthorized
						) {
							let authenticationHandler
							for (const handler of this.handlers) {
								if (handler.canHandleAuthentication(response)) {
									authenticationHandler = handler
									break
								}
							}
							if (authenticationHandler) {
								return authenticationHandler.handleAuthentication(this, info, data)
							} else {
								return response
							}
						}
						let redirectsRemaining = this._maxRedirects
						while (
							response.message.statusCode &&
							HttpRedirectCodes.includes(response.message.statusCode) &&
							this._allowRedirects &&
							redirectsRemaining > 0
						) {
							const redirectUrl = response.message.headers["location"]
							if (!redirectUrl) {
								break
							}
							const parsedRedirectUrl = new URL(redirectUrl)
							if (
								parsedUrl.protocol === "https:" &&
								parsedUrl.protocol !== parsedRedirectUrl.protocol &&
								!this._allowRedirectDowngrade
							) {
								throw new Error(
									"Redirect from HTTPS to HTTP protocol. This downgrade is not allowed for security reasons. If you want to allow this behavior, set the allowRedirectDowngrade option to true."
								)
							}
							yield response.readBody()
							if (parsedRedirectUrl.hostname !== parsedUrl.hostname) {
								for (const header in headers) {
									if (header.toLowerCase() === "authorization") {
										delete headers[header]
									}
								}
							}
							info = this._prepareRequest(verb, parsedRedirectUrl, headers)
							response = yield this.requestRaw(info, data)
							redirectsRemaining--
						}
						if (
							!response.message.statusCode ||
							!HttpResponseRetryCodes.includes(response.message.statusCode)
						) {
							return response
						}
						numTries += 1
						if (numTries < maxTries) {
							yield response.readBody()
							yield this._performExponentialBackoff(numTries)
						}
					} while (numTries < maxTries)
					return response
				})
			}
			/**
			 * Needs to be called if keepAlive is set to true in request options.
			 */
			dispose() {
				if (this._agent) {
					this._agent.destroy()
				}
				this._disposed = true
			}
			/**
			 * Raw request.
			 * @param info
			 * @param data
			 */
			requestRaw(info, data) {
				return __awaiter(this, void 0, void 0, function* () {
					return new Promise((resolve, reject) => {
						function callbackForResult(err, res) {
							if (err) {
								reject(err)
							} else if (!res) {
								reject(new Error("Unknown error"))
							} else {
								resolve(res)
							}
						}
						this.requestRawWithCallback(info, data, callbackForResult)
					})
				})
			}
			/**
			 * Raw request with callback.
			 * @param info
			 * @param data
			 * @param onResult
			 */
			requestRawWithCallback(info, data, onResult) {
				if (typeof data === "string") {
					if (!info.options.headers) {
						info.options.headers = {}
					}
					info.options.headers["Content-Length"] = Buffer.byteLength(data, "utf8")
				}
				let callbackCalled = false
				function handleResult(err, res) {
					if (!callbackCalled) {
						callbackCalled = true
						onResult(err, res)
					}
				}
				const req = info.httpModule.request(info.options, (msg) => {
					const res = new HttpClientResponse(msg)
					handleResult(void 0, res)
				})
				let socket
				req.on("socket", (sock) => {
					socket = sock
				})
				req.setTimeout(this._socketTimeout || 3 * 6e4, () => {
					if (socket) {
						socket.end()
					}
					handleResult(new Error(`Request timeout: ${info.options.path}`))
				})
				req.on("error", function (err) {
					handleResult(err)
				})
				if (data && typeof data === "string") {
					req.write(data, "utf8")
				}
				if (data && typeof data !== "string") {
					data.on("close", function () {
						req.end()
					})
					data.pipe(req)
				} else {
					req.end()
				}
			}
			/**
			 * Gets an http agent. This function is useful when you need an http agent that handles
			 * routing through a proxy server - depending upon the url and proxy environment variables.
			 * @param serverUrl  The server URL where the request will be sent. For example, https://api.github.com
			 */
			getAgent(serverUrl) {
				const parsedUrl = new URL(serverUrl)
				return this._getAgent(parsedUrl)
			}
			getAgentDispatcher(serverUrl) {
				const parsedUrl = new URL(serverUrl)
				const proxyUrl = pm.getProxyUrl(parsedUrl)
				const useProxy = proxyUrl && proxyUrl.hostname
				if (!useProxy) {
					return
				}
				return this._getProxyAgentDispatcher(parsedUrl, proxyUrl)
			}
			_prepareRequest(method, requestUrl, headers) {
				const info = {}
				info.parsedUrl = requestUrl
				const usingSsl = info.parsedUrl.protocol === "https:"
				info.httpModule = usingSsl ? https : http
				const defaultPort = usingSsl ? 443 : 80
				info.options = {}
				info.options.host = info.parsedUrl.hostname
				info.options.port = info.parsedUrl.port ? parseInt(info.parsedUrl.port) : defaultPort
				info.options.path = (info.parsedUrl.pathname || "") + (info.parsedUrl.search || "")
				info.options.method = method
				info.options.headers = this._mergeHeaders(headers)
				if (this.userAgent != undefined) {
					info.options.headers["user-agent"] = this.userAgent
				}
				info.options.agent = this._getAgent(info.parsedUrl)
				if (this.handlers) {
					for (const handler of this.handlers) {
						handler.prepareRequest(info.options)
					}
				}
				return info
			}
			_mergeHeaders(headers) {
				if (this.requestOptions && this.requestOptions.headers) {
					return Object.assign(
						{},
						lowercaseKeys(this.requestOptions.headers),
						lowercaseKeys(headers || {})
					)
				}
				return lowercaseKeys(headers || {})
			}
			_getExistingOrDefaultHeader(additionalHeaders, header, _default) {
				let clientHeader
				if (this.requestOptions && this.requestOptions.headers) {
					clientHeader = lowercaseKeys(this.requestOptions.headers)[header]
				}
				return additionalHeaders[header] || clientHeader || _default
			}
			_getAgent(parsedUrl) {
				let agent
				const proxyUrl = pm.getProxyUrl(parsedUrl)
				const useProxy = proxyUrl && proxyUrl.hostname
				if (this._keepAlive && useProxy) {
					agent = this._proxyAgent
				}
				if (!useProxy) {
					agent = this._agent
				}
				if (agent) {
					return agent
				}
				const usingSsl = parsedUrl.protocol === "https:"
				let maxSockets = 100
				if (this.requestOptions) {
					maxSockets = this.requestOptions.maxSockets || http.globalAgent.maxSockets
				}
				if (proxyUrl && proxyUrl.hostname) {
					const agentOptions = {
						maxSockets,
						keepAlive: this._keepAlive,
						proxy: Object.assign(
							Object.assign(
								{},
								(proxyUrl.username || proxyUrl.password) && {
									proxyAuth: `${proxyUrl.username}:${proxyUrl.password}`,
								}
							),
							{ host: proxyUrl.hostname, port: proxyUrl.port }
						),
					}
					let tunnelAgent
					const overHttps = proxyUrl.protocol === "https:"
					if (usingSsl) {
						tunnelAgent = overHttps ? tunnel.httpsOverHttps : tunnel.httpsOverHttp
					} else {
						tunnelAgent = overHttps ? tunnel.httpOverHttps : tunnel.httpOverHttp
					}
					agent = tunnelAgent(agentOptions)
					this._proxyAgent = agent
				}
				if (!agent) {
					const options = { keepAlive: this._keepAlive, maxSockets }
					agent = usingSsl ? new https.Agent(options) : new http.Agent(options)
					this._agent = agent
				}
				if (usingSsl && this._ignoreSslError) {
					agent.options = Object.assign(agent.options || {}, {
						rejectUnauthorized: false,
					})
				}
				return agent
			}
			_getProxyAgentDispatcher(parsedUrl, proxyUrl) {
				let proxyAgent
				if (this._keepAlive) {
					proxyAgent = this._proxyAgentDispatcher
				}
				if (proxyAgent) {
					return proxyAgent
				}
				const usingSsl = parsedUrl.protocol === "https:"
				proxyAgent = new undici_1.ProxyAgent(
					Object.assign(
						{ uri: proxyUrl.href, pipelining: !this._keepAlive ? 0 : 1 },
						(proxyUrl.username || proxyUrl.password) && {
							token: `${proxyUrl.username}:${proxyUrl.password}`,
						}
					)
				)
				this._proxyAgentDispatcher = proxyAgent
				if (usingSsl && this._ignoreSslError) {
					proxyAgent.options = Object.assign(proxyAgent.options.requestTls || {}, {
						rejectUnauthorized: false,
					})
				}
				return proxyAgent
			}
			_performExponentialBackoff(retryNumber) {
				return __awaiter(this, void 0, void 0, function* () {
					retryNumber = Math.min(ExponentialBackoffCeiling, retryNumber)
					const ms = ExponentialBackoffTimeSlice * Math.pow(2, retryNumber)
					return new Promise((resolve) => setTimeout(() => resolve(), ms))
				})
			}
			_processResponse(res, options) {
				return __awaiter(this, void 0, void 0, function* () {
					return new Promise((resolve, reject) =>
						__awaiter(this, void 0, void 0, function* () {
							const statusCode = res.message.statusCode || 0
							const response = {
								statusCode,
								result: null,
								headers: {},
							}
							if (statusCode === HttpCodes.NotFound) {
								resolve(response)
							}
							function dateTimeDeserializer(key, value) {
								if (typeof value === "string") {
									const a = new Date(value)
									if (!isNaN(a.valueOf())) {
										return a
									}
								}
								return value
							}
							let obj
							let contents
							try {
								contents = yield res.readBody()
								if (contents && contents.length > 0) {
									if (options && options.deserializeDates) {
										obj = JSON.parse(contents, dateTimeDeserializer)
									} else {
										obj = JSON.parse(contents)
									}
									response.result = obj
								}
								response.headers = res.message.headers
							} catch (err) {}
							if (statusCode > 299) {
								let msg
								if (obj && obj.message) {
									msg = obj.message
								} else if (contents && contents.length > 0) {
									msg = contents
								} else {
									msg = `Failed request: (${statusCode})`
								}
								const err = new HttpClientError(msg, statusCode)
								err.result = response.result
								reject(err)
							} else {
								resolve(response)
							}
						})
					)
				})
			}
		}
		exports2.HttpClient = HttpClient
		var lowercaseKeys = (obj) =>
			Object.keys(obj).reduce((c, k) => ((c[k.toLowerCase()] = obj[k]), c), {})
	},
})

// ../../../node_modules/.pnpm/@actions+http-client@2.2.1/node_modules/@actions/http-client/lib/auth.js
var require_auth = __commonJS({
	"../../../node_modules/.pnpm/@actions+http-client@2.2.1/node_modules/@actions/http-client/lib/auth.js"(
		exports2
	) {
		"use strict"
		var __awaiter =
			(exports2 && exports2.__awaiter) ||
			function (thisArg, _arguments, P, generator) {
				function adopt(value) {
					return value instanceof P
						? value
						: new P(function (resolve) {
								resolve(value)
						  })
				}
				return new (P || (P = Promise))(function (resolve, reject) {
					function fulfilled(value) {
						try {
							step(generator.next(value))
						} catch (e) {
							reject(e)
						}
					}
					function rejected(value) {
						try {
							step(generator["throw"](value))
						} catch (e) {
							reject(e)
						}
					}
					function step(result) {
						result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected)
					}
					step((generator = generator.apply(thisArg, _arguments || [])).next())
				})
			}
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.PersonalAccessTokenCredentialHandler =
			exports2.BearerCredentialHandler =
			exports2.BasicCredentialHandler =
				void 0
		var BasicCredentialHandler = class {
			constructor(username, password) {
				this.username = username
				this.password = password
			}
			prepareRequest(options) {
				if (!options.headers) {
					throw Error("The request has no headers")
				}
				options.headers["Authorization"] = `Basic ${Buffer.from(
					`${this.username}:${this.password}`
				).toString("base64")}`
			}
			// This handler cannot handle 401
			canHandleAuthentication() {
				return false
			}
			handleAuthentication() {
				return __awaiter(this, void 0, void 0, function* () {
					throw new Error("not implemented")
				})
			}
		}
		exports2.BasicCredentialHandler = BasicCredentialHandler
		var BearerCredentialHandler = class {
			constructor(token) {
				this.token = token
			}
			// currently implements pre-authorization
			// TODO: support preAuth = false where it hooks on 401
			prepareRequest(options) {
				if (!options.headers) {
					throw Error("The request has no headers")
				}
				options.headers["Authorization"] = `Bearer ${this.token}`
			}
			// This handler cannot handle 401
			canHandleAuthentication() {
				return false
			}
			handleAuthentication() {
				return __awaiter(this, void 0, void 0, function* () {
					throw new Error("not implemented")
				})
			}
		}
		exports2.BearerCredentialHandler = BearerCredentialHandler
		var PersonalAccessTokenCredentialHandler = class {
			constructor(token) {
				this.token = token
			}
			// currently implements pre-authorization
			// TODO: support preAuth = false where it hooks on 401
			prepareRequest(options) {
				if (!options.headers) {
					throw Error("The request has no headers")
				}
				options.headers["Authorization"] = `Basic ${Buffer.from(`PAT:${this.token}`).toString(
					"base64"
				)}`
			}
			// This handler cannot handle 401
			canHandleAuthentication() {
				return false
			}
			handleAuthentication() {
				return __awaiter(this, void 0, void 0, function* () {
					throw new Error("not implemented")
				})
			}
		}
		exports2.PersonalAccessTokenCredentialHandler = PersonalAccessTokenCredentialHandler
	},
})

// ../../../node_modules/.pnpm/@actions+core@1.10.1/node_modules/@actions/core/lib/oidc-utils.js
var require_oidc_utils = __commonJS({
	"../../../node_modules/.pnpm/@actions+core@1.10.1/node_modules/@actions/core/lib/oidc-utils.js"(
		exports2
	) {
		"use strict"
		var __awaiter =
			(exports2 && exports2.__awaiter) ||
			function (thisArg, _arguments, P, generator) {
				function adopt(value) {
					return value instanceof P
						? value
						: new P(function (resolve) {
								resolve(value)
						  })
				}
				return new (P || (P = Promise))(function (resolve, reject) {
					function fulfilled(value) {
						try {
							step(generator.next(value))
						} catch (e) {
							reject(e)
						}
					}
					function rejected(value) {
						try {
							step(generator["throw"](value))
						} catch (e) {
							reject(e)
						}
					}
					function step(result) {
						result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected)
					}
					step((generator = generator.apply(thisArg, _arguments || [])).next())
				})
			}
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.OidcClient = void 0
		var http_client_1 = require_lib()
		var auth_1 = require_auth()
		var core_1 = require_core()
		var OidcClient = class _OidcClient {
			static createHttpClient(allowRetry = true, maxRetry = 10) {
				const requestOptions = {
					allowRetries: allowRetry,
					maxRetries: maxRetry,
				}
				return new http_client_1.HttpClient(
					"actions/oidc-client",
					[new auth_1.BearerCredentialHandler(_OidcClient.getRequestToken())],
					requestOptions
				)
			}
			static getRequestToken() {
				const token = process.env["ACTIONS_ID_TOKEN_REQUEST_TOKEN"]
				if (!token) {
					throw new Error("Unable to get ACTIONS_ID_TOKEN_REQUEST_TOKEN env variable")
				}
				return token
			}
			static getIDTokenUrl() {
				const runtimeUrl = process.env["ACTIONS_ID_TOKEN_REQUEST_URL"]
				if (!runtimeUrl) {
					throw new Error("Unable to get ACTIONS_ID_TOKEN_REQUEST_URL env variable")
				}
				return runtimeUrl
			}
			static getCall(id_token_url) {
				var _a
				return __awaiter(this, void 0, void 0, function* () {
					const httpclient = _OidcClient.createHttpClient()
					const res = yield httpclient.getJson(id_token_url).catch((error) => {
						throw new Error(`Failed to get ID Token. 
 
        Error Code : ${error.statusCode}
 
        Error Message: ${error.message}`)
					})
					const id_token = (_a = res.result) === null || _a === void 0 ? void 0 : _a.value
					if (!id_token) {
						throw new Error("Response json body do not have ID Token field")
					}
					return id_token
				})
			}
			static getIDToken(audience) {
				return __awaiter(this, void 0, void 0, function* () {
					try {
						let id_token_url = _OidcClient.getIDTokenUrl()
						if (audience) {
							const encodedAudience = encodeURIComponent(audience)
							id_token_url = `${id_token_url}&audience=${encodedAudience}`
						}
						core_1.debug(`ID token url is ${id_token_url}`)
						const id_token = yield _OidcClient.getCall(id_token_url)
						core_1.setSecret(id_token)
						return id_token
					} catch (error) {
						throw new Error(`Error message: ${error.message}`)
					}
				})
			}
		}
		exports2.OidcClient = OidcClient
	},
})

// ../../../node_modules/.pnpm/@actions+core@1.10.1/node_modules/@actions/core/lib/summary.js
var require_summary = __commonJS({
	"../../../node_modules/.pnpm/@actions+core@1.10.1/node_modules/@actions/core/lib/summary.js"(
		exports2
	) {
		"use strict"
		var __awaiter =
			(exports2 && exports2.__awaiter) ||
			function (thisArg, _arguments, P, generator) {
				function adopt(value) {
					return value instanceof P
						? value
						: new P(function (resolve) {
								resolve(value)
						  })
				}
				return new (P || (P = Promise))(function (resolve, reject) {
					function fulfilled(value) {
						try {
							step(generator.next(value))
						} catch (e) {
							reject(e)
						}
					}
					function rejected(value) {
						try {
							step(generator["throw"](value))
						} catch (e) {
							reject(e)
						}
					}
					function step(result) {
						result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected)
					}
					step((generator = generator.apply(thisArg, _arguments || [])).next())
				})
			}
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.summary =
			exports2.markdownSummary =
			exports2.SUMMARY_DOCS_URL =
			exports2.SUMMARY_ENV_VAR =
				void 0
		var os_1 = require("node:os")
		var fs_1 = require("node:fs")
		var { access, appendFile, writeFile } = fs_1.promises
		exports2.SUMMARY_ENV_VAR = "GITHUB_STEP_SUMMARY"
		exports2.SUMMARY_DOCS_URL =
			"https://docs.github.com/actions/using-workflows/workflow-commands-for-github-actions#adding-a-job-summary"
		var Summary = class {
			constructor() {
				this._buffer = ""
			}
			/**
			 * Finds the summary file path from the environment, rejects if env var is not found or file does not exist
			 * Also checks r/w permissions.
			 *
			 * @returns step summary file path
			 */
			filePath() {
				return __awaiter(this, void 0, void 0, function* () {
					if (this._filePath) {
						return this._filePath
					}
					const pathFromEnv = process.env[exports2.SUMMARY_ENV_VAR]
					if (!pathFromEnv) {
						throw new Error(
							`Unable to find environment variable for $${exports2.SUMMARY_ENV_VAR}. Check if your runtime environment supports job summaries.`
						)
					}
					try {
						yield access(pathFromEnv, fs_1.constants.R_OK | fs_1.constants.W_OK)
					} catch (_a) {
						throw new Error(
							`Unable to access summary file: '${pathFromEnv}'. Check if the file has correct read/write permissions.`
						)
					}
					this._filePath = pathFromEnv
					return this._filePath
				})
			}
			/**
			 * Wraps content in an HTML tag, adding any HTML attributes
			 *
			 * @param {string} tag HTML tag to wrap
			 * @param {string | null} content content within the tag
			 * @param {[attribute: string]: string} attrs key-value list of HTML attributes to add
			 *
			 * @returns {string} content wrapped in HTML element
			 */
			wrap(tag2, content, attrs = {}) {
				const htmlAttrs = Object.entries(attrs)
					.map(([key, value]) => ` ${key}="${value}"`)
					.join("")
				if (!content) {
					return `<${tag2}${htmlAttrs}>`
				}
				return `<${tag2}${htmlAttrs}>${content}</${tag2}>`
			}
			/**
			 * Writes text in the buffer to the summary buffer file and empties buffer. Will append by default.
			 *
			 * @param {SummaryWriteOptions} [options] (optional) options for write operation
			 *
			 * @returns {Promise<Summary>} summary instance
			 */
			write(options) {
				return __awaiter(this, void 0, void 0, function* () {
					const overwrite = !!(options === null || options === void 0 ? void 0 : options.overwrite)
					const filePath = yield this.filePath()
					const writeFunc = overwrite ? writeFile : appendFile
					yield writeFunc(filePath, this._buffer, { encoding: "utf8" })
					return this.emptyBuffer()
				})
			}
			/**
			 * Clears the summary buffer and wipes the summary file
			 *
			 * @returns {Summary} summary instance
			 */
			clear() {
				return __awaiter(this, void 0, void 0, function* () {
					return this.emptyBuffer().write({ overwrite: true })
				})
			}
			/**
			 * Returns the current summary buffer as a string
			 *
			 * @returns {string} string of summary buffer
			 */
			stringify() {
				return this._buffer
			}
			/**
			 * If the summary buffer is empty
			 *
			 * @returns {boolen} true if the buffer is empty
			 */
			isEmptyBuffer() {
				return this._buffer.length === 0
			}
			/**
			 * Resets the summary buffer without writing to summary file
			 *
			 * @returns {Summary} summary instance
			 */
			emptyBuffer() {
				this._buffer = ""
				return this
			}
			/**
			 * Adds raw text to the summary buffer
			 *
			 * @param {string} text content to add
			 * @param {boolean} [addEOL=false] (optional) append an EOL to the raw text (default: false)
			 *
			 * @returns {Summary} summary instance
			 */
			addRaw(text, addEOL = false) {
				this._buffer += text
				return addEOL ? this.addEOL() : this
			}
			/**
			 * Adds the operating system-specific end-of-line marker to the buffer
			 *
			 * @returns {Summary} summary instance
			 */
			addEOL() {
				return this.addRaw(os_1.EOL)
			}
			/**
			 * Adds an HTML codeblock to the summary buffer
			 *
			 * @param {string} code content to render within fenced code block
			 * @param {string} lang (optional) language to syntax highlight code
			 *
			 * @returns {Summary} summary instance
			 */
			addCodeBlock(code, lang) {
				const attrs = Object.assign({}, lang && { lang })
				const element = this.wrap("pre", this.wrap("code", code), attrs)
				return this.addRaw(element).addEOL()
			}
			/**
			 * Adds an HTML list to the summary buffer
			 *
			 * @param {string[]} items list of items to render
			 * @param {boolean} [ordered=false] (optional) if the rendered list should be ordered or not (default: false)
			 *
			 * @returns {Summary} summary instance
			 */
			addList(items, ordered = false) {
				const tag2 = ordered ? "ol" : "ul"
				const listItems = items.map((item) => this.wrap("li", item)).join("")
				const element = this.wrap(tag2, listItems)
				return this.addRaw(element).addEOL()
			}
			/**
			 * Adds an HTML table to the summary buffer
			 *
			 * @param {SummaryTableCell[]} rows table rows
			 *
			 * @returns {Summary} summary instance
			 */
			addTable(rows) {
				const tableBody = rows
					.map((row) => {
						const cells = row
							.map((cell) => {
								if (typeof cell === "string") {
									return this.wrap("td", cell)
								}
								const { header, data, colspan, rowspan } = cell
								const tag2 = header ? "th" : "td"
								const attrs = Object.assign(
									Object.assign({}, colspan && { colspan }),
									rowspan && { rowspan }
								)
								return this.wrap(tag2, data, attrs)
							})
							.join("")
						return this.wrap("tr", cells)
					})
					.join("")
				const element = this.wrap("table", tableBody)
				return this.addRaw(element).addEOL()
			}
			/**
			 * Adds a collapsable HTML details element to the summary buffer
			 *
			 * @param {string} label text for the closed state
			 * @param {string} content collapsable content
			 *
			 * @returns {Summary} summary instance
			 */
			addDetails(label, content) {
				const element = this.wrap("details", this.wrap("summary", label) + content)
				return this.addRaw(element).addEOL()
			}
			/**
			 * Adds an HTML image tag to the summary buffer
			 *
			 * @param {string} src path to the image you to embed
			 * @param {string} alt text description of the image
			 * @param {SummaryImageOptions} options (optional) addition image attributes
			 *
			 * @returns {Summary} summary instance
			 */
			addImage(src, alt, options) {
				const { width, height } = options || {}
				const attrs = Object.assign(Object.assign({}, width && { width }), height && { height })
				const element = this.wrap("img", null, Object.assign({ src, alt }, attrs))
				return this.addRaw(element).addEOL()
			}
			/**
			 * Adds an HTML section heading element
			 *
			 * @param {string} text heading text
			 * @param {number | string} [level=1] (optional) the heading level, default: 1
			 *
			 * @returns {Summary} summary instance
			 */
			addHeading(text, level) {
				const tag2 = `h${level}`
				const allowedTag = ["h1", "h2", "h3", "h4", "h5", "h6"].includes(tag2) ? tag2 : "h1"
				const element = this.wrap(allowedTag, text)
				return this.addRaw(element).addEOL()
			}
			/**
			 * Adds an HTML thematic break (<hr>) to the summary buffer
			 *
			 * @returns {Summary} summary instance
			 */
			addSeparator() {
				const element = this.wrap("hr", null)
				return this.addRaw(element).addEOL()
			}
			/**
			 * Adds an HTML line break (<br>) to the summary buffer
			 *
			 * @returns {Summary} summary instance
			 */
			addBreak() {
				const element = this.wrap("br", null)
				return this.addRaw(element).addEOL()
			}
			/**
			 * Adds an HTML blockquote to the summary buffer
			 *
			 * @param {string} text quote text
			 * @param {string} cite (optional) citation url
			 *
			 * @returns {Summary} summary instance
			 */
			addQuote(text, cite) {
				const attrs = Object.assign({}, cite && { cite })
				const element = this.wrap("blockquote", text, attrs)
				return this.addRaw(element).addEOL()
			}
			/**
			 * Adds an HTML anchor tag to the summary buffer
			 *
			 * @param {string} text link text/content
			 * @param {string} href hyperlink
			 *
			 * @returns {Summary} summary instance
			 */
			addLink(text, href) {
				const element = this.wrap("a", text, { href })
				return this.addRaw(element).addEOL()
			}
		}
		var _summary = new Summary()
		exports2.markdownSummary = _summary
		exports2.summary = _summary
	},
})

// ../../../node_modules/.pnpm/@actions+core@1.10.1/node_modules/@actions/core/lib/path-utils.js
var require_path_utils = __commonJS({
	"../../../node_modules/.pnpm/@actions+core@1.10.1/node_modules/@actions/core/lib/path-utils.js"(
		exports2
	) {
		"use strict"
		var __createBinding =
			(exports2 && exports2.__createBinding) ||
			(Object.create
				? function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						Object.defineProperty(o, k2, {
							enumerable: true,
							get: function () {
								return m[k]
							},
						})
				  }
				: function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						o[k2] = m[k]
				  })
		var __setModuleDefault =
			(exports2 && exports2.__setModuleDefault) ||
			(Object.create
				? function (o, v) {
						Object.defineProperty(o, "default", { enumerable: true, value: v })
				  }
				: function (o, v) {
						o["default"] = v
				  })
		var __importStar =
			(exports2 && exports2.__importStar) ||
			function (mod) {
				if (mod && mod.__esModule) return mod
				var result = {}
				if (mod != undefined) {
					for (var k in mod)
						if (k !== "default" && Object.hasOwnProperty.call(mod, k))
							__createBinding(result, mod, k)
				}
				__setModuleDefault(result, mod)
				return result
			}
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.toPlatformPath = exports2.toWin32Path = exports2.toPosixPath = void 0
		var path = __importStar(require("node:path"))
		function toPosixPath(pth) {
			return pth.replace(/[\\]/g, "/")
		}
		exports2.toPosixPath = toPosixPath
		function toWin32Path(pth) {
			return pth.replace(/[/]/g, "\\")
		}
		exports2.toWin32Path = toWin32Path
		function toPlatformPath(pth) {
			return pth.replace(/[/\\]/g, path.sep)
		}
		exports2.toPlatformPath = toPlatformPath
	},
})

// ../../../node_modules/.pnpm/@actions+core@1.10.1/node_modules/@actions/core/lib/core.js
var require_core = __commonJS({
	"../../../node_modules/.pnpm/@actions+core@1.10.1/node_modules/@actions/core/lib/core.js"(
		exports2
	) {
		"use strict"
		var __createBinding =
			(exports2 && exports2.__createBinding) ||
			(Object.create
				? function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						Object.defineProperty(o, k2, {
							enumerable: true,
							get: function () {
								return m[k]
							},
						})
				  }
				: function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						o[k2] = m[k]
				  })
		var __setModuleDefault =
			(exports2 && exports2.__setModuleDefault) ||
			(Object.create
				? function (o, v) {
						Object.defineProperty(o, "default", { enumerable: true, value: v })
				  }
				: function (o, v) {
						o["default"] = v
				  })
		var __importStar =
			(exports2 && exports2.__importStar) ||
			function (mod) {
				if (mod && mod.__esModule) return mod
				var result = {}
				if (mod != undefined) {
					for (var k in mod)
						if (k !== "default" && Object.hasOwnProperty.call(mod, k))
							__createBinding(result, mod, k)
				}
				__setModuleDefault(result, mod)
				return result
			}
		var __awaiter =
			(exports2 && exports2.__awaiter) ||
			function (thisArg, _arguments, P, generator) {
				function adopt(value) {
					return value instanceof P
						? value
						: new P(function (resolve) {
								resolve(value)
						  })
				}
				return new (P || (P = Promise))(function (resolve, reject) {
					function fulfilled(value) {
						try {
							step(generator.next(value))
						} catch (e) {
							reject(e)
						}
					}
					function rejected(value) {
						try {
							step(generator["throw"](value))
						} catch (e) {
							reject(e)
						}
					}
					function step(result) {
						result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected)
					}
					step((generator = generator.apply(thisArg, _arguments || [])).next())
				})
			}
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.getIDToken =
			exports2.getState =
			exports2.saveState =
			exports2.group =
			exports2.endGroup =
			exports2.startGroup =
			exports2.info =
			exports2.notice =
			exports2.warning =
			exports2.error =
			exports2.debug =
			exports2.isDebug =
			exports2.setFailed =
			exports2.setCommandEcho =
			exports2.setOutput =
			exports2.getBooleanInput =
			exports2.getMultilineInput =
			exports2.getInput =
			exports2.addPath =
			exports2.setSecret =
			exports2.exportVariable =
			exports2.ExitCode =
				void 0
		var command_1 = require_command()
		var file_command_1 = require_file_command()
		var utils_1 = require_utils()
		var os = __importStar(require("node:os"))
		var path = __importStar(require("node:path"))
		var oidc_utils_1 = require_oidc_utils()
		var ExitCode
		;(function (ExitCode2) {
			ExitCode2[(ExitCode2["Success"] = 0)] = "Success"
			ExitCode2[(ExitCode2["Failure"] = 1)] = "Failure"
		})((ExitCode = exports2.ExitCode || (exports2.ExitCode = {})))
		function exportVariable(name, val) {
			const convertedVal = utils_1.toCommandValue(val)
			process.env[name] = convertedVal
			const filePath = process.env["GITHUB_ENV"] || ""
			if (filePath) {
				return file_command_1.issueFileCommand(
					"ENV",
					file_command_1.prepareKeyValueMessage(name, val)
				)
			}
			command_1.issueCommand("set-env", { name }, convertedVal)
		}
		exports2.exportVariable = exportVariable
		function setSecret(secret) {
			command_1.issueCommand("add-mask", {}, secret)
		}
		exports2.setSecret = setSecret
		function addPath(inputPath) {
			const filePath = process.env["GITHUB_PATH"] || ""
			if (filePath) {
				file_command_1.issueFileCommand("PATH", inputPath)
			} else {
				command_1.issueCommand("add-path", {}, inputPath)
			}
			process.env["PATH"] = `${inputPath}${path.delimiter}${process.env["PATH"]}`
		}
		exports2.addPath = addPath
		function getInput(name, options) {
			const val = process.env[`INPUT_${name.replace(/ /g, "_").toUpperCase()}`] || ""
			if (options && options.required && !val) {
				throw new Error(`Input required and not supplied: ${name}`)
			}
			if (options && options.trimWhitespace === false) {
				return val
			}
			return val.trim()
		}
		exports2.getInput = getInput
		function getMultilineInput(name, options) {
			const inputs = getInput(name, options)
				.split("\n")
				.filter((x) => x !== "")
			if (options && options.trimWhitespace === false) {
				return inputs
			}
			return inputs.map((input) => input.trim())
		}
		exports2.getMultilineInput = getMultilineInput
		function getBooleanInput(name, options) {
			const trueValue = ["true", "True", "TRUE"]
			const falseValue = ["false", "False", "FALSE"]
			const val = getInput(name, options)
			if (trueValue.includes(val)) return true
			if (falseValue.includes(val)) return false
			throw new TypeError(`Input does not meet YAML 1.2 "Core Schema" specification: ${name}
Support boolean input list: \`true | True | TRUE | false | False | FALSE\``)
		}
		exports2.getBooleanInput = getBooleanInput
		function setOutput(name, value) {
			const filePath = process.env["GITHUB_OUTPUT"] || ""
			if (filePath) {
				return file_command_1.issueFileCommand(
					"OUTPUT",
					file_command_1.prepareKeyValueMessage(name, value)
				)
			}
			process.stdout.write(os.EOL)
			command_1.issueCommand("set-output", { name }, utils_1.toCommandValue(value))
		}
		exports2.setOutput = setOutput
		function setCommandEcho(enabled) {
			command_1.issue("echo", enabled ? "on" : "off")
		}
		exports2.setCommandEcho = setCommandEcho
		function setFailed2(message) {
			process.exitCode = ExitCode.Failure
			error(message)
		}
		exports2.setFailed = setFailed2
		function isDebug() {
			return process.env["RUNNER_DEBUG"] === "1"
		}
		exports2.isDebug = isDebug
		function debug10(message) {
			command_1.issueCommand("debug", {}, message)
		}
		exports2.debug = debug10
		function error(message, properties = {}) {
			command_1.issueCommand(
				"error",
				utils_1.toCommandProperties(properties),
				message instanceof Error ? message.toString() : message
			)
		}
		exports2.error = error
		function warning(message, properties = {}) {
			command_1.issueCommand(
				"warning",
				utils_1.toCommandProperties(properties),
				message instanceof Error ? message.toString() : message
			)
		}
		exports2.warning = warning
		function notice(message, properties = {}) {
			command_1.issueCommand(
				"notice",
				utils_1.toCommandProperties(properties),
				message instanceof Error ? message.toString() : message
			)
		}
		exports2.notice = notice
		function info(message) {
			process.stdout.write(message + os.EOL)
		}
		exports2.info = info
		function startGroup(name) {
			command_1.issue("group", name)
		}
		exports2.startGroup = startGroup
		function endGroup() {
			command_1.issue("endgroup")
		}
		exports2.endGroup = endGroup
		function group(name, fn) {
			return __awaiter(this, void 0, void 0, function* () {
				startGroup(name)
				let result
				try {
					result = yield fn()
				} finally {
					endGroup()
				}
				return result
			})
		}
		exports2.group = group
		function saveState(name, value) {
			const filePath = process.env["GITHUB_STATE"] || ""
			if (filePath) {
				return file_command_1.issueFileCommand(
					"STATE",
					file_command_1.prepareKeyValueMessage(name, value)
				)
			}
			command_1.issueCommand("save-state", { name }, utils_1.toCommandValue(value))
		}
		exports2.saveState = saveState
		function getState(name) {
			return process.env[`STATE_${name}`] || ""
		}
		exports2.getState = getState
		function getIDToken(aud) {
			return __awaiter(this, void 0, void 0, function* () {
				return yield oidc_utils_1.OidcClient.getIDToken(aud)
			})
		}
		exports2.getIDToken = getIDToken
		var summary_1 = require_summary()
		Object.defineProperty(exports2, "summary", {
			enumerable: true,
			get: function () {
				return summary_1.summary
			},
		})
		var summary_2 = require_summary()
		Object.defineProperty(exports2, "markdownSummary", {
			enumerable: true,
			get: function () {
				return summary_2.markdownSummary
			},
		})
		var path_utils_1 = require_path_utils()
		Object.defineProperty(exports2, "toPosixPath", {
			enumerable: true,
			get: function () {
				return path_utils_1.toPosixPath
			},
		})
		Object.defineProperty(exports2, "toWin32Path", {
			enumerable: true,
			get: function () {
				return path_utils_1.toWin32Path
			},
		})
		Object.defineProperty(exports2, "toPlatformPath", {
			enumerable: true,
			get: function () {
				return path_utils_1.toPlatformPath
			},
		})
	},
})

// ../../../node_modules/.pnpm/@actions+github@6.0.0/node_modules/@actions/github/lib/context.js
var require_context = __commonJS({
	"../../../node_modules/.pnpm/@actions+github@6.0.0/node_modules/@actions/github/lib/context.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.Context = void 0
		var fs_1 = require("node:fs")
		var os_1 = require("node:os")
		var Context = class {
			/**
			 * Hydrate the context from the environment
			 */
			constructor() {
				var _a, _b, _c
				this.payload = {}
				if (process.env.GITHUB_EVENT_PATH) {
					if ((0, fs_1.existsSync)(process.env.GITHUB_EVENT_PATH)) {
						this.payload = JSON.parse(
							(0, fs_1.readFileSync)(process.env.GITHUB_EVENT_PATH, { encoding: "utf8" })
						)
					} else {
						const path = process.env.GITHUB_EVENT_PATH
						process.stdout.write(`GITHUB_EVENT_PATH ${path} does not exist${os_1.EOL}`)
					}
				}
				this.eventName = process.env.GITHUB_EVENT_NAME
				this.sha = process.env.GITHUB_SHA
				this.ref = process.env.GITHUB_REF
				this.workflow = process.env.GITHUB_WORKFLOW
				this.action = process.env.GITHUB_ACTION
				this.actor = process.env.GITHUB_ACTOR
				this.job = process.env.GITHUB_JOB
				this.runNumber = parseInt(process.env.GITHUB_RUN_NUMBER, 10)
				this.runId = parseInt(process.env.GITHUB_RUN_ID, 10)
				this.apiUrl =
					(_a = process.env.GITHUB_API_URL) !== null && _a !== void 0
						? _a
						: `https://api.github.com`
				this.serverUrl =
					(_b = process.env.GITHUB_SERVER_URL) !== null && _b !== void 0 ? _b : `https://github.com`
				this.graphqlUrl =
					(_c = process.env.GITHUB_GRAPHQL_URL) !== null && _c !== void 0
						? _c
						: `https://api.github.com/graphql`
			}
			get issue() {
				const payload = this.payload
				return Object.assign(Object.assign({}, this.repo), {
					number: (payload.issue || payload.pull_request || payload).number,
				})
			}
			get repo() {
				if (process.env.GITHUB_REPOSITORY) {
					const [owner, repo] = process.env.GITHUB_REPOSITORY.split("/")
					return { owner, repo }
				}
				if (this.payload.repository) {
					return {
						owner: this.payload.repository.owner.login,
						repo: this.payload.repository.name,
					}
				}
				throw new Error(
					"context.repo requires a GITHUB_REPOSITORY environment variable like 'owner/repo'"
				)
			}
		}
		exports2.Context = Context
	},
})

// ../../../node_modules/.pnpm/@actions+github@6.0.0/node_modules/@actions/github/lib/internal/utils.js
var require_utils3 = __commonJS({
	"../../../node_modules/.pnpm/@actions+github@6.0.0/node_modules/@actions/github/lib/internal/utils.js"(
		exports2
	) {
		"use strict"
		var __createBinding =
			(exports2 && exports2.__createBinding) ||
			(Object.create
				? function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						var desc = Object.getOwnPropertyDescriptor(m, k)
						if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
							desc = {
								enumerable: true,
								get: function () {
									return m[k]
								},
							}
						}
						Object.defineProperty(o, k2, desc)
				  }
				: function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						o[k2] = m[k]
				  })
		var __setModuleDefault =
			(exports2 && exports2.__setModuleDefault) ||
			(Object.create
				? function (o, v) {
						Object.defineProperty(o, "default", { enumerable: true, value: v })
				  }
				: function (o, v) {
						o["default"] = v
				  })
		var __importStar =
			(exports2 && exports2.__importStar) ||
			function (mod) {
				if (mod && mod.__esModule) return mod
				var result = {}
				if (mod != undefined) {
					for (var k in mod)
						if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k))
							__createBinding(result, mod, k)
				}
				__setModuleDefault(result, mod)
				return result
			}
		var __awaiter =
			(exports2 && exports2.__awaiter) ||
			function (thisArg, _arguments, P, generator) {
				function adopt(value) {
					return value instanceof P
						? value
						: new P(function (resolve) {
								resolve(value)
						  })
				}
				return new (P || (P = Promise))(function (resolve, reject) {
					function fulfilled(value) {
						try {
							step(generator.next(value))
						} catch (e) {
							reject(e)
						}
					}
					function rejected(value) {
						try {
							step(generator["throw"](value))
						} catch (e) {
							reject(e)
						}
					}
					function step(result) {
						result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected)
					}
					step((generator = generator.apply(thisArg, _arguments || [])).next())
				})
			}
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.getApiBaseUrl =
			exports2.getProxyFetch =
			exports2.getProxyAgentDispatcher =
			exports2.getProxyAgent =
			exports2.getAuthString =
				void 0
		var httpClient = __importStar(require_lib())
		var undici_1 = require_undici()
		function getAuthString(token, options) {
			if (!token && !options.auth) {
				throw new Error("Parameter token or opts.auth is required")
			} else if (token && options.auth) {
				throw new Error("Parameters token and opts.auth may not both be specified")
			}
			return typeof options.auth === "string" ? options.auth : `token ${token}`
		}
		exports2.getAuthString = getAuthString
		function getProxyAgent(destinationUrl) {
			const hc = new httpClient.HttpClient()
			return hc.getAgent(destinationUrl)
		}
		exports2.getProxyAgent = getProxyAgent
		function getProxyAgentDispatcher(destinationUrl) {
			const hc = new httpClient.HttpClient()
			return hc.getAgentDispatcher(destinationUrl)
		}
		exports2.getProxyAgentDispatcher = getProxyAgentDispatcher
		function getProxyFetch(destinationUrl) {
			const httpDispatcher = getProxyAgentDispatcher(destinationUrl)
			const proxyFetch = (url, opts) =>
				__awaiter(this, void 0, void 0, function* () {
					return (0,
					undici_1.fetch)(url, Object.assign(Object.assign({}, opts), { dispatcher: httpDispatcher }))
				})
			return proxyFetch
		}
		exports2.getProxyFetch = getProxyFetch
		function getApiBaseUrl() {
			return process.env["GITHUB_API_URL"] || "https://api.github.com"
		}
		exports2.getApiBaseUrl = getApiBaseUrl
	},
})

// ../../../node_modules/.pnpm/universal-user-agent@6.0.1/node_modules/universal-user-agent/dist-node/index.js
var require_dist_node = __commonJS({
	"../../../node_modules/.pnpm/universal-user-agent@6.0.1/node_modules/universal-user-agent/dist-node/index.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		function getUserAgent() {
			if (typeof navigator === "object" && "userAgent" in navigator) {
				return navigator.userAgent
			}
			if (typeof process === "object" && process.version !== void 0) {
				return `Node.js/${process.version.slice(1)} (${process.platform}; ${process.arch})`
			}
			return "<environment undetectable>"
		}
		exports2.getUserAgent = getUserAgent
	},
})

// ../../../node_modules/.pnpm/before-after-hook@2.2.3/node_modules/before-after-hook/lib/register.js
var require_register = __commonJS({
	"../../../node_modules/.pnpm/before-after-hook@2.2.3/node_modules/before-after-hook/lib/register.js"(
		exports2,
		module2
	) {
		module2.exports = register
		function register(state, name, method, options) {
			if (typeof method !== "function") {
				throw new Error("method for before hook must be a function")
			}
			if (!options) {
				options = {}
			}
			if (Array.isArray(name)) {
				return name.reverse().reduce(function (callback, name2) {
					return register.bind(null, state, name2, callback, options)
				}, method)()
			}
			return Promise.resolve().then(function () {
				if (!state.registry[name]) {
					return method(options)
				}
				return state.registry[name].reduce(function (method2, registered) {
					return registered.hook.bind(null, method2, options)
				}, method)()
			})
		}
	},
})

// ../../../node_modules/.pnpm/before-after-hook@2.2.3/node_modules/before-after-hook/lib/add.js
var require_add = __commonJS({
	"../../../node_modules/.pnpm/before-after-hook@2.2.3/node_modules/before-after-hook/lib/add.js"(
		exports2,
		module2
	) {
		module2.exports = addHook
		function addHook(state, kind, name, hook) {
			var orig = hook
			if (!state.registry[name]) {
				state.registry[name] = []
			}
			if (kind === "before") {
				hook = function (method, options) {
					return Promise.resolve().then(orig.bind(null, options)).then(method.bind(null, options))
				}
			}
			if (kind === "after") {
				hook = function (method, options) {
					var result
					return Promise.resolve()
						.then(method.bind(null, options))
						.then(function (result_) {
							result = result_
							return orig(result, options)
						})
						.then(function () {
							return result
						})
				}
			}
			if (kind === "error") {
				hook = function (method, options) {
					return Promise.resolve()
						.then(method.bind(null, options))
						.catch(function (error) {
							return orig(error, options)
						})
				}
			}
			state.registry[name].push({
				hook,
				orig,
			})
		}
	},
})

// ../../../node_modules/.pnpm/before-after-hook@2.2.3/node_modules/before-after-hook/lib/remove.js
var require_remove = __commonJS({
	"../../../node_modules/.pnpm/before-after-hook@2.2.3/node_modules/before-after-hook/lib/remove.js"(
		exports2,
		module2
	) {
		module2.exports = removeHook
		function removeHook(state, name, method) {
			if (!state.registry[name]) {
				return
			}
			var index2 = state.registry[name]
				.map(function (registered) {
					return registered.orig
				})
				.indexOf(method)
			if (index2 === -1) {
				return
			}
			state.registry[name].splice(index2, 1)
		}
	},
})

// ../../../node_modules/.pnpm/before-after-hook@2.2.3/node_modules/before-after-hook/index.js
var require_before_after_hook = __commonJS({
	"../../../node_modules/.pnpm/before-after-hook@2.2.3/node_modules/before-after-hook/index.js"(
		exports2,
		module2
	) {
		var register = require_register()
		var addHook = require_add()
		var removeHook = require_remove()
		var bind = Function.bind
		var bindable = bind.bind(bind)
		function bindApi(hook, state, name) {
			var removeHookRef = bindable(removeHook, null).apply(null, name ? [state, name] : [state])
			hook.api = { remove: removeHookRef }
			hook.remove = removeHookRef
			for (const kind of ["before", "error", "after", "wrap"]) {
				var args = name ? [state, kind, name] : [state, kind]
				hook[kind] = hook.api[kind] = bindable(addHook, null).apply(null, args)
			}
		}
		function HookSingular() {
			var singularHookName = "h"
			var singularHookState = {
				registry: {},
			}
			var singularHook = register.bind(null, singularHookState, singularHookName)
			bindApi(singularHook, singularHookState, singularHookName)
			return singularHook
		}
		function HookCollection() {
			var state = {
				registry: {},
			}
			var hook = register.bind(null, state)
			bindApi(hook, state)
			return hook
		}
		var collectionHookDeprecationMessageDisplayed = false
		function Hook() {
			if (!collectionHookDeprecationMessageDisplayed) {
				console.warn(
					'[before-after-hook]: "Hook()" repurposing warning, use "Hook.Collection()". Read more: https://git.io/upgrade-before-after-hook-to-1.4'
				)
				collectionHookDeprecationMessageDisplayed = true
			}
			return HookCollection()
		}
		Hook.Singular = HookSingular.bind()
		Hook.Collection = HookCollection.bind()
		module2.exports = Hook
		module2.exports.Hook = Hook
		module2.exports.Singular = Hook.Singular
		module2.exports.Collection = Hook.Collection
	},
})

// ../../../node_modules/.pnpm/@octokit+endpoint@9.0.5/node_modules/@octokit/endpoint/dist-node/index.js
var require_dist_node2 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+endpoint@9.0.5/node_modules/@octokit/endpoint/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			endpoint: () => endpoint,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var import_universal_user_agent = require_dist_node()
		var VERSION = "9.0.5"
		var userAgent = `octokit-endpoint.js/${VERSION} ${(0,
		import_universal_user_agent.getUserAgent)()}`
		var DEFAULTS = {
			method: "GET",
			baseUrl: "https://api.github.com",
			headers: {
				accept: "application/vnd.github.v3+json",
				"user-agent": userAgent,
			},
			mediaType: {
				format: "",
			},
		}
		function lowercaseKeys(object) {
			if (!object) {
				return {}
			}
			return Object.keys(object).reduce((newObj, key) => {
				newObj[key.toLowerCase()] = object[key]
				return newObj
			}, {})
		}
		function isPlainObject(value) {
			if (typeof value !== "object" || value === null) return false
			if (Object.prototype.toString.call(value) !== "[object Object]") return false
			const proto = Object.getPrototypeOf(value)
			if (proto === null) return true
			const Ctor = Object.prototype.hasOwnProperty.call(proto, "constructor") && proto.constructor
			return (
				typeof Ctor === "function" &&
				Ctor instanceof Ctor &&
				Function.prototype.call(Ctor) === Function.prototype.call(value)
			)
		}
		function mergeDeep(defaults, options) {
			const result = Object.assign({}, defaults)
			for (const key of Object.keys(options)) {
				if (isPlainObject(options[key])) {
					if (!(key in defaults)) Object.assign(result, { [key]: options[key] })
					else result[key] = mergeDeep(defaults[key], options[key])
				} else {
					Object.assign(result, { [key]: options[key] })
				}
			}
			return result
		}
		function removeUndefinedProperties(obj) {
			for (const key in obj) {
				if (obj[key] === void 0) {
					delete obj[key]
				}
			}
			return obj
		}
		function merge2(defaults, route, options) {
			if (typeof route === "string") {
				let [method, url] = route.split(" ")
				options = Object.assign(url ? { method, url } : { url: method }, options)
			} else {
				options = Object.assign({}, route)
			}
			options.headers = lowercaseKeys(options.headers)
			removeUndefinedProperties(options)
			removeUndefinedProperties(options.headers)
			const mergedOptions = mergeDeep(defaults || {}, options)
			if (options.url === "/graphql") {
				if (defaults && defaults.mediaType.previews?.length) {
					mergedOptions.mediaType.previews = defaults.mediaType.previews
						.filter((preview) => !mergedOptions.mediaType.previews.includes(preview))
						.concat(mergedOptions.mediaType.previews)
				}
				mergedOptions.mediaType.previews = (mergedOptions.mediaType.previews || []).map((preview) =>
					preview.replace(/-preview/, "")
				)
			}
			return mergedOptions
		}
		function addQueryParameters(url, parameters) {
			const separator = /\?/.test(url) ? "&" : "?"
			const names = Object.keys(parameters)
			if (names.length === 0) {
				return url
			}
			return (
				url +
				separator +
				names
					.map((name) => {
						if (name === "q") {
							return "q=" + parameters.q.split("+").map(encodeURIComponent).join("+")
						}
						return `${name}=${encodeURIComponent(parameters[name])}`
					})
					.join("&")
			)
		}
		var urlVariableRegex = /\{[^}]+\}/g
		function removeNonChars(variableName) {
			return variableName.replace(/^\W+|\W+$/g, "").split(/,/)
		}
		function extractUrlVariableNames(url) {
			const matches = url.match(urlVariableRegex)
			if (!matches) {
				return []
			}
			return matches.map(removeNonChars).reduce((a, b) => a.concat(b), [])
		}
		function omit(object, keysToOmit) {
			const result = { __proto__: null }
			for (const key of Object.keys(object)) {
				if (!keysToOmit.includes(key)) {
					result[key] = object[key]
				}
			}
			return result
		}
		function encodeReserved(str) {
			return str
				.split(/(%[0-9A-Fa-f]{2})/g)
				.map(function (part) {
					if (!/%[0-9A-Fa-f]/.test(part)) {
						part = encodeURI(part).replace(/%5B/g, "[").replace(/%5D/g, "]")
					}
					return part
				})
				.join("")
		}
		function encodeUnreserved(str) {
			return encodeURIComponent(str).replace(/[!'()*]/g, function (c) {
				return "%" + c.charCodeAt(0).toString(16).toUpperCase()
			})
		}
		function encodeValue(operator, value, key) {
			value = operator === "+" || operator === "#" ? encodeReserved(value) : encodeUnreserved(value)
			if (key) {
				return encodeUnreserved(key) + "=" + value
			} else {
				return value
			}
		}
		function isDefined(value) {
			return value !== void 0 && value !== null
		}
		function isKeyOperator(operator) {
			return operator === ";" || operator === "&" || operator === "?"
		}
		function getValues(context2, operator, key, modifier) {
			var value = context2[key],
				result = []
			if (isDefined(value) && value !== "") {
				if (typeof value === "string" || typeof value === "number" || typeof value === "boolean") {
					value = value.toString()
					if (modifier && modifier !== "*") {
						value = value.slice(0, Math.max(0, parseInt(modifier, 10)))
					}
					result.push(encodeValue(operator, value, isKeyOperator(operator) ? key : ""))
				} else {
					if (modifier === "*") {
						if (Array.isArray(value)) {
							for (const value2 of value.filter(isDefined)) {
								result.push(encodeValue(operator, value2, isKeyOperator(operator) ? key : ""))
							}
						} else {
							for (const k of Object.keys(value)) {
								if (isDefined(value[k])) {
									result.push(encodeValue(operator, value[k], k))
								}
							}
						}
					} else {
						const tmp = []
						if (Array.isArray(value)) {
							for (const value2 of value.filter(isDefined)) {
								tmp.push(encodeValue(operator, value2))
							}
						} else {
							for (const k of Object.keys(value)) {
								if (isDefined(value[k])) {
									tmp.push(encodeUnreserved(k))
									tmp.push(encodeValue(operator, value[k].toString()))
								}
							}
						}
						if (isKeyOperator(operator)) {
							result.push(encodeUnreserved(key) + "=" + tmp.join(","))
						} else if (tmp.length !== 0) {
							result.push(tmp.join(","))
						}
					}
				}
			} else {
				if (operator === ";") {
					if (isDefined(value)) {
						result.push(encodeUnreserved(key))
					}
				} else if (value === "" && (operator === "&" || operator === "?")) {
					result.push(encodeUnreserved(key) + "=")
				} else if (value === "") {
					result.push("")
				}
			}
			return result
		}
		function parseUrl(template) {
			return {
				expand: expand.bind(null, template),
			}
		}
		function expand(template, context2) {
			var operators = ["+", "#", ".", "/", ";", "?", "&"]
			template = template.replace(/\{([^\{\}]+)\}|([^\{\}]+)/g, function (_, expression, literal) {
				if (expression) {
					let operator = ""
					const values = []
					if (operators.includes(expression.charAt(0))) {
						operator = expression.charAt(0)
						expression = expression.slice(1)
					}
					for (const variable of expression.split(/,/g)) {
						var tmp = /([^:\*]*)(?::(\d+)|(\*))?/.exec(variable)
						values.push(getValues(context2, operator, tmp[1], tmp[2] || tmp[3]))
					}
					if (operator && operator !== "+") {
						var separator = ","
						if (operator === "?") {
							separator = "&"
						} else if (operator !== "#") {
							separator = operator
						}
						return (values.length !== 0 ? operator : "") + values.join(separator)
					} else {
						return values.join(",")
					}
				} else {
					return encodeReserved(literal)
				}
			})
			if (template === "/") {
				return template
			} else {
				return template.replace(/\/$/, "")
			}
		}
		function parse2(options) {
			let method = options.method.toUpperCase()
			let url = (options.url || "/").replace(/:([a-z]\w+)/g, "{$1}")
			let headers = Object.assign({}, options.headers)
			let body
			let parameters = omit(options, [
				"method",
				"baseUrl",
				"url",
				"headers",
				"request",
				"mediaType",
			])
			const urlVariableNames = extractUrlVariableNames(url)
			url = parseUrl(url).expand(parameters)
			if (!/^http/.test(url)) {
				url = options.baseUrl + url
			}
			const omittedParameters = [
				...Object.keys(options).filter((option) => urlVariableNames.includes(option)),
				"baseUrl",
			]
			const remainingParameters = omit(parameters, omittedParameters)
			const isBinaryRequest = /application\/octet-stream/i.test(headers.accept)
			if (!isBinaryRequest) {
				if (options.mediaType.format) {
					headers.accept = headers.accept
						.split(/,/)
						.map((format) =>
							format.replace(
								/application\/vnd(\.\w+)(\.v3)?(\.\w+)?(\+json)?$/,
								`application/vnd$1$2.${options.mediaType.format}`
							)
						)
						.join(",")
				}
				if (url.endsWith("/graphql") && options.mediaType.previews?.length) {
					const previewsFromAcceptHeader = headers.accept.match(/[\w-]+(?=-preview)/g) || []
					headers.accept = previewsFromAcceptHeader
						.concat(options.mediaType.previews)
						.map((preview) => {
							const format = options.mediaType.format ? `.${options.mediaType.format}` : "+json"
							return `application/vnd.github.${preview}-preview${format}`
						})
						.join(",")
				}
			}
			if (["GET", "HEAD"].includes(method)) {
				url = addQueryParameters(url, remainingParameters)
			} else {
				if ("data" in remainingParameters) {
					body = remainingParameters.data
				} else {
					if (Object.keys(remainingParameters).length) {
						body = remainingParameters
					}
				}
			}
			if (!headers["content-type"] && typeof body !== "undefined") {
				headers["content-type"] = "application/json; charset=utf-8"
			}
			if (["PATCH", "PUT"].includes(method) && typeof body === "undefined") {
				body = ""
			}
			return Object.assign(
				{ method, url, headers },
				typeof body !== "undefined" ? { body } : null,
				options.request ? { request: options.request } : null
			)
		}
		function endpointWithDefaults(defaults, route, options) {
			return parse2(merge2(defaults, route, options))
		}
		function withDefaults(oldDefaults, newDefaults) {
			const DEFAULTS2 = merge2(oldDefaults, newDefaults)
			const endpoint2 = endpointWithDefaults.bind(null, DEFAULTS2)
			return Object.assign(endpoint2, {
				DEFAULTS: DEFAULTS2,
				defaults: withDefaults.bind(null, DEFAULTS2),
				merge: merge2.bind(null, DEFAULTS2),
				parse: parse2,
			})
		}
		var endpoint = withDefaults(null, DEFAULTS)
	},
})

// ../../../node_modules/.pnpm/deprecation@2.3.1/node_modules/deprecation/dist-node/index.js
var require_dist_node3 = __commonJS({
	"../../../node_modules/.pnpm/deprecation@2.3.1/node_modules/deprecation/dist-node/index.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		var Deprecation = class extends Error {
			constructor(message) {
				super(message)
				if (Error.captureStackTrace) {
					Error.captureStackTrace(this, this.constructor)
				}
				this.name = "Deprecation"
			}
		}
		exports2.Deprecation = Deprecation
	},
})

// ../../../node_modules/.pnpm/wrappy@1.0.2/node_modules/wrappy/wrappy.js
var require_wrappy = __commonJS({
	"../../../node_modules/.pnpm/wrappy@1.0.2/node_modules/wrappy/wrappy.js"(exports2, module2) {
		module2.exports = wrappy
		function wrappy(fn, cb) {
			if (fn && cb) return wrappy(fn)(cb)
			if (typeof fn !== "function") throw new TypeError("need wrapper function")
			for (const k of Object.keys(fn)) {
				wrapper[k] = fn[k]
			}
			return wrapper
			function wrapper() {
				var args = new Array(arguments.length)
				for (var i = 0; i < args.length; i++) {
					args[i] = arguments[i]
				}
				var ret = fn.apply(this, args)
				var cb2 = args.at(-1)
				if (typeof ret === "function" && ret !== cb2) {
					for (const k of Object.keys(cb2)) {
						ret[k] = cb2[k]
					}
				}
				return ret
			}
		}
	},
})

// ../../../node_modules/.pnpm/once@1.4.0/node_modules/once/once.js
var require_once = __commonJS({
	"../../../node_modules/.pnpm/once@1.4.0/node_modules/once/once.js"(exports2, module2) {
		var wrappy = require_wrappy()
		module2.exports = wrappy(once)
		module2.exports.strict = wrappy(onceStrict)
		once.proto = once(function () {
			Object.defineProperty(Function.prototype, "once", {
				value: function () {
					return once(this)
				},
				configurable: true,
			})
			Object.defineProperty(Function.prototype, "onceStrict", {
				value: function () {
					return onceStrict(this)
				},
				configurable: true,
			})
		})
		function once(fn) {
			var f = function () {
				if (f.called) return f.value
				f.called = true
				return (f.value = fn.apply(this, arguments))
			}
			f.called = false
			return f
		}
		function onceStrict(fn) {
			var f = function () {
				if (f.called) throw new Error(f.onceError)
				f.called = true
				return (f.value = fn.apply(this, arguments))
			}
			var name = fn.name || "Function wrapped with `once`"
			f.onceError = name + " shouldn't be called more than once"
			f.called = false
			return f
		}
	},
})

// ../../../node_modules/.pnpm/@octokit+request-error@5.1.0/node_modules/@octokit/request-error/dist-node/index.js
var require_dist_node4 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+request-error@5.1.0/node_modules/@octokit/request-error/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __create2 = Object.create
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __getProtoOf2 = Object.getPrototypeOf
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toESM2 = (mod, isNodeMode, target) => (
			(target = mod != undefined ? __create2(__getProtoOf2(mod)) : {}),
			__copyProps2(
				// If the importer is in node compatibility mode or this is not an ESM
				// file that has been converted to a CommonJS file using a Babel-
				// compatible transform (i.e. "__esModule" has not been set), then set
				// "default" to the CommonJS "module.exports" for node compatibility.
				isNodeMode || !mod || !mod.__esModule
					? __defProp2(target, "default", { value: mod, enumerable: true })
					: target,
				mod
			)
		)
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			RequestError: () => RequestError2,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var import_deprecation = require_dist_node3()
		var import_once = __toESM2(require_once())
		var logOnceCode = (0, import_once.default)((deprecation) => console.warn(deprecation))
		var logOnceHeaders = (0, import_once.default)((deprecation) => console.warn(deprecation))
		var RequestError2 = class extends Error {
			constructor(message, statusCode, options) {
				super(message)
				if (Error.captureStackTrace) {
					Error.captureStackTrace(this, this.constructor)
				}
				this.name = "HttpError"
				this.status = statusCode
				let headers
				if ("headers" in options && typeof options.headers !== "undefined") {
					headers = options.headers
				}
				if ("response" in options) {
					this.response = options.response
					headers = options.response.headers
				}
				const requestCopy = Object.assign({}, options.request)
				if (options.request.headers.authorization) {
					requestCopy.headers = Object.assign({}, options.request.headers, {
						authorization: options.request.headers.authorization.replace(/ .*$/, " [REDACTED]"),
					})
				}
				requestCopy.url = requestCopy.url
					.replace(/\bclient_secret=\w+/g, "client_secret=[REDACTED]")
					.replace(/\baccess_token=\w+/g, "access_token=[REDACTED]")
				this.request = requestCopy
				Object.defineProperty(this, "code", {
					get() {
						logOnceCode(
							new import_deprecation.Deprecation(
								"[@octokit/request-error] `error.code` is deprecated, use `error.status`."
							)
						)
						return statusCode
					},
				})
				Object.defineProperty(this, "headers", {
					get() {
						logOnceHeaders(
							new import_deprecation.Deprecation(
								"[@octokit/request-error] `error.headers` is deprecated, use `error.response.headers`."
							)
						)
						return headers || {}
					},
				})
			}
		}
	},
})

// ../../../node_modules/.pnpm/@octokit+request@8.4.0/node_modules/@octokit/request/dist-node/index.js
var require_dist_node5 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+request@8.4.0/node_modules/@octokit/request/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			request: () => request,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var import_endpoint = require_dist_node2()
		var import_universal_user_agent = require_dist_node()
		var VERSION = "8.4.0"
		function isPlainObject(value) {
			if (typeof value !== "object" || value === null) return false
			if (Object.prototype.toString.call(value) !== "[object Object]") return false
			const proto = Object.getPrototypeOf(value)
			if (proto === null) return true
			const Ctor = Object.prototype.hasOwnProperty.call(proto, "constructor") && proto.constructor
			return (
				typeof Ctor === "function" &&
				Ctor instanceof Ctor &&
				Function.prototype.call(Ctor) === Function.prototype.call(value)
			)
		}
		var import_request_error = require_dist_node4()
		function getBufferResponse(response) {
			return response.arrayBuffer()
		}
		function fetchWrapper(requestOptions) {
			var _a, _b, _c, _d
			const log3 =
				requestOptions.request && requestOptions.request.log ? requestOptions.request.log : console
			const parseSuccessResponseBody =
				((_a = requestOptions.request) == undefined ? void 0 : _a.parseSuccessResponseBody) !==
				false
			if (isPlainObject(requestOptions.body) || Array.isArray(requestOptions.body)) {
				requestOptions.body = JSON.stringify(requestOptions.body)
			}
			let headers = {}
			let status3
			let url
			let { fetch: fetch3 } = globalThis
			if ((_b = requestOptions.request) == undefined ? void 0 : _b.fetch) {
				fetch3 = requestOptions.request.fetch
			}
			if (!fetch3) {
				throw new Error(
					"fetch is not set. Please pass a fetch implementation as new Octokit({ request: { fetch }}). Learn more at https://github.com/octokit/octokit.js/#fetch-missing"
				)
			}
			return fetch3(requestOptions.url, {
				method: requestOptions.method,
				body: requestOptions.body,
				redirect: (_c = requestOptions.request) == undefined ? void 0 : _c.redirect,
				headers: requestOptions.headers,
				signal: (_d = requestOptions.request) == undefined ? void 0 : _d.signal,
				// duplex must be set if request.body is ReadableStream or Async Iterables.
				// See https://fetch.spec.whatwg.org/#dom-requestinit-duplex.
				...(requestOptions.body && { duplex: "half" }),
			})
				.then(async (response) => {
					url = response.url
					status3 = response.status
					for (const keyAndValue of response.headers) {
						headers[keyAndValue[0]] = keyAndValue[1]
					}
					if ("deprecation" in headers) {
						const matches = headers.link && headers.link.match(/<([^>]+)>; rel="deprecation"/)
						const deprecationLink = matches && matches.pop()
						log3.warn(
							`[@octokit/request] "${requestOptions.method} ${
								requestOptions.url
							}" is deprecated. It is scheduled to be removed on ${headers.sunset}${
								deprecationLink ? `. See ${deprecationLink}` : ""
							}`
						)
					}
					if (status3 === 204 || status3 === 205) {
						return
					}
					if (requestOptions.method === "HEAD") {
						if (status3 < 400) {
							return
						}
						throw new import_request_error.RequestError(response.statusText, status3, {
							response: {
								url,
								status: status3,
								headers,
								data: void 0,
							},
							request: requestOptions,
						})
					}
					if (status3 === 304) {
						throw new import_request_error.RequestError("Not modified", status3, {
							response: {
								url,
								status: status3,
								headers,
								data: await getResponseData(response),
							},
							request: requestOptions,
						})
					}
					if (status3 >= 400) {
						const data = await getResponseData(response)
						const error = new import_request_error.RequestError(toErrorMessage(data), status3, {
							response: {
								url,
								status: status3,
								headers,
								data,
							},
							request: requestOptions,
						})
						throw error
					}
					return parseSuccessResponseBody ? await getResponseData(response) : response.body
				})
				.then((data) => {
					return {
						status: status3,
						url,
						headers,
						data,
					}
				})
				.catch((error) => {
					if (error instanceof import_request_error.RequestError) throw error
					else if (error.name === "AbortError") throw error
					let message = error.message
					if (error.name === "TypeError" && "cause" in error) {
						if (error.cause instanceof Error) {
							message = error.cause.message
						} else if (typeof error.cause === "string") {
							message = error.cause
						}
					}
					throw new import_request_error.RequestError(message, 500, {
						request: requestOptions,
					})
				})
		}
		async function getResponseData(response) {
			const contentType = response.headers.get("content-type")
			if (/application\/json/.test(contentType)) {
				return response
					.json()
					.catch(() => response.text())
					.catch(() => "")
			}
			if (!contentType || /^text\/|charset=utf-8$/.test(contentType)) {
				return response.text()
			}
			return getBufferResponse(response)
		}
		function toErrorMessage(data) {
			if (typeof data === "string") return data
			let suffix
			if ("documentation_url" in data) {
				suffix = ` - ${data.documentation_url}`
			} else {
				suffix = ""
			}
			if ("message" in data) {
				if (Array.isArray(data.errors)) {
					return `${data.message}: ${data.errors.map(JSON.stringify).join(", ")}${suffix}`
				}
				return `${data.message}${suffix}`
			}
			return `Unknown error: ${JSON.stringify(data)}`
		}
		function withDefaults(oldEndpoint, newDefaults) {
			const endpoint2 = oldEndpoint.defaults(newDefaults)
			const newApi = function (route, parameters) {
				const endpointOptions = endpoint2.merge(route, parameters)
				if (!endpointOptions.request || !endpointOptions.request.hook) {
					return fetchWrapper(endpoint2.parse(endpointOptions))
				}
				const request2 = (route2, parameters2) => {
					return fetchWrapper(endpoint2.parse(endpoint2.merge(route2, parameters2)))
				}
				Object.assign(request2, {
					endpoint: endpoint2,
					defaults: withDefaults.bind(null, endpoint2),
				})
				return endpointOptions.request.hook(request2, endpointOptions)
			}
			return Object.assign(newApi, {
				endpoint: endpoint2,
				defaults: withDefaults.bind(null, endpoint2),
			})
		}
		var request = withDefaults(import_endpoint.endpoint, {
			headers: {
				"user-agent": `octokit-request.js/${VERSION} ${(0,
				import_universal_user_agent.getUserAgent)()}`,
			},
		})
	},
})

// ../../../node_modules/.pnpm/@octokit+graphql@7.1.0/node_modules/@octokit/graphql/dist-node/index.js
var require_dist_node6 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+graphql@7.1.0/node_modules/@octokit/graphql/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			GraphqlResponseError: () => GraphqlResponseError,
			graphql: () => graphql2,
			withCustomRequest: () => withCustomRequest,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var import_request3 = require_dist_node5()
		var import_universal_user_agent = require_dist_node()
		var VERSION = "7.1.0"
		var import_request2 = require_dist_node5()
		var import_request = require_dist_node5()
		function _buildMessageForResponseErrors(data) {
			return (
				`Request failed due to following response errors:
` + data.errors.map((e) => ` - ${e.message}`).join("\n")
			)
		}
		var GraphqlResponseError = class extends Error {
			constructor(request2, headers, response) {
				super(_buildMessageForResponseErrors(response))
				this.request = request2
				this.headers = headers
				this.response = response
				this.name = "GraphqlResponseError"
				this.errors = response.errors
				this.data = response.data
				if (Error.captureStackTrace) {
					Error.captureStackTrace(this, this.constructor)
				}
			}
		}
		var NON_VARIABLE_OPTIONS = [
			"method",
			"baseUrl",
			"url",
			"headers",
			"request",
			"query",
			"mediaType",
		]
		var FORBIDDEN_VARIABLE_OPTIONS = ["query", "method", "url"]
		var GHES_V3_SUFFIX_REGEX = /\/api\/v3\/?$/
		function graphql(request2, query, options) {
			if (options) {
				if (typeof query === "string" && "query" in options) {
					return Promise.reject(
						new Error(`[@octokit/graphql] "query" cannot be used as variable name`)
					)
				}
				for (const key in options) {
					if (!FORBIDDEN_VARIABLE_OPTIONS.includes(key)) continue
					return Promise.reject(
						new Error(`[@octokit/graphql] "${key}" cannot be used as variable name`)
					)
				}
			}
			const parsedOptions = typeof query === "string" ? Object.assign({ query }, options) : query
			const requestOptions = Object.keys(parsedOptions).reduce((result, key) => {
				if (NON_VARIABLE_OPTIONS.includes(key)) {
					result[key] = parsedOptions[key]
					return result
				}
				if (!result.variables) {
					result.variables = {}
				}
				result.variables[key] = parsedOptions[key]
				return result
			}, {})
			const baseUrl = parsedOptions.baseUrl || request2.endpoint.DEFAULTS.baseUrl
			if (GHES_V3_SUFFIX_REGEX.test(baseUrl)) {
				requestOptions.url = baseUrl.replace(GHES_V3_SUFFIX_REGEX, "/api/graphql")
			}
			return request2(requestOptions).then((response) => {
				if (response.data.errors) {
					const headers = {}
					for (const key of Object.keys(response.headers)) {
						headers[key] = response.headers[key]
					}
					throw new GraphqlResponseError(requestOptions, headers, response.data)
				}
				return response.data.data
			})
		}
		function withDefaults(request2, newDefaults) {
			const newRequest = request2.defaults(newDefaults)
			const newApi = (query, options) => {
				return graphql(newRequest, query, options)
			}
			return Object.assign(newApi, {
				defaults: withDefaults.bind(null, newRequest),
				endpoint: newRequest.endpoint,
			})
		}
		var graphql2 = withDefaults(import_request3.request, {
			headers: {
				"user-agent": `octokit-graphql.js/${VERSION} ${(0,
				import_universal_user_agent.getUserAgent)()}`,
			},
			method: "POST",
			url: "/graphql",
		})
		function withCustomRequest(customRequest) {
			return withDefaults(customRequest, {
				method: "POST",
				url: "/graphql",
			})
		}
	},
})

// ../../../node_modules/.pnpm/@octokit+auth-token@4.0.0/node_modules/@octokit/auth-token/dist-node/index.js
var require_dist_node7 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+auth-token@4.0.0/node_modules/@octokit/auth-token/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			createTokenAuth: () => createTokenAuth,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var REGEX_IS_INSTALLATION_LEGACY = /^v1\./
		var REGEX_IS_INSTALLATION = /^ghs_/
		var REGEX_IS_USER_TO_SERVER = /^ghu_/
		async function auth(token) {
			const isApp = token.split(/\./).length === 3
			const isInstallation =
				REGEX_IS_INSTALLATION_LEGACY.test(token) || REGEX_IS_INSTALLATION.test(token)
			const isUserToServer = REGEX_IS_USER_TO_SERVER.test(token)
			const tokenType = isApp
				? "app"
				: isInstallation
				? "installation"
				: isUserToServer
				? "user-to-server"
				: "oauth"
			return {
				type: "token",
				token,
				tokenType,
			}
		}
		function withAuthorizationPrefix(token) {
			if (token.split(/\./).length === 3) {
				return `bearer ${token}`
			}
			return `token ${token}`
		}
		async function hook(token, request, route, parameters) {
			const endpoint = request.endpoint.merge(route, parameters)
			endpoint.headers.authorization = withAuthorizationPrefix(token)
			return request(endpoint)
		}
		var createTokenAuth = function createTokenAuth2(token) {
			if (!token) {
				throw new Error("[@octokit/auth-token] No token passed to createTokenAuth")
			}
			if (typeof token !== "string") {
				throw new Error("[@octokit/auth-token] Token passed to createTokenAuth is not a string")
			}
			token = token.replace(/^(token|bearer) +/i, "")
			return Object.assign(auth.bind(null, token), {
				hook: hook.bind(null, token),
			})
		}
	},
})

// ../../../node_modules/.pnpm/@octokit+core@5.2.0/node_modules/@octokit/core/dist-node/index.js
var require_dist_node8 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+core@5.2.0/node_modules/@octokit/core/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			Octokit: () => Octokit2,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var import_universal_user_agent = require_dist_node()
		var import_before_after_hook = require_before_after_hook()
		var import_request = require_dist_node5()
		var import_graphql = require_dist_node6()
		var import_auth_token = require_dist_node7()
		var VERSION = "5.2.0"
		var noop = () => {}
		var consoleWarn = console.warn.bind(console)
		var consoleError = console.error.bind(console)
		var userAgentTrail = `octokit-core.js/${VERSION} ${(0,
		import_universal_user_agent.getUserAgent)()}`
		var Octokit2 = class {
			static {
				this.VERSION = VERSION
			}
			static defaults(defaults) {
				const OctokitWithDefaults = class extends this {
					constructor(...args) {
						const options = args[0] || {}
						if (typeof defaults === "function") {
							super(defaults(options))
							return
						}
						super(
							Object.assign(
								{},
								defaults,
								options,
								options.userAgent && defaults.userAgent
									? {
											userAgent: `${options.userAgent} ${defaults.userAgent}`,
									  }
									: null
							)
						)
					}
				}
				return OctokitWithDefaults
			}
			static {
				this.plugins = []
			}
			/**
			 * Attach a plugin (or many) to your Octokit instance.
			 *
			 * @example
			 * const API = Octokit.plugin(plugin1, plugin2, plugin3, ...)
			 */
			static plugin(...newPlugins) {
				const currentPlugins = this.plugins
				const NewOctokit = class extends this {
					static {
						this.plugins = currentPlugins.concat(
							newPlugins.filter((plugin) => !currentPlugins.includes(plugin))
						)
					}
				}
				return NewOctokit
			}
			constructor(options = {}) {
				const hook = new import_before_after_hook.Collection()
				const requestDefaults = {
					baseUrl: import_request.request.endpoint.DEFAULTS.baseUrl,
					headers: {},
					request: Object.assign({}, options.request, {
						// @ts-ignore internal usage only, no need to type
						hook: hook.bind(null, "request"),
					}),
					mediaType: {
						previews: [],
						format: "",
					},
				}
				requestDefaults.headers["user-agent"] = options.userAgent
					? `${options.userAgent} ${userAgentTrail}`
					: userAgentTrail
				if (options.baseUrl) {
					requestDefaults.baseUrl = options.baseUrl
				}
				if (options.previews) {
					requestDefaults.mediaType.previews = options.previews
				}
				if (options.timeZone) {
					requestDefaults.headers["time-zone"] = options.timeZone
				}
				this.request = import_request.request.defaults(requestDefaults)
				this.graphql = (0, import_graphql.withCustomRequest)(this.request).defaults(requestDefaults)
				this.log = Object.assign(
					{
						debug: noop,
						info: noop,
						warn: consoleWarn,
						error: consoleError,
					},
					options.log
				)
				this.hook = hook
				if (!options.authStrategy) {
					if (!options.auth) {
						this.auth = async () => ({
							type: "unauthenticated",
						})
					} else {
						const auth = (0, import_auth_token.createTokenAuth)(options.auth)
						hook.wrap("request", auth.hook)
						this.auth = auth
					}
				} else {
					const { authStrategy, ...otherOptions } = options
					const auth = authStrategy(
						Object.assign(
							{
								request: this.request,
								log: this.log,
								// we pass the current octokit instance as well as its constructor options
								// to allow for authentication strategies that return a new octokit instance
								// that shares the same internal state as the current one. The original
								// requirement for this was the "event-octokit" authentication strategy
								// of https://github.com/probot/octokit-auth-probot.
								octokit: this,
								octokitOptions: otherOptions,
							},
							options.auth
						)
					)
					hook.wrap("request", auth.hook)
					this.auth = auth
				}
				const classConstructor = this.constructor
				for (let i = 0; i < classConstructor.plugins.length; ++i) {
					Object.assign(this, classConstructor.plugins[i](this, options))
				}
			}
		}
	},
})

// ../../../node_modules/.pnpm/@octokit+plugin-rest-endpoint-methods@10.4.1_@octokit+core@5.2.0/node_modules/@octokit/plugin-rest-endpoint-methods/dist-node/index.js
var require_dist_node9 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+plugin-rest-endpoint-methods@10.4.1_@octokit+core@5.2.0/node_modules/@octokit/plugin-rest-endpoint-methods/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			legacyRestEndpointMethods: () => legacyRestEndpointMethods,
			restEndpointMethods: () => restEndpointMethods,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var VERSION = "10.4.1"
		var Endpoints = {
			actions: {
				addCustomLabelsToSelfHostedRunnerForOrg: [
					"POST /orgs/{org}/actions/runners/{runner_id}/labels",
				],
				addCustomLabelsToSelfHostedRunnerForRepo: [
					"POST /repos/{owner}/{repo}/actions/runners/{runner_id}/labels",
				],
				addSelectedRepoToOrgSecret: [
					"PUT /orgs/{org}/actions/secrets/{secret_name}/repositories/{repository_id}",
				],
				addSelectedRepoToOrgVariable: [
					"PUT /orgs/{org}/actions/variables/{name}/repositories/{repository_id}",
				],
				approveWorkflowRun: ["POST /repos/{owner}/{repo}/actions/runs/{run_id}/approve"],
				cancelWorkflowRun: ["POST /repos/{owner}/{repo}/actions/runs/{run_id}/cancel"],
				createEnvironmentVariable: [
					"POST /repositories/{repository_id}/environments/{environment_name}/variables",
				],
				createOrUpdateEnvironmentSecret: [
					"PUT /repositories/{repository_id}/environments/{environment_name}/secrets/{secret_name}",
				],
				createOrUpdateOrgSecret: ["PUT /orgs/{org}/actions/secrets/{secret_name}"],
				createOrUpdateRepoSecret: ["PUT /repos/{owner}/{repo}/actions/secrets/{secret_name}"],
				createOrgVariable: ["POST /orgs/{org}/actions/variables"],
				createRegistrationTokenForOrg: ["POST /orgs/{org}/actions/runners/registration-token"],
				createRegistrationTokenForRepo: [
					"POST /repos/{owner}/{repo}/actions/runners/registration-token",
				],
				createRemoveTokenForOrg: ["POST /orgs/{org}/actions/runners/remove-token"],
				createRemoveTokenForRepo: ["POST /repos/{owner}/{repo}/actions/runners/remove-token"],
				createRepoVariable: ["POST /repos/{owner}/{repo}/actions/variables"],
				createWorkflowDispatch: [
					"POST /repos/{owner}/{repo}/actions/workflows/{workflow_id}/dispatches",
				],
				deleteActionsCacheById: ["DELETE /repos/{owner}/{repo}/actions/caches/{cache_id}"],
				deleteActionsCacheByKey: ["DELETE /repos/{owner}/{repo}/actions/caches{?key,ref}"],
				deleteArtifact: ["DELETE /repos/{owner}/{repo}/actions/artifacts/{artifact_id}"],
				deleteEnvironmentSecret: [
					"DELETE /repositories/{repository_id}/environments/{environment_name}/secrets/{secret_name}",
				],
				deleteEnvironmentVariable: [
					"DELETE /repositories/{repository_id}/environments/{environment_name}/variables/{name}",
				],
				deleteOrgSecret: ["DELETE /orgs/{org}/actions/secrets/{secret_name}"],
				deleteOrgVariable: ["DELETE /orgs/{org}/actions/variables/{name}"],
				deleteRepoSecret: ["DELETE /repos/{owner}/{repo}/actions/secrets/{secret_name}"],
				deleteRepoVariable: ["DELETE /repos/{owner}/{repo}/actions/variables/{name}"],
				deleteSelfHostedRunnerFromOrg: ["DELETE /orgs/{org}/actions/runners/{runner_id}"],
				deleteSelfHostedRunnerFromRepo: [
					"DELETE /repos/{owner}/{repo}/actions/runners/{runner_id}",
				],
				deleteWorkflowRun: ["DELETE /repos/{owner}/{repo}/actions/runs/{run_id}"],
				deleteWorkflowRunLogs: ["DELETE /repos/{owner}/{repo}/actions/runs/{run_id}/logs"],
				disableSelectedRepositoryGithubActionsOrganization: [
					"DELETE /orgs/{org}/actions/permissions/repositories/{repository_id}",
				],
				disableWorkflow: ["PUT /repos/{owner}/{repo}/actions/workflows/{workflow_id}/disable"],
				downloadArtifact: [
					"GET /repos/{owner}/{repo}/actions/artifacts/{artifact_id}/{archive_format}",
				],
				downloadJobLogsForWorkflowRun: ["GET /repos/{owner}/{repo}/actions/jobs/{job_id}/logs"],
				downloadWorkflowRunAttemptLogs: [
					"GET /repos/{owner}/{repo}/actions/runs/{run_id}/attempts/{attempt_number}/logs",
				],
				downloadWorkflowRunLogs: ["GET /repos/{owner}/{repo}/actions/runs/{run_id}/logs"],
				enableSelectedRepositoryGithubActionsOrganization: [
					"PUT /orgs/{org}/actions/permissions/repositories/{repository_id}",
				],
				enableWorkflow: ["PUT /repos/{owner}/{repo}/actions/workflows/{workflow_id}/enable"],
				forceCancelWorkflowRun: ["POST /repos/{owner}/{repo}/actions/runs/{run_id}/force-cancel"],
				generateRunnerJitconfigForOrg: ["POST /orgs/{org}/actions/runners/generate-jitconfig"],
				generateRunnerJitconfigForRepo: [
					"POST /repos/{owner}/{repo}/actions/runners/generate-jitconfig",
				],
				getActionsCacheList: ["GET /repos/{owner}/{repo}/actions/caches"],
				getActionsCacheUsage: ["GET /repos/{owner}/{repo}/actions/cache/usage"],
				getActionsCacheUsageByRepoForOrg: ["GET /orgs/{org}/actions/cache/usage-by-repository"],
				getActionsCacheUsageForOrg: ["GET /orgs/{org}/actions/cache/usage"],
				getAllowedActionsOrganization: ["GET /orgs/{org}/actions/permissions/selected-actions"],
				getAllowedActionsRepository: [
					"GET /repos/{owner}/{repo}/actions/permissions/selected-actions",
				],
				getArtifact: ["GET /repos/{owner}/{repo}/actions/artifacts/{artifact_id}"],
				getCustomOidcSubClaimForRepo: ["GET /repos/{owner}/{repo}/actions/oidc/customization/sub"],
				getEnvironmentPublicKey: [
					"GET /repositories/{repository_id}/environments/{environment_name}/secrets/public-key",
				],
				getEnvironmentSecret: [
					"GET /repositories/{repository_id}/environments/{environment_name}/secrets/{secret_name}",
				],
				getEnvironmentVariable: [
					"GET /repositories/{repository_id}/environments/{environment_name}/variables/{name}",
				],
				getGithubActionsDefaultWorkflowPermissionsOrganization: [
					"GET /orgs/{org}/actions/permissions/workflow",
				],
				getGithubActionsDefaultWorkflowPermissionsRepository: [
					"GET /repos/{owner}/{repo}/actions/permissions/workflow",
				],
				getGithubActionsPermissionsOrganization: ["GET /orgs/{org}/actions/permissions"],
				getGithubActionsPermissionsRepository: ["GET /repos/{owner}/{repo}/actions/permissions"],
				getJobForWorkflowRun: ["GET /repos/{owner}/{repo}/actions/jobs/{job_id}"],
				getOrgPublicKey: ["GET /orgs/{org}/actions/secrets/public-key"],
				getOrgSecret: ["GET /orgs/{org}/actions/secrets/{secret_name}"],
				getOrgVariable: ["GET /orgs/{org}/actions/variables/{name}"],
				getPendingDeploymentsForRun: [
					"GET /repos/{owner}/{repo}/actions/runs/{run_id}/pending_deployments",
				],
				getRepoPermissions: [
					"GET /repos/{owner}/{repo}/actions/permissions",
					{},
					{ renamed: ["actions", "getGithubActionsPermissionsRepository"] },
				],
				getRepoPublicKey: ["GET /repos/{owner}/{repo}/actions/secrets/public-key"],
				getRepoSecret: ["GET /repos/{owner}/{repo}/actions/secrets/{secret_name}"],
				getRepoVariable: ["GET /repos/{owner}/{repo}/actions/variables/{name}"],
				getReviewsForRun: ["GET /repos/{owner}/{repo}/actions/runs/{run_id}/approvals"],
				getSelfHostedRunnerForOrg: ["GET /orgs/{org}/actions/runners/{runner_id}"],
				getSelfHostedRunnerForRepo: ["GET /repos/{owner}/{repo}/actions/runners/{runner_id}"],
				getWorkflow: ["GET /repos/{owner}/{repo}/actions/workflows/{workflow_id}"],
				getWorkflowAccessToRepository: ["GET /repos/{owner}/{repo}/actions/permissions/access"],
				getWorkflowRun: ["GET /repos/{owner}/{repo}/actions/runs/{run_id}"],
				getWorkflowRunAttempt: [
					"GET /repos/{owner}/{repo}/actions/runs/{run_id}/attempts/{attempt_number}",
				],
				getWorkflowRunUsage: ["GET /repos/{owner}/{repo}/actions/runs/{run_id}/timing"],
				getWorkflowUsage: ["GET /repos/{owner}/{repo}/actions/workflows/{workflow_id}/timing"],
				listArtifactsForRepo: ["GET /repos/{owner}/{repo}/actions/artifacts"],
				listEnvironmentSecrets: [
					"GET /repositories/{repository_id}/environments/{environment_name}/secrets",
				],
				listEnvironmentVariables: [
					"GET /repositories/{repository_id}/environments/{environment_name}/variables",
				],
				listJobsForWorkflowRun: ["GET /repos/{owner}/{repo}/actions/runs/{run_id}/jobs"],
				listJobsForWorkflowRunAttempt: [
					"GET /repos/{owner}/{repo}/actions/runs/{run_id}/attempts/{attempt_number}/jobs",
				],
				listLabelsForSelfHostedRunnerForOrg: ["GET /orgs/{org}/actions/runners/{runner_id}/labels"],
				listLabelsForSelfHostedRunnerForRepo: [
					"GET /repos/{owner}/{repo}/actions/runners/{runner_id}/labels",
				],
				listOrgSecrets: ["GET /orgs/{org}/actions/secrets"],
				listOrgVariables: ["GET /orgs/{org}/actions/variables"],
				listRepoOrganizationSecrets: ["GET /repos/{owner}/{repo}/actions/organization-secrets"],
				listRepoOrganizationVariables: ["GET /repos/{owner}/{repo}/actions/organization-variables"],
				listRepoSecrets: ["GET /repos/{owner}/{repo}/actions/secrets"],
				listRepoVariables: ["GET /repos/{owner}/{repo}/actions/variables"],
				listRepoWorkflows: ["GET /repos/{owner}/{repo}/actions/workflows"],
				listRunnerApplicationsForOrg: ["GET /orgs/{org}/actions/runners/downloads"],
				listRunnerApplicationsForRepo: ["GET /repos/{owner}/{repo}/actions/runners/downloads"],
				listSelectedReposForOrgSecret: [
					"GET /orgs/{org}/actions/secrets/{secret_name}/repositories",
				],
				listSelectedReposForOrgVariable: ["GET /orgs/{org}/actions/variables/{name}/repositories"],
				listSelectedRepositoriesEnabledGithubActionsOrganization: [
					"GET /orgs/{org}/actions/permissions/repositories",
				],
				listSelfHostedRunnersForOrg: ["GET /orgs/{org}/actions/runners"],
				listSelfHostedRunnersForRepo: ["GET /repos/{owner}/{repo}/actions/runners"],
				listWorkflowRunArtifacts: ["GET /repos/{owner}/{repo}/actions/runs/{run_id}/artifacts"],
				listWorkflowRuns: ["GET /repos/{owner}/{repo}/actions/workflows/{workflow_id}/runs"],
				listWorkflowRunsForRepo: ["GET /repos/{owner}/{repo}/actions/runs"],
				reRunJobForWorkflowRun: ["POST /repos/{owner}/{repo}/actions/jobs/{job_id}/rerun"],
				reRunWorkflow: ["POST /repos/{owner}/{repo}/actions/runs/{run_id}/rerun"],
				reRunWorkflowFailedJobs: [
					"POST /repos/{owner}/{repo}/actions/runs/{run_id}/rerun-failed-jobs",
				],
				removeAllCustomLabelsFromSelfHostedRunnerForOrg: [
					"DELETE /orgs/{org}/actions/runners/{runner_id}/labels",
				],
				removeAllCustomLabelsFromSelfHostedRunnerForRepo: [
					"DELETE /repos/{owner}/{repo}/actions/runners/{runner_id}/labels",
				],
				removeCustomLabelFromSelfHostedRunnerForOrg: [
					"DELETE /orgs/{org}/actions/runners/{runner_id}/labels/{name}",
				],
				removeCustomLabelFromSelfHostedRunnerForRepo: [
					"DELETE /repos/{owner}/{repo}/actions/runners/{runner_id}/labels/{name}",
				],
				removeSelectedRepoFromOrgSecret: [
					"DELETE /orgs/{org}/actions/secrets/{secret_name}/repositories/{repository_id}",
				],
				removeSelectedRepoFromOrgVariable: [
					"DELETE /orgs/{org}/actions/variables/{name}/repositories/{repository_id}",
				],
				reviewCustomGatesForRun: [
					"POST /repos/{owner}/{repo}/actions/runs/{run_id}/deployment_protection_rule",
				],
				reviewPendingDeploymentsForRun: [
					"POST /repos/{owner}/{repo}/actions/runs/{run_id}/pending_deployments",
				],
				setAllowedActionsOrganization: ["PUT /orgs/{org}/actions/permissions/selected-actions"],
				setAllowedActionsRepository: [
					"PUT /repos/{owner}/{repo}/actions/permissions/selected-actions",
				],
				setCustomLabelsForSelfHostedRunnerForOrg: [
					"PUT /orgs/{org}/actions/runners/{runner_id}/labels",
				],
				setCustomLabelsForSelfHostedRunnerForRepo: [
					"PUT /repos/{owner}/{repo}/actions/runners/{runner_id}/labels",
				],
				setCustomOidcSubClaimForRepo: ["PUT /repos/{owner}/{repo}/actions/oidc/customization/sub"],
				setGithubActionsDefaultWorkflowPermissionsOrganization: [
					"PUT /orgs/{org}/actions/permissions/workflow",
				],
				setGithubActionsDefaultWorkflowPermissionsRepository: [
					"PUT /repos/{owner}/{repo}/actions/permissions/workflow",
				],
				setGithubActionsPermissionsOrganization: ["PUT /orgs/{org}/actions/permissions"],
				setGithubActionsPermissionsRepository: ["PUT /repos/{owner}/{repo}/actions/permissions"],
				setSelectedReposForOrgSecret: [
					"PUT /orgs/{org}/actions/secrets/{secret_name}/repositories",
				],
				setSelectedReposForOrgVariable: ["PUT /orgs/{org}/actions/variables/{name}/repositories"],
				setSelectedRepositoriesEnabledGithubActionsOrganization: [
					"PUT /orgs/{org}/actions/permissions/repositories",
				],
				setWorkflowAccessToRepository: ["PUT /repos/{owner}/{repo}/actions/permissions/access"],
				updateEnvironmentVariable: [
					"PATCH /repositories/{repository_id}/environments/{environment_name}/variables/{name}",
				],
				updateOrgVariable: ["PATCH /orgs/{org}/actions/variables/{name}"],
				updateRepoVariable: ["PATCH /repos/{owner}/{repo}/actions/variables/{name}"],
			},
			activity: {
				checkRepoIsStarredByAuthenticatedUser: ["GET /user/starred/{owner}/{repo}"],
				deleteRepoSubscription: ["DELETE /repos/{owner}/{repo}/subscription"],
				deleteThreadSubscription: ["DELETE /notifications/threads/{thread_id}/subscription"],
				getFeeds: ["GET /feeds"],
				getRepoSubscription: ["GET /repos/{owner}/{repo}/subscription"],
				getThread: ["GET /notifications/threads/{thread_id}"],
				getThreadSubscriptionForAuthenticatedUser: [
					"GET /notifications/threads/{thread_id}/subscription",
				],
				listEventsForAuthenticatedUser: ["GET /users/{username}/events"],
				listNotificationsForAuthenticatedUser: ["GET /notifications"],
				listOrgEventsForAuthenticatedUser: ["GET /users/{username}/events/orgs/{org}"],
				listPublicEvents: ["GET /events"],
				listPublicEventsForRepoNetwork: ["GET /networks/{owner}/{repo}/events"],
				listPublicEventsForUser: ["GET /users/{username}/events/public"],
				listPublicOrgEvents: ["GET /orgs/{org}/events"],
				listReceivedEventsForUser: ["GET /users/{username}/received_events"],
				listReceivedPublicEventsForUser: ["GET /users/{username}/received_events/public"],
				listRepoEvents: ["GET /repos/{owner}/{repo}/events"],
				listRepoNotificationsForAuthenticatedUser: ["GET /repos/{owner}/{repo}/notifications"],
				listReposStarredByAuthenticatedUser: ["GET /user/starred"],
				listReposStarredByUser: ["GET /users/{username}/starred"],
				listReposWatchedByUser: ["GET /users/{username}/subscriptions"],
				listStargazersForRepo: ["GET /repos/{owner}/{repo}/stargazers"],
				listWatchedReposForAuthenticatedUser: ["GET /user/subscriptions"],
				listWatchersForRepo: ["GET /repos/{owner}/{repo}/subscribers"],
				markNotificationsAsRead: ["PUT /notifications"],
				markRepoNotificationsAsRead: ["PUT /repos/{owner}/{repo}/notifications"],
				markThreadAsDone: ["DELETE /notifications/threads/{thread_id}"],
				markThreadAsRead: ["PATCH /notifications/threads/{thread_id}"],
				setRepoSubscription: ["PUT /repos/{owner}/{repo}/subscription"],
				setThreadSubscription: ["PUT /notifications/threads/{thread_id}/subscription"],
				starRepoForAuthenticatedUser: ["PUT /user/starred/{owner}/{repo}"],
				unstarRepoForAuthenticatedUser: ["DELETE /user/starred/{owner}/{repo}"],
			},
			apps: {
				addRepoToInstallation: [
					"PUT /user/installations/{installation_id}/repositories/{repository_id}",
					{},
					{ renamed: ["apps", "addRepoToInstallationForAuthenticatedUser"] },
				],
				addRepoToInstallationForAuthenticatedUser: [
					"PUT /user/installations/{installation_id}/repositories/{repository_id}",
				],
				checkToken: ["POST /applications/{client_id}/token"],
				createFromManifest: ["POST /app-manifests/{code}/conversions"],
				createInstallationAccessToken: ["POST /app/installations/{installation_id}/access_tokens"],
				deleteAuthorization: ["DELETE /applications/{client_id}/grant"],
				deleteInstallation: ["DELETE /app/installations/{installation_id}"],
				deleteToken: ["DELETE /applications/{client_id}/token"],
				getAuthenticated: ["GET /app"],
				getBySlug: ["GET /apps/{app_slug}"],
				getInstallation: ["GET /app/installations/{installation_id}"],
				getOrgInstallation: ["GET /orgs/{org}/installation"],
				getRepoInstallation: ["GET /repos/{owner}/{repo}/installation"],
				getSubscriptionPlanForAccount: ["GET /marketplace_listing/accounts/{account_id}"],
				getSubscriptionPlanForAccountStubbed: [
					"GET /marketplace_listing/stubbed/accounts/{account_id}",
				],
				getUserInstallation: ["GET /users/{username}/installation"],
				getWebhookConfigForApp: ["GET /app/hook/config"],
				getWebhookDelivery: ["GET /app/hook/deliveries/{delivery_id}"],
				listAccountsForPlan: ["GET /marketplace_listing/plans/{plan_id}/accounts"],
				listAccountsForPlanStubbed: ["GET /marketplace_listing/stubbed/plans/{plan_id}/accounts"],
				listInstallationReposForAuthenticatedUser: [
					"GET /user/installations/{installation_id}/repositories",
				],
				listInstallationRequestsForAuthenticatedApp: ["GET /app/installation-requests"],
				listInstallations: ["GET /app/installations"],
				listInstallationsForAuthenticatedUser: ["GET /user/installations"],
				listPlans: ["GET /marketplace_listing/plans"],
				listPlansStubbed: ["GET /marketplace_listing/stubbed/plans"],
				listReposAccessibleToInstallation: ["GET /installation/repositories"],
				listSubscriptionsForAuthenticatedUser: ["GET /user/marketplace_purchases"],
				listSubscriptionsForAuthenticatedUserStubbed: ["GET /user/marketplace_purchases/stubbed"],
				listWebhookDeliveries: ["GET /app/hook/deliveries"],
				redeliverWebhookDelivery: ["POST /app/hook/deliveries/{delivery_id}/attempts"],
				removeRepoFromInstallation: [
					"DELETE /user/installations/{installation_id}/repositories/{repository_id}",
					{},
					{ renamed: ["apps", "removeRepoFromInstallationForAuthenticatedUser"] },
				],
				removeRepoFromInstallationForAuthenticatedUser: [
					"DELETE /user/installations/{installation_id}/repositories/{repository_id}",
				],
				resetToken: ["PATCH /applications/{client_id}/token"],
				revokeInstallationAccessToken: ["DELETE /installation/token"],
				scopeToken: ["POST /applications/{client_id}/token/scoped"],
				suspendInstallation: ["PUT /app/installations/{installation_id}/suspended"],
				unsuspendInstallation: ["DELETE /app/installations/{installation_id}/suspended"],
				updateWebhookConfigForApp: ["PATCH /app/hook/config"],
			},
			billing: {
				getGithubActionsBillingOrg: ["GET /orgs/{org}/settings/billing/actions"],
				getGithubActionsBillingUser: ["GET /users/{username}/settings/billing/actions"],
				getGithubPackagesBillingOrg: ["GET /orgs/{org}/settings/billing/packages"],
				getGithubPackagesBillingUser: ["GET /users/{username}/settings/billing/packages"],
				getSharedStorageBillingOrg: ["GET /orgs/{org}/settings/billing/shared-storage"],
				getSharedStorageBillingUser: ["GET /users/{username}/settings/billing/shared-storage"],
			},
			checks: {
				create: ["POST /repos/{owner}/{repo}/check-runs"],
				createSuite: ["POST /repos/{owner}/{repo}/check-suites"],
				get: ["GET /repos/{owner}/{repo}/check-runs/{check_run_id}"],
				getSuite: ["GET /repos/{owner}/{repo}/check-suites/{check_suite_id}"],
				listAnnotations: ["GET /repos/{owner}/{repo}/check-runs/{check_run_id}/annotations"],
				listForRef: ["GET /repos/{owner}/{repo}/commits/{ref}/check-runs"],
				listForSuite: ["GET /repos/{owner}/{repo}/check-suites/{check_suite_id}/check-runs"],
				listSuitesForRef: ["GET /repos/{owner}/{repo}/commits/{ref}/check-suites"],
				rerequestRun: ["POST /repos/{owner}/{repo}/check-runs/{check_run_id}/rerequest"],
				rerequestSuite: ["POST /repos/{owner}/{repo}/check-suites/{check_suite_id}/rerequest"],
				setSuitesPreferences: ["PATCH /repos/{owner}/{repo}/check-suites/preferences"],
				update: ["PATCH /repos/{owner}/{repo}/check-runs/{check_run_id}"],
			},
			codeScanning: {
				deleteAnalysis: [
					"DELETE /repos/{owner}/{repo}/code-scanning/analyses/{analysis_id}{?confirm_delete}",
				],
				getAlert: [
					"GET /repos/{owner}/{repo}/code-scanning/alerts/{alert_number}",
					{},
					{ renamedParameters: { alert_id: "alert_number" } },
				],
				getAnalysis: ["GET /repos/{owner}/{repo}/code-scanning/analyses/{analysis_id}"],
				getCodeqlDatabase: ["GET /repos/{owner}/{repo}/code-scanning/codeql/databases/{language}"],
				getDefaultSetup: ["GET /repos/{owner}/{repo}/code-scanning/default-setup"],
				getSarif: ["GET /repos/{owner}/{repo}/code-scanning/sarifs/{sarif_id}"],
				listAlertInstances: [
					"GET /repos/{owner}/{repo}/code-scanning/alerts/{alert_number}/instances",
				],
				listAlertsForOrg: ["GET /orgs/{org}/code-scanning/alerts"],
				listAlertsForRepo: ["GET /repos/{owner}/{repo}/code-scanning/alerts"],
				listAlertsInstances: [
					"GET /repos/{owner}/{repo}/code-scanning/alerts/{alert_number}/instances",
					{},
					{ renamed: ["codeScanning", "listAlertInstances"] },
				],
				listCodeqlDatabases: ["GET /repos/{owner}/{repo}/code-scanning/codeql/databases"],
				listRecentAnalyses: ["GET /repos/{owner}/{repo}/code-scanning/analyses"],
				updateAlert: ["PATCH /repos/{owner}/{repo}/code-scanning/alerts/{alert_number}"],
				updateDefaultSetup: ["PATCH /repos/{owner}/{repo}/code-scanning/default-setup"],
				uploadSarif: ["POST /repos/{owner}/{repo}/code-scanning/sarifs"],
			},
			codesOfConduct: {
				getAllCodesOfConduct: ["GET /codes_of_conduct"],
				getConductCode: ["GET /codes_of_conduct/{key}"],
			},
			codespaces: {
				addRepositoryForSecretForAuthenticatedUser: [
					"PUT /user/codespaces/secrets/{secret_name}/repositories/{repository_id}",
				],
				addSelectedRepoToOrgSecret: [
					"PUT /orgs/{org}/codespaces/secrets/{secret_name}/repositories/{repository_id}",
				],
				checkPermissionsForDevcontainer: ["GET /repos/{owner}/{repo}/codespaces/permissions_check"],
				codespaceMachinesForAuthenticatedUser: ["GET /user/codespaces/{codespace_name}/machines"],
				createForAuthenticatedUser: ["POST /user/codespaces"],
				createOrUpdateOrgSecret: ["PUT /orgs/{org}/codespaces/secrets/{secret_name}"],
				createOrUpdateRepoSecret: ["PUT /repos/{owner}/{repo}/codespaces/secrets/{secret_name}"],
				createOrUpdateSecretForAuthenticatedUser: ["PUT /user/codespaces/secrets/{secret_name}"],
				createWithPrForAuthenticatedUser: [
					"POST /repos/{owner}/{repo}/pulls/{pull_number}/codespaces",
				],
				createWithRepoForAuthenticatedUser: ["POST /repos/{owner}/{repo}/codespaces"],
				deleteForAuthenticatedUser: ["DELETE /user/codespaces/{codespace_name}"],
				deleteFromOrganization: [
					"DELETE /orgs/{org}/members/{username}/codespaces/{codespace_name}",
				],
				deleteOrgSecret: ["DELETE /orgs/{org}/codespaces/secrets/{secret_name}"],
				deleteRepoSecret: ["DELETE /repos/{owner}/{repo}/codespaces/secrets/{secret_name}"],
				deleteSecretForAuthenticatedUser: ["DELETE /user/codespaces/secrets/{secret_name}"],
				exportForAuthenticatedUser: ["POST /user/codespaces/{codespace_name}/exports"],
				getCodespacesForUserInOrg: ["GET /orgs/{org}/members/{username}/codespaces"],
				getExportDetailsForAuthenticatedUser: [
					"GET /user/codespaces/{codespace_name}/exports/{export_id}",
				],
				getForAuthenticatedUser: ["GET /user/codespaces/{codespace_name}"],
				getOrgPublicKey: ["GET /orgs/{org}/codespaces/secrets/public-key"],
				getOrgSecret: ["GET /orgs/{org}/codespaces/secrets/{secret_name}"],
				getPublicKeyForAuthenticatedUser: ["GET /user/codespaces/secrets/public-key"],
				getRepoPublicKey: ["GET /repos/{owner}/{repo}/codespaces/secrets/public-key"],
				getRepoSecret: ["GET /repos/{owner}/{repo}/codespaces/secrets/{secret_name}"],
				getSecretForAuthenticatedUser: ["GET /user/codespaces/secrets/{secret_name}"],
				listDevcontainersInRepositoryForAuthenticatedUser: [
					"GET /repos/{owner}/{repo}/codespaces/devcontainers",
				],
				listForAuthenticatedUser: ["GET /user/codespaces"],
				listInOrganization: [
					"GET /orgs/{org}/codespaces",
					{},
					{ renamedParameters: { org_id: "org" } },
				],
				listInRepositoryForAuthenticatedUser: ["GET /repos/{owner}/{repo}/codespaces"],
				listOrgSecrets: ["GET /orgs/{org}/codespaces/secrets"],
				listRepoSecrets: ["GET /repos/{owner}/{repo}/codespaces/secrets"],
				listRepositoriesForSecretForAuthenticatedUser: [
					"GET /user/codespaces/secrets/{secret_name}/repositories",
				],
				listSecretsForAuthenticatedUser: ["GET /user/codespaces/secrets"],
				listSelectedReposForOrgSecret: [
					"GET /orgs/{org}/codespaces/secrets/{secret_name}/repositories",
				],
				preFlightWithRepoForAuthenticatedUser: ["GET /repos/{owner}/{repo}/codespaces/new"],
				publishForAuthenticatedUser: ["POST /user/codespaces/{codespace_name}/publish"],
				removeRepositoryForSecretForAuthenticatedUser: [
					"DELETE /user/codespaces/secrets/{secret_name}/repositories/{repository_id}",
				],
				removeSelectedRepoFromOrgSecret: [
					"DELETE /orgs/{org}/codespaces/secrets/{secret_name}/repositories/{repository_id}",
				],
				repoMachinesForAuthenticatedUser: ["GET /repos/{owner}/{repo}/codespaces/machines"],
				setRepositoriesForSecretForAuthenticatedUser: [
					"PUT /user/codespaces/secrets/{secret_name}/repositories",
				],
				setSelectedReposForOrgSecret: [
					"PUT /orgs/{org}/codespaces/secrets/{secret_name}/repositories",
				],
				startForAuthenticatedUser: ["POST /user/codespaces/{codespace_name}/start"],
				stopForAuthenticatedUser: ["POST /user/codespaces/{codespace_name}/stop"],
				stopInOrganization: [
					"POST /orgs/{org}/members/{username}/codespaces/{codespace_name}/stop",
				],
				updateForAuthenticatedUser: ["PATCH /user/codespaces/{codespace_name}"],
			},
			copilot: {
				addCopilotSeatsForTeams: ["POST /orgs/{org}/copilot/billing/selected_teams"],
				addCopilotSeatsForUsers: ["POST /orgs/{org}/copilot/billing/selected_users"],
				cancelCopilotSeatAssignmentForTeams: ["DELETE /orgs/{org}/copilot/billing/selected_teams"],
				cancelCopilotSeatAssignmentForUsers: ["DELETE /orgs/{org}/copilot/billing/selected_users"],
				getCopilotOrganizationDetails: ["GET /orgs/{org}/copilot/billing"],
				getCopilotSeatDetailsForUser: ["GET /orgs/{org}/members/{username}/copilot"],
				listCopilotSeats: ["GET /orgs/{org}/copilot/billing/seats"],
			},
			dependabot: {
				addSelectedRepoToOrgSecret: [
					"PUT /orgs/{org}/dependabot/secrets/{secret_name}/repositories/{repository_id}",
				],
				createOrUpdateOrgSecret: ["PUT /orgs/{org}/dependabot/secrets/{secret_name}"],
				createOrUpdateRepoSecret: ["PUT /repos/{owner}/{repo}/dependabot/secrets/{secret_name}"],
				deleteOrgSecret: ["DELETE /orgs/{org}/dependabot/secrets/{secret_name}"],
				deleteRepoSecret: ["DELETE /repos/{owner}/{repo}/dependabot/secrets/{secret_name}"],
				getAlert: ["GET /repos/{owner}/{repo}/dependabot/alerts/{alert_number}"],
				getOrgPublicKey: ["GET /orgs/{org}/dependabot/secrets/public-key"],
				getOrgSecret: ["GET /orgs/{org}/dependabot/secrets/{secret_name}"],
				getRepoPublicKey: ["GET /repos/{owner}/{repo}/dependabot/secrets/public-key"],
				getRepoSecret: ["GET /repos/{owner}/{repo}/dependabot/secrets/{secret_name}"],
				listAlertsForEnterprise: ["GET /enterprises/{enterprise}/dependabot/alerts"],
				listAlertsForOrg: ["GET /orgs/{org}/dependabot/alerts"],
				listAlertsForRepo: ["GET /repos/{owner}/{repo}/dependabot/alerts"],
				listOrgSecrets: ["GET /orgs/{org}/dependabot/secrets"],
				listRepoSecrets: ["GET /repos/{owner}/{repo}/dependabot/secrets"],
				listSelectedReposForOrgSecret: [
					"GET /orgs/{org}/dependabot/secrets/{secret_name}/repositories",
				],
				removeSelectedRepoFromOrgSecret: [
					"DELETE /orgs/{org}/dependabot/secrets/{secret_name}/repositories/{repository_id}",
				],
				setSelectedReposForOrgSecret: [
					"PUT /orgs/{org}/dependabot/secrets/{secret_name}/repositories",
				],
				updateAlert: ["PATCH /repos/{owner}/{repo}/dependabot/alerts/{alert_number}"],
			},
			dependencyGraph: {
				createRepositorySnapshot: ["POST /repos/{owner}/{repo}/dependency-graph/snapshots"],
				diffRange: ["GET /repos/{owner}/{repo}/dependency-graph/compare/{basehead}"],
				exportSbom: ["GET /repos/{owner}/{repo}/dependency-graph/sbom"],
			},
			emojis: { get: ["GET /emojis"] },
			gists: {
				checkIsStarred: ["GET /gists/{gist_id}/star"],
				create: ["POST /gists"],
				createComment: ["POST /gists/{gist_id}/comments"],
				delete: ["DELETE /gists/{gist_id}"],
				deleteComment: ["DELETE /gists/{gist_id}/comments/{comment_id}"],
				fork: ["POST /gists/{gist_id}/forks"],
				get: ["GET /gists/{gist_id}"],
				getComment: ["GET /gists/{gist_id}/comments/{comment_id}"],
				getRevision: ["GET /gists/{gist_id}/{sha}"],
				list: ["GET /gists"],
				listComments: ["GET /gists/{gist_id}/comments"],
				listCommits: ["GET /gists/{gist_id}/commits"],
				listForUser: ["GET /users/{username}/gists"],
				listForks: ["GET /gists/{gist_id}/forks"],
				listPublic: ["GET /gists/public"],
				listStarred: ["GET /gists/starred"],
				star: ["PUT /gists/{gist_id}/star"],
				unstar: ["DELETE /gists/{gist_id}/star"],
				update: ["PATCH /gists/{gist_id}"],
				updateComment: ["PATCH /gists/{gist_id}/comments/{comment_id}"],
			},
			git: {
				createBlob: ["POST /repos/{owner}/{repo}/git/blobs"],
				createCommit: ["POST /repos/{owner}/{repo}/git/commits"],
				createRef: ["POST /repos/{owner}/{repo}/git/refs"],
				createTag: ["POST /repos/{owner}/{repo}/git/tags"],
				createTree: ["POST /repos/{owner}/{repo}/git/trees"],
				deleteRef: ["DELETE /repos/{owner}/{repo}/git/refs/{ref}"],
				getBlob: ["GET /repos/{owner}/{repo}/git/blobs/{file_sha}"],
				getCommit: ["GET /repos/{owner}/{repo}/git/commits/{commit_sha}"],
				getRef: ["GET /repos/{owner}/{repo}/git/ref/{ref}"],
				getTag: ["GET /repos/{owner}/{repo}/git/tags/{tag_sha}"],
				getTree: ["GET /repos/{owner}/{repo}/git/trees/{tree_sha}"],
				listMatchingRefs: ["GET /repos/{owner}/{repo}/git/matching-refs/{ref}"],
				updateRef: ["PATCH /repos/{owner}/{repo}/git/refs/{ref}"],
			},
			gitignore: {
				getAllTemplates: ["GET /gitignore/templates"],
				getTemplate: ["GET /gitignore/templates/{name}"],
			},
			interactions: {
				getRestrictionsForAuthenticatedUser: ["GET /user/interaction-limits"],
				getRestrictionsForOrg: ["GET /orgs/{org}/interaction-limits"],
				getRestrictionsForRepo: ["GET /repos/{owner}/{repo}/interaction-limits"],
				getRestrictionsForYourPublicRepos: [
					"GET /user/interaction-limits",
					{},
					{ renamed: ["interactions", "getRestrictionsForAuthenticatedUser"] },
				],
				removeRestrictionsForAuthenticatedUser: ["DELETE /user/interaction-limits"],
				removeRestrictionsForOrg: ["DELETE /orgs/{org}/interaction-limits"],
				removeRestrictionsForRepo: ["DELETE /repos/{owner}/{repo}/interaction-limits"],
				removeRestrictionsForYourPublicRepos: [
					"DELETE /user/interaction-limits",
					{},
					{ renamed: ["interactions", "removeRestrictionsForAuthenticatedUser"] },
				],
				setRestrictionsForAuthenticatedUser: ["PUT /user/interaction-limits"],
				setRestrictionsForOrg: ["PUT /orgs/{org}/interaction-limits"],
				setRestrictionsForRepo: ["PUT /repos/{owner}/{repo}/interaction-limits"],
				setRestrictionsForYourPublicRepos: [
					"PUT /user/interaction-limits",
					{},
					{ renamed: ["interactions", "setRestrictionsForAuthenticatedUser"] },
				],
			},
			issues: {
				addAssignees: ["POST /repos/{owner}/{repo}/issues/{issue_number}/assignees"],
				addLabels: ["POST /repos/{owner}/{repo}/issues/{issue_number}/labels"],
				checkUserCanBeAssigned: ["GET /repos/{owner}/{repo}/assignees/{assignee}"],
				checkUserCanBeAssignedToIssue: [
					"GET /repos/{owner}/{repo}/issues/{issue_number}/assignees/{assignee}",
				],
				create: ["POST /repos/{owner}/{repo}/issues"],
				createComment: ["POST /repos/{owner}/{repo}/issues/{issue_number}/comments"],
				createLabel: ["POST /repos/{owner}/{repo}/labels"],
				createMilestone: ["POST /repos/{owner}/{repo}/milestones"],
				deleteComment: ["DELETE /repos/{owner}/{repo}/issues/comments/{comment_id}"],
				deleteLabel: ["DELETE /repos/{owner}/{repo}/labels/{name}"],
				deleteMilestone: ["DELETE /repos/{owner}/{repo}/milestones/{milestone_number}"],
				get: ["GET /repos/{owner}/{repo}/issues/{issue_number}"],
				getComment: ["GET /repos/{owner}/{repo}/issues/comments/{comment_id}"],
				getEvent: ["GET /repos/{owner}/{repo}/issues/events/{event_id}"],
				getLabel: ["GET /repos/{owner}/{repo}/labels/{name}"],
				getMilestone: ["GET /repos/{owner}/{repo}/milestones/{milestone_number}"],
				list: ["GET /issues"],
				listAssignees: ["GET /repos/{owner}/{repo}/assignees"],
				listComments: ["GET /repos/{owner}/{repo}/issues/{issue_number}/comments"],
				listCommentsForRepo: ["GET /repos/{owner}/{repo}/issues/comments"],
				listEvents: ["GET /repos/{owner}/{repo}/issues/{issue_number}/events"],
				listEventsForRepo: ["GET /repos/{owner}/{repo}/issues/events"],
				listEventsForTimeline: ["GET /repos/{owner}/{repo}/issues/{issue_number}/timeline"],
				listForAuthenticatedUser: ["GET /user/issues"],
				listForOrg: ["GET /orgs/{org}/issues"],
				listForRepo: ["GET /repos/{owner}/{repo}/issues"],
				listLabelsForMilestone: ["GET /repos/{owner}/{repo}/milestones/{milestone_number}/labels"],
				listLabelsForRepo: ["GET /repos/{owner}/{repo}/labels"],
				listLabelsOnIssue: ["GET /repos/{owner}/{repo}/issues/{issue_number}/labels"],
				listMilestones: ["GET /repos/{owner}/{repo}/milestones"],
				lock: ["PUT /repos/{owner}/{repo}/issues/{issue_number}/lock"],
				removeAllLabels: ["DELETE /repos/{owner}/{repo}/issues/{issue_number}/labels"],
				removeAssignees: ["DELETE /repos/{owner}/{repo}/issues/{issue_number}/assignees"],
				removeLabel: ["DELETE /repos/{owner}/{repo}/issues/{issue_number}/labels/{name}"],
				setLabels: ["PUT /repos/{owner}/{repo}/issues/{issue_number}/labels"],
				unlock: ["DELETE /repos/{owner}/{repo}/issues/{issue_number}/lock"],
				update: ["PATCH /repos/{owner}/{repo}/issues/{issue_number}"],
				updateComment: ["PATCH /repos/{owner}/{repo}/issues/comments/{comment_id}"],
				updateLabel: ["PATCH /repos/{owner}/{repo}/labels/{name}"],
				updateMilestone: ["PATCH /repos/{owner}/{repo}/milestones/{milestone_number}"],
			},
			licenses: {
				get: ["GET /licenses/{license}"],
				getAllCommonlyUsed: ["GET /licenses"],
				getForRepo: ["GET /repos/{owner}/{repo}/license"],
			},
			markdown: {
				render: ["POST /markdown"],
				renderRaw: [
					"POST /markdown/raw",
					{ headers: { "content-type": "text/plain; charset=utf-8" } },
				],
			},
			meta: {
				get: ["GET /meta"],
				getAllVersions: ["GET /versions"],
				getOctocat: ["GET /octocat"],
				getZen: ["GET /zen"],
				root: ["GET /"],
			},
			migrations: {
				cancelImport: [
					"DELETE /repos/{owner}/{repo}/import",
					{},
					{
						deprecated:
							"octokit.rest.migrations.cancelImport() is deprecated, see https://docs.github.com/rest/migrations/source-imports#cancel-an-import",
					},
				],
				deleteArchiveForAuthenticatedUser: ["DELETE /user/migrations/{migration_id}/archive"],
				deleteArchiveForOrg: ["DELETE /orgs/{org}/migrations/{migration_id}/archive"],
				downloadArchiveForOrg: ["GET /orgs/{org}/migrations/{migration_id}/archive"],
				getArchiveForAuthenticatedUser: ["GET /user/migrations/{migration_id}/archive"],
				getCommitAuthors: [
					"GET /repos/{owner}/{repo}/import/authors",
					{},
					{
						deprecated:
							"octokit.rest.migrations.getCommitAuthors() is deprecated, see https://docs.github.com/rest/migrations/source-imports#get-commit-authors",
					},
				],
				getImportStatus: [
					"GET /repos/{owner}/{repo}/import",
					{},
					{
						deprecated:
							"octokit.rest.migrations.getImportStatus() is deprecated, see https://docs.github.com/rest/migrations/source-imports#get-an-import-status",
					},
				],
				getLargeFiles: [
					"GET /repos/{owner}/{repo}/import/large_files",
					{},
					{
						deprecated:
							"octokit.rest.migrations.getLargeFiles() is deprecated, see https://docs.github.com/rest/migrations/source-imports#get-large-files",
					},
				],
				getStatusForAuthenticatedUser: ["GET /user/migrations/{migration_id}"],
				getStatusForOrg: ["GET /orgs/{org}/migrations/{migration_id}"],
				listForAuthenticatedUser: ["GET /user/migrations"],
				listForOrg: ["GET /orgs/{org}/migrations"],
				listReposForAuthenticatedUser: ["GET /user/migrations/{migration_id}/repositories"],
				listReposForOrg: ["GET /orgs/{org}/migrations/{migration_id}/repositories"],
				listReposForUser: [
					"GET /user/migrations/{migration_id}/repositories",
					{},
					{ renamed: ["migrations", "listReposForAuthenticatedUser"] },
				],
				mapCommitAuthor: [
					"PATCH /repos/{owner}/{repo}/import/authors/{author_id}",
					{},
					{
						deprecated:
							"octokit.rest.migrations.mapCommitAuthor() is deprecated, see https://docs.github.com/rest/migrations/source-imports#map-a-commit-author",
					},
				],
				setLfsPreference: [
					"PATCH /repos/{owner}/{repo}/import/lfs",
					{},
					{
						deprecated:
							"octokit.rest.migrations.setLfsPreference() is deprecated, see https://docs.github.com/rest/migrations/source-imports#update-git-lfs-preference",
					},
				],
				startForAuthenticatedUser: ["POST /user/migrations"],
				startForOrg: ["POST /orgs/{org}/migrations"],
				startImport: [
					"PUT /repos/{owner}/{repo}/import",
					{},
					{
						deprecated:
							"octokit.rest.migrations.startImport() is deprecated, see https://docs.github.com/rest/migrations/source-imports#start-an-import",
					},
				],
				unlockRepoForAuthenticatedUser: [
					"DELETE /user/migrations/{migration_id}/repos/{repo_name}/lock",
				],
				unlockRepoForOrg: ["DELETE /orgs/{org}/migrations/{migration_id}/repos/{repo_name}/lock"],
				updateImport: [
					"PATCH /repos/{owner}/{repo}/import",
					{},
					{
						deprecated:
							"octokit.rest.migrations.updateImport() is deprecated, see https://docs.github.com/rest/migrations/source-imports#update-an-import",
					},
				],
			},
			oidc: {
				getOidcCustomSubTemplateForOrg: ["GET /orgs/{org}/actions/oidc/customization/sub"],
				updateOidcCustomSubTemplateForOrg: ["PUT /orgs/{org}/actions/oidc/customization/sub"],
			},
			orgs: {
				addSecurityManagerTeam: ["PUT /orgs/{org}/security-managers/teams/{team_slug}"],
				assignTeamToOrgRole: ["PUT /orgs/{org}/organization-roles/teams/{team_slug}/{role_id}"],
				assignUserToOrgRole: ["PUT /orgs/{org}/organization-roles/users/{username}/{role_id}"],
				blockUser: ["PUT /orgs/{org}/blocks/{username}"],
				cancelInvitation: ["DELETE /orgs/{org}/invitations/{invitation_id}"],
				checkBlockedUser: ["GET /orgs/{org}/blocks/{username}"],
				checkMembershipForUser: ["GET /orgs/{org}/members/{username}"],
				checkPublicMembershipForUser: ["GET /orgs/{org}/public_members/{username}"],
				convertMemberToOutsideCollaborator: ["PUT /orgs/{org}/outside_collaborators/{username}"],
				createCustomOrganizationRole: ["POST /orgs/{org}/organization-roles"],
				createInvitation: ["POST /orgs/{org}/invitations"],
				createOrUpdateCustomProperties: ["PATCH /orgs/{org}/properties/schema"],
				createOrUpdateCustomPropertiesValuesForRepos: ["PATCH /orgs/{org}/properties/values"],
				createOrUpdateCustomProperty: ["PUT /orgs/{org}/properties/schema/{custom_property_name}"],
				createWebhook: ["POST /orgs/{org}/hooks"],
				delete: ["DELETE /orgs/{org}"],
				deleteCustomOrganizationRole: ["DELETE /orgs/{org}/organization-roles/{role_id}"],
				deleteWebhook: ["DELETE /orgs/{org}/hooks/{hook_id}"],
				enableOrDisableSecurityProductOnAllOrgRepos: [
					"POST /orgs/{org}/{security_product}/{enablement}",
				],
				get: ["GET /orgs/{org}"],
				getAllCustomProperties: ["GET /orgs/{org}/properties/schema"],
				getCustomProperty: ["GET /orgs/{org}/properties/schema/{custom_property_name}"],
				getMembershipForAuthenticatedUser: ["GET /user/memberships/orgs/{org}"],
				getMembershipForUser: ["GET /orgs/{org}/memberships/{username}"],
				getOrgRole: ["GET /orgs/{org}/organization-roles/{role_id}"],
				getWebhook: ["GET /orgs/{org}/hooks/{hook_id}"],
				getWebhookConfigForOrg: ["GET /orgs/{org}/hooks/{hook_id}/config"],
				getWebhookDelivery: ["GET /orgs/{org}/hooks/{hook_id}/deliveries/{delivery_id}"],
				list: ["GET /organizations"],
				listAppInstallations: ["GET /orgs/{org}/installations"],
				listBlockedUsers: ["GET /orgs/{org}/blocks"],
				listCustomPropertiesValuesForRepos: ["GET /orgs/{org}/properties/values"],
				listFailedInvitations: ["GET /orgs/{org}/failed_invitations"],
				listForAuthenticatedUser: ["GET /user/orgs"],
				listForUser: ["GET /users/{username}/orgs"],
				listInvitationTeams: ["GET /orgs/{org}/invitations/{invitation_id}/teams"],
				listMembers: ["GET /orgs/{org}/members"],
				listMembershipsForAuthenticatedUser: ["GET /user/memberships/orgs"],
				listOrgRoleTeams: ["GET /orgs/{org}/organization-roles/{role_id}/teams"],
				listOrgRoleUsers: ["GET /orgs/{org}/organization-roles/{role_id}/users"],
				listOrgRoles: ["GET /orgs/{org}/organization-roles"],
				listOrganizationFineGrainedPermissions: [
					"GET /orgs/{org}/organization-fine-grained-permissions",
				],
				listOutsideCollaborators: ["GET /orgs/{org}/outside_collaborators"],
				listPatGrantRepositories: ["GET /orgs/{org}/personal-access-tokens/{pat_id}/repositories"],
				listPatGrantRequestRepositories: [
					"GET /orgs/{org}/personal-access-token-requests/{pat_request_id}/repositories",
				],
				listPatGrantRequests: ["GET /orgs/{org}/personal-access-token-requests"],
				listPatGrants: ["GET /orgs/{org}/personal-access-tokens"],
				listPendingInvitations: ["GET /orgs/{org}/invitations"],
				listPublicMembers: ["GET /orgs/{org}/public_members"],
				listSecurityManagerTeams: ["GET /orgs/{org}/security-managers"],
				listWebhookDeliveries: ["GET /orgs/{org}/hooks/{hook_id}/deliveries"],
				listWebhooks: ["GET /orgs/{org}/hooks"],
				patchCustomOrganizationRole: ["PATCH /orgs/{org}/organization-roles/{role_id}"],
				pingWebhook: ["POST /orgs/{org}/hooks/{hook_id}/pings"],
				redeliverWebhookDelivery: [
					"POST /orgs/{org}/hooks/{hook_id}/deliveries/{delivery_id}/attempts",
				],
				removeCustomProperty: ["DELETE /orgs/{org}/properties/schema/{custom_property_name}"],
				removeMember: ["DELETE /orgs/{org}/members/{username}"],
				removeMembershipForUser: ["DELETE /orgs/{org}/memberships/{username}"],
				removeOutsideCollaborator: ["DELETE /orgs/{org}/outside_collaborators/{username}"],
				removePublicMembershipForAuthenticatedUser: [
					"DELETE /orgs/{org}/public_members/{username}",
				],
				removeSecurityManagerTeam: ["DELETE /orgs/{org}/security-managers/teams/{team_slug}"],
				reviewPatGrantRequest: ["POST /orgs/{org}/personal-access-token-requests/{pat_request_id}"],
				reviewPatGrantRequestsInBulk: ["POST /orgs/{org}/personal-access-token-requests"],
				revokeAllOrgRolesTeam: ["DELETE /orgs/{org}/organization-roles/teams/{team_slug}"],
				revokeAllOrgRolesUser: ["DELETE /orgs/{org}/organization-roles/users/{username}"],
				revokeOrgRoleTeam: ["DELETE /orgs/{org}/organization-roles/teams/{team_slug}/{role_id}"],
				revokeOrgRoleUser: ["DELETE /orgs/{org}/organization-roles/users/{username}/{role_id}"],
				setMembershipForUser: ["PUT /orgs/{org}/memberships/{username}"],
				setPublicMembershipForAuthenticatedUser: ["PUT /orgs/{org}/public_members/{username}"],
				unblockUser: ["DELETE /orgs/{org}/blocks/{username}"],
				update: ["PATCH /orgs/{org}"],
				updateMembershipForAuthenticatedUser: ["PATCH /user/memberships/orgs/{org}"],
				updatePatAccess: ["POST /orgs/{org}/personal-access-tokens/{pat_id}"],
				updatePatAccesses: ["POST /orgs/{org}/personal-access-tokens"],
				updateWebhook: ["PATCH /orgs/{org}/hooks/{hook_id}"],
				updateWebhookConfigForOrg: ["PATCH /orgs/{org}/hooks/{hook_id}/config"],
			},
			packages: {
				deletePackageForAuthenticatedUser: ["DELETE /user/packages/{package_type}/{package_name}"],
				deletePackageForOrg: ["DELETE /orgs/{org}/packages/{package_type}/{package_name}"],
				deletePackageForUser: ["DELETE /users/{username}/packages/{package_type}/{package_name}"],
				deletePackageVersionForAuthenticatedUser: [
					"DELETE /user/packages/{package_type}/{package_name}/versions/{package_version_id}",
				],
				deletePackageVersionForOrg: [
					"DELETE /orgs/{org}/packages/{package_type}/{package_name}/versions/{package_version_id}",
				],
				deletePackageVersionForUser: [
					"DELETE /users/{username}/packages/{package_type}/{package_name}/versions/{package_version_id}",
				],
				getAllPackageVersionsForAPackageOwnedByAnOrg: [
					"GET /orgs/{org}/packages/{package_type}/{package_name}/versions",
					{},
					{ renamed: ["packages", "getAllPackageVersionsForPackageOwnedByOrg"] },
				],
				getAllPackageVersionsForAPackageOwnedByTheAuthenticatedUser: [
					"GET /user/packages/{package_type}/{package_name}/versions",
					{},
					{
						renamed: ["packages", "getAllPackageVersionsForPackageOwnedByAuthenticatedUser"],
					},
				],
				getAllPackageVersionsForPackageOwnedByAuthenticatedUser: [
					"GET /user/packages/{package_type}/{package_name}/versions",
				],
				getAllPackageVersionsForPackageOwnedByOrg: [
					"GET /orgs/{org}/packages/{package_type}/{package_name}/versions",
				],
				getAllPackageVersionsForPackageOwnedByUser: [
					"GET /users/{username}/packages/{package_type}/{package_name}/versions",
				],
				getPackageForAuthenticatedUser: ["GET /user/packages/{package_type}/{package_name}"],
				getPackageForOrganization: ["GET /orgs/{org}/packages/{package_type}/{package_name}"],
				getPackageForUser: ["GET /users/{username}/packages/{package_type}/{package_name}"],
				getPackageVersionForAuthenticatedUser: [
					"GET /user/packages/{package_type}/{package_name}/versions/{package_version_id}",
				],
				getPackageVersionForOrganization: [
					"GET /orgs/{org}/packages/{package_type}/{package_name}/versions/{package_version_id}",
				],
				getPackageVersionForUser: [
					"GET /users/{username}/packages/{package_type}/{package_name}/versions/{package_version_id}",
				],
				listDockerMigrationConflictingPackagesForAuthenticatedUser: ["GET /user/docker/conflicts"],
				listDockerMigrationConflictingPackagesForOrganization: ["GET /orgs/{org}/docker/conflicts"],
				listDockerMigrationConflictingPackagesForUser: ["GET /users/{username}/docker/conflicts"],
				listPackagesForAuthenticatedUser: ["GET /user/packages"],
				listPackagesForOrganization: ["GET /orgs/{org}/packages"],
				listPackagesForUser: ["GET /users/{username}/packages"],
				restorePackageForAuthenticatedUser: [
					"POST /user/packages/{package_type}/{package_name}/restore{?token}",
				],
				restorePackageForOrg: [
					"POST /orgs/{org}/packages/{package_type}/{package_name}/restore{?token}",
				],
				restorePackageForUser: [
					"POST /users/{username}/packages/{package_type}/{package_name}/restore{?token}",
				],
				restorePackageVersionForAuthenticatedUser: [
					"POST /user/packages/{package_type}/{package_name}/versions/{package_version_id}/restore",
				],
				restorePackageVersionForOrg: [
					"POST /orgs/{org}/packages/{package_type}/{package_name}/versions/{package_version_id}/restore",
				],
				restorePackageVersionForUser: [
					"POST /users/{username}/packages/{package_type}/{package_name}/versions/{package_version_id}/restore",
				],
			},
			projects: {
				addCollaborator: ["PUT /projects/{project_id}/collaborators/{username}"],
				createCard: ["POST /projects/columns/{column_id}/cards"],
				createColumn: ["POST /projects/{project_id}/columns"],
				createForAuthenticatedUser: ["POST /user/projects"],
				createForOrg: ["POST /orgs/{org}/projects"],
				createForRepo: ["POST /repos/{owner}/{repo}/projects"],
				delete: ["DELETE /projects/{project_id}"],
				deleteCard: ["DELETE /projects/columns/cards/{card_id}"],
				deleteColumn: ["DELETE /projects/columns/{column_id}"],
				get: ["GET /projects/{project_id}"],
				getCard: ["GET /projects/columns/cards/{card_id}"],
				getColumn: ["GET /projects/columns/{column_id}"],
				getPermissionForUser: ["GET /projects/{project_id}/collaborators/{username}/permission"],
				listCards: ["GET /projects/columns/{column_id}/cards"],
				listCollaborators: ["GET /projects/{project_id}/collaborators"],
				listColumns: ["GET /projects/{project_id}/columns"],
				listForOrg: ["GET /orgs/{org}/projects"],
				listForRepo: ["GET /repos/{owner}/{repo}/projects"],
				listForUser: ["GET /users/{username}/projects"],
				moveCard: ["POST /projects/columns/cards/{card_id}/moves"],
				moveColumn: ["POST /projects/columns/{column_id}/moves"],
				removeCollaborator: ["DELETE /projects/{project_id}/collaborators/{username}"],
				update: ["PATCH /projects/{project_id}"],
				updateCard: ["PATCH /projects/columns/cards/{card_id}"],
				updateColumn: ["PATCH /projects/columns/{column_id}"],
			},
			pulls: {
				checkIfMerged: ["GET /repos/{owner}/{repo}/pulls/{pull_number}/merge"],
				create: ["POST /repos/{owner}/{repo}/pulls"],
				createReplyForReviewComment: [
					"POST /repos/{owner}/{repo}/pulls/{pull_number}/comments/{comment_id}/replies",
				],
				createReview: ["POST /repos/{owner}/{repo}/pulls/{pull_number}/reviews"],
				createReviewComment: ["POST /repos/{owner}/{repo}/pulls/{pull_number}/comments"],
				deletePendingReview: [
					"DELETE /repos/{owner}/{repo}/pulls/{pull_number}/reviews/{review_id}",
				],
				deleteReviewComment: ["DELETE /repos/{owner}/{repo}/pulls/comments/{comment_id}"],
				dismissReview: [
					"PUT /repos/{owner}/{repo}/pulls/{pull_number}/reviews/{review_id}/dismissals",
				],
				get: ["GET /repos/{owner}/{repo}/pulls/{pull_number}"],
				getReview: ["GET /repos/{owner}/{repo}/pulls/{pull_number}/reviews/{review_id}"],
				getReviewComment: ["GET /repos/{owner}/{repo}/pulls/comments/{comment_id}"],
				list: ["GET /repos/{owner}/{repo}/pulls"],
				listCommentsForReview: [
					"GET /repos/{owner}/{repo}/pulls/{pull_number}/reviews/{review_id}/comments",
				],
				listCommits: ["GET /repos/{owner}/{repo}/pulls/{pull_number}/commits"],
				listFiles: ["GET /repos/{owner}/{repo}/pulls/{pull_number}/files"],
				listRequestedReviewers: [
					"GET /repos/{owner}/{repo}/pulls/{pull_number}/requested_reviewers",
				],
				listReviewComments: ["GET /repos/{owner}/{repo}/pulls/{pull_number}/comments"],
				listReviewCommentsForRepo: ["GET /repos/{owner}/{repo}/pulls/comments"],
				listReviews: ["GET /repos/{owner}/{repo}/pulls/{pull_number}/reviews"],
				merge: ["PUT /repos/{owner}/{repo}/pulls/{pull_number}/merge"],
				removeRequestedReviewers: [
					"DELETE /repos/{owner}/{repo}/pulls/{pull_number}/requested_reviewers",
				],
				requestReviewers: ["POST /repos/{owner}/{repo}/pulls/{pull_number}/requested_reviewers"],
				submitReview: ["POST /repos/{owner}/{repo}/pulls/{pull_number}/reviews/{review_id}/events"],
				update: ["PATCH /repos/{owner}/{repo}/pulls/{pull_number}"],
				updateBranch: ["PUT /repos/{owner}/{repo}/pulls/{pull_number}/update-branch"],
				updateReview: ["PUT /repos/{owner}/{repo}/pulls/{pull_number}/reviews/{review_id}"],
				updateReviewComment: ["PATCH /repos/{owner}/{repo}/pulls/comments/{comment_id}"],
			},
			rateLimit: { get: ["GET /rate_limit"] },
			reactions: {
				createForCommitComment: ["POST /repos/{owner}/{repo}/comments/{comment_id}/reactions"],
				createForIssue: ["POST /repos/{owner}/{repo}/issues/{issue_number}/reactions"],
				createForIssueComment: [
					"POST /repos/{owner}/{repo}/issues/comments/{comment_id}/reactions",
				],
				createForPullRequestReviewComment: [
					"POST /repos/{owner}/{repo}/pulls/comments/{comment_id}/reactions",
				],
				createForRelease: ["POST /repos/{owner}/{repo}/releases/{release_id}/reactions"],
				createForTeamDiscussionCommentInOrg: [
					"POST /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}/comments/{comment_number}/reactions",
				],
				createForTeamDiscussionInOrg: [
					"POST /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}/reactions",
				],
				deleteForCommitComment: [
					"DELETE /repos/{owner}/{repo}/comments/{comment_id}/reactions/{reaction_id}",
				],
				deleteForIssue: [
					"DELETE /repos/{owner}/{repo}/issues/{issue_number}/reactions/{reaction_id}",
				],
				deleteForIssueComment: [
					"DELETE /repos/{owner}/{repo}/issues/comments/{comment_id}/reactions/{reaction_id}",
				],
				deleteForPullRequestComment: [
					"DELETE /repos/{owner}/{repo}/pulls/comments/{comment_id}/reactions/{reaction_id}",
				],
				deleteForRelease: [
					"DELETE /repos/{owner}/{repo}/releases/{release_id}/reactions/{reaction_id}",
				],
				deleteForTeamDiscussion: [
					"DELETE /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}/reactions/{reaction_id}",
				],
				deleteForTeamDiscussionComment: [
					"DELETE /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}/comments/{comment_number}/reactions/{reaction_id}",
				],
				listForCommitComment: ["GET /repos/{owner}/{repo}/comments/{comment_id}/reactions"],
				listForIssue: ["GET /repos/{owner}/{repo}/issues/{issue_number}/reactions"],
				listForIssueComment: ["GET /repos/{owner}/{repo}/issues/comments/{comment_id}/reactions"],
				listForPullRequestReviewComment: [
					"GET /repos/{owner}/{repo}/pulls/comments/{comment_id}/reactions",
				],
				listForRelease: ["GET /repos/{owner}/{repo}/releases/{release_id}/reactions"],
				listForTeamDiscussionCommentInOrg: [
					"GET /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}/comments/{comment_number}/reactions",
				],
				listForTeamDiscussionInOrg: [
					"GET /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}/reactions",
				],
			},
			repos: {
				acceptInvitation: [
					"PATCH /user/repository_invitations/{invitation_id}",
					{},
					{ renamed: ["repos", "acceptInvitationForAuthenticatedUser"] },
				],
				acceptInvitationForAuthenticatedUser: [
					"PATCH /user/repository_invitations/{invitation_id}",
				],
				addAppAccessRestrictions: [
					"POST /repos/{owner}/{repo}/branches/{branch}/protection/restrictions/apps",
					{},
					{ mapToData: "apps" },
				],
				addCollaborator: ["PUT /repos/{owner}/{repo}/collaborators/{username}"],
				addStatusCheckContexts: [
					"POST /repos/{owner}/{repo}/branches/{branch}/protection/required_status_checks/contexts",
					{},
					{ mapToData: "contexts" },
				],
				addTeamAccessRestrictions: [
					"POST /repos/{owner}/{repo}/branches/{branch}/protection/restrictions/teams",
					{},
					{ mapToData: "teams" },
				],
				addUserAccessRestrictions: [
					"POST /repos/{owner}/{repo}/branches/{branch}/protection/restrictions/users",
					{},
					{ mapToData: "users" },
				],
				cancelPagesDeployment: [
					"POST /repos/{owner}/{repo}/pages/deployments/{pages_deployment_id}/cancel",
				],
				checkAutomatedSecurityFixes: ["GET /repos/{owner}/{repo}/automated-security-fixes"],
				checkCollaborator: ["GET /repos/{owner}/{repo}/collaborators/{username}"],
				checkVulnerabilityAlerts: ["GET /repos/{owner}/{repo}/vulnerability-alerts"],
				codeownersErrors: ["GET /repos/{owner}/{repo}/codeowners/errors"],
				compareCommits: ["GET /repos/{owner}/{repo}/compare/{base}...{head}"],
				compareCommitsWithBasehead: ["GET /repos/{owner}/{repo}/compare/{basehead}"],
				createAutolink: ["POST /repos/{owner}/{repo}/autolinks"],
				createCommitComment: ["POST /repos/{owner}/{repo}/commits/{commit_sha}/comments"],
				createCommitSignatureProtection: [
					"POST /repos/{owner}/{repo}/branches/{branch}/protection/required_signatures",
				],
				createCommitStatus: ["POST /repos/{owner}/{repo}/statuses/{sha}"],
				createDeployKey: ["POST /repos/{owner}/{repo}/keys"],
				createDeployment: ["POST /repos/{owner}/{repo}/deployments"],
				createDeploymentBranchPolicy: [
					"POST /repos/{owner}/{repo}/environments/{environment_name}/deployment-branch-policies",
				],
				createDeploymentProtectionRule: [
					"POST /repos/{owner}/{repo}/environments/{environment_name}/deployment_protection_rules",
				],
				createDeploymentStatus: ["POST /repos/{owner}/{repo}/deployments/{deployment_id}/statuses"],
				createDispatchEvent: ["POST /repos/{owner}/{repo}/dispatches"],
				createForAuthenticatedUser: ["POST /user/repos"],
				createFork: ["POST /repos/{owner}/{repo}/forks"],
				createInOrg: ["POST /orgs/{org}/repos"],
				createOrUpdateCustomPropertiesValues: ["PATCH /repos/{owner}/{repo}/properties/values"],
				createOrUpdateEnvironment: ["PUT /repos/{owner}/{repo}/environments/{environment_name}"],
				createOrUpdateFileContents: ["PUT /repos/{owner}/{repo}/contents/{path}"],
				createOrgRuleset: ["POST /orgs/{org}/rulesets"],
				createPagesDeployment: ["POST /repos/{owner}/{repo}/pages/deployments"],
				createPagesSite: ["POST /repos/{owner}/{repo}/pages"],
				createRelease: ["POST /repos/{owner}/{repo}/releases"],
				createRepoRuleset: ["POST /repos/{owner}/{repo}/rulesets"],
				createTagProtection: ["POST /repos/{owner}/{repo}/tags/protection"],
				createUsingTemplate: ["POST /repos/{template_owner}/{template_repo}/generate"],
				createWebhook: ["POST /repos/{owner}/{repo}/hooks"],
				declineInvitation: [
					"DELETE /user/repository_invitations/{invitation_id}",
					{},
					{ renamed: ["repos", "declineInvitationForAuthenticatedUser"] },
				],
				declineInvitationForAuthenticatedUser: [
					"DELETE /user/repository_invitations/{invitation_id}",
				],
				delete: ["DELETE /repos/{owner}/{repo}"],
				deleteAccessRestrictions: [
					"DELETE /repos/{owner}/{repo}/branches/{branch}/protection/restrictions",
				],
				deleteAdminBranchProtection: [
					"DELETE /repos/{owner}/{repo}/branches/{branch}/protection/enforce_admins",
				],
				deleteAnEnvironment: ["DELETE /repos/{owner}/{repo}/environments/{environment_name}"],
				deleteAutolink: ["DELETE /repos/{owner}/{repo}/autolinks/{autolink_id}"],
				deleteBranchProtection: ["DELETE /repos/{owner}/{repo}/branches/{branch}/protection"],
				deleteCommitComment: ["DELETE /repos/{owner}/{repo}/comments/{comment_id}"],
				deleteCommitSignatureProtection: [
					"DELETE /repos/{owner}/{repo}/branches/{branch}/protection/required_signatures",
				],
				deleteDeployKey: ["DELETE /repos/{owner}/{repo}/keys/{key_id}"],
				deleteDeployment: ["DELETE /repos/{owner}/{repo}/deployments/{deployment_id}"],
				deleteDeploymentBranchPolicy: [
					"DELETE /repos/{owner}/{repo}/environments/{environment_name}/deployment-branch-policies/{branch_policy_id}",
				],
				deleteFile: ["DELETE /repos/{owner}/{repo}/contents/{path}"],
				deleteInvitation: ["DELETE /repos/{owner}/{repo}/invitations/{invitation_id}"],
				deleteOrgRuleset: ["DELETE /orgs/{org}/rulesets/{ruleset_id}"],
				deletePagesSite: ["DELETE /repos/{owner}/{repo}/pages"],
				deletePullRequestReviewProtection: [
					"DELETE /repos/{owner}/{repo}/branches/{branch}/protection/required_pull_request_reviews",
				],
				deleteRelease: ["DELETE /repos/{owner}/{repo}/releases/{release_id}"],
				deleteReleaseAsset: ["DELETE /repos/{owner}/{repo}/releases/assets/{asset_id}"],
				deleteRepoRuleset: ["DELETE /repos/{owner}/{repo}/rulesets/{ruleset_id}"],
				deleteTagProtection: ["DELETE /repos/{owner}/{repo}/tags/protection/{tag_protection_id}"],
				deleteWebhook: ["DELETE /repos/{owner}/{repo}/hooks/{hook_id}"],
				disableAutomatedSecurityFixes: ["DELETE /repos/{owner}/{repo}/automated-security-fixes"],
				disableDeploymentProtectionRule: [
					"DELETE /repos/{owner}/{repo}/environments/{environment_name}/deployment_protection_rules/{protection_rule_id}",
				],
				disablePrivateVulnerabilityReporting: [
					"DELETE /repos/{owner}/{repo}/private-vulnerability-reporting",
				],
				disableVulnerabilityAlerts: ["DELETE /repos/{owner}/{repo}/vulnerability-alerts"],
				downloadArchive: [
					"GET /repos/{owner}/{repo}/zipball/{ref}",
					{},
					{ renamed: ["repos", "downloadZipballArchive"] },
				],
				downloadTarballArchive: ["GET /repos/{owner}/{repo}/tarball/{ref}"],
				downloadZipballArchive: ["GET /repos/{owner}/{repo}/zipball/{ref}"],
				enableAutomatedSecurityFixes: ["PUT /repos/{owner}/{repo}/automated-security-fixes"],
				enablePrivateVulnerabilityReporting: [
					"PUT /repos/{owner}/{repo}/private-vulnerability-reporting",
				],
				enableVulnerabilityAlerts: ["PUT /repos/{owner}/{repo}/vulnerability-alerts"],
				generateReleaseNotes: ["POST /repos/{owner}/{repo}/releases/generate-notes"],
				get: ["GET /repos/{owner}/{repo}"],
				getAccessRestrictions: [
					"GET /repos/{owner}/{repo}/branches/{branch}/protection/restrictions",
				],
				getAdminBranchProtection: [
					"GET /repos/{owner}/{repo}/branches/{branch}/protection/enforce_admins",
				],
				getAllDeploymentProtectionRules: [
					"GET /repos/{owner}/{repo}/environments/{environment_name}/deployment_protection_rules",
				],
				getAllEnvironments: ["GET /repos/{owner}/{repo}/environments"],
				getAllStatusCheckContexts: [
					"GET /repos/{owner}/{repo}/branches/{branch}/protection/required_status_checks/contexts",
				],
				getAllTopics: ["GET /repos/{owner}/{repo}/topics"],
				getAppsWithAccessToProtectedBranch: [
					"GET /repos/{owner}/{repo}/branches/{branch}/protection/restrictions/apps",
				],
				getAutolink: ["GET /repos/{owner}/{repo}/autolinks/{autolink_id}"],
				getBranch: ["GET /repos/{owner}/{repo}/branches/{branch}"],
				getBranchProtection: ["GET /repos/{owner}/{repo}/branches/{branch}/protection"],
				getBranchRules: ["GET /repos/{owner}/{repo}/rules/branches/{branch}"],
				getClones: ["GET /repos/{owner}/{repo}/traffic/clones"],
				getCodeFrequencyStats: ["GET /repos/{owner}/{repo}/stats/code_frequency"],
				getCollaboratorPermissionLevel: [
					"GET /repos/{owner}/{repo}/collaborators/{username}/permission",
				],
				getCombinedStatusForRef: ["GET /repos/{owner}/{repo}/commits/{ref}/status"],
				getCommit: ["GET /repos/{owner}/{repo}/commits/{ref}"],
				getCommitActivityStats: ["GET /repos/{owner}/{repo}/stats/commit_activity"],
				getCommitComment: ["GET /repos/{owner}/{repo}/comments/{comment_id}"],
				getCommitSignatureProtection: [
					"GET /repos/{owner}/{repo}/branches/{branch}/protection/required_signatures",
				],
				getCommunityProfileMetrics: ["GET /repos/{owner}/{repo}/community/profile"],
				getContent: ["GET /repos/{owner}/{repo}/contents/{path}"],
				getContributorsStats: ["GET /repos/{owner}/{repo}/stats/contributors"],
				getCustomDeploymentProtectionRule: [
					"GET /repos/{owner}/{repo}/environments/{environment_name}/deployment_protection_rules/{protection_rule_id}",
				],
				getCustomPropertiesValues: ["GET /repos/{owner}/{repo}/properties/values"],
				getDeployKey: ["GET /repos/{owner}/{repo}/keys/{key_id}"],
				getDeployment: ["GET /repos/{owner}/{repo}/deployments/{deployment_id}"],
				getDeploymentBranchPolicy: [
					"GET /repos/{owner}/{repo}/environments/{environment_name}/deployment-branch-policies/{branch_policy_id}",
				],
				getDeploymentStatus: [
					"GET /repos/{owner}/{repo}/deployments/{deployment_id}/statuses/{status_id}",
				],
				getEnvironment: ["GET /repos/{owner}/{repo}/environments/{environment_name}"],
				getLatestPagesBuild: ["GET /repos/{owner}/{repo}/pages/builds/latest"],
				getLatestRelease: ["GET /repos/{owner}/{repo}/releases/latest"],
				getOrgRuleSuite: ["GET /orgs/{org}/rulesets/rule-suites/{rule_suite_id}"],
				getOrgRuleSuites: ["GET /orgs/{org}/rulesets/rule-suites"],
				getOrgRuleset: ["GET /orgs/{org}/rulesets/{ruleset_id}"],
				getOrgRulesets: ["GET /orgs/{org}/rulesets"],
				getPages: ["GET /repos/{owner}/{repo}/pages"],
				getPagesBuild: ["GET /repos/{owner}/{repo}/pages/builds/{build_id}"],
				getPagesDeployment: ["GET /repos/{owner}/{repo}/pages/deployments/{pages_deployment_id}"],
				getPagesHealthCheck: ["GET /repos/{owner}/{repo}/pages/health"],
				getParticipationStats: ["GET /repos/{owner}/{repo}/stats/participation"],
				getPullRequestReviewProtection: [
					"GET /repos/{owner}/{repo}/branches/{branch}/protection/required_pull_request_reviews",
				],
				getPunchCardStats: ["GET /repos/{owner}/{repo}/stats/punch_card"],
				getReadme: ["GET /repos/{owner}/{repo}/readme"],
				getReadmeInDirectory: ["GET /repos/{owner}/{repo}/readme/{dir}"],
				getRelease: ["GET /repos/{owner}/{repo}/releases/{release_id}"],
				getReleaseAsset: ["GET /repos/{owner}/{repo}/releases/assets/{asset_id}"],
				getReleaseByTag: ["GET /repos/{owner}/{repo}/releases/tags/{tag}"],
				getRepoRuleSuite: ["GET /repos/{owner}/{repo}/rulesets/rule-suites/{rule_suite_id}"],
				getRepoRuleSuites: ["GET /repos/{owner}/{repo}/rulesets/rule-suites"],
				getRepoRuleset: ["GET /repos/{owner}/{repo}/rulesets/{ruleset_id}"],
				getRepoRulesets: ["GET /repos/{owner}/{repo}/rulesets"],
				getStatusChecksProtection: [
					"GET /repos/{owner}/{repo}/branches/{branch}/protection/required_status_checks",
				],
				getTeamsWithAccessToProtectedBranch: [
					"GET /repos/{owner}/{repo}/branches/{branch}/protection/restrictions/teams",
				],
				getTopPaths: ["GET /repos/{owner}/{repo}/traffic/popular/paths"],
				getTopReferrers: ["GET /repos/{owner}/{repo}/traffic/popular/referrers"],
				getUsersWithAccessToProtectedBranch: [
					"GET /repos/{owner}/{repo}/branches/{branch}/protection/restrictions/users",
				],
				getViews: ["GET /repos/{owner}/{repo}/traffic/views"],
				getWebhook: ["GET /repos/{owner}/{repo}/hooks/{hook_id}"],
				getWebhookConfigForRepo: ["GET /repos/{owner}/{repo}/hooks/{hook_id}/config"],
				getWebhookDelivery: ["GET /repos/{owner}/{repo}/hooks/{hook_id}/deliveries/{delivery_id}"],
				listActivities: ["GET /repos/{owner}/{repo}/activity"],
				listAutolinks: ["GET /repos/{owner}/{repo}/autolinks"],
				listBranches: ["GET /repos/{owner}/{repo}/branches"],
				listBranchesForHeadCommit: [
					"GET /repos/{owner}/{repo}/commits/{commit_sha}/branches-where-head",
				],
				listCollaborators: ["GET /repos/{owner}/{repo}/collaborators"],
				listCommentsForCommit: ["GET /repos/{owner}/{repo}/commits/{commit_sha}/comments"],
				listCommitCommentsForRepo: ["GET /repos/{owner}/{repo}/comments"],
				listCommitStatusesForRef: ["GET /repos/{owner}/{repo}/commits/{ref}/statuses"],
				listCommits: ["GET /repos/{owner}/{repo}/commits"],
				listContributors: ["GET /repos/{owner}/{repo}/contributors"],
				listCustomDeploymentRuleIntegrations: [
					"GET /repos/{owner}/{repo}/environments/{environment_name}/deployment_protection_rules/apps",
				],
				listDeployKeys: ["GET /repos/{owner}/{repo}/keys"],
				listDeploymentBranchPolicies: [
					"GET /repos/{owner}/{repo}/environments/{environment_name}/deployment-branch-policies",
				],
				listDeploymentStatuses: ["GET /repos/{owner}/{repo}/deployments/{deployment_id}/statuses"],
				listDeployments: ["GET /repos/{owner}/{repo}/deployments"],
				listForAuthenticatedUser: ["GET /user/repos"],
				listForOrg: ["GET /orgs/{org}/repos"],
				listForUser: ["GET /users/{username}/repos"],
				listForks: ["GET /repos/{owner}/{repo}/forks"],
				listInvitations: ["GET /repos/{owner}/{repo}/invitations"],
				listInvitationsForAuthenticatedUser: ["GET /user/repository_invitations"],
				listLanguages: ["GET /repos/{owner}/{repo}/languages"],
				listPagesBuilds: ["GET /repos/{owner}/{repo}/pages/builds"],
				listPublic: ["GET /repositories"],
				listPullRequestsAssociatedWithCommit: [
					"GET /repos/{owner}/{repo}/commits/{commit_sha}/pulls",
				],
				listReleaseAssets: ["GET /repos/{owner}/{repo}/releases/{release_id}/assets"],
				listReleases: ["GET /repos/{owner}/{repo}/releases"],
				listTagProtection: ["GET /repos/{owner}/{repo}/tags/protection"],
				listTags: ["GET /repos/{owner}/{repo}/tags"],
				listTeams: ["GET /repos/{owner}/{repo}/teams"],
				listWebhookDeliveries: ["GET /repos/{owner}/{repo}/hooks/{hook_id}/deliveries"],
				listWebhooks: ["GET /repos/{owner}/{repo}/hooks"],
				merge: ["POST /repos/{owner}/{repo}/merges"],
				mergeUpstream: ["POST /repos/{owner}/{repo}/merge-upstream"],
				pingWebhook: ["POST /repos/{owner}/{repo}/hooks/{hook_id}/pings"],
				redeliverWebhookDelivery: [
					"POST /repos/{owner}/{repo}/hooks/{hook_id}/deliveries/{delivery_id}/attempts",
				],
				removeAppAccessRestrictions: [
					"DELETE /repos/{owner}/{repo}/branches/{branch}/protection/restrictions/apps",
					{},
					{ mapToData: "apps" },
				],
				removeCollaborator: ["DELETE /repos/{owner}/{repo}/collaborators/{username}"],
				removeStatusCheckContexts: [
					"DELETE /repos/{owner}/{repo}/branches/{branch}/protection/required_status_checks/contexts",
					{},
					{ mapToData: "contexts" },
				],
				removeStatusCheckProtection: [
					"DELETE /repos/{owner}/{repo}/branches/{branch}/protection/required_status_checks",
				],
				removeTeamAccessRestrictions: [
					"DELETE /repos/{owner}/{repo}/branches/{branch}/protection/restrictions/teams",
					{},
					{ mapToData: "teams" },
				],
				removeUserAccessRestrictions: [
					"DELETE /repos/{owner}/{repo}/branches/{branch}/protection/restrictions/users",
					{},
					{ mapToData: "users" },
				],
				renameBranch: ["POST /repos/{owner}/{repo}/branches/{branch}/rename"],
				replaceAllTopics: ["PUT /repos/{owner}/{repo}/topics"],
				requestPagesBuild: ["POST /repos/{owner}/{repo}/pages/builds"],
				setAdminBranchProtection: [
					"POST /repos/{owner}/{repo}/branches/{branch}/protection/enforce_admins",
				],
				setAppAccessRestrictions: [
					"PUT /repos/{owner}/{repo}/branches/{branch}/protection/restrictions/apps",
					{},
					{ mapToData: "apps" },
				],
				setStatusCheckContexts: [
					"PUT /repos/{owner}/{repo}/branches/{branch}/protection/required_status_checks/contexts",
					{},
					{ mapToData: "contexts" },
				],
				setTeamAccessRestrictions: [
					"PUT /repos/{owner}/{repo}/branches/{branch}/protection/restrictions/teams",
					{},
					{ mapToData: "teams" },
				],
				setUserAccessRestrictions: [
					"PUT /repos/{owner}/{repo}/branches/{branch}/protection/restrictions/users",
					{},
					{ mapToData: "users" },
				],
				testPushWebhook: ["POST /repos/{owner}/{repo}/hooks/{hook_id}/tests"],
				transfer: ["POST /repos/{owner}/{repo}/transfer"],
				update: ["PATCH /repos/{owner}/{repo}"],
				updateBranchProtection: ["PUT /repos/{owner}/{repo}/branches/{branch}/protection"],
				updateCommitComment: ["PATCH /repos/{owner}/{repo}/comments/{comment_id}"],
				updateDeploymentBranchPolicy: [
					"PUT /repos/{owner}/{repo}/environments/{environment_name}/deployment-branch-policies/{branch_policy_id}",
				],
				updateInformationAboutPagesSite: ["PUT /repos/{owner}/{repo}/pages"],
				updateInvitation: ["PATCH /repos/{owner}/{repo}/invitations/{invitation_id}"],
				updateOrgRuleset: ["PUT /orgs/{org}/rulesets/{ruleset_id}"],
				updatePullRequestReviewProtection: [
					"PATCH /repos/{owner}/{repo}/branches/{branch}/protection/required_pull_request_reviews",
				],
				updateRelease: ["PATCH /repos/{owner}/{repo}/releases/{release_id}"],
				updateReleaseAsset: ["PATCH /repos/{owner}/{repo}/releases/assets/{asset_id}"],
				updateRepoRuleset: ["PUT /repos/{owner}/{repo}/rulesets/{ruleset_id}"],
				updateStatusCheckPotection: [
					"PATCH /repos/{owner}/{repo}/branches/{branch}/protection/required_status_checks",
					{},
					{ renamed: ["repos", "updateStatusCheckProtection"] },
				],
				updateStatusCheckProtection: [
					"PATCH /repos/{owner}/{repo}/branches/{branch}/protection/required_status_checks",
				],
				updateWebhook: ["PATCH /repos/{owner}/{repo}/hooks/{hook_id}"],
				updateWebhookConfigForRepo: ["PATCH /repos/{owner}/{repo}/hooks/{hook_id}/config"],
				uploadReleaseAsset: [
					"POST /repos/{owner}/{repo}/releases/{release_id}/assets{?name,label}",
					{ baseUrl: "https://uploads.github.com" },
				],
			},
			search: {
				code: ["GET /search/code"],
				commits: ["GET /search/commits"],
				issuesAndPullRequests: ["GET /search/issues"],
				labels: ["GET /search/labels"],
				repos: ["GET /search/repositories"],
				topics: ["GET /search/topics"],
				users: ["GET /search/users"],
			},
			secretScanning: {
				getAlert: ["GET /repos/{owner}/{repo}/secret-scanning/alerts/{alert_number}"],
				listAlertsForEnterprise: ["GET /enterprises/{enterprise}/secret-scanning/alerts"],
				listAlertsForOrg: ["GET /orgs/{org}/secret-scanning/alerts"],
				listAlertsForRepo: ["GET /repos/{owner}/{repo}/secret-scanning/alerts"],
				listLocationsForAlert: [
					"GET /repos/{owner}/{repo}/secret-scanning/alerts/{alert_number}/locations",
				],
				updateAlert: ["PATCH /repos/{owner}/{repo}/secret-scanning/alerts/{alert_number}"],
			},
			securityAdvisories: {
				createFork: ["POST /repos/{owner}/{repo}/security-advisories/{ghsa_id}/forks"],
				createPrivateVulnerabilityReport: [
					"POST /repos/{owner}/{repo}/security-advisories/reports",
				],
				createRepositoryAdvisory: ["POST /repos/{owner}/{repo}/security-advisories"],
				createRepositoryAdvisoryCveRequest: [
					"POST /repos/{owner}/{repo}/security-advisories/{ghsa_id}/cve",
				],
				getGlobalAdvisory: ["GET /advisories/{ghsa_id}"],
				getRepositoryAdvisory: ["GET /repos/{owner}/{repo}/security-advisories/{ghsa_id}"],
				listGlobalAdvisories: ["GET /advisories"],
				listOrgRepositoryAdvisories: ["GET /orgs/{org}/security-advisories"],
				listRepositoryAdvisories: ["GET /repos/{owner}/{repo}/security-advisories"],
				updateRepositoryAdvisory: ["PATCH /repos/{owner}/{repo}/security-advisories/{ghsa_id}"],
			},
			teams: {
				addOrUpdateMembershipForUserInOrg: [
					"PUT /orgs/{org}/teams/{team_slug}/memberships/{username}",
				],
				addOrUpdateProjectPermissionsInOrg: [
					"PUT /orgs/{org}/teams/{team_slug}/projects/{project_id}",
				],
				addOrUpdateRepoPermissionsInOrg: ["PUT /orgs/{org}/teams/{team_slug}/repos/{owner}/{repo}"],
				checkPermissionsForProjectInOrg: [
					"GET /orgs/{org}/teams/{team_slug}/projects/{project_id}",
				],
				checkPermissionsForRepoInOrg: ["GET /orgs/{org}/teams/{team_slug}/repos/{owner}/{repo}"],
				create: ["POST /orgs/{org}/teams"],
				createDiscussionCommentInOrg: [
					"POST /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}/comments",
				],
				createDiscussionInOrg: ["POST /orgs/{org}/teams/{team_slug}/discussions"],
				deleteDiscussionCommentInOrg: [
					"DELETE /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}/comments/{comment_number}",
				],
				deleteDiscussionInOrg: [
					"DELETE /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}",
				],
				deleteInOrg: ["DELETE /orgs/{org}/teams/{team_slug}"],
				getByName: ["GET /orgs/{org}/teams/{team_slug}"],
				getDiscussionCommentInOrg: [
					"GET /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}/comments/{comment_number}",
				],
				getDiscussionInOrg: ["GET /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}"],
				getMembershipForUserInOrg: ["GET /orgs/{org}/teams/{team_slug}/memberships/{username}"],
				list: ["GET /orgs/{org}/teams"],
				listChildInOrg: ["GET /orgs/{org}/teams/{team_slug}/teams"],
				listDiscussionCommentsInOrg: [
					"GET /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}/comments",
				],
				listDiscussionsInOrg: ["GET /orgs/{org}/teams/{team_slug}/discussions"],
				listForAuthenticatedUser: ["GET /user/teams"],
				listMembersInOrg: ["GET /orgs/{org}/teams/{team_slug}/members"],
				listPendingInvitationsInOrg: ["GET /orgs/{org}/teams/{team_slug}/invitations"],
				listProjectsInOrg: ["GET /orgs/{org}/teams/{team_slug}/projects"],
				listReposInOrg: ["GET /orgs/{org}/teams/{team_slug}/repos"],
				removeMembershipForUserInOrg: [
					"DELETE /orgs/{org}/teams/{team_slug}/memberships/{username}",
				],
				removeProjectInOrg: ["DELETE /orgs/{org}/teams/{team_slug}/projects/{project_id}"],
				removeRepoInOrg: ["DELETE /orgs/{org}/teams/{team_slug}/repos/{owner}/{repo}"],
				updateDiscussionCommentInOrg: [
					"PATCH /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}/comments/{comment_number}",
				],
				updateDiscussionInOrg: [
					"PATCH /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}",
				],
				updateInOrg: ["PATCH /orgs/{org}/teams/{team_slug}"],
			},
			users: {
				addEmailForAuthenticated: [
					"POST /user/emails",
					{},
					{ renamed: ["users", "addEmailForAuthenticatedUser"] },
				],
				addEmailForAuthenticatedUser: ["POST /user/emails"],
				addSocialAccountForAuthenticatedUser: ["POST /user/social_accounts"],
				block: ["PUT /user/blocks/{username}"],
				checkBlocked: ["GET /user/blocks/{username}"],
				checkFollowingForUser: ["GET /users/{username}/following/{target_user}"],
				checkPersonIsFollowedByAuthenticated: ["GET /user/following/{username}"],
				createGpgKeyForAuthenticated: [
					"POST /user/gpg_keys",
					{},
					{ renamed: ["users", "createGpgKeyForAuthenticatedUser"] },
				],
				createGpgKeyForAuthenticatedUser: ["POST /user/gpg_keys"],
				createPublicSshKeyForAuthenticated: [
					"POST /user/keys",
					{},
					{ renamed: ["users", "createPublicSshKeyForAuthenticatedUser"] },
				],
				createPublicSshKeyForAuthenticatedUser: ["POST /user/keys"],
				createSshSigningKeyForAuthenticatedUser: ["POST /user/ssh_signing_keys"],
				deleteEmailForAuthenticated: [
					"DELETE /user/emails",
					{},
					{ renamed: ["users", "deleteEmailForAuthenticatedUser"] },
				],
				deleteEmailForAuthenticatedUser: ["DELETE /user/emails"],
				deleteGpgKeyForAuthenticated: [
					"DELETE /user/gpg_keys/{gpg_key_id}",
					{},
					{ renamed: ["users", "deleteGpgKeyForAuthenticatedUser"] },
				],
				deleteGpgKeyForAuthenticatedUser: ["DELETE /user/gpg_keys/{gpg_key_id}"],
				deletePublicSshKeyForAuthenticated: [
					"DELETE /user/keys/{key_id}",
					{},
					{ renamed: ["users", "deletePublicSshKeyForAuthenticatedUser"] },
				],
				deletePublicSshKeyForAuthenticatedUser: ["DELETE /user/keys/{key_id}"],
				deleteSocialAccountForAuthenticatedUser: ["DELETE /user/social_accounts"],
				deleteSshSigningKeyForAuthenticatedUser: [
					"DELETE /user/ssh_signing_keys/{ssh_signing_key_id}",
				],
				follow: ["PUT /user/following/{username}"],
				getAuthenticated: ["GET /user"],
				getByUsername: ["GET /users/{username}"],
				getContextForUser: ["GET /users/{username}/hovercard"],
				getGpgKeyForAuthenticated: [
					"GET /user/gpg_keys/{gpg_key_id}",
					{},
					{ renamed: ["users", "getGpgKeyForAuthenticatedUser"] },
				],
				getGpgKeyForAuthenticatedUser: ["GET /user/gpg_keys/{gpg_key_id}"],
				getPublicSshKeyForAuthenticated: [
					"GET /user/keys/{key_id}",
					{},
					{ renamed: ["users", "getPublicSshKeyForAuthenticatedUser"] },
				],
				getPublicSshKeyForAuthenticatedUser: ["GET /user/keys/{key_id}"],
				getSshSigningKeyForAuthenticatedUser: ["GET /user/ssh_signing_keys/{ssh_signing_key_id}"],
				list: ["GET /users"],
				listBlockedByAuthenticated: [
					"GET /user/blocks",
					{},
					{ renamed: ["users", "listBlockedByAuthenticatedUser"] },
				],
				listBlockedByAuthenticatedUser: ["GET /user/blocks"],
				listEmailsForAuthenticated: [
					"GET /user/emails",
					{},
					{ renamed: ["users", "listEmailsForAuthenticatedUser"] },
				],
				listEmailsForAuthenticatedUser: ["GET /user/emails"],
				listFollowedByAuthenticated: [
					"GET /user/following",
					{},
					{ renamed: ["users", "listFollowedByAuthenticatedUser"] },
				],
				listFollowedByAuthenticatedUser: ["GET /user/following"],
				listFollowersForAuthenticatedUser: ["GET /user/followers"],
				listFollowersForUser: ["GET /users/{username}/followers"],
				listFollowingForUser: ["GET /users/{username}/following"],
				listGpgKeysForAuthenticated: [
					"GET /user/gpg_keys",
					{},
					{ renamed: ["users", "listGpgKeysForAuthenticatedUser"] },
				],
				listGpgKeysForAuthenticatedUser: ["GET /user/gpg_keys"],
				listGpgKeysForUser: ["GET /users/{username}/gpg_keys"],
				listPublicEmailsForAuthenticated: [
					"GET /user/public_emails",
					{},
					{ renamed: ["users", "listPublicEmailsForAuthenticatedUser"] },
				],
				listPublicEmailsForAuthenticatedUser: ["GET /user/public_emails"],
				listPublicKeysForUser: ["GET /users/{username}/keys"],
				listPublicSshKeysForAuthenticated: [
					"GET /user/keys",
					{},
					{ renamed: ["users", "listPublicSshKeysForAuthenticatedUser"] },
				],
				listPublicSshKeysForAuthenticatedUser: ["GET /user/keys"],
				listSocialAccountsForAuthenticatedUser: ["GET /user/social_accounts"],
				listSocialAccountsForUser: ["GET /users/{username}/social_accounts"],
				listSshSigningKeysForAuthenticatedUser: ["GET /user/ssh_signing_keys"],
				listSshSigningKeysForUser: ["GET /users/{username}/ssh_signing_keys"],
				setPrimaryEmailVisibilityForAuthenticated: [
					"PATCH /user/email/visibility",
					{},
					{ renamed: ["users", "setPrimaryEmailVisibilityForAuthenticatedUser"] },
				],
				setPrimaryEmailVisibilityForAuthenticatedUser: ["PATCH /user/email/visibility"],
				unblock: ["DELETE /user/blocks/{username}"],
				unfollow: ["DELETE /user/following/{username}"],
				updateAuthenticated: ["PATCH /user"],
			},
		}
		var endpoints_default = Endpoints
		var endpointMethodsMap = /* @__PURE__ */ new Map()
		for (const [scope, endpoints] of Object.entries(endpoints_default)) {
			for (const [methodName, endpoint] of Object.entries(endpoints)) {
				const [route, defaults, decorations] = endpoint
				const [method, url] = route.split(/ /)
				const endpointDefaults = Object.assign(
					{
						method,
						url,
					},
					defaults
				)
				if (!endpointMethodsMap.has(scope)) {
					endpointMethodsMap.set(scope, /* @__PURE__ */ new Map())
				}
				endpointMethodsMap.get(scope).set(methodName, {
					scope,
					methodName,
					endpointDefaults,
					decorations,
				})
			}
		}
		var handler = {
			has({ scope }, methodName) {
				return endpointMethodsMap.get(scope).has(methodName)
			},
			getOwnPropertyDescriptor(target, methodName) {
				return {
					value: this.get(target, methodName),
					// ensures method is in the cache
					configurable: true,
					writable: true,
					enumerable: true,
				}
			},
			defineProperty(target, methodName, descriptor) {
				Object.defineProperty(target.cache, methodName, descriptor)
				return true
			},
			deleteProperty(target, methodName) {
				delete target.cache[methodName]
				return true
			},
			ownKeys({ scope }) {
				return [...endpointMethodsMap.get(scope).keys()]
			},
			set(target, methodName, value) {
				return (target.cache[methodName] = value)
			},
			get({ octokit, scope, cache: cache2 }, methodName) {
				if (cache2[methodName]) {
					return cache2[methodName]
				}
				const method = endpointMethodsMap.get(scope).get(methodName)
				if (!method) {
					return void 0
				}
				const { endpointDefaults, decorations } = method
				if (decorations) {
					cache2[methodName] = decorate(octokit, scope, methodName, endpointDefaults, decorations)
				} else {
					cache2[methodName] = octokit.request.defaults(endpointDefaults)
				}
				return cache2[methodName]
			},
		}
		function endpointsToMethods(octokit) {
			const newMethods = {}
			for (const scope of endpointMethodsMap.keys()) {
				newMethods[scope] = new Proxy({ octokit, scope, cache: {} }, handler)
			}
			return newMethods
		}
		function decorate(octokit, scope, methodName, defaults, decorations) {
			const requestWithDefaults = octokit.request.defaults(defaults)
			function withDecorations(...args) {
				let options = requestWithDefaults.endpoint.merge(...args)
				if (decorations.mapToData) {
					options = Object.assign({}, options, {
						data: options[decorations.mapToData],
						[decorations.mapToData]: void 0,
					})
					return requestWithDefaults(options)
				}
				if (decorations.renamed) {
					const [newScope, newMethodName] = decorations.renamed
					octokit.log.warn(
						`octokit.${scope}.${methodName}() has been renamed to octokit.${newScope}.${newMethodName}()`
					)
				}
				if (decorations.deprecated) {
					octokit.log.warn(decorations.deprecated)
				}
				if (decorations.renamedParameters) {
					const options2 = requestWithDefaults.endpoint.merge(...args)
					for (const [name, alias] of Object.entries(decorations.renamedParameters)) {
						if (name in options2) {
							octokit.log.warn(
								`"${name}" parameter is deprecated for "octokit.${scope}.${methodName}()". Use "${alias}" instead`
							)
							if (!(alias in options2)) {
								options2[alias] = options2[name]
							}
							delete options2[name]
						}
					}
					return requestWithDefaults(options2)
				}
				return requestWithDefaults(...args)
			}
			return Object.assign(withDecorations, requestWithDefaults)
		}
		function restEndpointMethods(octokit) {
			const api = endpointsToMethods(octokit)
			return {
				rest: api,
			}
		}
		restEndpointMethods.VERSION = VERSION
		function legacyRestEndpointMethods(octokit) {
			const api = endpointsToMethods(octokit)
			return {
				...api,
				rest: api,
			}
		}
		legacyRestEndpointMethods.VERSION = VERSION
	},
})

// ../../../node_modules/.pnpm/@octokit+plugin-paginate-rest@9.2.1_@octokit+core@5.2.0/node_modules/@octokit/plugin-paginate-rest/dist-node/index.js
var require_dist_node10 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+plugin-paginate-rest@9.2.1_@octokit+core@5.2.0/node_modules/@octokit/plugin-paginate-rest/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			composePaginateRest: () => composePaginateRest,
			isPaginatingEndpoint: () => isPaginatingEndpoint,
			paginateRest: () => paginateRest,
			paginatingEndpoints: () => paginatingEndpoints,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var VERSION = "9.2.1"
		function normalizePaginatedListResponse(response) {
			if (!response.data) {
				return {
					...response,
					data: [],
				}
			}
			const responseNeedsNormalization = "total_count" in response.data && !("url" in response.data)
			if (!responseNeedsNormalization) return response
			const incompleteResults = response.data.incomplete_results
			const repositorySelection = response.data.repository_selection
			const totalCount = response.data.total_count
			delete response.data.incomplete_results
			delete response.data.repository_selection
			delete response.data.total_count
			const namespaceKey = Object.keys(response.data)[0]
			const data = response.data[namespaceKey]
			response.data = data
			if (typeof incompleteResults !== "undefined") {
				response.data.incomplete_results = incompleteResults
			}
			if (typeof repositorySelection !== "undefined") {
				response.data.repository_selection = repositorySelection
			}
			response.data.total_count = totalCount
			return response
		}
		function iterator(octokit, route, parameters) {
			const options =
				typeof route === "function"
					? route.endpoint(parameters)
					: octokit.request.endpoint(route, parameters)
			const requestMethod = typeof route === "function" ? route : octokit.request
			const method = options.method
			const headers = options.headers
			let url = options.url
			return {
				[Symbol.asyncIterator]: () => ({
					async next() {
						if (!url) return { done: true }
						try {
							const response = await requestMethod({ method, url, headers })
							const normalizedResponse = normalizePaginatedListResponse(response)
							url = ((normalizedResponse.headers.link || "").match(/<([^>]+)>;\s*rel="next"/) ||
								[])[1]
							return { value: normalizedResponse }
						} catch (error) {
							if (error.status !== 409) throw error
							url = ""
							return {
								value: {
									status: 200,
									headers: {},
									data: [],
								},
							}
						}
					},
				}),
			}
		}
		function paginate(octokit, route, parameters, mapFn) {
			if (typeof parameters === "function") {
				mapFn = parameters
				parameters = void 0
			}
			return gather(
				octokit,
				[],
				iterator(octokit, route, parameters)[Symbol.asyncIterator](),
				mapFn
			)
		}
		function gather(octokit, results, iterator2, mapFn) {
			return iterator2.next().then((result) => {
				if (result.done) {
					return results
				}
				let earlyExit = false
				function done() {
					earlyExit = true
				}
				results = results.concat(mapFn ? mapFn(result.value, done) : result.value.data)
				if (earlyExit) {
					return results
				}
				return gather(octokit, results, iterator2, mapFn)
			})
		}
		var composePaginateRest = Object.assign(paginate, {
			iterator,
		})
		var paginatingEndpoints = [
			"GET /advisories",
			"GET /app/hook/deliveries",
			"GET /app/installation-requests",
			"GET /app/installations",
			"GET /assignments/{assignment_id}/accepted_assignments",
			"GET /classrooms",
			"GET /classrooms/{classroom_id}/assignments",
			"GET /enterprises/{enterprise}/dependabot/alerts",
			"GET /enterprises/{enterprise}/secret-scanning/alerts",
			"GET /events",
			"GET /gists",
			"GET /gists/public",
			"GET /gists/starred",
			"GET /gists/{gist_id}/comments",
			"GET /gists/{gist_id}/commits",
			"GET /gists/{gist_id}/forks",
			"GET /installation/repositories",
			"GET /issues",
			"GET /licenses",
			"GET /marketplace_listing/plans",
			"GET /marketplace_listing/plans/{plan_id}/accounts",
			"GET /marketplace_listing/stubbed/plans",
			"GET /marketplace_listing/stubbed/plans/{plan_id}/accounts",
			"GET /networks/{owner}/{repo}/events",
			"GET /notifications",
			"GET /organizations",
			"GET /orgs/{org}/actions/cache/usage-by-repository",
			"GET /orgs/{org}/actions/permissions/repositories",
			"GET /orgs/{org}/actions/runners",
			"GET /orgs/{org}/actions/secrets",
			"GET /orgs/{org}/actions/secrets/{secret_name}/repositories",
			"GET /orgs/{org}/actions/variables",
			"GET /orgs/{org}/actions/variables/{name}/repositories",
			"GET /orgs/{org}/blocks",
			"GET /orgs/{org}/code-scanning/alerts",
			"GET /orgs/{org}/codespaces",
			"GET /orgs/{org}/codespaces/secrets",
			"GET /orgs/{org}/codespaces/secrets/{secret_name}/repositories",
			"GET /orgs/{org}/copilot/billing/seats",
			"GET /orgs/{org}/dependabot/alerts",
			"GET /orgs/{org}/dependabot/secrets",
			"GET /orgs/{org}/dependabot/secrets/{secret_name}/repositories",
			"GET /orgs/{org}/events",
			"GET /orgs/{org}/failed_invitations",
			"GET /orgs/{org}/hooks",
			"GET /orgs/{org}/hooks/{hook_id}/deliveries",
			"GET /orgs/{org}/installations",
			"GET /orgs/{org}/invitations",
			"GET /orgs/{org}/invitations/{invitation_id}/teams",
			"GET /orgs/{org}/issues",
			"GET /orgs/{org}/members",
			"GET /orgs/{org}/members/{username}/codespaces",
			"GET /orgs/{org}/migrations",
			"GET /orgs/{org}/migrations/{migration_id}/repositories",
			"GET /orgs/{org}/organization-roles/{role_id}/teams",
			"GET /orgs/{org}/organization-roles/{role_id}/users",
			"GET /orgs/{org}/outside_collaborators",
			"GET /orgs/{org}/packages",
			"GET /orgs/{org}/packages/{package_type}/{package_name}/versions",
			"GET /orgs/{org}/personal-access-token-requests",
			"GET /orgs/{org}/personal-access-token-requests/{pat_request_id}/repositories",
			"GET /orgs/{org}/personal-access-tokens",
			"GET /orgs/{org}/personal-access-tokens/{pat_id}/repositories",
			"GET /orgs/{org}/projects",
			"GET /orgs/{org}/properties/values",
			"GET /orgs/{org}/public_members",
			"GET /orgs/{org}/repos",
			"GET /orgs/{org}/rulesets",
			"GET /orgs/{org}/rulesets/rule-suites",
			"GET /orgs/{org}/secret-scanning/alerts",
			"GET /orgs/{org}/security-advisories",
			"GET /orgs/{org}/teams",
			"GET /orgs/{org}/teams/{team_slug}/discussions",
			"GET /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}/comments",
			"GET /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}/comments/{comment_number}/reactions",
			"GET /orgs/{org}/teams/{team_slug}/discussions/{discussion_number}/reactions",
			"GET /orgs/{org}/teams/{team_slug}/invitations",
			"GET /orgs/{org}/teams/{team_slug}/members",
			"GET /orgs/{org}/teams/{team_slug}/projects",
			"GET /orgs/{org}/teams/{team_slug}/repos",
			"GET /orgs/{org}/teams/{team_slug}/teams",
			"GET /projects/columns/{column_id}/cards",
			"GET /projects/{project_id}/collaborators",
			"GET /projects/{project_id}/columns",
			"GET /repos/{owner}/{repo}/actions/artifacts",
			"GET /repos/{owner}/{repo}/actions/caches",
			"GET /repos/{owner}/{repo}/actions/organization-secrets",
			"GET /repos/{owner}/{repo}/actions/organization-variables",
			"GET /repos/{owner}/{repo}/actions/runners",
			"GET /repos/{owner}/{repo}/actions/runs",
			"GET /repos/{owner}/{repo}/actions/runs/{run_id}/artifacts",
			"GET /repos/{owner}/{repo}/actions/runs/{run_id}/attempts/{attempt_number}/jobs",
			"GET /repos/{owner}/{repo}/actions/runs/{run_id}/jobs",
			"GET /repos/{owner}/{repo}/actions/secrets",
			"GET /repos/{owner}/{repo}/actions/variables",
			"GET /repos/{owner}/{repo}/actions/workflows",
			"GET /repos/{owner}/{repo}/actions/workflows/{workflow_id}/runs",
			"GET /repos/{owner}/{repo}/activity",
			"GET /repos/{owner}/{repo}/assignees",
			"GET /repos/{owner}/{repo}/branches",
			"GET /repos/{owner}/{repo}/check-runs/{check_run_id}/annotations",
			"GET /repos/{owner}/{repo}/check-suites/{check_suite_id}/check-runs",
			"GET /repos/{owner}/{repo}/code-scanning/alerts",
			"GET /repos/{owner}/{repo}/code-scanning/alerts/{alert_number}/instances",
			"GET /repos/{owner}/{repo}/code-scanning/analyses",
			"GET /repos/{owner}/{repo}/codespaces",
			"GET /repos/{owner}/{repo}/codespaces/devcontainers",
			"GET /repos/{owner}/{repo}/codespaces/secrets",
			"GET /repos/{owner}/{repo}/collaborators",
			"GET /repos/{owner}/{repo}/comments",
			"GET /repos/{owner}/{repo}/comments/{comment_id}/reactions",
			"GET /repos/{owner}/{repo}/commits",
			"GET /repos/{owner}/{repo}/commits/{commit_sha}/comments",
			"GET /repos/{owner}/{repo}/commits/{commit_sha}/pulls",
			"GET /repos/{owner}/{repo}/commits/{ref}/check-runs",
			"GET /repos/{owner}/{repo}/commits/{ref}/check-suites",
			"GET /repos/{owner}/{repo}/commits/{ref}/status",
			"GET /repos/{owner}/{repo}/commits/{ref}/statuses",
			"GET /repos/{owner}/{repo}/contributors",
			"GET /repos/{owner}/{repo}/dependabot/alerts",
			"GET /repos/{owner}/{repo}/dependabot/secrets",
			"GET /repos/{owner}/{repo}/deployments",
			"GET /repos/{owner}/{repo}/deployments/{deployment_id}/statuses",
			"GET /repos/{owner}/{repo}/environments",
			"GET /repos/{owner}/{repo}/environments/{environment_name}/deployment-branch-policies",
			"GET /repos/{owner}/{repo}/environments/{environment_name}/deployment_protection_rules/apps",
			"GET /repos/{owner}/{repo}/events",
			"GET /repos/{owner}/{repo}/forks",
			"GET /repos/{owner}/{repo}/hooks",
			"GET /repos/{owner}/{repo}/hooks/{hook_id}/deliveries",
			"GET /repos/{owner}/{repo}/invitations",
			"GET /repos/{owner}/{repo}/issues",
			"GET /repos/{owner}/{repo}/issues/comments",
			"GET /repos/{owner}/{repo}/issues/comments/{comment_id}/reactions",
			"GET /repos/{owner}/{repo}/issues/events",
			"GET /repos/{owner}/{repo}/issues/{issue_number}/comments",
			"GET /repos/{owner}/{repo}/issues/{issue_number}/events",
			"GET /repos/{owner}/{repo}/issues/{issue_number}/labels",
			"GET /repos/{owner}/{repo}/issues/{issue_number}/reactions",
			"GET /repos/{owner}/{repo}/issues/{issue_number}/timeline",
			"GET /repos/{owner}/{repo}/keys",
			"GET /repos/{owner}/{repo}/labels",
			"GET /repos/{owner}/{repo}/milestones",
			"GET /repos/{owner}/{repo}/milestones/{milestone_number}/labels",
			"GET /repos/{owner}/{repo}/notifications",
			"GET /repos/{owner}/{repo}/pages/builds",
			"GET /repos/{owner}/{repo}/projects",
			"GET /repos/{owner}/{repo}/pulls",
			"GET /repos/{owner}/{repo}/pulls/comments",
			"GET /repos/{owner}/{repo}/pulls/comments/{comment_id}/reactions",
			"GET /repos/{owner}/{repo}/pulls/{pull_number}/comments",
			"GET /repos/{owner}/{repo}/pulls/{pull_number}/commits",
			"GET /repos/{owner}/{repo}/pulls/{pull_number}/files",
			"GET /repos/{owner}/{repo}/pulls/{pull_number}/reviews",
			"GET /repos/{owner}/{repo}/pulls/{pull_number}/reviews/{review_id}/comments",
			"GET /repos/{owner}/{repo}/releases",
			"GET /repos/{owner}/{repo}/releases/{release_id}/assets",
			"GET /repos/{owner}/{repo}/releases/{release_id}/reactions",
			"GET /repos/{owner}/{repo}/rules/branches/{branch}",
			"GET /repos/{owner}/{repo}/rulesets",
			"GET /repos/{owner}/{repo}/rulesets/rule-suites",
			"GET /repos/{owner}/{repo}/secret-scanning/alerts",
			"GET /repos/{owner}/{repo}/secret-scanning/alerts/{alert_number}/locations",
			"GET /repos/{owner}/{repo}/security-advisories",
			"GET /repos/{owner}/{repo}/stargazers",
			"GET /repos/{owner}/{repo}/subscribers",
			"GET /repos/{owner}/{repo}/tags",
			"GET /repos/{owner}/{repo}/teams",
			"GET /repos/{owner}/{repo}/topics",
			"GET /repositories",
			"GET /repositories/{repository_id}/environments/{environment_name}/secrets",
			"GET /repositories/{repository_id}/environments/{environment_name}/variables",
			"GET /search/code",
			"GET /search/commits",
			"GET /search/issues",
			"GET /search/labels",
			"GET /search/repositories",
			"GET /search/topics",
			"GET /search/users",
			"GET /teams/{team_id}/discussions",
			"GET /teams/{team_id}/discussions/{discussion_number}/comments",
			"GET /teams/{team_id}/discussions/{discussion_number}/comments/{comment_number}/reactions",
			"GET /teams/{team_id}/discussions/{discussion_number}/reactions",
			"GET /teams/{team_id}/invitations",
			"GET /teams/{team_id}/members",
			"GET /teams/{team_id}/projects",
			"GET /teams/{team_id}/repos",
			"GET /teams/{team_id}/teams",
			"GET /user/blocks",
			"GET /user/codespaces",
			"GET /user/codespaces/secrets",
			"GET /user/emails",
			"GET /user/followers",
			"GET /user/following",
			"GET /user/gpg_keys",
			"GET /user/installations",
			"GET /user/installations/{installation_id}/repositories",
			"GET /user/issues",
			"GET /user/keys",
			"GET /user/marketplace_purchases",
			"GET /user/marketplace_purchases/stubbed",
			"GET /user/memberships/orgs",
			"GET /user/migrations",
			"GET /user/migrations/{migration_id}/repositories",
			"GET /user/orgs",
			"GET /user/packages",
			"GET /user/packages/{package_type}/{package_name}/versions",
			"GET /user/public_emails",
			"GET /user/repos",
			"GET /user/repository_invitations",
			"GET /user/social_accounts",
			"GET /user/ssh_signing_keys",
			"GET /user/starred",
			"GET /user/subscriptions",
			"GET /user/teams",
			"GET /users",
			"GET /users/{username}/events",
			"GET /users/{username}/events/orgs/{org}",
			"GET /users/{username}/events/public",
			"GET /users/{username}/followers",
			"GET /users/{username}/following",
			"GET /users/{username}/gists",
			"GET /users/{username}/gpg_keys",
			"GET /users/{username}/keys",
			"GET /users/{username}/orgs",
			"GET /users/{username}/packages",
			"GET /users/{username}/projects",
			"GET /users/{username}/received_events",
			"GET /users/{username}/received_events/public",
			"GET /users/{username}/repos",
			"GET /users/{username}/social_accounts",
			"GET /users/{username}/ssh_signing_keys",
			"GET /users/{username}/starred",
			"GET /users/{username}/subscriptions",
		]
		function isPaginatingEndpoint(arg) {
			if (typeof arg === "string") {
				return paginatingEndpoints.includes(arg)
			} else {
				return false
			}
		}
		function paginateRest(octokit) {
			return {
				paginate: Object.assign(paginate.bind(null, octokit), {
					iterator: iterator.bind(null, octokit),
				}),
			}
		}
		paginateRest.VERSION = VERSION
	},
})

// ../../../node_modules/.pnpm/@actions+github@6.0.0/node_modules/@actions/github/lib/utils.js
var require_utils4 = __commonJS({
	"../../../node_modules/.pnpm/@actions+github@6.0.0/node_modules/@actions/github/lib/utils.js"(
		exports2
	) {
		"use strict"
		var __createBinding =
			(exports2 && exports2.__createBinding) ||
			(Object.create
				? function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						var desc = Object.getOwnPropertyDescriptor(m, k)
						if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
							desc = {
								enumerable: true,
								get: function () {
									return m[k]
								},
							}
						}
						Object.defineProperty(o, k2, desc)
				  }
				: function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						o[k2] = m[k]
				  })
		var __setModuleDefault =
			(exports2 && exports2.__setModuleDefault) ||
			(Object.create
				? function (o, v) {
						Object.defineProperty(o, "default", { enumerable: true, value: v })
				  }
				: function (o, v) {
						o["default"] = v
				  })
		var __importStar =
			(exports2 && exports2.__importStar) ||
			function (mod) {
				if (mod && mod.__esModule) return mod
				var result = {}
				if (mod != undefined) {
					for (var k in mod)
						if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k))
							__createBinding(result, mod, k)
				}
				__setModuleDefault(result, mod)
				return result
			}
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.getOctokitOptions = exports2.GitHub = exports2.defaults = exports2.context = void 0
		var Context = __importStar(require_context())
		var Utils = __importStar(require_utils3())
		var core_1 = require_dist_node8()
		var plugin_rest_endpoint_methods_1 = require_dist_node9()
		var plugin_paginate_rest_1 = require_dist_node10()
		exports2.context = new Context.Context()
		var baseUrl = Utils.getApiBaseUrl()
		exports2.defaults = {
			baseUrl,
			request: {
				agent: Utils.getProxyAgent(baseUrl),
				fetch: Utils.getProxyFetch(baseUrl),
			},
		}
		exports2.GitHub = core_1.Octokit.plugin(
			plugin_rest_endpoint_methods_1.restEndpointMethods,
			plugin_paginate_rest_1.paginateRest
		).defaults(exports2.defaults)
		function getOctokitOptions(token, options) {
			const opts = Object.assign({}, options || {})
			const auth = Utils.getAuthString(token, opts)
			if (auth) {
				opts.auth = auth
			}
			return opts
		}
		exports2.getOctokitOptions = getOctokitOptions
	},
})

// ../../../node_modules/.pnpm/@actions+github@6.0.0/node_modules/@actions/github/lib/github.js
var require_github = __commonJS({
	"../../../node_modules/.pnpm/@actions+github@6.0.0/node_modules/@actions/github/lib/github.js"(
		exports2
	) {
		"use strict"
		var __createBinding =
			(exports2 && exports2.__createBinding) ||
			(Object.create
				? function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						var desc = Object.getOwnPropertyDescriptor(m, k)
						if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
							desc = {
								enumerable: true,
								get: function () {
									return m[k]
								},
							}
						}
						Object.defineProperty(o, k2, desc)
				  }
				: function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						o[k2] = m[k]
				  })
		var __setModuleDefault =
			(exports2 && exports2.__setModuleDefault) ||
			(Object.create
				? function (o, v) {
						Object.defineProperty(o, "default", { enumerable: true, value: v })
				  }
				: function (o, v) {
						o["default"] = v
				  })
		var __importStar =
			(exports2 && exports2.__importStar) ||
			function (mod) {
				if (mod && mod.__esModule) return mod
				var result = {}
				if (mod != undefined) {
					for (var k in mod)
						if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k))
							__createBinding(result, mod, k)
				}
				__setModuleDefault(result, mod)
				return result
			}
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.getOctokit = exports2.context = void 0
		var Context = __importStar(require_context())
		var utils_1 = require_utils4()
		exports2.context = new Context.Context()
		function getOctokit2(token, options, ...additionalPlugins) {
			const GitHubWithPlugins = utils_1.GitHub.plugin(...additionalPlugins)
			return new GitHubWithPlugins((0, utils_1.getOctokitOptions)(token, options))
		}
		exports2.getOctokit = getOctokit2
	},
})

// ../../../node_modules/.pnpm/async-lock@1.4.1/node_modules/async-lock/lib/index.js
var require_lib2 = __commonJS({
	"../../../node_modules/.pnpm/async-lock@1.4.1/node_modules/async-lock/lib/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var AsyncLock2 = function (opts) {
			opts = opts || {}
			this.Promise = opts.Promise || Promise
			this.queues = /* @__PURE__ */ Object.create(null)
			this.domainReentrant = opts.domainReentrant || false
			if (this.domainReentrant) {
				if (typeof process === "undefined" || typeof process.domain === "undefined") {
					throw new Error(
						"Domain-reentrant locks require `process.domain` to exist. Please flip `opts.domainReentrant = false`, use a NodeJS version that still implements Domain, or install a browser polyfill."
					)
				}
				this.domains = /* @__PURE__ */ Object.create(null)
			}
			this.timeout = opts.timeout || AsyncLock2.DEFAULT_TIMEOUT
			this.maxOccupationTime = opts.maxOccupationTime || AsyncLock2.DEFAULT_MAX_OCCUPATION_TIME
			this.maxExecutionTime = opts.maxExecutionTime || AsyncLock2.DEFAULT_MAX_EXECUTION_TIME
			if (
				opts.maxPending === Infinity ||
				(Number.isInteger(opts.maxPending) && opts.maxPending >= 0)
			) {
				this.maxPending = opts.maxPending
			} else {
				this.maxPending = AsyncLock2.DEFAULT_MAX_PENDING
			}
		}
		AsyncLock2.DEFAULT_TIMEOUT = 0
		AsyncLock2.DEFAULT_MAX_OCCUPATION_TIME = 0
		AsyncLock2.DEFAULT_MAX_EXECUTION_TIME = 0
		AsyncLock2.DEFAULT_MAX_PENDING = 1e3
		AsyncLock2.prototype.acquire = function (key, fn, cb, opts) {
			if (Array.isArray(key)) {
				return this._acquireBatch(key, fn, cb, opts)
			}
			if (typeof fn !== "function") {
				throw new Error("You must pass a function to execute")
			}
			var deferredResolve = null
			var deferredReject = null
			var deferred = null
			if (typeof cb !== "function") {
				opts = cb
				cb = null
				deferred = new this.Promise(function (resolve, reject) {
					deferredResolve = resolve
					deferredReject = reject
				})
			}
			opts = opts || {}
			var resolved = false
			var timer = null
			var occupationTimer = null
			var executionTimer = null
			var self2 = this
			var done = function (locked, err, ret) {
				if (occupationTimer) {
					clearTimeout(occupationTimer)
					occupationTimer = null
				}
				if (executionTimer) {
					clearTimeout(executionTimer)
					executionTimer = null
				}
				if (locked) {
					if (!!self2.queues[key] && self2.queues[key].length === 0) {
						delete self2.queues[key]
					}
					if (self2.domainReentrant) {
						delete self2.domains[key]
					}
				}
				if (!resolved) {
					if (!deferred) {
						if (typeof cb === "function") {
							cb(err, ret)
						}
					} else {
						if (err) {
							deferredReject(err)
						} else {
							deferredResolve(ret)
						}
					}
					resolved = true
				}
				if (locked && !!self2.queues[key] && self2.queues[key].length > 0) {
					self2.queues[key].shift()()
				}
			}
			var exec = function (locked) {
				if (resolved) {
					return done(locked)
				}
				if (timer) {
					clearTimeout(timer)
					timer = null
				}
				if (self2.domainReentrant && locked) {
					self2.domains[key] = process.domain
				}
				var maxExecutionTime = opts.maxExecutionTime || self2.maxExecutionTime
				if (maxExecutionTime) {
					executionTimer = setTimeout(function () {
						if (self2.queues[key]) {
							done(locked, new Error("Maximum execution time is exceeded " + key))
						}
					}, maxExecutionTime)
				}
				if (fn.length === 1) {
					var called = false
					try {
						fn(function (err, ret) {
							if (!called) {
								called = true
								done(locked, err, ret)
							}
						})
					} catch (err) {
						if (!called) {
							called = true
							done(locked, err)
						}
					}
				} else {
					self2
						._promiseTry(function () {
							return fn()
						})
						.then(
							function (ret) {
								done(locked, void 0, ret)
							},
							function (error) {
								done(locked, error)
							}
						)
				}
			}
			if (self2.domainReentrant && !!process.domain) {
				exec = process.domain.bind(exec)
			}
			var maxPending = opts.maxPending || self2.maxPending
			if (!self2.queues[key]) {
				self2.queues[key] = []
				exec(true)
			} else if (
				self2.domainReentrant &&
				!!process.domain &&
				process.domain === self2.domains[key]
			) {
				exec(false)
			} else if (self2.queues[key].length >= maxPending) {
				done(false, new Error("Too many pending tasks in queue " + key))
			} else {
				var taskFn = function () {
					exec(true)
				}
				if (opts.skipQueue) {
					self2.queues[key].unshift(taskFn)
				} else {
					self2.queues[key].push(taskFn)
				}
				var timeout = opts.timeout || self2.timeout
				if (timeout) {
					timer = setTimeout(function () {
						timer = null
						done(false, new Error("async-lock timed out in queue " + key))
					}, timeout)
				}
			}
			var maxOccupationTime = opts.maxOccupationTime || self2.maxOccupationTime
			if (maxOccupationTime) {
				occupationTimer = setTimeout(function () {
					if (self2.queues[key]) {
						done(false, new Error("Maximum occupation time is exceeded in queue " + key))
					}
				}, maxOccupationTime)
			}
			if (deferred) {
				return deferred
			}
		}
		AsyncLock2.prototype._acquireBatch = function (keys, fn, cb, opts) {
			if (typeof cb !== "function") {
				opts = cb
				cb = null
			}
			var self2 = this
			var getFn = function (key, fn2) {
				return function (cb2) {
					self2.acquire(key, fn2, cb2, opts)
				}
			}
			var fnx = keys.reduceRight(function (prev, key) {
				return getFn(key, prev)
			}, fn)
			if (typeof cb === "function") {
				fnx(cb)
			} else {
				return new this.Promise(function (resolve, reject) {
					if (fnx.length === 1) {
						fnx(function (err, ret) {
							if (err) {
								reject(err)
							} else {
								resolve(ret)
							}
						})
					} else {
						resolve(fnx())
					}
				})
			}
		}
		AsyncLock2.prototype.isBusy = function (key) {
			if (!key) {
				return Object.keys(this.queues).length > 0
			} else {
				return !!this.queues[key]
			}
		}
		AsyncLock2.prototype._promiseTry = function (fn) {
			try {
				return this.Promise.resolve(fn())
			} catch (e) {
				return this.Promise.reject(e)
			}
		}
		module2.exports = AsyncLock2
	},
})

// ../../../node_modules/.pnpm/async-lock@1.4.1/node_modules/async-lock/index.js
var require_async_lock = __commonJS({
	"../../../node_modules/.pnpm/async-lock@1.4.1/node_modules/async-lock/index.js"(
		exports2,
		module2
	) {
		"use strict"
		module2.exports = require_lib2()
	},
})

// ../../../node_modules/.pnpm/inherits@2.0.4/node_modules/inherits/inherits_browser.js
var require_inherits_browser = __commonJS({
	"../../../node_modules/.pnpm/inherits@2.0.4/node_modules/inherits/inherits_browser.js"(
		exports2,
		module2
	) {
		if (typeof Object.create === "function") {
			module2.exports = function inherits(ctor, superCtor) {
				if (superCtor) {
					ctor.super_ = superCtor
					ctor.prototype = Object.create(superCtor.prototype, {
						constructor: {
							value: ctor,
							enumerable: false,
							writable: true,
							configurable: true,
						},
					})
				}
			}
		} else {
			module2.exports = function inherits(ctor, superCtor) {
				if (superCtor) {
					ctor.super_ = superCtor
					var TempCtor = function () {}
					TempCtor.prototype = superCtor.prototype
					ctor.prototype = new TempCtor()
					ctor.prototype.constructor = ctor
				}
			}
		}
	},
})

// ../../../node_modules/.pnpm/inherits@2.0.4/node_modules/inherits/inherits.js
var require_inherits = __commonJS({
	"../../../node_modules/.pnpm/inherits@2.0.4/node_modules/inherits/inherits.js"(
		exports2,
		module2
	) {
		try {
			util = require("node:util")
			if (typeof util.inherits !== "function") throw ""
			module2.exports = util.inherits
		} catch (e) {
			module2.exports = require_inherits_browser()
		}
		var util
	},
})

// ../../../node_modules/.pnpm/safe-buffer@5.2.1/node_modules/safe-buffer/index.js
var require_safe_buffer = __commonJS({
	"../../../node_modules/.pnpm/safe-buffer@5.2.1/node_modules/safe-buffer/index.js"(
		exports2,
		module2
	) {
		var buffer = require("node:buffer")
		var Buffer2 = buffer.Buffer
		function copyProps(src, dst) {
			for (var key in src) {
				dst[key] = src[key]
			}
		}
		if (Buffer2.from && Buffer2.alloc && Buffer2.allocUnsafe && Buffer2.allocUnsafeSlow) {
			module2.exports = buffer
		} else {
			copyProps(buffer, exports2)
			exports2.Buffer = SafeBuffer
		}
		function SafeBuffer(arg, encodingOrOffset, length) {
			return Buffer2(arg, encodingOrOffset, length)
		}
		SafeBuffer.prototype = Object.create(Buffer2.prototype)
		copyProps(Buffer2, SafeBuffer)
		SafeBuffer.from = function (arg, encodingOrOffset, length) {
			if (typeof arg === "number") {
				throw new TypeError("Argument must not be a number")
			}
			return Buffer2(arg, encodingOrOffset, length)
		}
		SafeBuffer.alloc = function (size, fill, encoding) {
			if (typeof size !== "number") {
				throw new TypeError("Argument must be a number")
			}
			var buf = Buffer2(size)
			if (fill !== void 0) {
				if (typeof encoding === "string") {
					buf.fill(fill, encoding)
				} else {
					buf.fill(fill)
				}
			} else {
				buf.fill(0)
			}
			return buf
		}
		SafeBuffer.allocUnsafe = function (size) {
			if (typeof size !== "number") {
				throw new TypeError("Argument must be a number")
			}
			return Buffer2(size)
		}
		SafeBuffer.allocUnsafeSlow = function (size) {
			if (typeof size !== "number") {
				throw new TypeError("Argument must be a number")
			}
			return buffer.SlowBuffer(size)
		}
	},
})

// ../../../node_modules/.pnpm/sha.js@2.4.11/node_modules/sha.js/hash.js
var require_hash = __commonJS({
	"../../../node_modules/.pnpm/sha.js@2.4.11/node_modules/sha.js/hash.js"(exports2, module2) {
		var Buffer2 = require_safe_buffer().Buffer
		function Hash2(blockSize, finalSize) {
			this._block = Buffer2.alloc(blockSize)
			this._finalSize = finalSize
			this._blockSize = blockSize
			this._len = 0
		}
		Hash2.prototype.update = function (data, enc) {
			if (typeof data === "string") {
				enc = enc || "utf8"
				data = Buffer2.from(data, enc)
			}
			var block = this._block
			var blockSize = this._blockSize
			var length = data.length
			var accum = this._len
			for (var offset = 0; offset < length; ) {
				var assigned = accum % blockSize
				var remainder = Math.min(length - offset, blockSize - assigned)
				for (var i = 0; i < remainder; i++) {
					block[assigned + i] = data[offset + i]
				}
				accum += remainder
				offset += remainder
				if (accum % blockSize === 0) {
					this._update(block)
				}
			}
			this._len += length
			return this
		}
		Hash2.prototype.digest = function (enc) {
			var rem = this._len % this._blockSize
			this._block[rem] = 128
			this._block.fill(0, rem + 1)
			if (rem >= this._finalSize) {
				this._update(this._block)
				this._block.fill(0)
			}
			var bits = this._len * 8
			if (bits <= 4294967295) {
				this._block.writeUInt32BE(bits, this._blockSize - 4)
			} else {
				var lowBits = (bits & 4294967295) >>> 0
				var highBits = (bits - lowBits) / 4294967296
				this._block.writeUInt32BE(highBits, this._blockSize - 8)
				this._block.writeUInt32BE(lowBits, this._blockSize - 4)
			}
			this._update(this._block)
			var hash2 = this._hash()
			return enc ? hash2.toString(enc) : hash2
		}
		Hash2.prototype._update = function () {
			throw new Error("_update must be implemented by subclass")
		}
		module2.exports = Hash2
	},
})

// ../../../node_modules/.pnpm/sha.js@2.4.11/node_modules/sha.js/sha1.js
var require_sha1 = __commonJS({
	"../../../node_modules/.pnpm/sha.js@2.4.11/node_modules/sha.js/sha1.js"(exports2, module2) {
		var inherits = require_inherits()
		var Hash2 = require_hash()
		var Buffer2 = require_safe_buffer().Buffer
		var K = [1518500249, 1859775393, 2400959708 | 0, 3395469782 | 0]
		var W = new Array(80)
		function Sha1() {
			this.init()
			this._w = W
			Hash2.call(this, 64, 56)
		}
		inherits(Sha1, Hash2)
		Sha1.prototype.init = function () {
			this._a = 1732584193
			this._b = 4023233417
			this._c = 2562383102
			this._d = 271733878
			this._e = 3285377520
			return this
		}
		function rotl1(num2) {
			return (num2 << 1) | (num2 >>> 31)
		}
		function rotl5(num2) {
			return (num2 << 5) | (num2 >>> 27)
		}
		function rotl30(num2) {
			return (num2 << 30) | (num2 >>> 2)
		}
		function ft(s, b, c, d) {
			if (s === 0) return (b & c) | (~b & d)
			if (s === 2) return (b & c) | (b & d) | (c & d)
			return b ^ c ^ d
		}
		Sha1.prototype._update = function (M) {
			var W2 = this._w
			var a = this._a | 0
			var b = this._b | 0
			var c = this._c | 0
			var d = this._d | 0
			var e = this._e | 0
			for (var i = 0; i < 16; ++i) W2[i] = M.readInt32BE(i * 4)
			for (; i < 80; ++i) W2[i] = rotl1(W2[i - 3] ^ W2[i - 8] ^ W2[i - 14] ^ W2[i - 16])
			for (var j = 0; j < 80; ++j) {
				var s = ~~(j / 20)
				var t = (rotl5(a) + ft(s, b, c, d) + e + W2[j] + K[s]) | 0
				e = d
				d = c
				c = rotl30(b)
				b = a
				a = t
			}
			this._a = (a + this._a) | 0
			this._b = (b + this._b) | 0
			this._c = (c + this._c) | 0
			this._d = (d + this._d) | 0
			this._e = (e + this._e) | 0
		}
		Sha1.prototype._hash = function () {
			var H = Buffer2.allocUnsafe(20)
			H.writeInt32BE(this._a | 0, 0)
			H.writeInt32BE(this._b | 0, 4)
			H.writeInt32BE(this._c | 0, 8)
			H.writeInt32BE(this._d | 0, 12)
			H.writeInt32BE(this._e | 0, 16)
			return H
		}
		module2.exports = Sha1
	},
})

// ../../../node_modules/.pnpm/crc-32@1.2.2/node_modules/crc-32/crc32.js
var require_crc32 = __commonJS({
	"../../../node_modules/.pnpm/crc-32@1.2.2/node_modules/crc-32/crc32.js"(exports2) {
		var CRC32
		;(function (factory) {
			if (typeof DO_NOT_EXPORT_CRC === "undefined") {
				if ("object" === typeof exports2) {
					factory(exports2)
				} else if ("function" === typeof define && define.amd) {
					define(function () {
						var module3 = {}
						factory(module3)
						return module3
					})
				} else {
					factory((CRC32 = {}))
				}
			} else {
				factory((CRC32 = {}))
			}
		})(function (CRC322) {
			CRC322.version = "1.2.2"
			function signed_crc_table() {
				var c = 0,
					table = new Array(256)
				for (var n = 0; n != 256; ++n) {
					c = n
					c = c & 1 ? -306674912 ^ (c >>> 1) : c >>> 1
					c = c & 1 ? -306674912 ^ (c >>> 1) : c >>> 1
					c = c & 1 ? -306674912 ^ (c >>> 1) : c >>> 1
					c = c & 1 ? -306674912 ^ (c >>> 1) : c >>> 1
					c = c & 1 ? -306674912 ^ (c >>> 1) : c >>> 1
					c = c & 1 ? -306674912 ^ (c >>> 1) : c >>> 1
					c = c & 1 ? -306674912 ^ (c >>> 1) : c >>> 1
					c = c & 1 ? -306674912 ^ (c >>> 1) : c >>> 1
					table[n] = c
				}
				return typeof Int32Array !== "undefined" ? new Int32Array(table) : table
			}
			var T0 = signed_crc_table()
			function slice_by_16_tables(T) {
				var c = 0,
					v = 0,
					n = 0,
					table = typeof Int32Array !== "undefined" ? new Int32Array(4096) : new Array(4096)
				for (n = 0; n != 256; ++n) table[n] = T[n]
				for (n = 0; n != 256; ++n) {
					v = T[n]
					for (c = 256 + n; c < 4096; c += 256) v = table[c] = (v >>> 8) ^ T[v & 255]
				}
				var out = []
				for (n = 1; n != 16; ++n)
					out[n - 1] =
						typeof Int32Array !== "undefined"
							? table.subarray(n * 256, n * 256 + 256)
							: table.slice(n * 256, n * 256 + 256)
				return out
			}
			var TT = slice_by_16_tables(T0)
			var T1 = TT[0],
				T2 = TT[1],
				T3 = TT[2],
				T4 = TT[3],
				T5 = TT[4]
			var T6 = TT[5],
				T7 = TT[6],
				T8 = TT[7],
				T9 = TT[8],
				Ta = TT[9]
			var Tb = TT[10],
				Tc = TT[11],
				Td = TT[12],
				Te = TT[13],
				Tf = TT[14]
			function crc32_bstr(bstr, seed) {
				var C = seed ^ -1
				for (var i = 0, L = bstr.length; i < L; )
					C = (C >>> 8) ^ T0[(C ^ bstr.charCodeAt(i++)) & 255]
				return ~C
			}
			function crc32_buf(B, seed) {
				var C = seed ^ -1,
					L = B.length - 15,
					i = 0
				for (; i < L; )
					C =
						Tf[B[i++] ^ (C & 255)] ^
						Te[B[i++] ^ ((C >> 8) & 255)] ^
						Td[B[i++] ^ ((C >> 16) & 255)] ^
						Tc[B[i++] ^ (C >>> 24)] ^
						Tb[B[i++]] ^
						Ta[B[i++]] ^
						T9[B[i++]] ^
						T8[B[i++]] ^
						T7[B[i++]] ^
						T6[B[i++]] ^
						T5[B[i++]] ^
						T4[B[i++]] ^
						T3[B[i++]] ^
						T2[B[i++]] ^
						T1[B[i++]] ^
						T0[B[i++]]
				L += 15
				while (i < L) C = (C >>> 8) ^ T0[(C ^ B[i++]) & 255]
				return ~C
			}
			function crc32_str(str, seed) {
				var C = seed ^ -1
				for (var i = 0, L = str.length, c = 0, d = 0; i < L; ) {
					c = str.charCodeAt(i++)
					if (c < 128) {
						C = (C >>> 8) ^ T0[(C ^ c) & 255]
					} else if (c < 2048) {
						C = (C >>> 8) ^ T0[(C ^ (192 | ((c >> 6) & 31))) & 255]
						C = (C >>> 8) ^ T0[(C ^ (128 | (c & 63))) & 255]
					} else if (c >= 55296 && c < 57344) {
						c = (c & 1023) + 64
						d = str.charCodeAt(i++) & 1023
						C = (C >>> 8) ^ T0[(C ^ (240 | ((c >> 8) & 7))) & 255]
						C = (C >>> 8) ^ T0[(C ^ (128 | ((c >> 2) & 63))) & 255]
						C = (C >>> 8) ^ T0[(C ^ (128 | ((d >> 6) & 15) | ((c & 3) << 4))) & 255]
						C = (C >>> 8) ^ T0[(C ^ (128 | (d & 63))) & 255]
					} else {
						C = (C >>> 8) ^ T0[(C ^ (224 | ((c >> 12) & 15))) & 255]
						C = (C >>> 8) ^ T0[(C ^ (128 | ((c >> 6) & 63))) & 255]
						C = (C >>> 8) ^ T0[(C ^ (128 | (c & 63))) & 255]
					}
				}
				return ~C
			}
			CRC322.table = T0
			CRC322.bstr = crc32_bstr
			CRC322.buf = crc32_buf
			CRC322.str = crc32_str
		})
	},
})

// ../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/utils/common.js
var require_common = __commonJS({
	"../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/utils/common.js"(exports2) {
		"use strict"
		var TYPED_OK =
			typeof Uint8Array !== "undefined" &&
			typeof Uint16Array !== "undefined" &&
			typeof Int32Array !== "undefined"
		function _has(obj, key) {
			return Object.prototype.hasOwnProperty.call(obj, key)
		}
		exports2.assign = function (obj) {
			var sources = Array.prototype.slice.call(arguments, 1)
			while (sources.length) {
				var source = sources.shift()
				if (!source) {
					continue
				}
				if (typeof source !== "object") {
					throw new TypeError(source + "must be non-object")
				}
				for (var p in source) {
					if (_has(source, p)) {
						obj[p] = source[p]
					}
				}
			}
			return obj
		}
		exports2.shrinkBuf = function (buf, size) {
			if (buf.length === size) {
				return buf
			}
			if (buf.subarray) {
				return buf.subarray(0, size)
			}
			buf.length = size
			return buf
		}
		var fnTyped = {
			arraySet: function (dest, src, src_offs, len, dest_offs) {
				if (src.subarray && dest.subarray) {
					dest.set(src.subarray(src_offs, src_offs + len), dest_offs)
					return
				}
				for (var i = 0; i < len; i++) {
					dest[dest_offs + i] = src[src_offs + i]
				}
			},
			// Join array of chunks to single array.
			flattenChunks: function (chunks) {
				var i, l, len, pos, chunk, result
				len = 0
				for (i = 0, l = chunks.length; i < l; i++) {
					len += chunks[i].length
				}
				result = new Uint8Array(len)
				pos = 0
				for (i = 0, l = chunks.length; i < l; i++) {
					chunk = chunks[i]
					result.set(chunk, pos)
					pos += chunk.length
				}
				return result
			},
		}
		var fnUntyped = {
			arraySet: function (dest, src, src_offs, len, dest_offs) {
				for (var i = 0; i < len; i++) {
					dest[dest_offs + i] = src[src_offs + i]
				}
			},
			// Join array of chunks to single array.
			flattenChunks: function (chunks) {
				return [].concat.apply([], chunks)
			},
		}
		exports2.setTyped = function (on) {
			if (on) {
				exports2.Buf8 = Uint8Array
				exports2.Buf16 = Uint16Array
				exports2.Buf32 = Int32Array
				exports2.assign(exports2, fnTyped)
			} else {
				exports2.Buf8 = Array
				exports2.Buf16 = Array
				exports2.Buf32 = Array
				exports2.assign(exports2, fnUntyped)
			}
		}
		exports2.setTyped(TYPED_OK)
	},
})

// ../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/trees.js
var require_trees = __commonJS({
	"../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/trees.js"(exports2) {
		"use strict"
		var utils = require_common()
		var Z_FIXED = 4
		var Z_BINARY = 0
		var Z_TEXT = 1
		var Z_UNKNOWN = 2
		function zero(buf) {
			var len = buf.length
			while (--len >= 0) {
				buf[len] = 0
			}
		}
		var STORED_BLOCK = 0
		var STATIC_TREES = 1
		var DYN_TREES = 2
		var MIN_MATCH = 3
		var MAX_MATCH = 258
		var LENGTH_CODES = 29
		var LITERALS = 256
		var L_CODES = LITERALS + 1 + LENGTH_CODES
		var D_CODES = 30
		var BL_CODES = 19
		var HEAP_SIZE = 2 * L_CODES + 1
		var MAX_BITS = 15
		var Buf_size = 16
		var MAX_BL_BITS = 7
		var END_BLOCK = 256
		var REP_3_6 = 16
		var REPZ_3_10 = 17
		var REPZ_11_138 = 18
		var extra_lbits =
			/* extra bits for each length code */
			[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 0]
		var extra_dbits =
			/* extra bits for each distance code */
			[
				0, 0, 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12,
				13, 13,
			]
		var extra_blbits =
			/* extra bits for each bit length code */
			[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 7]
		var bl_order = [16, 17, 18, 0, 8, 7, 9, 6, 10, 5, 11, 4, 12, 3, 13, 2, 14, 1, 15]
		var DIST_CODE_LEN = 512
		var static_ltree = new Array((L_CODES + 2) * 2)
		zero(static_ltree)
		var static_dtree = new Array(D_CODES * 2)
		zero(static_dtree)
		var _dist_code = new Array(DIST_CODE_LEN)
		zero(_dist_code)
		var _length_code = new Array(MAX_MATCH - MIN_MATCH + 1)
		zero(_length_code)
		var base_length = new Array(LENGTH_CODES)
		zero(base_length)
		var base_dist = new Array(D_CODES)
		zero(base_dist)
		function StaticTreeDesc(static_tree, extra_bits, extra_base, elems, max_length) {
			this.static_tree = static_tree
			this.extra_bits = extra_bits
			this.extra_base = extra_base
			this.elems = elems
			this.max_length = max_length
			this.has_stree = static_tree && static_tree.length
		}
		var static_l_desc
		var static_d_desc
		var static_bl_desc
		function TreeDesc(dyn_tree, stat_desc) {
			this.dyn_tree = dyn_tree
			this.max_code = 0
			this.stat_desc = stat_desc
		}
		function d_code(dist) {
			return dist < 256 ? _dist_code[dist] : _dist_code[256 + (dist >>> 7)]
		}
		function put_short(s, w) {
			s.pending_buf[s.pending++] = w & 255
			s.pending_buf[s.pending++] = (w >>> 8) & 255
		}
		function send_bits(s, value, length) {
			if (s.bi_valid > Buf_size - length) {
				s.bi_buf |= (value << s.bi_valid) & 65535
				put_short(s, s.bi_buf)
				s.bi_buf = value >> (Buf_size - s.bi_valid)
				s.bi_valid += length - Buf_size
			} else {
				s.bi_buf |= (value << s.bi_valid) & 65535
				s.bi_valid += length
			}
		}
		function send_code(s, c, tree) {
			send_bits(
				s,
				tree[c * 2],
				tree[c * 2 + 1]
				/*.Len*/
			)
		}
		function bi_reverse(code, len) {
			var res = 0
			do {
				res |= code & 1
				code >>>= 1
				res <<= 1
			} while (--len > 0)
			return res >>> 1
		}
		function bi_flush(s) {
			if (s.bi_valid === 16) {
				put_short(s, s.bi_buf)
				s.bi_buf = 0
				s.bi_valid = 0
			} else if (s.bi_valid >= 8) {
				s.pending_buf[s.pending++] = s.bi_buf & 255
				s.bi_buf >>= 8
				s.bi_valid -= 8
			}
		}
		function gen_bitlen(s, desc) {
			var tree = desc.dyn_tree
			var max_code = desc.max_code
			var stree = desc.stat_desc.static_tree
			var has_stree = desc.stat_desc.has_stree
			var extra = desc.stat_desc.extra_bits
			var base = desc.stat_desc.extra_base
			var max_length = desc.stat_desc.max_length
			var h
			var n, m
			var bits
			var xbits
			var f
			var overflow = 0
			for (bits = 0; bits <= MAX_BITS; bits++) {
				s.bl_count[bits] = 0
			}
			tree[s.heap[s.heap_max] * 2 + 1] = 0
			for (h = s.heap_max + 1; h < HEAP_SIZE; h++) {
				n = s.heap[h]
				bits = tree[tree[n * 2 + 1] * 2 + 1] + 1
				if (bits > max_length) {
					bits = max_length
					overflow++
				}
				tree[n * 2 + 1] = bits
				if (n > max_code) {
					continue
				}
				s.bl_count[bits]++
				xbits = 0
				if (n >= base) {
					xbits = extra[n - base]
				}
				f = tree[n * 2]
				s.opt_len += f * (bits + xbits)
				if (has_stree) {
					s.static_len += f * (stree[n * 2 + 1] + xbits)
				}
			}
			if (overflow === 0) {
				return
			}
			do {
				bits = max_length - 1
				while (s.bl_count[bits] === 0) {
					bits--
				}
				s.bl_count[bits]--
				s.bl_count[bits + 1] += 2
				s.bl_count[max_length]--
				overflow -= 2
			} while (overflow > 0)
			for (bits = max_length; bits !== 0; bits--) {
				n = s.bl_count[bits]
				while (n !== 0) {
					m = s.heap[--h]
					if (m > max_code) {
						continue
					}
					if (tree[m * 2 + 1] !== bits) {
						s.opt_len += (bits - tree[m * 2 + 1]) * tree[m * 2]
						tree[m * 2 + 1] = bits
					}
					n--
				}
			}
		}
		function gen_codes(tree, max_code, bl_count) {
			var next_code = new Array(MAX_BITS + 1)
			var code = 0
			var bits
			var n
			for (bits = 1; bits <= MAX_BITS; bits++) {
				next_code[bits] = code = (code + bl_count[bits - 1]) << 1
			}
			for (n = 0; n <= max_code; n++) {
				var len = tree[n * 2 + 1]
				if (len === 0) {
					continue
				}
				tree[n * 2] = bi_reverse(next_code[len]++, len)
			}
		}
		function tr_static_init() {
			var n
			var bits
			var length
			var code
			var dist
			var bl_count = new Array(MAX_BITS + 1)
			length = 0
			for (code = 0; code < LENGTH_CODES - 1; code++) {
				base_length[code] = length
				for (n = 0; n < 1 << extra_lbits[code]; n++) {
					_length_code[length++] = code
				}
			}
			_length_code[length - 1] = code
			dist = 0
			for (code = 0; code < 16; code++) {
				base_dist[code] = dist
				for (n = 0; n < 1 << extra_dbits[code]; n++) {
					_dist_code[dist++] = code
				}
			}
			dist >>= 7
			for (; code < D_CODES; code++) {
				base_dist[code] = dist << 7
				for (n = 0; n < 1 << (extra_dbits[code] - 7); n++) {
					_dist_code[256 + dist++] = code
				}
			}
			for (bits = 0; bits <= MAX_BITS; bits++) {
				bl_count[bits] = 0
			}
			n = 0
			while (n <= 143) {
				static_ltree[n * 2 + 1] = 8
				n++
				bl_count[8]++
			}
			while (n <= 255) {
				static_ltree[n * 2 + 1] = 9
				n++
				bl_count[9]++
			}
			while (n <= 279) {
				static_ltree[n * 2 + 1] = 7
				n++
				bl_count[7]++
			}
			while (n <= 287) {
				static_ltree[n * 2 + 1] = 8
				n++
				bl_count[8]++
			}
			gen_codes(static_ltree, L_CODES + 1, bl_count)
			for (n = 0; n < D_CODES; n++) {
				static_dtree[n * 2 + 1] = 5
				static_dtree[n * 2] = bi_reverse(n, 5)
			}
			static_l_desc = new StaticTreeDesc(static_ltree, extra_lbits, LITERALS + 1, L_CODES, MAX_BITS)
			static_d_desc = new StaticTreeDesc(static_dtree, extra_dbits, 0, D_CODES, MAX_BITS)
			static_bl_desc = new StaticTreeDesc(new Array(0), extra_blbits, 0, BL_CODES, MAX_BL_BITS)
		}
		function init_block(s) {
			var n
			for (n = 0; n < L_CODES; n++) {
				s.dyn_ltree[n * 2] = 0
			}
			for (n = 0; n < D_CODES; n++) {
				s.dyn_dtree[n * 2] = 0
			}
			for (n = 0; n < BL_CODES; n++) {
				s.bl_tree[n * 2] = 0
			}
			s.dyn_ltree[END_BLOCK * 2] = 1
			s.opt_len = s.static_len = 0
			s.last_lit = s.matches = 0
		}
		function bi_windup(s) {
			if (s.bi_valid > 8) {
				put_short(s, s.bi_buf)
			} else if (s.bi_valid > 0) {
				s.pending_buf[s.pending++] = s.bi_buf
			}
			s.bi_buf = 0
			s.bi_valid = 0
		}
		function copy_block(s, buf, len, header) {
			bi_windup(s)
			if (header) {
				put_short(s, len)
				put_short(s, ~len)
			}
			utils.arraySet(s.pending_buf, s.window, buf, len, s.pending)
			s.pending += len
		}
		function smaller(tree, n, m, depth) {
			var _n2 = n * 2
			var _m2 = m * 2
			return tree[_n2] < tree[_m2] || (tree[_n2] === tree[_m2] && depth[n] <= depth[m])
		}
		function pqdownheap(s, tree, k) {
			var v = s.heap[k]
			var j = k << 1
			while (j <= s.heap_len) {
				if (j < s.heap_len && smaller(tree, s.heap[j + 1], s.heap[j], s.depth)) {
					j++
				}
				if (smaller(tree, v, s.heap[j], s.depth)) {
					break
				}
				s.heap[k] = s.heap[j]
				k = j
				j <<= 1
			}
			s.heap[k] = v
		}
		function compress_block(s, ltree, dtree) {
			var dist
			var lc
			var lx = 0
			var code
			var extra
			if (s.last_lit !== 0) {
				do {
					dist = (s.pending_buf[s.d_buf + lx * 2] << 8) | s.pending_buf[s.d_buf + lx * 2 + 1]
					lc = s.pending_buf[s.l_buf + lx]
					lx++
					if (dist === 0) {
						send_code(s, lc, ltree)
					} else {
						code = _length_code[lc]
						send_code(s, code + LITERALS + 1, ltree)
						extra = extra_lbits[code]
						if (extra !== 0) {
							lc -= base_length[code]
							send_bits(s, lc, extra)
						}
						dist--
						code = d_code(dist)
						send_code(s, code, dtree)
						extra = extra_dbits[code]
						if (extra !== 0) {
							dist -= base_dist[code]
							send_bits(s, dist, extra)
						}
					}
				} while (lx < s.last_lit)
			}
			send_code(s, END_BLOCK, ltree)
		}
		function build_tree(s, desc) {
			var tree = desc.dyn_tree
			var stree = desc.stat_desc.static_tree
			var has_stree = desc.stat_desc.has_stree
			var elems = desc.stat_desc.elems
			var n, m
			var max_code = -1
			var node
			s.heap_len = 0
			s.heap_max = HEAP_SIZE
			for (n = 0; n < elems; n++) {
				if (tree[n * 2] !== 0) {
					s.heap[++s.heap_len] = max_code = n
					s.depth[n] = 0
				} else {
					tree[n * 2 + 1] = 0
				}
			}
			while (s.heap_len < 2) {
				node = s.heap[++s.heap_len] = max_code < 2 ? ++max_code : 0
				tree[node * 2] = 1
				s.depth[node] = 0
				s.opt_len--
				if (has_stree) {
					s.static_len -= stree[node * 2 + 1]
				}
			}
			desc.max_code = max_code
			for (n = s.heap_len >> 1; n >= 1; n--) {
				pqdownheap(s, tree, n)
			}
			node = elems
			do {
				n =
					s.heap[1]
					/*SMALLEST*/
				s.heap[1] = s.heap[s.heap_len--]
				/*SMALLEST*/
				pqdownheap(
					s,
					tree,
					1
					/*SMALLEST*/
				)
				m =
					s.heap[1]
					/*SMALLEST*/
				s.heap[--s.heap_max] = n
				s.heap[--s.heap_max] = m
				tree[node * 2] = tree[n * 2] + tree[m * 2]
				s.depth[node] = (s.depth[n] >= s.depth[m] ? s.depth[n] : s.depth[m]) + 1
				tree[n * 2 + 1] = tree[m * 2 + 1] = node
				s.heap[1] =
				/*SMALLEST*/
					node++
				pqdownheap(
					s,
					tree,
					1
					/*SMALLEST*/
				)
			} while (s.heap_len >= 2)
			s.heap[--s.heap_max] =
				s.heap[1]
				/*SMALLEST*/
			gen_bitlen(s, desc)
			gen_codes(tree, max_code, s.bl_count)
		}
		function scan_tree(s, tree, max_code) {
			var n
			var prevlen = -1
			var curlen
			var nextlen = tree[0 * 2 + 1]
			var count = 0
			var max_count = 7
			var min_count = 4
			if (nextlen === 0) {
				max_count = 138
				min_count = 3
			}
			tree[(max_code + 1) * 2 + 1] = 65535
			for (n = 0; n <= max_code; n++) {
				curlen = nextlen
				nextlen = tree[(n + 1) * 2 + 1]
				if (++count < max_count && curlen === nextlen) {
					continue
				} else if (count < min_count) {
					s.bl_tree[curlen * 2] += count
				} else if (curlen !== 0) {
					if (curlen !== prevlen) {
						s.bl_tree[curlen * 2]++
					}
					s.bl_tree[REP_3_6 * 2]++
				} else if (count <= 10) {
					s.bl_tree[REPZ_3_10 * 2]++
				} else {
					s.bl_tree[REPZ_11_138 * 2]++
				}
				count = 0
				prevlen = curlen
				if (nextlen === 0) {
					max_count = 138
					min_count = 3
				} else if (curlen === nextlen) {
					max_count = 6
					min_count = 3
				} else {
					max_count = 7
					min_count = 4
				}
			}
		}
		function send_tree(s, tree, max_code) {
			var n
			var prevlen = -1
			var curlen
			var nextlen = tree[0 * 2 + 1]
			var count = 0
			var max_count = 7
			var min_count = 4
			if (nextlen === 0) {
				max_count = 138
				min_count = 3
			}
			for (n = 0; n <= max_code; n++) {
				curlen = nextlen
				nextlen = tree[(n + 1) * 2 + 1]
				if (++count < max_count && curlen === nextlen) {
					continue
				} else if (count < min_count) {
					do {
						send_code(s, curlen, s.bl_tree)
					} while (--count !== 0)
				} else if (curlen !== 0) {
					if (curlen !== prevlen) {
						send_code(s, curlen, s.bl_tree)
						count--
					}
					send_code(s, REP_3_6, s.bl_tree)
					send_bits(s, count - 3, 2)
				} else if (count <= 10) {
					send_code(s, REPZ_3_10, s.bl_tree)
					send_bits(s, count - 3, 3)
				} else {
					send_code(s, REPZ_11_138, s.bl_tree)
					send_bits(s, count - 11, 7)
				}
				count = 0
				prevlen = curlen
				if (nextlen === 0) {
					max_count = 138
					min_count = 3
				} else if (curlen === nextlen) {
					max_count = 6
					min_count = 3
				} else {
					max_count = 7
					min_count = 4
				}
			}
		}
		function build_bl_tree(s) {
			var max_blindex
			scan_tree(s, s.dyn_ltree, s.l_desc.max_code)
			scan_tree(s, s.dyn_dtree, s.d_desc.max_code)
			build_tree(s, s.bl_desc)
			for (max_blindex = BL_CODES - 1; max_blindex >= 3; max_blindex--) {
				if (s.bl_tree[bl_order[max_blindex] * 2 + 1] !== 0) {
					break
				}
			}
			s.opt_len += 3 * (max_blindex + 1) + 5 + 5 + 4
			return max_blindex
		}
		function send_all_trees(s, lcodes, dcodes, blcodes) {
			var rank
			send_bits(s, lcodes - 257, 5)
			send_bits(s, dcodes - 1, 5)
			send_bits(s, blcodes - 4, 4)
			for (rank = 0; rank < blcodes; rank++) {
				send_bits(s, s.bl_tree[bl_order[rank] * 2 + 1], 3)
			}
			send_tree(s, s.dyn_ltree, lcodes - 1)
			send_tree(s, s.dyn_dtree, dcodes - 1)
		}
		function detect_data_type(s) {
			var black_mask = 4093624447
			var n
			for (n = 0; n <= 31; n++, black_mask >>>= 1) {
				if (black_mask & 1 && s.dyn_ltree[n * 2] !== 0) {
					return Z_BINARY
				}
			}
			if (s.dyn_ltree[9 * 2] !== 0 || s.dyn_ltree[10 * 2] !== 0 || s.dyn_ltree[13 * 2] !== 0) {
				return Z_TEXT
			}
			for (n = 32; n < LITERALS; n++) {
				if (s.dyn_ltree[n * 2] !== 0) {
					return Z_TEXT
				}
			}
			return Z_BINARY
		}
		var static_init_done = false
		function _tr_init(s) {
			if (!static_init_done) {
				tr_static_init()
				static_init_done = true
			}
			s.l_desc = new TreeDesc(s.dyn_ltree, static_l_desc)
			s.d_desc = new TreeDesc(s.dyn_dtree, static_d_desc)
			s.bl_desc = new TreeDesc(s.bl_tree, static_bl_desc)
			s.bi_buf = 0
			s.bi_valid = 0
			init_block(s)
		}
		function _tr_stored_block(s, buf, stored_len, last) {
			send_bits(s, (STORED_BLOCK << 1) + (last ? 1 : 0), 3)
			copy_block(s, buf, stored_len, true)
		}
		function _tr_align(s) {
			send_bits(s, STATIC_TREES << 1, 3)
			send_code(s, END_BLOCK, static_ltree)
			bi_flush(s)
		}
		function _tr_flush_block(s, buf, stored_len, last) {
			var opt_lenb, static_lenb
			var max_blindex = 0
			if (s.level > 0) {
				if (s.strm.data_type === Z_UNKNOWN) {
					s.strm.data_type = detect_data_type(s)
				}
				build_tree(s, s.l_desc)
				build_tree(s, s.d_desc)
				max_blindex = build_bl_tree(s)
				opt_lenb = (s.opt_len + 3 + 7) >>> 3
				static_lenb = (s.static_len + 3 + 7) >>> 3
				if (static_lenb <= opt_lenb) {
					opt_lenb = static_lenb
				}
			} else {
				opt_lenb = static_lenb = stored_len + 5
			}
			if (stored_len + 4 <= opt_lenb && buf !== -1) {
				_tr_stored_block(s, buf, stored_len, last)
			} else if (s.strategy === Z_FIXED || static_lenb === opt_lenb) {
				send_bits(s, (STATIC_TREES << 1) + (last ? 1 : 0), 3)
				compress_block(s, static_ltree, static_dtree)
			} else {
				send_bits(s, (DYN_TREES << 1) + (last ? 1 : 0), 3)
				send_all_trees(s, s.l_desc.max_code + 1, s.d_desc.max_code + 1, max_blindex + 1)
				compress_block(s, s.dyn_ltree, s.dyn_dtree)
			}
			init_block(s)
			if (last) {
				bi_windup(s)
			}
		}
		function _tr_tally(s, dist, lc) {
			s.pending_buf[s.d_buf + s.last_lit * 2] = (dist >>> 8) & 255
			s.pending_buf[s.d_buf + s.last_lit * 2 + 1] = dist & 255
			s.pending_buf[s.l_buf + s.last_lit] = lc & 255
			s.last_lit++
			if (dist === 0) {
				s.dyn_ltree[lc * 2]++
			} else {
				s.matches++
				dist--
				s.dyn_ltree[(_length_code[lc] + LITERALS + 1) * 2]++
				s.dyn_dtree[d_code(dist) * 2]++
			}
			return s.last_lit === s.lit_bufsize - 1
		}
		exports2._tr_init = _tr_init
		exports2._tr_stored_block = _tr_stored_block
		exports2._tr_flush_block = _tr_flush_block
		exports2._tr_tally = _tr_tally
		exports2._tr_align = _tr_align
	},
})

// ../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/adler32.js
var require_adler32 = __commonJS({
	"../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/adler32.js"(
		exports2,
		module2
	) {
		"use strict"
		function adler32(adler, buf, len, pos) {
			var s1 = (adler & 65535) | 0,
				s2 = ((adler >>> 16) & 65535) | 0,
				n = 0
			while (len !== 0) {
				n = len > 2e3 ? 2e3 : len
				len -= n
				do {
					s1 = (s1 + buf[pos++]) | 0
					s2 = (s2 + s1) | 0
				} while (--n)
				s1 %= 65521
				s2 %= 65521
			}
			return s1 | (s2 << 16) | 0
		}
		module2.exports = adler32
	},
})

// ../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/crc32.js
var require_crc322 = __commonJS({
	"../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/crc32.js"(exports2, module2) {
		"use strict"
		function makeTable() {
			var c,
				table = []
			for (var n = 0; n < 256; n++) {
				c = n
				for (var k = 0; k < 8; k++) {
					c = c & 1 ? 3988292384 ^ (c >>> 1) : c >>> 1
				}
				table[n] = c
			}
			return table
		}
		var crcTable = makeTable()
		function crc322(crc, buf, len, pos) {
			var t = crcTable,
				end = pos + len
			crc ^= -1
			for (var i = pos; i < end; i++) {
				crc = (crc >>> 8) ^ t[(crc ^ buf[i]) & 255]
			}
			return crc ^ -1
		}
		module2.exports = crc322
	},
})

// ../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/messages.js
var require_messages = __commonJS({
	"../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/messages.js"(
		exports2,
		module2
	) {
		"use strict"
		module2.exports = {
			2: "need dictionary",
			/* Z_NEED_DICT       2  */
			1: "stream end",
			/* Z_STREAM_END      1  */
			0: "",
			/* Z_OK              0  */
			"-1": "file error",
			/* Z_ERRNO         (-1) */
			"-2": "stream error",
			/* Z_STREAM_ERROR  (-2) */
			"-3": "data error",
			/* Z_DATA_ERROR    (-3) */
			"-4": "insufficient memory",
			/* Z_MEM_ERROR     (-4) */
			"-5": "buffer error",
			/* Z_BUF_ERROR     (-5) */
			"-6": "incompatible version",
			/* Z_VERSION_ERROR (-6) */
		}
	},
})

// ../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/deflate.js
var require_deflate = __commonJS({
	"../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/deflate.js"(exports2) {
		"use strict"
		var utils = require_common()
		var trees = require_trees()
		var adler32 = require_adler32()
		var crc322 = require_crc322()
		var msg = require_messages()
		var Z_NO_FLUSH = 0
		var Z_PARTIAL_FLUSH = 1
		var Z_FULL_FLUSH = 3
		var Z_FINISH = 4
		var Z_BLOCK = 5
		var Z_OK = 0
		var Z_STREAM_END = 1
		var Z_STREAM_ERROR = -2
		var Z_DATA_ERROR = -3
		var Z_BUF_ERROR = -5
		var Z_DEFAULT_COMPRESSION = -1
		var Z_FILTERED = 1
		var Z_HUFFMAN_ONLY = 2
		var Z_RLE = 3
		var Z_FIXED = 4
		var Z_DEFAULT_STRATEGY = 0
		var Z_UNKNOWN = 2
		var Z_DEFLATED = 8
		var MAX_MEM_LEVEL = 9
		var MAX_WBITS = 15
		var DEF_MEM_LEVEL = 8
		var LENGTH_CODES = 29
		var LITERALS = 256
		var L_CODES = LITERALS + 1 + LENGTH_CODES
		var D_CODES = 30
		var BL_CODES = 19
		var HEAP_SIZE = 2 * L_CODES + 1
		var MAX_BITS = 15
		var MIN_MATCH = 3
		var MAX_MATCH = 258
		var MIN_LOOKAHEAD = MAX_MATCH + MIN_MATCH + 1
		var PRESET_DICT = 32
		var INIT_STATE = 42
		var EXTRA_STATE = 69
		var NAME_STATE = 73
		var COMMENT_STATE = 91
		var HCRC_STATE = 103
		var BUSY_STATE = 113
		var FINISH_STATE = 666
		var BS_NEED_MORE = 1
		var BS_BLOCK_DONE = 2
		var BS_FINISH_STARTED = 3
		var BS_FINISH_DONE = 4
		var OS_CODE = 3
		function err(strm, errorCode) {
			strm.msg = msg[errorCode]
			return errorCode
		}
		function rank(f) {
			return (f << 1) - (f > 4 ? 9 : 0)
		}
		function zero(buf) {
			var len = buf.length
			while (--len >= 0) {
				buf[len] = 0
			}
		}
		function flush_pending(strm) {
			var s = strm.state
			var len = s.pending
			if (len > strm.avail_out) {
				len = strm.avail_out
			}
			if (len === 0) {
				return
			}
			utils.arraySet(strm.output, s.pending_buf, s.pending_out, len, strm.next_out)
			strm.next_out += len
			s.pending_out += len
			strm.total_out += len
			strm.avail_out -= len
			s.pending -= len
			if (s.pending === 0) {
				s.pending_out = 0
			}
		}
		function flush_block_only(s, last) {
			trees._tr_flush_block(
				s,
				s.block_start >= 0 ? s.block_start : -1,
				s.strstart - s.block_start,
				last
			)
			s.block_start = s.strstart
			flush_pending(s.strm)
		}
		function put_byte(s, b) {
			s.pending_buf[s.pending++] = b
		}
		function putShortMSB(s, b) {
			s.pending_buf[s.pending++] = (b >>> 8) & 255
			s.pending_buf[s.pending++] = b & 255
		}
		function read_buf(strm, buf, start, size) {
			var len = strm.avail_in
			if (len > size) {
				len = size
			}
			if (len === 0) {
				return 0
			}
			strm.avail_in -= len
			utils.arraySet(buf, strm.input, strm.next_in, len, start)
			if (strm.state.wrap === 1) {
				strm.adler = adler32(strm.adler, buf, len, start)
			} else if (strm.state.wrap === 2) {
				strm.adler = crc322(strm.adler, buf, len, start)
			}
			strm.next_in += len
			strm.total_in += len
			return len
		}
		function longest_match(s, cur_match) {
			var chain_length = s.max_chain_length
			var scan = s.strstart
			var match
			var len
			var best_len = s.prev_length
			var nice_match = s.nice_match
			var limit =
				s.strstart > s.w_size - MIN_LOOKAHEAD ? s.strstart - (s.w_size - MIN_LOOKAHEAD) : 0
			var _win = s.window
			var wmask = s.w_mask
			var prev = s.prev
			var strend = s.strstart + MAX_MATCH
			var scan_end1 = _win[scan + best_len - 1]
			var scan_end = _win[scan + best_len]
			if (s.prev_length >= s.good_match) {
				chain_length >>= 2
			}
			if (nice_match > s.lookahead) {
				nice_match = s.lookahead
			}
			do {
				match = cur_match
				if (
					_win[match + best_len] !== scan_end ||
					_win[match + best_len - 1] !== scan_end1 ||
					_win[match] !== _win[scan] ||
					_win[++match] !== _win[scan + 1]
				) {
					continue
				}
				scan += 2
				match++
				do {} while (
					_win[++scan] === _win[++match] &&
					_win[++scan] === _win[++match] &&
					_win[++scan] === _win[++match] &&
					_win[++scan] === _win[++match] &&
					_win[++scan] === _win[++match] &&
					_win[++scan] === _win[++match] &&
					_win[++scan] === _win[++match] &&
					_win[++scan] === _win[++match] &&
					scan < strend
				)
				len = MAX_MATCH - (strend - scan)
				scan = strend - MAX_MATCH
				if (len > best_len) {
					s.match_start = cur_match
					best_len = len
					if (len >= nice_match) {
						break
					}
					scan_end1 = _win[scan + best_len - 1]
					scan_end = _win[scan + best_len]
				}
			} while ((cur_match = prev[cur_match & wmask]) > limit && --chain_length !== 0)
			if (best_len <= s.lookahead) {
				return best_len
			}
			return s.lookahead
		}
		function fill_window(s) {
			var _w_size = s.w_size
			var p, n, m, more, str
			do {
				more = s.window_size - s.lookahead - s.strstart
				if (s.strstart >= _w_size + (_w_size - MIN_LOOKAHEAD)) {
					utils.arraySet(s.window, s.window, _w_size, _w_size, 0)
					s.match_start -= _w_size
					s.strstart -= _w_size
					s.block_start -= _w_size
					n = s.hash_size
					p = n
					do {
						m = s.head[--p]
						s.head[p] = m >= _w_size ? m - _w_size : 0
					} while (--n)
					n = _w_size
					p = n
					do {
						m = s.prev[--p]
						s.prev[p] = m >= _w_size ? m - _w_size : 0
					} while (--n)
					more += _w_size
				}
				if (s.strm.avail_in === 0) {
					break
				}
				n = read_buf(s.strm, s.window, s.strstart + s.lookahead, more)
				s.lookahead += n
				if (s.lookahead + s.insert >= MIN_MATCH) {
					str = s.strstart - s.insert
					s.ins_h = s.window[str]
					s.ins_h = ((s.ins_h << s.hash_shift) ^ s.window[str + 1]) & s.hash_mask
					while (s.insert) {
						s.ins_h = ((s.ins_h << s.hash_shift) ^ s.window[str + MIN_MATCH - 1]) & s.hash_mask
						s.prev[str & s.w_mask] = s.head[s.ins_h]
						s.head[s.ins_h] = str
						str++
						s.insert--
						if (s.lookahead + s.insert < MIN_MATCH) {
							break
						}
					}
				}
			} while (s.lookahead < MIN_LOOKAHEAD && s.strm.avail_in !== 0)
		}
		function deflate_stored(s, flush) {
			var max_block_size = 65535
			if (max_block_size > s.pending_buf_size - 5) {
				max_block_size = s.pending_buf_size - 5
			}
			for (;;) {
				if (s.lookahead <= 1) {
					fill_window(s)
					if (s.lookahead === 0 && flush === Z_NO_FLUSH) {
						return BS_NEED_MORE
					}
					if (s.lookahead === 0) {
						break
					}
				}
				s.strstart += s.lookahead
				s.lookahead = 0
				var max_start = s.block_start + max_block_size
				if (s.strstart === 0 || s.strstart >= max_start) {
					s.lookahead = s.strstart - max_start
					s.strstart = max_start
					flush_block_only(s, false)
					if (s.strm.avail_out === 0) {
						return BS_NEED_MORE
					}
				}
				if (s.strstart - s.block_start >= s.w_size - MIN_LOOKAHEAD) {
					flush_block_only(s, false)
					if (s.strm.avail_out === 0) {
						return BS_NEED_MORE
					}
				}
			}
			s.insert = 0
			if (flush === Z_FINISH) {
				flush_block_only(s, true)
				if (s.strm.avail_out === 0) {
					return BS_FINISH_STARTED
				}
				return BS_FINISH_DONE
			}
			if (s.strstart > s.block_start) {
				flush_block_only(s, false)
				if (s.strm.avail_out === 0) {
					return BS_NEED_MORE
				}
			}
			return BS_NEED_MORE
		}
		function deflate_fast(s, flush) {
			var hash_head
			var bflush
			for (;;) {
				if (s.lookahead < MIN_LOOKAHEAD) {
					fill_window(s)
					if (s.lookahead < MIN_LOOKAHEAD && flush === Z_NO_FLUSH) {
						return BS_NEED_MORE
					}
					if (s.lookahead === 0) {
						break
					}
				}
				hash_head = 0
				if (s.lookahead >= MIN_MATCH) {
					s.ins_h = ((s.ins_h << s.hash_shift) ^ s.window[s.strstart + MIN_MATCH - 1]) & s.hash_mask
					hash_head = s.prev[s.strstart & s.w_mask] = s.head[s.ins_h]
					s.head[s.ins_h] = s.strstart
				}
				if (hash_head !== 0 && s.strstart - hash_head <= s.w_size - MIN_LOOKAHEAD) {
					s.match_length = longest_match(s, hash_head)
				}
				if (s.match_length >= MIN_MATCH) {
					bflush = trees._tr_tally(s, s.strstart - s.match_start, s.match_length - MIN_MATCH)
					s.lookahead -= s.match_length
					if (s.match_length <= s.max_lazy_match && s.lookahead >= MIN_MATCH) {
						s.match_length--
						do {
							s.strstart++
							s.ins_h =
								((s.ins_h << s.hash_shift) ^ s.window[s.strstart + MIN_MATCH - 1]) & s.hash_mask
							hash_head = s.prev[s.strstart & s.w_mask] = s.head[s.ins_h]
							s.head[s.ins_h] = s.strstart
						} while (--s.match_length !== 0)
						s.strstart++
					} else {
						s.strstart += s.match_length
						s.match_length = 0
						s.ins_h = s.window[s.strstart]
						s.ins_h = ((s.ins_h << s.hash_shift) ^ s.window[s.strstart + 1]) & s.hash_mask
					}
				} else {
					bflush = trees._tr_tally(s, 0, s.window[s.strstart])
					s.lookahead--
					s.strstart++
				}
				if (bflush) {
					flush_block_only(s, false)
					if (s.strm.avail_out === 0) {
						return BS_NEED_MORE
					}
				}
			}
			s.insert = s.strstart < MIN_MATCH - 1 ? s.strstart : MIN_MATCH - 1
			if (flush === Z_FINISH) {
				flush_block_only(s, true)
				if (s.strm.avail_out === 0) {
					return BS_FINISH_STARTED
				}
				return BS_FINISH_DONE
			}
			if (s.last_lit) {
				flush_block_only(s, false)
				if (s.strm.avail_out === 0) {
					return BS_NEED_MORE
				}
			}
			return BS_BLOCK_DONE
		}
		function deflate_slow(s, flush) {
			var hash_head
			var bflush
			var max_insert
			for (;;) {
				if (s.lookahead < MIN_LOOKAHEAD) {
					fill_window(s)
					if (s.lookahead < MIN_LOOKAHEAD && flush === Z_NO_FLUSH) {
						return BS_NEED_MORE
					}
					if (s.lookahead === 0) {
						break
					}
				}
				hash_head = 0
				if (s.lookahead >= MIN_MATCH) {
					s.ins_h = ((s.ins_h << s.hash_shift) ^ s.window[s.strstart + MIN_MATCH - 1]) & s.hash_mask
					hash_head = s.prev[s.strstart & s.w_mask] = s.head[s.ins_h]
					s.head[s.ins_h] = s.strstart
				}
				s.prev_length = s.match_length
				s.prev_match = s.match_start
				s.match_length = MIN_MATCH - 1
				if (
					hash_head !== 0 &&
					s.prev_length < s.max_lazy_match &&
					s.strstart - hash_head <= s.w_size - MIN_LOOKAHEAD
				) {
					s.match_length = longest_match(s, hash_head)
					if (
						s.match_length <= 5 &&
						(s.strategy === Z_FILTERED ||
							(s.match_length === MIN_MATCH && s.strstart - s.match_start > 4096))
					) {
						s.match_length = MIN_MATCH - 1
					}
				}
				if (s.prev_length >= MIN_MATCH && s.match_length <= s.prev_length) {
					max_insert = s.strstart + s.lookahead - MIN_MATCH
					bflush = trees._tr_tally(s, s.strstart - 1 - s.prev_match, s.prev_length - MIN_MATCH)
					s.lookahead -= s.prev_length - 1
					s.prev_length -= 2
					do {
						if (++s.strstart <= max_insert) {
							s.ins_h =
								((s.ins_h << s.hash_shift) ^ s.window[s.strstart + MIN_MATCH - 1]) & s.hash_mask
							hash_head = s.prev[s.strstart & s.w_mask] = s.head[s.ins_h]
							s.head[s.ins_h] = s.strstart
						}
					} while (--s.prev_length !== 0)
					s.match_available = 0
					s.match_length = MIN_MATCH - 1
					s.strstart++
					if (bflush) {
						flush_block_only(s, false)
						if (s.strm.avail_out === 0) {
							return BS_NEED_MORE
						}
					}
				} else if (s.match_available) {
					bflush = trees._tr_tally(s, 0, s.window[s.strstart - 1])
					if (bflush) {
						flush_block_only(s, false)
					}
					s.strstart++
					s.lookahead--
					if (s.strm.avail_out === 0) {
						return BS_NEED_MORE
					}
				} else {
					s.match_available = 1
					s.strstart++
					s.lookahead--
				}
			}
			if (s.match_available) {
				bflush = trees._tr_tally(s, 0, s.window[s.strstart - 1])
				s.match_available = 0
			}
			s.insert = s.strstart < MIN_MATCH - 1 ? s.strstart : MIN_MATCH - 1
			if (flush === Z_FINISH) {
				flush_block_only(s, true)
				if (s.strm.avail_out === 0) {
					return BS_FINISH_STARTED
				}
				return BS_FINISH_DONE
			}
			if (s.last_lit) {
				flush_block_only(s, false)
				if (s.strm.avail_out === 0) {
					return BS_NEED_MORE
				}
			}
			return BS_BLOCK_DONE
		}
		function deflate_rle(s, flush) {
			var bflush
			var prev
			var scan, strend
			var _win = s.window
			for (;;) {
				if (s.lookahead <= MAX_MATCH) {
					fill_window(s)
					if (s.lookahead <= MAX_MATCH && flush === Z_NO_FLUSH) {
						return BS_NEED_MORE
					}
					if (s.lookahead === 0) {
						break
					}
				}
				s.match_length = 0
				if (s.lookahead >= MIN_MATCH && s.strstart > 0) {
					scan = s.strstart - 1
					prev = _win[scan]
					if (prev === _win[++scan] && prev === _win[++scan] && prev === _win[++scan]) {
						strend = s.strstart + MAX_MATCH
						do {} while (
							prev === _win[++scan] &&
							prev === _win[++scan] &&
							prev === _win[++scan] &&
							prev === _win[++scan] &&
							prev === _win[++scan] &&
							prev === _win[++scan] &&
							prev === _win[++scan] &&
							prev === _win[++scan] &&
							scan < strend
						)
						s.match_length = MAX_MATCH - (strend - scan)
						if (s.match_length > s.lookahead) {
							s.match_length = s.lookahead
						}
					}
				}
				if (s.match_length >= MIN_MATCH) {
					bflush = trees._tr_tally(s, 1, s.match_length - MIN_MATCH)
					s.lookahead -= s.match_length
					s.strstart += s.match_length
					s.match_length = 0
				} else {
					bflush = trees._tr_tally(s, 0, s.window[s.strstart])
					s.lookahead--
					s.strstart++
				}
				if (bflush) {
					flush_block_only(s, false)
					if (s.strm.avail_out === 0) {
						return BS_NEED_MORE
					}
				}
			}
			s.insert = 0
			if (flush === Z_FINISH) {
				flush_block_only(s, true)
				if (s.strm.avail_out === 0) {
					return BS_FINISH_STARTED
				}
				return BS_FINISH_DONE
			}
			if (s.last_lit) {
				flush_block_only(s, false)
				if (s.strm.avail_out === 0) {
					return BS_NEED_MORE
				}
			}
			return BS_BLOCK_DONE
		}
		function deflate_huff(s, flush) {
			var bflush
			for (;;) {
				if (s.lookahead === 0) {
					fill_window(s)
					if (s.lookahead === 0) {
						if (flush === Z_NO_FLUSH) {
							return BS_NEED_MORE
						}
						break
					}
				}
				s.match_length = 0
				bflush = trees._tr_tally(s, 0, s.window[s.strstart])
				s.lookahead--
				s.strstart++
				if (bflush) {
					flush_block_only(s, false)
					if (s.strm.avail_out === 0) {
						return BS_NEED_MORE
					}
				}
			}
			s.insert = 0
			if (flush === Z_FINISH) {
				flush_block_only(s, true)
				if (s.strm.avail_out === 0) {
					return BS_FINISH_STARTED
				}
				return BS_FINISH_DONE
			}
			if (s.last_lit) {
				flush_block_only(s, false)
				if (s.strm.avail_out === 0) {
					return BS_NEED_MORE
				}
			}
			return BS_BLOCK_DONE
		}
		function Config(good_length, max_lazy, nice_length, max_chain, func) {
			this.good_length = good_length
			this.max_lazy = max_lazy
			this.nice_length = nice_length
			this.max_chain = max_chain
			this.func = func
		}
		var configuration_table
		configuration_table = [
			/*      good lazy nice chain */
			new Config(0, 0, 0, 0, deflate_stored),
			/* 0 store only */
			new Config(4, 4, 8, 4, deflate_fast),
			/* 1 max speed, no lazy matches */
			new Config(4, 5, 16, 8, deflate_fast),
			/* 2 */
			new Config(4, 6, 32, 32, deflate_fast),
			/* 3 */
			new Config(4, 4, 16, 16, deflate_slow),
			/* 4 lazy matches */
			new Config(8, 16, 32, 32, deflate_slow),
			/* 5 */
			new Config(8, 16, 128, 128, deflate_slow),
			/* 6 */
			new Config(8, 32, 128, 256, deflate_slow),
			/* 7 */
			new Config(32, 128, 258, 1024, deflate_slow),
			/* 8 */
			new Config(32, 258, 258, 4096, deflate_slow),
			/* 9 max compression */
		]
		function lm_init(s) {
			s.window_size = 2 * s.w_size
			zero(s.head)
			s.max_lazy_match = configuration_table[s.level].max_lazy
			s.good_match = configuration_table[s.level].good_length
			s.nice_match = configuration_table[s.level].nice_length
			s.max_chain_length = configuration_table[s.level].max_chain
			s.strstart = 0
			s.block_start = 0
			s.lookahead = 0
			s.insert = 0
			s.match_length = s.prev_length = MIN_MATCH - 1
			s.match_available = 0
			s.ins_h = 0
		}
		function DeflateState() {
			this.strm = null
			this.status = 0
			this.pending_buf = null
			this.pending_buf_size = 0
			this.pending_out = 0
			this.pending = 0
			this.wrap = 0
			this.gzhead = null
			this.gzindex = 0
			this.method = Z_DEFLATED
			this.last_flush = -1
			this.w_size = 0
			this.w_bits = 0
			this.w_mask = 0
			this.window = null
			this.window_size = 0
			this.prev = null
			this.head = null
			this.ins_h = 0
			this.hash_size = 0
			this.hash_bits = 0
			this.hash_mask = 0
			this.hash_shift = 0
			this.block_start = 0
			this.match_length = 0
			this.prev_match = 0
			this.match_available = 0
			this.strstart = 0
			this.match_start = 0
			this.lookahead = 0
			this.prev_length = 0
			this.max_chain_length = 0
			this.max_lazy_match = 0
			this.level = 0
			this.strategy = 0
			this.good_match = 0
			this.nice_match = 0
			this.dyn_ltree = new utils.Buf16(HEAP_SIZE * 2)
			this.dyn_dtree = new utils.Buf16((2 * D_CODES + 1) * 2)
			this.bl_tree = new utils.Buf16((2 * BL_CODES + 1) * 2)
			zero(this.dyn_ltree)
			zero(this.dyn_dtree)
			zero(this.bl_tree)
			this.l_desc = null
			this.d_desc = null
			this.bl_desc = null
			this.bl_count = new utils.Buf16(MAX_BITS + 1)
			this.heap = new utils.Buf16(2 * L_CODES + 1)
			zero(this.heap)
			this.heap_len = 0
			this.heap_max = 0
			this.depth = new utils.Buf16(2 * L_CODES + 1)
			zero(this.depth)
			this.l_buf = 0
			this.lit_bufsize = 0
			this.last_lit = 0
			this.d_buf = 0
			this.opt_len = 0
			this.static_len = 0
			this.matches = 0
			this.insert = 0
			this.bi_buf = 0
			this.bi_valid = 0
		}
		function deflateResetKeep(strm) {
			var s
			if (!strm || !strm.state) {
				return err(strm, Z_STREAM_ERROR)
			}
			strm.total_in = strm.total_out = 0
			strm.data_type = Z_UNKNOWN
			s = strm.state
			s.pending = 0
			s.pending_out = 0
			if (s.wrap < 0) {
				s.wrap = -s.wrap
			}
			s.status = s.wrap ? INIT_STATE : BUSY_STATE
			strm.adler = s.wrap === 2 ? 0 : 1
			s.last_flush = Z_NO_FLUSH
			trees._tr_init(s)
			return Z_OK
		}
		function deflateReset(strm) {
			var ret = deflateResetKeep(strm)
			if (ret === Z_OK) {
				lm_init(strm.state)
			}
			return ret
		}
		function deflateSetHeader(strm, head) {
			if (!strm || !strm.state) {
				return Z_STREAM_ERROR
			}
			if (strm.state.wrap !== 2) {
				return Z_STREAM_ERROR
			}
			strm.state.gzhead = head
			return Z_OK
		}
		function deflateInit2(strm, level, method, windowBits, memLevel, strategy) {
			if (!strm) {
				return Z_STREAM_ERROR
			}
			var wrap = 1
			if (level === Z_DEFAULT_COMPRESSION) {
				level = 6
			}
			if (windowBits < 0) {
				wrap = 0
				windowBits = -windowBits
			} else if (windowBits > 15) {
				wrap = 2
				windowBits -= 16
			}
			if (
				memLevel < 1 ||
				memLevel > MAX_MEM_LEVEL ||
				method !== Z_DEFLATED ||
				windowBits < 8 ||
				windowBits > 15 ||
				level < 0 ||
				level > 9 ||
				strategy < 0 ||
				strategy > Z_FIXED
			) {
				return err(strm, Z_STREAM_ERROR)
			}
			if (windowBits === 8) {
				windowBits = 9
			}
			var s = new DeflateState()
			strm.state = s
			s.strm = strm
			s.wrap = wrap
			s.gzhead = null
			s.w_bits = windowBits
			s.w_size = 1 << s.w_bits
			s.w_mask = s.w_size - 1
			s.hash_bits = memLevel + 7
			s.hash_size = 1 << s.hash_bits
			s.hash_mask = s.hash_size - 1
			s.hash_shift = ~~((s.hash_bits + MIN_MATCH - 1) / MIN_MATCH)
			s.window = new utils.Buf8(s.w_size * 2)
			s.head = new utils.Buf16(s.hash_size)
			s.prev = new utils.Buf16(s.w_size)
			s.lit_bufsize = 1 << (memLevel + 6)
			s.pending_buf_size = s.lit_bufsize * 4
			s.pending_buf = new utils.Buf8(s.pending_buf_size)
			s.d_buf = 1 * s.lit_bufsize
			s.l_buf = (1 + 2) * s.lit_bufsize
			s.level = level
			s.strategy = strategy
			s.method = method
			return deflateReset(strm)
		}
		function deflateInit(strm, level) {
			return deflateInit2(strm, level, Z_DEFLATED, MAX_WBITS, DEF_MEM_LEVEL, Z_DEFAULT_STRATEGY)
		}
		function deflate2(strm, flush) {
			var old_flush, s
			var beg, val
			if (!strm || !strm.state || flush > Z_BLOCK || flush < 0) {
				return strm ? err(strm, Z_STREAM_ERROR) : Z_STREAM_ERROR
			}
			s = strm.state
			if (
				!strm.output ||
				(!strm.input && strm.avail_in !== 0) ||
				(s.status === FINISH_STATE && flush !== Z_FINISH)
			) {
				return err(strm, strm.avail_out === 0 ? Z_BUF_ERROR : Z_STREAM_ERROR)
			}
			s.strm = strm
			old_flush = s.last_flush
			s.last_flush = flush
			if (s.status === INIT_STATE) {
				if (s.wrap === 2) {
					strm.adler = 0
					put_byte(s, 31)
					put_byte(s, 139)
					put_byte(s, 8)
					if (!s.gzhead) {
						put_byte(s, 0)
						put_byte(s, 0)
						put_byte(s, 0)
						put_byte(s, 0)
						put_byte(s, 0)
						put_byte(s, s.level === 9 ? 2 : s.strategy >= Z_HUFFMAN_ONLY || s.level < 2 ? 4 : 0)
						put_byte(s, OS_CODE)
						s.status = BUSY_STATE
					} else {
						put_byte(
							s,
							(s.gzhead.text ? 1 : 0) +
								(s.gzhead.hcrc ? 2 : 0) +
								(!s.gzhead.extra ? 0 : 4) +
								(!s.gzhead.name ? 0 : 8) +
								(!s.gzhead.comment ? 0 : 16)
						)
						put_byte(s, s.gzhead.time & 255)
						put_byte(s, (s.gzhead.time >> 8) & 255)
						put_byte(s, (s.gzhead.time >> 16) & 255)
						put_byte(s, (s.gzhead.time >> 24) & 255)
						put_byte(s, s.level === 9 ? 2 : s.strategy >= Z_HUFFMAN_ONLY || s.level < 2 ? 4 : 0)
						put_byte(s, s.gzhead.os & 255)
						if (s.gzhead.extra && s.gzhead.extra.length) {
							put_byte(s, s.gzhead.extra.length & 255)
							put_byte(s, (s.gzhead.extra.length >> 8) & 255)
						}
						if (s.gzhead.hcrc) {
							strm.adler = crc322(strm.adler, s.pending_buf, s.pending, 0)
						}
						s.gzindex = 0
						s.status = EXTRA_STATE
					}
				} else {
					var header = (Z_DEFLATED + ((s.w_bits - 8) << 4)) << 8
					var level_flags = -1
					if (s.strategy >= Z_HUFFMAN_ONLY || s.level < 2) {
						level_flags = 0
					} else if (s.level < 6) {
						level_flags = 1
					} else if (s.level === 6) {
						level_flags = 2
					} else {
						level_flags = 3
					}
					header |= level_flags << 6
					if (s.strstart !== 0) {
						header |= PRESET_DICT
					}
					header += 31 - (header % 31)
					s.status = BUSY_STATE
					putShortMSB(s, header)
					if (s.strstart !== 0) {
						putShortMSB(s, strm.adler >>> 16)
						putShortMSB(s, strm.adler & 65535)
					}
					strm.adler = 1
				}
			}
			if (s.status === EXTRA_STATE) {
				if (s.gzhead.extra) {
					beg = s.pending
					while (s.gzindex < (s.gzhead.extra.length & 65535)) {
						if (s.pending === s.pending_buf_size) {
							if (s.gzhead.hcrc && s.pending > beg) {
								strm.adler = crc322(strm.adler, s.pending_buf, s.pending - beg, beg)
							}
							flush_pending(strm)
							beg = s.pending
							if (s.pending === s.pending_buf_size) {
								break
							}
						}
						put_byte(s, s.gzhead.extra[s.gzindex] & 255)
						s.gzindex++
					}
					if (s.gzhead.hcrc && s.pending > beg) {
						strm.adler = crc322(strm.adler, s.pending_buf, s.pending - beg, beg)
					}
					if (s.gzindex === s.gzhead.extra.length) {
						s.gzindex = 0
						s.status = NAME_STATE
					}
				} else {
					s.status = NAME_STATE
				}
			}
			if (s.status === NAME_STATE) {
				if (s.gzhead.name) {
					beg = s.pending
					do {
						if (s.pending === s.pending_buf_size) {
							if (s.gzhead.hcrc && s.pending > beg) {
								strm.adler = crc322(strm.adler, s.pending_buf, s.pending - beg, beg)
							}
							flush_pending(strm)
							beg = s.pending
							if (s.pending === s.pending_buf_size) {
								val = 1
								break
							}
						}
						if (s.gzindex < s.gzhead.name.length) {
							val = s.gzhead.name.charCodeAt(s.gzindex++) & 255
						} else {
							val = 0
						}
						put_byte(s, val)
					} while (val !== 0)
					if (s.gzhead.hcrc && s.pending > beg) {
						strm.adler = crc322(strm.adler, s.pending_buf, s.pending - beg, beg)
					}
					if (val === 0) {
						s.gzindex = 0
						s.status = COMMENT_STATE
					}
				} else {
					s.status = COMMENT_STATE
				}
			}
			if (s.status === COMMENT_STATE) {
				if (s.gzhead.comment) {
					beg = s.pending
					do {
						if (s.pending === s.pending_buf_size) {
							if (s.gzhead.hcrc && s.pending > beg) {
								strm.adler = crc322(strm.adler, s.pending_buf, s.pending - beg, beg)
							}
							flush_pending(strm)
							beg = s.pending
							if (s.pending === s.pending_buf_size) {
								val = 1
								break
							}
						}
						if (s.gzindex < s.gzhead.comment.length) {
							val = s.gzhead.comment.charCodeAt(s.gzindex++) & 255
						} else {
							val = 0
						}
						put_byte(s, val)
					} while (val !== 0)
					if (s.gzhead.hcrc && s.pending > beg) {
						strm.adler = crc322(strm.adler, s.pending_buf, s.pending - beg, beg)
					}
					if (val === 0) {
						s.status = HCRC_STATE
					}
				} else {
					s.status = HCRC_STATE
				}
			}
			if (s.status === HCRC_STATE) {
				if (s.gzhead.hcrc) {
					if (s.pending + 2 > s.pending_buf_size) {
						flush_pending(strm)
					}
					if (s.pending + 2 <= s.pending_buf_size) {
						put_byte(s, strm.adler & 255)
						put_byte(s, (strm.adler >> 8) & 255)
						strm.adler = 0
						s.status = BUSY_STATE
					}
				} else {
					s.status = BUSY_STATE
				}
			}
			if (s.pending !== 0) {
				flush_pending(strm)
				if (strm.avail_out === 0) {
					s.last_flush = -1
					return Z_OK
				}
			} else if (strm.avail_in === 0 && rank(flush) <= rank(old_flush) && flush !== Z_FINISH) {
				return err(strm, Z_BUF_ERROR)
			}
			if (s.status === FINISH_STATE && strm.avail_in !== 0) {
				return err(strm, Z_BUF_ERROR)
			}
			if (
				strm.avail_in !== 0 ||
				s.lookahead !== 0 ||
				(flush !== Z_NO_FLUSH && s.status !== FINISH_STATE)
			) {
				var bstate =
					s.strategy === Z_HUFFMAN_ONLY
						? deflate_huff(s, flush)
						: s.strategy === Z_RLE
						? deflate_rle(s, flush)
						: configuration_table[s.level].func(s, flush)
				if (bstate === BS_FINISH_STARTED || bstate === BS_FINISH_DONE) {
					s.status = FINISH_STATE
				}
				if (bstate === BS_NEED_MORE || bstate === BS_FINISH_STARTED) {
					if (strm.avail_out === 0) {
						s.last_flush = -1
					}
					return Z_OK
				}
				if (bstate === BS_BLOCK_DONE) {
					if (flush === Z_PARTIAL_FLUSH) {
						trees._tr_align(s)
					} else if (flush !== Z_BLOCK) {
						trees._tr_stored_block(s, 0, 0, false)
						if (flush === Z_FULL_FLUSH) {
							zero(s.head)
							if (s.lookahead === 0) {
								s.strstart = 0
								s.block_start = 0
								s.insert = 0
							}
						}
					}
					flush_pending(strm)
					if (strm.avail_out === 0) {
						s.last_flush = -1
						return Z_OK
					}
				}
			}
			if (flush !== Z_FINISH) {
				return Z_OK
			}
			if (s.wrap <= 0) {
				return Z_STREAM_END
			}
			if (s.wrap === 2) {
				put_byte(s, strm.adler & 255)
				put_byte(s, (strm.adler >> 8) & 255)
				put_byte(s, (strm.adler >> 16) & 255)
				put_byte(s, (strm.adler >> 24) & 255)
				put_byte(s, strm.total_in & 255)
				put_byte(s, (strm.total_in >> 8) & 255)
				put_byte(s, (strm.total_in >> 16) & 255)
				put_byte(s, (strm.total_in >> 24) & 255)
			} else {
				putShortMSB(s, strm.adler >>> 16)
				putShortMSB(s, strm.adler & 65535)
			}
			flush_pending(strm)
			if (s.wrap > 0) {
				s.wrap = -s.wrap
			}
			return s.pending !== 0 ? Z_OK : Z_STREAM_END
		}
		function deflateEnd(strm) {
			var status3
			if (!strm || !strm.state) {
				return Z_STREAM_ERROR
			}
			status3 = strm.state.status
			if (
				status3 !== INIT_STATE &&
				status3 !== EXTRA_STATE &&
				status3 !== NAME_STATE &&
				status3 !== COMMENT_STATE &&
				status3 !== HCRC_STATE &&
				status3 !== BUSY_STATE &&
				status3 !== FINISH_STATE
			) {
				return err(strm, Z_STREAM_ERROR)
			}
			strm.state = null
			return status3 === BUSY_STATE ? err(strm, Z_DATA_ERROR) : Z_OK
		}
		function deflateSetDictionary(strm, dictionary) {
			var dictLength = dictionary.length
			var s
			var str, n
			var wrap
			var avail
			var next
			var input
			var tmpDict
			if (!strm || !strm.state) {
				return Z_STREAM_ERROR
			}
			s = strm.state
			wrap = s.wrap
			if (wrap === 2 || (wrap === 1 && s.status !== INIT_STATE) || s.lookahead) {
				return Z_STREAM_ERROR
			}
			if (wrap === 1) {
				strm.adler = adler32(strm.adler, dictionary, dictLength, 0)
			}
			s.wrap = 0
			if (dictLength >= s.w_size) {
				if (wrap === 0) {
					zero(s.head)
					s.strstart = 0
					s.block_start = 0
					s.insert = 0
				}
				tmpDict = new utils.Buf8(s.w_size)
				utils.arraySet(tmpDict, dictionary, dictLength - s.w_size, s.w_size, 0)
				dictionary = tmpDict
				dictLength = s.w_size
			}
			avail = strm.avail_in
			next = strm.next_in
			input = strm.input
			strm.avail_in = dictLength
			strm.next_in = 0
			strm.input = dictionary
			fill_window(s)
			while (s.lookahead >= MIN_MATCH) {
				str = s.strstart
				n = s.lookahead - (MIN_MATCH - 1)
				do {
					s.ins_h = ((s.ins_h << s.hash_shift) ^ s.window[str + MIN_MATCH - 1]) & s.hash_mask
					s.prev[str & s.w_mask] = s.head[s.ins_h]
					s.head[s.ins_h] = str
					str++
				} while (--n)
				s.strstart = str
				s.lookahead = MIN_MATCH - 1
				fill_window(s)
			}
			s.strstart += s.lookahead
			s.block_start = s.strstart
			s.insert = s.lookahead
			s.lookahead = 0
			s.match_length = s.prev_length = MIN_MATCH - 1
			s.match_available = 0
			strm.next_in = next
			strm.input = input
			strm.avail_in = avail
			s.wrap = wrap
			return Z_OK
		}
		exports2.deflateInit = deflateInit
		exports2.deflateInit2 = deflateInit2
		exports2.deflateReset = deflateReset
		exports2.deflateResetKeep = deflateResetKeep
		exports2.deflateSetHeader = deflateSetHeader
		exports2.deflate = deflate2
		exports2.deflateEnd = deflateEnd
		exports2.deflateSetDictionary = deflateSetDictionary
		exports2.deflateInfo = "pako deflate (from Nodeca project)"
	},
})

// ../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/utils/strings.js
var require_strings = __commonJS({
	"../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/utils/strings.js"(exports2) {
		"use strict"
		var utils = require_common()
		var STR_APPLY_OK = true
		var STR_APPLY_UIA_OK = true
		try {
			String.fromCharCode.apply(null, [0])
		} catch (__) {
			STR_APPLY_OK = false
		}
		try {
			String.fromCharCode.apply(null, new Uint8Array(1))
		} catch (__) {
			STR_APPLY_UIA_OK = false
		}
		var _utf8len = new utils.Buf8(256)
		for (q = 0; q < 256; q++) {
			_utf8len[q] = q >= 252 ? 6 : q >= 248 ? 5 : q >= 240 ? 4 : q >= 224 ? 3 : q >= 192 ? 2 : 1
		}
		var q
		_utf8len[254] = _utf8len[254] = 1
		exports2.string2buf = function (str) {
			var buf,
				c,
				c2,
				m_pos,
				i,
				str_len = str.length,
				buf_len = 0
			for (m_pos = 0; m_pos < str_len; m_pos++) {
				c = str.charCodeAt(m_pos)
				if ((c & 64512) === 55296 && m_pos + 1 < str_len) {
					c2 = str.charCodeAt(m_pos + 1)
					if ((c2 & 64512) === 56320) {
						c = 65536 + ((c - 55296) << 10) + (c2 - 56320)
						m_pos++
					}
				}
				buf_len += c < 128 ? 1 : c < 2048 ? 2 : c < 65536 ? 3 : 4
			}
			buf = new utils.Buf8(buf_len)
			for (i = 0, m_pos = 0; i < buf_len; m_pos++) {
				c = str.charCodeAt(m_pos)
				if ((c & 64512) === 55296 && m_pos + 1 < str_len) {
					c2 = str.charCodeAt(m_pos + 1)
					if ((c2 & 64512) === 56320) {
						c = 65536 + ((c - 55296) << 10) + (c2 - 56320)
						m_pos++
					}
				}
				if (c < 128) {
					buf[i++] = c
				} else if (c < 2048) {
					buf[i++] = 192 | (c >>> 6)
					buf[i++] = 128 | (c & 63)
				} else if (c < 65536) {
					buf[i++] = 224 | (c >>> 12)
					buf[i++] = 128 | ((c >>> 6) & 63)
					buf[i++] = 128 | (c & 63)
				} else {
					buf[i++] = 240 | (c >>> 18)
					buf[i++] = 128 | ((c >>> 12) & 63)
					buf[i++] = 128 | ((c >>> 6) & 63)
					buf[i++] = 128 | (c & 63)
				}
			}
			return buf
		}
		function buf2binstring(buf, len) {
			if (len < 65534 && ((buf.subarray && STR_APPLY_UIA_OK) || (!buf.subarray && STR_APPLY_OK))) {
				return String.fromCharCode.apply(null, utils.shrinkBuf(buf, len))
			}
			var result = ""
			for (var i = 0; i < len; i++) {
				result += String.fromCharCode(buf[i])
			}
			return result
		}
		exports2.buf2binstring = function (buf) {
			return buf2binstring(buf, buf.length)
		}
		exports2.binstring2buf = function (str) {
			var buf = new utils.Buf8(str.length)
			for (var i = 0, len = buf.length; i < len; i++) {
				buf[i] = str.charCodeAt(i)
			}
			return buf
		}
		exports2.buf2string = function (buf, max) {
			var i, out, c, c_len
			var len = max || buf.length
			var utf16buf = new Array(len * 2)
			for (out = 0, i = 0; i < len; ) {
				c = buf[i++]
				if (c < 128) {
					utf16buf[out++] = c
					continue
				}
				c_len = _utf8len[c]
				if (c_len > 4) {
					utf16buf[out++] = 65533
					i += c_len - 1
					continue
				}
				c &= c_len === 2 ? 31 : c_len === 3 ? 15 : 7
				while (c_len > 1 && i < len) {
					c = (c << 6) | (buf[i++] & 63)
					c_len--
				}
				if (c_len > 1) {
					utf16buf[out++] = 65533
					continue
				}
				if (c < 65536) {
					utf16buf[out++] = c
				} else {
					c -= 65536
					utf16buf[out++] = 55296 | ((c >> 10) & 1023)
					utf16buf[out++] = 56320 | (c & 1023)
				}
			}
			return buf2binstring(utf16buf, out)
		}
		exports2.utf8border = function (buf, max) {
			var pos
			max = max || buf.length
			if (max > buf.length) {
				max = buf.length
			}
			pos = max - 1
			while (pos >= 0 && (buf[pos] & 192) === 128) {
				pos--
			}
			if (pos < 0) {
				return max
			}
			if (pos === 0) {
				return max
			}
			return pos + _utf8len[buf[pos]] > max ? pos : max
		}
	},
})

// ../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/zstream.js
var require_zstream = __commonJS({
	"../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/zstream.js"(
		exports2,
		module2
	) {
		"use strict"
		function ZStream() {
			this.input = null
			this.next_in = 0
			this.avail_in = 0
			this.total_in = 0
			this.output = null
			this.next_out = 0
			this.avail_out = 0
			this.total_out = 0
			this.msg = ""
			this.state = null
			this.data_type = 2
			this.adler = 0
		}
		module2.exports = ZStream
	},
})

// ../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/deflate.js
var require_deflate2 = __commonJS({
	"../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/deflate.js"(exports2) {
		"use strict"
		var zlib_deflate = require_deflate()
		var utils = require_common()
		var strings = require_strings()
		var msg = require_messages()
		var ZStream = require_zstream()
		var toString = Object.prototype.toString
		var Z_NO_FLUSH = 0
		var Z_FINISH = 4
		var Z_OK = 0
		var Z_STREAM_END = 1
		var Z_SYNC_FLUSH = 2
		var Z_DEFAULT_COMPRESSION = -1
		var Z_DEFAULT_STRATEGY = 0
		var Z_DEFLATED = 8
		function Deflate(options) {
			if (!(this instanceof Deflate)) return new Deflate(options)
			this.options = utils.assign(
				{
					level: Z_DEFAULT_COMPRESSION,
					method: Z_DEFLATED,
					chunkSize: 16384,
					windowBits: 15,
					memLevel: 8,
					strategy: Z_DEFAULT_STRATEGY,
					to: "",
				},
				options || {}
			)
			var opt = this.options
			if (opt.raw && opt.windowBits > 0) {
				opt.windowBits = -opt.windowBits
			} else if (opt.gzip && opt.windowBits > 0 && opt.windowBits < 16) {
				opt.windowBits += 16
			}
			this.err = 0
			this.msg = ""
			this.ended = false
			this.chunks = []
			this.strm = new ZStream()
			this.strm.avail_out = 0
			var status3 = zlib_deflate.deflateInit2(
				this.strm,
				opt.level,
				opt.method,
				opt.windowBits,
				opt.memLevel,
				opt.strategy
			)
			if (status3 !== Z_OK) {
				throw new Error(msg[status3])
			}
			if (opt.header) {
				zlib_deflate.deflateSetHeader(this.strm, opt.header)
			}
			if (opt.dictionary) {
				var dict
				if (typeof opt.dictionary === "string") {
					dict = strings.string2buf(opt.dictionary)
				} else if (toString.call(opt.dictionary) === "[object ArrayBuffer]") {
					dict = new Uint8Array(opt.dictionary)
				} else {
					dict = opt.dictionary
				}
				status3 = zlib_deflate.deflateSetDictionary(this.strm, dict)
				if (status3 !== Z_OK) {
					throw new Error(msg[status3])
				}
				this._dict_set = true
			}
		}
		Deflate.prototype.push = function (data, mode) {
			var strm = this.strm
			var chunkSize = this.options.chunkSize
			var status3, _mode
			if (this.ended) {
				return false
			}
			_mode = mode === ~~mode ? mode : mode === true ? Z_FINISH : Z_NO_FLUSH
			if (typeof data === "string") {
				strm.input = strings.string2buf(data)
			} else if (toString.call(data) === "[object ArrayBuffer]") {
				strm.input = new Uint8Array(data)
			} else {
				strm.input = data
			}
			strm.next_in = 0
			strm.avail_in = strm.input.length
			do {
				if (strm.avail_out === 0) {
					strm.output = new utils.Buf8(chunkSize)
					strm.next_out = 0
					strm.avail_out = chunkSize
				}
				status3 = zlib_deflate.deflate(strm, _mode)
				if (status3 !== Z_STREAM_END && status3 !== Z_OK) {
					this.onEnd(status3)
					this.ended = true
					return false
				}
				if (
					strm.avail_out === 0 ||
					(strm.avail_in === 0 && (_mode === Z_FINISH || _mode === Z_SYNC_FLUSH))
				) {
					if (this.options.to === "string") {
						this.onData(strings.buf2binstring(utils.shrinkBuf(strm.output, strm.next_out)))
					} else {
						this.onData(utils.shrinkBuf(strm.output, strm.next_out))
					}
				}
			} while ((strm.avail_in > 0 || strm.avail_out === 0) && status3 !== Z_STREAM_END)
			if (_mode === Z_FINISH) {
				status3 = zlib_deflate.deflateEnd(this.strm)
				this.onEnd(status3)
				this.ended = true
				return status3 === Z_OK
			}
			if (_mode === Z_SYNC_FLUSH) {
				this.onEnd(Z_OK)
				strm.avail_out = 0
				return true
			}
			return true
		}
		Deflate.prototype.onData = function (chunk) {
			this.chunks.push(chunk)
		}
		Deflate.prototype.onEnd = function (status3) {
			if (status3 === Z_OK) {
				if (this.options.to === "string") {
					this.result = this.chunks.join("")
				} else {
					this.result = utils.flattenChunks(this.chunks)
				}
			}
			this.chunks = []
			this.err = status3
			this.msg = this.strm.msg
		}
		function deflate2(input, options) {
			var deflator = new Deflate(options)
			deflator.push(input, true)
			if (deflator.err) {
				throw deflator.msg || msg[deflator.err]
			}
			return deflator.result
		}
		function deflateRaw(input, options) {
			options = options || {}
			options.raw = true
			return deflate2(input, options)
		}
		function gzip(input, options) {
			options = options || {}
			options.gzip = true
			return deflate2(input, options)
		}
		exports2.Deflate = Deflate
		exports2.deflate = deflate2
		exports2.deflateRaw = deflateRaw
		exports2.gzip = gzip
	},
})

// ../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/inffast.js
var require_inffast = __commonJS({
	"../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/inffast.js"(
		exports2,
		module2
	) {
		"use strict"
		var BAD = 30
		var TYPE = 12
		module2.exports = function inflate_fast(strm, start) {
			var state
			var _in
			var last
			var _out
			var beg
			var end
			var dmax
			var wsize
			var whave
			var wnext
			var s_window
			var hold
			var bits
			var lcode
			var dcode
			var lmask
			var dmask
			var here
			var op
			var len
			var dist
			var from2
			var from_source
			var input, output
			state = strm.state
			_in = strm.next_in
			input = strm.input
			last = _in + (strm.avail_in - 5)
			_out = strm.next_out
			output = strm.output
			beg = _out - (start - strm.avail_out)
			end = _out + (strm.avail_out - 257)
			dmax = state.dmax
			wsize = state.wsize
			whave = state.whave
			wnext = state.wnext
			s_window = state.window
			hold = state.hold
			bits = state.bits
			lcode = state.lencode
			dcode = state.distcode
			lmask = (1 << state.lenbits) - 1
			dmask = (1 << state.distbits) - 1
			top: do {
				if (bits < 15) {
					hold += input[_in++] << bits
					bits += 8
					hold += input[_in++] << bits
					bits += 8
				}
				here = lcode[hold & lmask]
				dolen: for (;;) {
					op = here >>> 24
					hold >>>= op
					bits -= op
					op = (here >>> 16) & 255
					if (op === 0) {
						output[_out++] = here & 65535
					} else if (op & 16) {
						len = here & 65535
						op &= 15
						if (op) {
							if (bits < op) {
								hold += input[_in++] << bits
								bits += 8
							}
							len += hold & ((1 << op) - 1)
							hold >>>= op
							bits -= op
						}
						if (bits < 15) {
							hold += input[_in++] << bits
							bits += 8
							hold += input[_in++] << bits
							bits += 8
						}
						here = dcode[hold & dmask]
						dodist: for (;;) {
							op = here >>> 24
							hold >>>= op
							bits -= op
							op = (here >>> 16) & 255
							if (op & 16) {
								dist = here & 65535
								op &= 15
								if (bits < op) {
									hold += input[_in++] << bits
									bits += 8
									if (bits < op) {
										hold += input[_in++] << bits
										bits += 8
									}
								}
								dist += hold & ((1 << op) - 1)
								if (dist > dmax) {
									strm.msg = "invalid distance too far back"
									state.mode = BAD
									break top
								}
								hold >>>= op
								bits -= op
								op = _out - beg
								if (dist > op) {
									op = dist - op
									if (op > whave && state.sane) {
										strm.msg = "invalid distance too far back"
										state.mode = BAD
										break top
									}
									from2 = 0
									from_source = s_window
									if (wnext === 0) {
										from2 += wsize - op
										if (op < len) {
											len -= op
											do {
												output[_out++] = s_window[from2++]
											} while (--op)
											from2 = _out - dist
											from_source = output
										}
									} else if (wnext < op) {
										from2 += wsize + wnext - op
										op -= wnext
										if (op < len) {
											len -= op
											do {
												output[_out++] = s_window[from2++]
											} while (--op)
											from2 = 0
											if (wnext < len) {
												op = wnext
												len -= op
												do {
													output[_out++] = s_window[from2++]
												} while (--op)
												from2 = _out - dist
												from_source = output
											}
										}
									} else {
										from2 += wnext - op
										if (op < len) {
											len -= op
											do {
												output[_out++] = s_window[from2++]
											} while (--op)
											from2 = _out - dist
											from_source = output
										}
									}
									while (len > 2) {
										output[_out++] = from_source[from2++]
										output[_out++] = from_source[from2++]
										output[_out++] = from_source[from2++]
										len -= 3
									}
									if (len) {
										output[_out++] = from_source[from2++]
										if (len > 1) {
											output[_out++] = from_source[from2++]
										}
									}
								} else {
									from2 = _out - dist
									do {
										output[_out++] = output[from2++]
										output[_out++] = output[from2++]
										output[_out++] = output[from2++]
										len -= 3
									} while (len > 2)
									if (len) {
										output[_out++] = output[from2++]
										if (len > 1) {
											output[_out++] = output[from2++]
										}
									}
								}
							} else if ((op & 64) === 0) {
								here = dcode[(here & 65535) + (hold & ((1 << op) - 1))]
								continue dodist
							} else {
								strm.msg = "invalid distance code"
								state.mode = BAD
								break top
							}
							break
						}
					} else if ((op & 64) === 0) {
						here = lcode[(here & 65535) + (hold & ((1 << op) - 1))]
						continue dolen
					} else if (op & 32) {
						state.mode = TYPE
						break top
					} else {
						strm.msg = "invalid literal/length code"
						state.mode = BAD
						break top
					}
					break
				}
			} while (_in < last && _out < end)
			len = bits >> 3
			_in -= len
			bits -= len << 3
			hold &= (1 << bits) - 1
			strm.next_in = _in
			strm.next_out = _out
			strm.avail_in = _in < last ? 5 + (last - _in) : 5 - (_in - last)
			strm.avail_out = _out < end ? 257 + (end - _out) : 257 - (_out - end)
			state.hold = hold
			state.bits = bits
			return
		}
	},
})

// ../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/inftrees.js
var require_inftrees = __commonJS({
	"../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/inftrees.js"(
		exports2,
		module2
	) {
		"use strict"
		var utils = require_common()
		var MAXBITS = 15
		var ENOUGH_LENS = 852
		var ENOUGH_DISTS = 592
		var CODES = 0
		var LENS = 1
		var DISTS = 2
		var lbase = [
			/* Length codes 257..285 base */
			3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 19, 23, 27, 31, 35, 43, 51, 59, 67, 83, 99, 115, 131,
			163, 195, 227, 258, 0, 0,
		]
		var lext = [
			/* Length codes 257..285 extra */
			16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19, 19, 20, 20, 20,
			20, 21, 21, 21, 21, 16, 72, 78,
		]
		var dbase = [
			/* Distance codes 0..29 base */
			1, 2, 3, 4, 5, 7, 9, 13, 17, 25, 33, 49, 65, 97, 129, 193, 257, 385, 513, 769, 1025, 1537,
			2049, 3073, 4097, 6145, 8193, 12289, 16385, 24577, 0, 0,
		]
		var dext = [
			/* Distance codes 0..29 extra */
			16, 16, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 22, 22, 23, 23, 24, 24, 25, 25, 26,
			26, 27, 27, 28, 28, 29, 29, 64, 64,
		]
		module2.exports = function inflate_table(
			type,
			lens,
			lens_index,
			codes,
			table,
			table_index,
			work,
			opts
		) {
			var bits = opts.bits
			var len = 0
			var sym = 0
			var min = 0,
				max = 0
			var root = 0
			var curr = 0
			var drop = 0
			var left = 0
			var used = 0
			var huff = 0
			var incr
			var fill
			var low
			var mask
			var next
			var base = null
			var base_index = 0
			var end
			var count = new utils.Buf16(MAXBITS + 1)
			var offs = new utils.Buf16(MAXBITS + 1)
			var extra = null
			var extra_index = 0
			var here_bits, here_op, here_val
			for (len = 0; len <= MAXBITS; len++) {
				count[len] = 0
			}
			for (sym = 0; sym < codes; sym++) {
				count[lens[lens_index + sym]]++
			}
			root = bits
			for (max = MAXBITS; max >= 1; max--) {
				if (count[max] !== 0) {
					break
				}
			}
			if (root > max) {
				root = max
			}
			if (max === 0) {
				table[table_index++] = (1 << 24) | (64 << 16) | 0
				table[table_index++] = (1 << 24) | (64 << 16) | 0
				opts.bits = 1
				return 0
			}
			for (min = 1; min < max; min++) {
				if (count[min] !== 0) {
					break
				}
			}
			if (root < min) {
				root = min
			}
			left = 1
			for (len = 1; len <= MAXBITS; len++) {
				left <<= 1
				left -= count[len]
				if (left < 0) {
					return -1
				}
			}
			if (left > 0 && (type === CODES || max !== 1)) {
				return -1
			}
			offs[1] = 0
			for (len = 1; len < MAXBITS; len++) {
				offs[len + 1] = offs[len] + count[len]
			}
			for (sym = 0; sym < codes; sym++) {
				if (lens[lens_index + sym] !== 0) {
					work[offs[lens[lens_index + sym]]++] = sym
				}
			}
			if (type === CODES) {
				base = extra = work
				end = 19
			} else if (type === LENS) {
				base = lbase
				base_index -= 257
				extra = lext
				extra_index -= 257
				end = 256
			} else {
				base = dbase
				extra = dext
				end = -1
			}
			huff = 0
			sym = 0
			len = min
			next = table_index
			curr = root
			drop = 0
			low = -1
			used = 1 << root
			mask = used - 1
			if ((type === LENS && used > ENOUGH_LENS) || (type === DISTS && used > ENOUGH_DISTS)) {
				return 1
			}
			for (;;) {
				here_bits = len - drop
				if (work[sym] < end) {
					here_op = 0
					here_val = work[sym]
				} else if (work[sym] > end) {
					here_op = extra[extra_index + work[sym]]
					here_val = base[base_index + work[sym]]
				} else {
					here_op = 32 + 64
					here_val = 0
				}
				incr = 1 << (len - drop)
				fill = 1 << curr
				min = fill
				do {
					fill -= incr
					table[next + (huff >> drop) + fill] = (here_bits << 24) | (here_op << 16) | here_val | 0
				} while (fill !== 0)
				incr = 1 << (len - 1)
				while (huff & incr) {
					incr >>= 1
				}
				if (incr !== 0) {
					huff &= incr - 1
					huff += incr
				} else {
					huff = 0
				}
				sym++
				if (--count[len] === 0) {
					if (len === max) {
						break
					}
					len = lens[lens_index + work[sym]]
				}
				if (len > root && (huff & mask) !== low) {
					if (drop === 0) {
						drop = root
					}
					next += min
					curr = len - drop
					left = 1 << curr
					while (curr + drop < max) {
						left -= count[curr + drop]
						if (left <= 0) {
							break
						}
						curr++
						left <<= 1
					}
					used += 1 << curr
					if ((type === LENS && used > ENOUGH_LENS) || (type === DISTS && used > ENOUGH_DISTS)) {
						return 1
					}
					low = huff & mask
					table[low] = (root << 24) | (curr << 16) | (next - table_index) | 0
				}
			}
			if (huff !== 0) {
				table[next + huff] = ((len - drop) << 24) | (64 << 16) | 0
			}
			opts.bits = root
			return 0
		}
	},
})

// ../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/inflate.js
var require_inflate = __commonJS({
	"../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/inflate.js"(exports2) {
		"use strict"
		var utils = require_common()
		var adler32 = require_adler32()
		var crc322 = require_crc322()
		var inflate_fast = require_inffast()
		var inflate_table = require_inftrees()
		var CODES = 0
		var LENS = 1
		var DISTS = 2
		var Z_FINISH = 4
		var Z_BLOCK = 5
		var Z_TREES = 6
		var Z_OK = 0
		var Z_STREAM_END = 1
		var Z_NEED_DICT = 2
		var Z_STREAM_ERROR = -2
		var Z_DATA_ERROR = -3
		var Z_MEM_ERROR = -4
		var Z_BUF_ERROR = -5
		var Z_DEFLATED = 8
		var HEAD = 1
		var FLAGS = 2
		var TIME = 3
		var OS = 4
		var EXLEN = 5
		var EXTRA = 6
		var NAME = 7
		var COMMENT = 8
		var HCRC = 9
		var DICTID = 10
		var DICT = 11
		var TYPE = 12
		var TYPEDO = 13
		var STORED = 14
		var COPY_ = 15
		var COPY = 16
		var TABLE = 17
		var LENLENS = 18
		var CODELENS = 19
		var LEN_ = 20
		var LEN = 21
		var LENEXT = 22
		var DIST = 23
		var DISTEXT = 24
		var MATCH = 25
		var LIT = 26
		var CHECK = 27
		var LENGTH = 28
		var DONE = 29
		var BAD = 30
		var MEM = 31
		var SYNC = 32
		var ENOUGH_LENS = 852
		var ENOUGH_DISTS = 592
		var MAX_WBITS = 15
		var DEF_WBITS = MAX_WBITS
		function zswap32(q) {
			return ((q >>> 24) & 255) + ((q >>> 8) & 65280) + ((q & 65280) << 8) + ((q & 255) << 24)
		}
		function InflateState() {
			this.mode = 0
			this.last = false
			this.wrap = 0
			this.havedict = false
			this.flags = 0
			this.dmax = 0
			this.check = 0
			this.total = 0
			this.head = null
			this.wbits = 0
			this.wsize = 0
			this.whave = 0
			this.wnext = 0
			this.window = null
			this.hold = 0
			this.bits = 0
			this.length = 0
			this.offset = 0
			this.extra = 0
			this.lencode = null
			this.distcode = null
			this.lenbits = 0
			this.distbits = 0
			this.ncode = 0
			this.nlen = 0
			this.ndist = 0
			this.have = 0
			this.next = null
			this.lens = new utils.Buf16(320)
			this.work = new utils.Buf16(288)
			this.lendyn = null
			this.distdyn = null
			this.sane = 0
			this.back = 0
			this.was = 0
		}
		function inflateResetKeep(strm) {
			var state
			if (!strm || !strm.state) {
				return Z_STREAM_ERROR
			}
			state = strm.state
			strm.total_in = strm.total_out = state.total = 0
			strm.msg = ""
			if (state.wrap) {
				strm.adler = state.wrap & 1
			}
			state.mode = HEAD
			state.last = 0
			state.havedict = 0
			state.dmax = 32768
			state.head = null
			state.hold = 0
			state.bits = 0
			state.lencode = state.lendyn = new utils.Buf32(ENOUGH_LENS)
			state.distcode = state.distdyn = new utils.Buf32(ENOUGH_DISTS)
			state.sane = 1
			state.back = -1
			return Z_OK
		}
		function inflateReset(strm) {
			var state
			if (!strm || !strm.state) {
				return Z_STREAM_ERROR
			}
			state = strm.state
			state.wsize = 0
			state.whave = 0
			state.wnext = 0
			return inflateResetKeep(strm)
		}
		function inflateReset2(strm, windowBits) {
			var wrap
			var state
			if (!strm || !strm.state) {
				return Z_STREAM_ERROR
			}
			state = strm.state
			if (windowBits < 0) {
				wrap = 0
				windowBits = -windowBits
			} else {
				wrap = (windowBits >> 4) + 1
				if (windowBits < 48) {
					windowBits &= 15
				}
			}
			if (windowBits && (windowBits < 8 || windowBits > 15)) {
				return Z_STREAM_ERROR
			}
			if (state.window !== null && state.wbits !== windowBits) {
				state.window = null
			}
			state.wrap = wrap
			state.wbits = windowBits
			return inflateReset(strm)
		}
		function inflateInit2(strm, windowBits) {
			var ret
			var state
			if (!strm) {
				return Z_STREAM_ERROR
			}
			state = new InflateState()
			strm.state = state
			state.window = null
			ret = inflateReset2(strm, windowBits)
			if (ret !== Z_OK) {
				strm.state = null
			}
			return ret
		}
		function inflateInit(strm) {
			return inflateInit2(strm, DEF_WBITS)
		}
		var virgin = true
		var lenfix
		var distfix
		function fixedtables(state) {
			if (virgin) {
				var sym
				lenfix = new utils.Buf32(512)
				distfix = new utils.Buf32(32)
				sym = 0
				while (sym < 144) {
					state.lens[sym++] = 8
				}
				while (sym < 256) {
					state.lens[sym++] = 9
				}
				while (sym < 280) {
					state.lens[sym++] = 7
				}
				while (sym < 288) {
					state.lens[sym++] = 8
				}
				inflate_table(LENS, state.lens, 0, 288, lenfix, 0, state.work, { bits: 9 })
				sym = 0
				while (sym < 32) {
					state.lens[sym++] = 5
				}
				inflate_table(DISTS, state.lens, 0, 32, distfix, 0, state.work, { bits: 5 })
				virgin = false
			}
			state.lencode = lenfix
			state.lenbits = 9
			state.distcode = distfix
			state.distbits = 5
		}
		function updatewindow(strm, src, end, copy) {
			var dist
			var state = strm.state
			if (state.window === null) {
				state.wsize = 1 << state.wbits
				state.wnext = 0
				state.whave = 0
				state.window = new utils.Buf8(state.wsize)
			}
			if (copy >= state.wsize) {
				utils.arraySet(state.window, src, end - state.wsize, state.wsize, 0)
				state.wnext = 0
				state.whave = state.wsize
			} else {
				dist = state.wsize - state.wnext
				if (dist > copy) {
					dist = copy
				}
				utils.arraySet(state.window, src, end - copy, dist, state.wnext)
				copy -= dist
				if (copy) {
					utils.arraySet(state.window, src, end - copy, copy, 0)
					state.wnext = copy
					state.whave = state.wsize
				} else {
					state.wnext += dist
					if (state.wnext === state.wsize) {
						state.wnext = 0
					}
					if (state.whave < state.wsize) {
						state.whave += dist
					}
				}
			}
			return 0
		}
		function inflate2(strm, flush) {
			var state
			var input, output
			var next
			var put
			var have, left
			var hold
			var bits
			var _in, _out
			var copy
			var from2
			var from_source
			var here = 0
			var here_bits, here_op, here_val
			var last_bits, last_op, last_val
			var len
			var ret
			var hbuf = new utils.Buf8(4)
			var opts
			var n
			var order =
				/* permutation of code lengths */
				[16, 17, 18, 0, 8, 7, 9, 6, 10, 5, 11, 4, 12, 3, 13, 2, 14, 1, 15]
			if (!strm || !strm.state || !strm.output || (!strm.input && strm.avail_in !== 0)) {
				return Z_STREAM_ERROR
			}
			state = strm.state
			if (state.mode === TYPE) {
				state.mode = TYPEDO
			}
			put = strm.next_out
			output = strm.output
			left = strm.avail_out
			next = strm.next_in
			input = strm.input
			have = strm.avail_in
			hold = state.hold
			bits = state.bits
			_in = have
			_out = left
			ret = Z_OK
			inf_leave: for (;;) {
				switch (state.mode) {
					case HEAD:
						if (state.wrap === 0) {
							state.mode = TYPEDO
							break
						}
						while (bits < 16) {
							if (have === 0) {
								break inf_leave
							}
							have--
							hold += input[next++] << bits
							bits += 8
						}
						if (state.wrap & 2 && hold === 35615) {
							state.check = 0
							hbuf[0] = hold & 255
							hbuf[1] = (hold >>> 8) & 255
							state.check = crc322(state.check, hbuf, 2, 0)
							hold = 0
							bits = 0
							state.mode = FLAGS
							break
						}
						state.flags = 0
						if (state.head) {
							state.head.done = false
						}
						if (
							!(state.wrap & 1) /* check if zlib header allowed */ ||
							(((hold & 255) << 8) + (hold >> 8)) % 31
						) {
							strm.msg = "incorrect header check"
							state.mode = BAD
							break
						}
						if ((hold & 15) !== Z_DEFLATED) {
							strm.msg = "unknown compression method"
							state.mode = BAD
							break
						}
						hold >>>= 4
						bits -= 4
						len = (hold & 15) + 8
						if (state.wbits === 0) {
							state.wbits = len
						} else if (len > state.wbits) {
							strm.msg = "invalid window size"
							state.mode = BAD
							break
						}
						state.dmax = 1 << len
						strm.adler = state.check = 1
						state.mode = hold & 512 ? DICTID : TYPE
						hold = 0
						bits = 0
						break
					case FLAGS:
						while (bits < 16) {
							if (have === 0) {
								break inf_leave
							}
							have--
							hold += input[next++] << bits
							bits += 8
						}
						state.flags = hold
						if ((state.flags & 255) !== Z_DEFLATED) {
							strm.msg = "unknown compression method"
							state.mode = BAD
							break
						}
						if (state.flags & 57344) {
							strm.msg = "unknown header flags set"
							state.mode = BAD
							break
						}
						if (state.head) {
							state.head.text = (hold >> 8) & 1
						}
						if (state.flags & 512) {
							hbuf[0] = hold & 255
							hbuf[1] = (hold >>> 8) & 255
							state.check = crc322(state.check, hbuf, 2, 0)
						}
						hold = 0
						bits = 0
						state.mode = TIME
					case TIME:
						while (bits < 32) {
							if (have === 0) {
								break inf_leave
							}
							have--
							hold += input[next++] << bits
							bits += 8
						}
						if (state.head) {
							state.head.time = hold
						}
						if (state.flags & 512) {
							hbuf[0] = hold & 255
							hbuf[1] = (hold >>> 8) & 255
							hbuf[2] = (hold >>> 16) & 255
							hbuf[3] = (hold >>> 24) & 255
							state.check = crc322(state.check, hbuf, 4, 0)
						}
						hold = 0
						bits = 0
						state.mode = OS
					case OS:
						while (bits < 16) {
							if (have === 0) {
								break inf_leave
							}
							have--
							hold += input[next++] << bits
							bits += 8
						}
						if (state.head) {
							state.head.xflags = hold & 255
							state.head.os = hold >> 8
						}
						if (state.flags & 512) {
							hbuf[0] = hold & 255
							hbuf[1] = (hold >>> 8) & 255
							state.check = crc322(state.check, hbuf, 2, 0)
						}
						hold = 0
						bits = 0
						state.mode = EXLEN
					case EXLEN:
						if (state.flags & 1024) {
							while (bits < 16) {
								if (have === 0) {
									break inf_leave
								}
								have--
								hold += input[next++] << bits
								bits += 8
							}
							state.length = hold
							if (state.head) {
								state.head.extra_len = hold
							}
							if (state.flags & 512) {
								hbuf[0] = hold & 255
								hbuf[1] = (hold >>> 8) & 255
								state.check = crc322(state.check, hbuf, 2, 0)
							}
							hold = 0
							bits = 0
						} else if (state.head) {
							state.head.extra = null
						}
						state.mode = EXTRA
					case EXTRA:
						if (state.flags & 1024) {
							copy = state.length
							if (copy > have) {
								copy = have
							}
							if (copy) {
								if (state.head) {
									len = state.head.extra_len - state.length
									if (!state.head.extra) {
										state.head.extra = new Array(state.head.extra_len)
									}
									utils.arraySet(
										state.head.extra,
										input,
										next,
										// extra field is limited to 65536 bytes
										// - no need for additional size check
										copy,
										/*len + copy > state.head.extra_max - len ? state.head.extra_max : copy,*/
										len
									)
								}
								if (state.flags & 512) {
									state.check = crc322(state.check, input, copy, next)
								}
								have -= copy
								next += copy
								state.length -= copy
							}
							if (state.length) {
								break inf_leave
							}
						}
						state.length = 0
						state.mode = NAME
					case NAME:
						if (state.flags & 2048) {
							if (have === 0) {
								break inf_leave
							}
							copy = 0
							do {
								len = input[next + copy++]
								if (state.head && len && state.length < 65536) {
									state.head.name += String.fromCharCode(len)
								}
							} while (len && copy < have)
							if (state.flags & 512) {
								state.check = crc322(state.check, input, copy, next)
							}
							have -= copy
							next += copy
							if (len) {
								break inf_leave
							}
						} else if (state.head) {
							state.head.name = null
						}
						state.length = 0
						state.mode = COMMENT
					case COMMENT:
						if (state.flags & 4096) {
							if (have === 0) {
								break inf_leave
							}
							copy = 0
							do {
								len = input[next + copy++]
								if (state.head && len && state.length < 65536) {
									state.head.comment += String.fromCharCode(len)
								}
							} while (len && copy < have)
							if (state.flags & 512) {
								state.check = crc322(state.check, input, copy, next)
							}
							have -= copy
							next += copy
							if (len) {
								break inf_leave
							}
						} else if (state.head) {
							state.head.comment = null
						}
						state.mode = HCRC
					case HCRC:
						if (state.flags & 512) {
							while (bits < 16) {
								if (have === 0) {
									break inf_leave
								}
								have--
								hold += input[next++] << bits
								bits += 8
							}
							if (hold !== (state.check & 65535)) {
								strm.msg = "header crc mismatch"
								state.mode = BAD
								break
							}
							hold = 0
							bits = 0
						}
						if (state.head) {
							state.head.hcrc = (state.flags >> 9) & 1
							state.head.done = true
						}
						strm.adler = state.check = 0
						state.mode = TYPE
						break
					case DICTID:
						while (bits < 32) {
							if (have === 0) {
								break inf_leave
							}
							have--
							hold += input[next++] << bits
							bits += 8
						}
						strm.adler = state.check = zswap32(hold)
						hold = 0
						bits = 0
						state.mode = DICT
					case DICT:
						if (state.havedict === 0) {
							strm.next_out = put
							strm.avail_out = left
							strm.next_in = next
							strm.avail_in = have
							state.hold = hold
							state.bits = bits
							return Z_NEED_DICT
						}
						strm.adler = state.check = 1
						state.mode = TYPE
					case TYPE:
						if (flush === Z_BLOCK || flush === Z_TREES) {
							break inf_leave
						}
					case TYPEDO:
						if (state.last) {
							hold >>>= bits & 7
							bits -= bits & 7
							state.mode = CHECK
							break
						}
						while (bits < 3) {
							if (have === 0) {
								break inf_leave
							}
							have--
							hold += input[next++] << bits
							bits += 8
						}
						state.last = hold & 1
						hold >>>= 1
						bits -= 1
						switch (hold & 3) {
							case 0:
								state.mode = STORED
								break
							case 1:
								fixedtables(state)
								state.mode = LEN_
								if (flush === Z_TREES) {
									hold >>>= 2
									bits -= 2
									break inf_leave
								}
								break
							case 2:
								state.mode = TABLE
								break
							case 3:
								strm.msg = "invalid block type"
								state.mode = BAD
						}
						hold >>>= 2
						bits -= 2
						break
					case STORED:
						hold >>>= bits & 7
						bits -= bits & 7
						while (bits < 32) {
							if (have === 0) {
								break inf_leave
							}
							have--
							hold += input[next++] << bits
							bits += 8
						}
						if ((hold & 65535) !== ((hold >>> 16) ^ 65535)) {
							strm.msg = "invalid stored block lengths"
							state.mode = BAD
							break
						}
						state.length = hold & 65535
						hold = 0
						bits = 0
						state.mode = COPY_
						if (flush === Z_TREES) {
							break inf_leave
						}
					case COPY_:
						state.mode = COPY
					case COPY:
						copy = state.length
						if (copy) {
							if (copy > have) {
								copy = have
							}
							if (copy > left) {
								copy = left
							}
							if (copy === 0) {
								break inf_leave
							}
							utils.arraySet(output, input, next, copy, put)
							have -= copy
							next += copy
							left -= copy
							put += copy
							state.length -= copy
							break
						}
						state.mode = TYPE
						break
					case TABLE:
						while (bits < 14) {
							if (have === 0) {
								break inf_leave
							}
							have--
							hold += input[next++] << bits
							bits += 8
						}
						state.nlen = (hold & 31) + 257
						hold >>>= 5
						bits -= 5
						state.ndist = (hold & 31) + 1
						hold >>>= 5
						bits -= 5
						state.ncode = (hold & 15) + 4
						hold >>>= 4
						bits -= 4
						if (state.nlen > 286 || state.ndist > 30) {
							strm.msg = "too many length or distance symbols"
							state.mode = BAD
							break
						}
						state.have = 0
						state.mode = LENLENS
					case LENLENS:
						while (state.have < state.ncode) {
							while (bits < 3) {
								if (have === 0) {
									break inf_leave
								}
								have--
								hold += input[next++] << bits
								bits += 8
							}
							state.lens[order[state.have++]] = hold & 7
							hold >>>= 3
							bits -= 3
						}
						while (state.have < 19) {
							state.lens[order[state.have++]] = 0
						}
						state.lencode = state.lendyn
						state.lenbits = 7
						opts = { bits: state.lenbits }
						ret = inflate_table(CODES, state.lens, 0, 19, state.lencode, 0, state.work, opts)
						state.lenbits = opts.bits
						if (ret) {
							strm.msg = "invalid code lengths set"
							state.mode = BAD
							break
						}
						state.have = 0
						state.mode = CODELENS
					case CODELENS:
						while (state.have < state.nlen + state.ndist) {
							for (;;) {
								here = state.lencode[hold & ((1 << state.lenbits) - 1)]
								here_bits = here >>> 24
								here_op = (here >>> 16) & 255
								here_val = here & 65535
								if (here_bits <= bits) {
									break
								}
								if (have === 0) {
									break inf_leave
								}
								have--
								hold += input[next++] << bits
								bits += 8
							}
							if (here_val < 16) {
								hold >>>= here_bits
								bits -= here_bits
								state.lens[state.have++] = here_val
							} else {
								if (here_val === 16) {
									n = here_bits + 2
									while (bits < n) {
										if (have === 0) {
											break inf_leave
										}
										have--
										hold += input[next++] << bits
										bits += 8
									}
									hold >>>= here_bits
									bits -= here_bits
									if (state.have === 0) {
										strm.msg = "invalid bit length repeat"
										state.mode = BAD
										break
									}
									len = state.lens[state.have - 1]
									copy = 3 + (hold & 3)
									hold >>>= 2
									bits -= 2
								} else if (here_val === 17) {
									n = here_bits + 3
									while (bits < n) {
										if (have === 0) {
											break inf_leave
										}
										have--
										hold += input[next++] << bits
										bits += 8
									}
									hold >>>= here_bits
									bits -= here_bits
									len = 0
									copy = 3 + (hold & 7)
									hold >>>= 3
									bits -= 3
								} else {
									n = here_bits + 7
									while (bits < n) {
										if (have === 0) {
											break inf_leave
										}
										have--
										hold += input[next++] << bits
										bits += 8
									}
									hold >>>= here_bits
									bits -= here_bits
									len = 0
									copy = 11 + (hold & 127)
									hold >>>= 7
									bits -= 7
								}
								if (state.have + copy > state.nlen + state.ndist) {
									strm.msg = "invalid bit length repeat"
									state.mode = BAD
									break
								}
								while (copy--) {
									state.lens[state.have++] = len
								}
							}
						}
						if (state.mode === BAD) {
							break
						}
						if (state.lens[256] === 0) {
							strm.msg = "invalid code -- missing end-of-block"
							state.mode = BAD
							break
						}
						state.lenbits = 9
						opts = { bits: state.lenbits }
						ret = inflate_table(LENS, state.lens, 0, state.nlen, state.lencode, 0, state.work, opts)
						state.lenbits = opts.bits
						if (ret) {
							strm.msg = "invalid literal/lengths set"
							state.mode = BAD
							break
						}
						state.distbits = 6
						state.distcode = state.distdyn
						opts = { bits: state.distbits }
						ret = inflate_table(
							DISTS,
							state.lens,
							state.nlen,
							state.ndist,
							state.distcode,
							0,
							state.work,
							opts
						)
						state.distbits = opts.bits
						if (ret) {
							strm.msg = "invalid distances set"
							state.mode = BAD
							break
						}
						state.mode = LEN_
						if (flush === Z_TREES) {
							break inf_leave
						}
					case LEN_:
						state.mode = LEN
					case LEN:
						if (have >= 6 && left >= 258) {
							strm.next_out = put
							strm.avail_out = left
							strm.next_in = next
							strm.avail_in = have
							state.hold = hold
							state.bits = bits
							inflate_fast(strm, _out)
							put = strm.next_out
							output = strm.output
							left = strm.avail_out
							next = strm.next_in
							input = strm.input
							have = strm.avail_in
							hold = state.hold
							bits = state.bits
							if (state.mode === TYPE) {
								state.back = -1
							}
							break
						}
						state.back = 0
						for (;;) {
							here = state.lencode[hold & ((1 << state.lenbits) - 1)]
							here_bits = here >>> 24
							here_op = (here >>> 16) & 255
							here_val = here & 65535
							if (here_bits <= bits) {
								break
							}
							if (have === 0) {
								break inf_leave
							}
							have--
							hold += input[next++] << bits
							bits += 8
						}
						if (here_op && (here_op & 240) === 0) {
							last_bits = here_bits
							last_op = here_op
							last_val = here_val
							for (;;) {
								here =
									state.lencode[
										last_val + ((hold & ((1 << (last_bits + last_op)) - 1)) >> last_bits)
									]
								here_bits = here >>> 24
								here_op = (here >>> 16) & 255
								here_val = here & 65535
								if (last_bits + here_bits <= bits) {
									break
								}
								if (have === 0) {
									break inf_leave
								}
								have--
								hold += input[next++] << bits
								bits += 8
							}
							hold >>>= last_bits
							bits -= last_bits
							state.back += last_bits
						}
						hold >>>= here_bits
						bits -= here_bits
						state.back += here_bits
						state.length = here_val
						if (here_op === 0) {
							state.mode = LIT
							break
						}
						if (here_op & 32) {
							state.back = -1
							state.mode = TYPE
							break
						}
						if (here_op & 64) {
							strm.msg = "invalid literal/length code"
							state.mode = BAD
							break
						}
						state.extra = here_op & 15
						state.mode = LENEXT
					case LENEXT:
						if (state.extra) {
							n = state.extra
							while (bits < n) {
								if (have === 0) {
									break inf_leave
								}
								have--
								hold += input[next++] << bits
								bits += 8
							}
							state.length += hold & ((1 << state.extra) - 1)
							hold >>>= state.extra
							bits -= state.extra
							state.back += state.extra
						}
						state.was = state.length
						state.mode = DIST
					case DIST:
						for (;;) {
							here = state.distcode[hold & ((1 << state.distbits) - 1)]
							here_bits = here >>> 24
							here_op = (here >>> 16) & 255
							here_val = here & 65535
							if (here_bits <= bits) {
								break
							}
							if (have === 0) {
								break inf_leave
							}
							have--
							hold += input[next++] << bits
							bits += 8
						}
						if ((here_op & 240) === 0) {
							last_bits = here_bits
							last_op = here_op
							last_val = here_val
							for (;;) {
								here =
									state.distcode[
										last_val + ((hold & ((1 << (last_bits + last_op)) - 1)) >> last_bits)
									]
								here_bits = here >>> 24
								here_op = (here >>> 16) & 255
								here_val = here & 65535
								if (last_bits + here_bits <= bits) {
									break
								}
								if (have === 0) {
									break inf_leave
								}
								have--
								hold += input[next++] << bits
								bits += 8
							}
							hold >>>= last_bits
							bits -= last_bits
							state.back += last_bits
						}
						hold >>>= here_bits
						bits -= here_bits
						state.back += here_bits
						if (here_op & 64) {
							strm.msg = "invalid distance code"
							state.mode = BAD
							break
						}
						state.offset = here_val
						state.extra = here_op & 15
						state.mode = DISTEXT
					case DISTEXT:
						if (state.extra) {
							n = state.extra
							while (bits < n) {
								if (have === 0) {
									break inf_leave
								}
								have--
								hold += input[next++] << bits
								bits += 8
							}
							state.offset += hold & ((1 << state.extra) - 1)
							hold >>>= state.extra
							bits -= state.extra
							state.back += state.extra
						}
						if (state.offset > state.dmax) {
							strm.msg = "invalid distance too far back"
							state.mode = BAD
							break
						}
						state.mode = MATCH
					case MATCH:
						if (left === 0) {
							break inf_leave
						}
						copy = _out - left
						if (state.offset > copy) {
							copy = state.offset - copy
							if (copy > state.whave && state.sane) {
								strm.msg = "invalid distance too far back"
								state.mode = BAD
								break
							}
							if (copy > state.wnext) {
								copy -= state.wnext
								from2 = state.wsize - copy
							} else {
								from2 = state.wnext - copy
							}
							if (copy > state.length) {
								copy = state.length
							}
							from_source = state.window
						} else {
							from_source = output
							from2 = put - state.offset
							copy = state.length
						}
						if (copy > left) {
							copy = left
						}
						left -= copy
						state.length -= copy
						do {
							output[put++] = from_source[from2++]
						} while (--copy)
						if (state.length === 0) {
							state.mode = LEN
						}
						break
					case LIT:
						if (left === 0) {
							break inf_leave
						}
						output[put++] = state.length
						left--
						state.mode = LEN
						break
					case CHECK:
						if (state.wrap) {
							while (bits < 32) {
								if (have === 0) {
									break inf_leave
								}
								have--
								hold |= input[next++] << bits
								bits += 8
							}
							_out -= left
							strm.total_out += _out
							state.total += _out
							if (_out) {
								strm.adler = state.check =
									/*UPDATE(state.check, put - _out, _out);*/
									state.flags
										? crc322(state.check, output, _out, put - _out)
										: adler32(state.check, output, _out, put - _out)
							}
							_out = left
							if ((state.flags ? hold : zswap32(hold)) !== state.check) {
								strm.msg = "incorrect data check"
								state.mode = BAD
								break
							}
							hold = 0
							bits = 0
						}
						state.mode = LENGTH
					case LENGTH:
						if (state.wrap && state.flags) {
							while (bits < 32) {
								if (have === 0) {
									break inf_leave
								}
								have--
								hold += input[next++] << bits
								bits += 8
							}
							if (hold !== (state.total & 4294967295)) {
								strm.msg = "incorrect length check"
								state.mode = BAD
								break
							}
							hold = 0
							bits = 0
						}
						state.mode = DONE
					case DONE:
						ret = Z_STREAM_END
						break inf_leave
					case BAD:
						ret = Z_DATA_ERROR
						break inf_leave
					case MEM:
						return Z_MEM_ERROR
					case SYNC:
					default:
						return Z_STREAM_ERROR
				}
			}
			strm.next_out = put
			strm.avail_out = left
			strm.next_in = next
			strm.avail_in = have
			state.hold = hold
			state.bits = bits
			if (
				(state.wsize ||
					(_out !== strm.avail_out &&
						state.mode < BAD &&
						(state.mode < CHECK || flush !== Z_FINISH))) &&
				updatewindow(strm, strm.output, strm.next_out, _out - strm.avail_out)
			) {
				state.mode = MEM
				return Z_MEM_ERROR
			}
			_in -= strm.avail_in
			_out -= strm.avail_out
			strm.total_in += _in
			strm.total_out += _out
			state.total += _out
			if (state.wrap && _out) {
				strm.adler = state.check =
					/*UPDATE(state.check, strm.next_out - _out, _out);*/
					state.flags
						? crc322(state.check, output, _out, strm.next_out - _out)
						: adler32(state.check, output, _out, strm.next_out - _out)
			}
			strm.data_type =
				state.bits +
				(state.last ? 64 : 0) +
				(state.mode === TYPE ? 128 : 0) +
				(state.mode === LEN_ || state.mode === COPY_ ? 256 : 0)
			if (((_in === 0 && _out === 0) || flush === Z_FINISH) && ret === Z_OK) {
				ret = Z_BUF_ERROR
			}
			return ret
		}
		function inflateEnd(strm) {
			if (!strm || !strm.state) {
				return Z_STREAM_ERROR
			}
			var state = strm.state
			if (state.window) {
				state.window = null
			}
			strm.state = null
			return Z_OK
		}
		function inflateGetHeader(strm, head) {
			var state
			if (!strm || !strm.state) {
				return Z_STREAM_ERROR
			}
			state = strm.state
			if ((state.wrap & 2) === 0) {
				return Z_STREAM_ERROR
			}
			state.head = head
			head.done = false
			return Z_OK
		}
		function inflateSetDictionary(strm, dictionary) {
			var dictLength = dictionary.length
			var state
			var dictid
			var ret
			if (!strm || !strm.state) {
				return Z_STREAM_ERROR
			}
			state = strm.state
			if (state.wrap !== 0 && state.mode !== DICT) {
				return Z_STREAM_ERROR
			}
			if (state.mode === DICT) {
				dictid = 1
				dictid = adler32(dictid, dictionary, dictLength, 0)
				if (dictid !== state.check) {
					return Z_DATA_ERROR
				}
			}
			ret = updatewindow(strm, dictionary, dictLength, dictLength)
			if (ret) {
				state.mode = MEM
				return Z_MEM_ERROR
			}
			state.havedict = 1
			return Z_OK
		}
		exports2.inflateReset = inflateReset
		exports2.inflateReset2 = inflateReset2
		exports2.inflateResetKeep = inflateResetKeep
		exports2.inflateInit = inflateInit
		exports2.inflateInit2 = inflateInit2
		exports2.inflate = inflate2
		exports2.inflateEnd = inflateEnd
		exports2.inflateGetHeader = inflateGetHeader
		exports2.inflateSetDictionary = inflateSetDictionary
		exports2.inflateInfo = "pako inflate (from Nodeca project)"
	},
})

// ../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/constants.js
var require_constants6 = __commonJS({
	"../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/constants.js"(
		exports2,
		module2
	) {
		"use strict"
		module2.exports = {
			/* Allowed flush values; see deflate() and inflate() below for details */
			Z_NO_FLUSH: 0,
			Z_PARTIAL_FLUSH: 1,
			Z_SYNC_FLUSH: 2,
			Z_FULL_FLUSH: 3,
			Z_FINISH: 4,
			Z_BLOCK: 5,
			Z_TREES: 6,
			/* Return codes for the compression/decompression functions. Negative values
			 * are errors, positive values are used for special but normal events.
			 */
			Z_OK: 0,
			Z_STREAM_END: 1,
			Z_NEED_DICT: 2,
			Z_ERRNO: -1,
			Z_STREAM_ERROR: -2,
			Z_DATA_ERROR: -3,
			//Z_MEM_ERROR:     -4,
			Z_BUF_ERROR: -5,
			//Z_VERSION_ERROR: -6,
			/* compression levels */
			Z_NO_COMPRESSION: 0,
			Z_BEST_SPEED: 1,
			Z_BEST_COMPRESSION: 9,
			Z_DEFAULT_COMPRESSION: -1,
			Z_FILTERED: 1,
			Z_HUFFMAN_ONLY: 2,
			Z_RLE: 3,
			Z_FIXED: 4,
			Z_DEFAULT_STRATEGY: 0,
			/* Possible values of the data_type field (though see inflate()) */
			Z_BINARY: 0,
			Z_TEXT: 1,
			//Z_ASCII:                1, // = Z_TEXT (deprecated)
			Z_UNKNOWN: 2,
			/* The deflate compression method */
			Z_DEFLATED: 8,
			//Z_NULL:                 null // Use -1 or null inline, depending on var type
		}
	},
})

// ../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/gzheader.js
var require_gzheader = __commonJS({
	"../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/zlib/gzheader.js"(
		exports2,
		module2
	) {
		"use strict"
		function GZheader() {
			this.text = 0
			this.time = 0
			this.xflags = 0
			this.os = 0
			this.extra = null
			this.extra_len = 0
			this.name = ""
			this.comment = ""
			this.hcrc = 0
			this.done = false
		}
		module2.exports = GZheader
	},
})

// ../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/inflate.js
var require_inflate2 = __commonJS({
	"../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/lib/inflate.js"(exports2) {
		"use strict"
		var zlib_inflate = require_inflate()
		var utils = require_common()
		var strings = require_strings()
		var c = require_constants6()
		var msg = require_messages()
		var ZStream = require_zstream()
		var GZheader = require_gzheader()
		var toString = Object.prototype.toString
		function Inflate(options) {
			if (!(this instanceof Inflate)) return new Inflate(options)
			this.options = utils.assign(
				{
					chunkSize: 16384,
					windowBits: 0,
					to: "",
				},
				options || {}
			)
			var opt = this.options
			if (opt.raw && opt.windowBits >= 0 && opt.windowBits < 16) {
				opt.windowBits = -opt.windowBits
				if (opt.windowBits === 0) {
					opt.windowBits = -15
				}
			}
			if (opt.windowBits >= 0 && opt.windowBits < 16 && !(options && options.windowBits)) {
				opt.windowBits += 32
			}
			if (opt.windowBits > 15 && opt.windowBits < 48 && (opt.windowBits & 15) === 0) {
				opt.windowBits |= 15
			}
			this.err = 0
			this.msg = ""
			this.ended = false
			this.chunks = []
			this.strm = new ZStream()
			this.strm.avail_out = 0
			var status3 = zlib_inflate.inflateInit2(this.strm, opt.windowBits)
			if (status3 !== c.Z_OK) {
				throw new Error(msg[status3])
			}
			this.header = new GZheader()
			zlib_inflate.inflateGetHeader(this.strm, this.header)
			if (opt.dictionary) {
				if (typeof opt.dictionary === "string") {
					opt.dictionary = strings.string2buf(opt.dictionary)
				} else if (toString.call(opt.dictionary) === "[object ArrayBuffer]") {
					opt.dictionary = new Uint8Array(opt.dictionary)
				}
				if (opt.raw) {
					status3 = zlib_inflate.inflateSetDictionary(this.strm, opt.dictionary)
					if (status3 !== c.Z_OK) {
						throw new Error(msg[status3])
					}
				}
			}
		}
		Inflate.prototype.push = function (data, mode) {
			var strm = this.strm
			var chunkSize = this.options.chunkSize
			var dictionary = this.options.dictionary
			var status3, _mode
			var next_out_utf8, tail, utf8str
			var allowBufError = false
			if (this.ended) {
				return false
			}
			_mode = mode === ~~mode ? mode : mode === true ? c.Z_FINISH : c.Z_NO_FLUSH
			if (typeof data === "string") {
				strm.input = strings.binstring2buf(data)
			} else if (toString.call(data) === "[object ArrayBuffer]") {
				strm.input = new Uint8Array(data)
			} else {
				strm.input = data
			}
			strm.next_in = 0
			strm.avail_in = strm.input.length
			do {
				if (strm.avail_out === 0) {
					strm.output = new utils.Buf8(chunkSize)
					strm.next_out = 0
					strm.avail_out = chunkSize
				}
				status3 = zlib_inflate.inflate(strm, c.Z_NO_FLUSH)
				if (status3 === c.Z_NEED_DICT && dictionary) {
					status3 = zlib_inflate.inflateSetDictionary(this.strm, dictionary)
				}
				if (status3 === c.Z_BUF_ERROR && allowBufError === true) {
					status3 = c.Z_OK
					allowBufError = false
				}
				if (status3 !== c.Z_STREAM_END && status3 !== c.Z_OK) {
					this.onEnd(status3)
					this.ended = true
					return false
				}
				if (
					strm.next_out &&
					(strm.avail_out === 0 ||
						status3 === c.Z_STREAM_END ||
						(strm.avail_in === 0 && (_mode === c.Z_FINISH || _mode === c.Z_SYNC_FLUSH)))
				) {
					if (this.options.to === "string") {
						next_out_utf8 = strings.utf8border(strm.output, strm.next_out)
						tail = strm.next_out - next_out_utf8
						utf8str = strings.buf2string(strm.output, next_out_utf8)
						strm.next_out = tail
						strm.avail_out = chunkSize - tail
						if (tail) {
							utils.arraySet(strm.output, strm.output, next_out_utf8, tail, 0)
						}
						this.onData(utf8str)
					} else {
						this.onData(utils.shrinkBuf(strm.output, strm.next_out))
					}
				}
				if (strm.avail_in === 0 && strm.avail_out === 0) {
					allowBufError = true
				}
			} while ((strm.avail_in > 0 || strm.avail_out === 0) && status3 !== c.Z_STREAM_END)
			if (status3 === c.Z_STREAM_END) {
				_mode = c.Z_FINISH
			}
			if (_mode === c.Z_FINISH) {
				status3 = zlib_inflate.inflateEnd(this.strm)
				this.onEnd(status3)
				this.ended = true
				return status3 === c.Z_OK
			}
			if (_mode === c.Z_SYNC_FLUSH) {
				this.onEnd(c.Z_OK)
				strm.avail_out = 0
				return true
			}
			return true
		}
		Inflate.prototype.onData = function (chunk) {
			this.chunks.push(chunk)
		}
		Inflate.prototype.onEnd = function (status3) {
			if (status3 === c.Z_OK) {
				if (this.options.to === "string") {
					this.result = this.chunks.join("")
				} else {
					this.result = utils.flattenChunks(this.chunks)
				}
			}
			this.chunks = []
			this.err = status3
			this.msg = this.strm.msg
		}
		function inflate2(input, options) {
			var inflator = new Inflate(options)
			inflator.push(input, true)
			if (inflator.err) {
				throw inflator.msg || msg[inflator.err]
			}
			return inflator.result
		}
		function inflateRaw(input, options) {
			options = options || {}
			options.raw = true
			return inflate2(input, options)
		}
		exports2.Inflate = Inflate
		exports2.inflate = inflate2
		exports2.inflateRaw = inflateRaw
		exports2.ungzip = inflate2
	},
})

// ../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/index.js
var require_pako = __commonJS({
	"../../../node_modules/.pnpm/pako@1.0.11/node_modules/pako/index.js"(exports2, module2) {
		"use strict"
		var assign = require_common().assign
		var deflate2 = require_deflate2()
		var inflate2 = require_inflate2()
		var constants = require_constants6()
		var pako2 = {}
		assign(pako2, deflate2, inflate2, constants)
		module2.exports = pako2
	},
})

// ../../../node_modules/.pnpm/pify@5.0.0/node_modules/pify/index.js
var require_pify = __commonJS({
	"../../../node_modules/.pnpm/pify@5.0.0/node_modules/pify/index.js"(exports2, module2) {
		"use strict"
		var processFn = (fn, options, proxy, unwrapped) =>
			function (...arguments_) {
				const P = options.promiseModule
				return new P((resolve, reject) => {
					if (options.multiArgs) {
						arguments_.push((...result) => {
							if (options.errorFirst) {
								if (result[0]) {
									reject(result)
								} else {
									result.shift()
									resolve(result)
								}
							} else {
								resolve(result)
							}
						})
					} else if (options.errorFirst) {
						arguments_.push((error, result) => {
							if (error) {
								reject(error)
							} else {
								resolve(result)
							}
						})
					} else {
						arguments_.push(resolve)
					}
					const self2 = this === proxy ? unwrapped : this
					Reflect.apply(fn, self2, arguments_)
				})
			}
		var filterCache = /* @__PURE__ */ new WeakMap()
		module2.exports = (input, options) => {
			options = {
				exclude: [/.+(?:Sync|Stream)$/],
				errorFirst: true,
				promiseModule: Promise,
				...options,
			}
			const objectType = typeof input
			if (!(input !== null && (objectType === "object" || objectType === "function"))) {
				throw new TypeError(
					`Expected \`input\` to be a \`Function\` or \`Object\`, got \`${
						input === null ? "null" : objectType
					}\``
				)
			}
			const filter = (target, key) => {
				let cached = filterCache.get(target)
				if (!cached) {
					cached = {}
					filterCache.set(target, cached)
				}
				if (key in cached) {
					return cached[key]
				}
				const match = (pattern3) =>
					typeof pattern3 === "string" || typeof key === "symbol"
						? key === pattern3
						: pattern3.test(key)
				const desc = Reflect.getOwnPropertyDescriptor(target, key)
				const writableOrConfigurableOwn = desc === void 0 || desc.writable || desc.configurable
				const included = options.include
					? options.include.some(match)
					: !options.exclude.some(match)
				const shouldFilter = included && writableOrConfigurableOwn
				cached[key] = shouldFilter
				return shouldFilter
			}
			const cache2 = /* @__PURE__ */ new WeakMap()
			const proxy = new Proxy(input, {
				apply(target, thisArg, args) {
					const cached = cache2.get(target)
					if (cached) {
						return Reflect.apply(cached, thisArg, args)
					}
					const pified = options.excludeMain ? target : processFn(target, options, proxy, target)
					cache2.set(target, pified)
					return Reflect.apply(pified, thisArg, args)
				},
				get(target, key) {
					const property = target[key]
					if (!filter(target, key) || property === Function.prototype[key]) {
						return property
					}
					const cached = cache2.get(property)
					if (cached) {
						return cached
					}
					if (typeof property === "function") {
						const pified = processFn(property, options, proxy, target)
						cache2.set(property, pified)
						return pified
					}
					return property
				},
			})
			return proxy
		}
	},
})

// ../../../node_modules/.pnpm/ignore@5.3.1/node_modules/ignore/index.js
var require_ignore = __commonJS({
	"../../../node_modules/.pnpm/ignore@5.3.1/node_modules/ignore/index.js"(exports2, module2) {
		function makeArray(subject) {
			return Array.isArray(subject) ? subject : [subject]
		}
		var EMPTY = ""
		var SPACE = " "
		var ESCAPE = "\\"
		var REGEX_TEST_BLANK_LINE = /^\s+$/
		var REGEX_INVALID_TRAILING_BACKSLASH = /(?:[^\\]|^)\\$/
		var REGEX_REPLACE_LEADING_EXCAPED_EXCLAMATION = /^\\!/
		var REGEX_REPLACE_LEADING_EXCAPED_HASH = /^\\#/
		var REGEX_SPLITALL_CRLF = /\r?\n/g
		var REGEX_TEST_INVALID_PATH = /^\.*\/|^\.+$/
		var SLASH = "/"
		var TMP_KEY_IGNORE = "node-ignore"
		if (typeof Symbol !== "undefined") {
			TMP_KEY_IGNORE = Symbol.for("node-ignore")
		}
		var KEY_IGNORE = TMP_KEY_IGNORE
		var define2 = (object, key, value) => Object.defineProperty(object, key, { value })
		var REGEX_REGEXP_RANGE = /([0-z])-([0-z])/g
		var RETURN_FALSE = () => false
		var sanitizeRange = (range) =>
			range.replace(REGEX_REGEXP_RANGE, (match, from2, to) =>
				from2.charCodeAt(0) <= to.charCodeAt(0) ? match : EMPTY
			)
		var cleanRangeBackSlash = (slashes) => {
			const { length } = slashes
			return slashes.slice(0, length - (length % 2))
		}
		var REPLACERS = [
			[
				// remove BOM
				// TODO:
				// Other similar zero-width characters?
				/^\uFEFF/,
				() => EMPTY,
			],
			// > Trailing spaces are ignored unless they are quoted with backslash ("\")
			[
				// (a\ ) -> (a )
				// (a  ) -> (a)
				// (a \ ) -> (a  )
				/\\?\s+$/,
				(match) => (match.indexOf("\\") === 0 ? SPACE : EMPTY),
			],
			// replace (\ ) with ' '
			[/\\\s/g, () => SPACE],
			// Escape metacharacters
			// which is written down by users but means special for regular expressions.
			// > There are 12 characters with special meanings:
			// > - the backslash \,
			// > - the caret ^,
			// > - the dollar sign $,
			// > - the period or dot .,
			// > - the vertical bar or pipe symbol |,
			// > - the question mark ?,
			// > - the asterisk or star *,
			// > - the plus sign +,
			// > - the opening parenthesis (,
			// > - the closing parenthesis ),
			// > - and the opening square bracket [,
			// > - the opening curly brace {,
			// > These special characters are often called "metacharacters".
			[/[\\$.|*+(){^]/g, (match) => `\\${match}`],
			[
				// > a question mark (?) matches a single character
				/(?!\\)\?/g,
				() => "[^/]",
			],
			// leading slash
			[
				// > A leading slash matches the beginning of the pathname.
				// > For example, "/*.c" matches "cat-file.c" but not "mozilla-sha1/sha1.c".
				// A leading slash matches the beginning of the pathname
				/^\//,
				() => "^",
			],
			// replace special metacharacter slash after the leading slash
			[/\//g, () => "\\/"],
			[
				// > A leading "**" followed by a slash means match in all directories.
				// > For example, "**/foo" matches file or directory "foo" anywhere,
				// > the same as pattern "foo".
				// > "**/foo/bar" matches file or directory "bar" anywhere that is directly
				// >   under directory "foo".
				// Notice that the '*'s have been replaced as '\\*'
				/^\^*\\\*\\\*\\\//,
				// '**/foo' <-> 'foo'
				() => "^(?:.*\\/)?",
			],
			// starting
			[
				// there will be no leading '/'
				//   (which has been replaced by section "leading slash")
				// If starts with '**', adding a '^' to the regular expression also works
				/^(?=[^^])/,
				function startingReplacer() {
					return !/\/(?!$)/.test(this) ? "(?:^|\\/)" : "^"
				},
			],
			// two globstars
			[
				// Use lookahead assertions so that we could match more than one `'/**'`
				/\\\/\\\*\\\*(?=\\\/|$)/g,
				// Zero, one or several directories
				// should not use '*', or it will be replaced by the next replacer
				// Check if it is not the last `'/**'`
				(_, index2, str) => (index2 + 6 < str.length ? "(?:\\/[^\\/]+)*" : "\\/.+"),
			],
			// normal intermediate wildcards
			[
				// Never replace escaped '*'
				// ignore rule '\*' will match the path '*'
				// 'abc.*/' -> go
				// 'abc.*'  -> skip this rule,
				//    coz trailing single wildcard will be handed by [trailing wildcard]
				/(^|[^\\]+)(\\\*)+(?=.+)/g,
				// '*.js' matches '.js'
				// '*.js' doesn't match 'abc'
				(_, p1, p2) => {
					const unescaped = p2.replace(/\\\*/g, "[^\\/]*")
					return p1 + unescaped
				},
			],
			[
				// unescape, revert step 3 except for back slash
				// For example, if a user escape a '\\*',
				// after step 3, the result will be '\\\\\\*'
				/\\\\\\(?=[$.|*+(){^])/g,
				() => ESCAPE,
			],
			[
				// '\\\\' -> '\\'
				/\\\\/g,
				() => ESCAPE,
			],
			[
				// > The range notation, e.g. [a-zA-Z],
				// > can be used to match one of the characters in a range.
				// `\` is escaped by step 3
				/(\\)?\[([^\]/]*?)(\\*)($|\])/g,
				(match, leadEscape, range, endEscape, close) =>
					leadEscape === ESCAPE
						? `\\[${range}${cleanRangeBackSlash(endEscape)}${close}`
						: close === "]"
						? endEscape.length % 2 === 0
							? `[${sanitizeRange(range)}${endEscape}]`
							: "[]"
						: "[]",
			],
			// ending
			[
				// 'js' will not match 'js.'
				// 'ab' will not match 'abc'
				/(?:[^*])$/,
				// WTF!
				// https://git-scm.com/docs/gitignore
				// changes in [2.22.1](https://git-scm.com/docs/gitignore/2.22.1)
				// which re-fixes #24, #38
				// > If there is a separator at the end of the pattern then the pattern
				// > will only match directories, otherwise the pattern can match both
				// > files and directories.
				// 'js*' will not match 'a.js'
				// 'js/' will not match 'a.js'
				// 'js' will match 'a.js' and 'a.js/'
				(match) => (/\/$/.test(match) ? `${match}$` : `${match}(?=$|\\/$)`),
			],
			// trailing wildcard
			[
				/(\^|\\\/)?\\\*$/,
				(_, p1) => {
					const prefix = p1 ? `${p1}[^/]+` : "[^/]*"
					return `${prefix}(?=$|\\/$)`
				},
			],
		]
		var regexCache = /* @__PURE__ */ Object.create(null)
		var makeRegex = (pattern3, ignoreCase) => {
			let source = regexCache[pattern3]
			if (!source) {
				source = REPLACERS.reduce(
					(prev, current) => prev.replace(current[0], current[1].bind(pattern3)),
					pattern3
				)
				regexCache[pattern3] = source
			}
			return ignoreCase ? new RegExp(source, "i") : new RegExp(source)
		}
		var isString = (subject) => typeof subject === "string"
		var checkPattern = (pattern3) =>
			pattern3 &&
			isString(pattern3) &&
			!REGEX_TEST_BLANK_LINE.test(pattern3) &&
			!REGEX_INVALID_TRAILING_BACKSLASH.test(pattern3) &&
			pattern3.indexOf("#") !== 0
		var splitPattern = (pattern3) => pattern3.split(REGEX_SPLITALL_CRLF)
		var IgnoreRule = class {
			constructor(origin, pattern3, negative, regex) {
				this.origin = origin
				this.pattern = pattern3
				this.negative = negative
				this.regex = regex
			}
		}
		var createRule = (pattern3, ignoreCase) => {
			const origin = pattern3
			let negative = false
			if (pattern3.indexOf("!") === 0) {
				negative = true
				pattern3 = pattern3.slice(1)
			}
			pattern3 = pattern3
				.replace(REGEX_REPLACE_LEADING_EXCAPED_EXCLAMATION, "!")
				.replace(REGEX_REPLACE_LEADING_EXCAPED_HASH, "#")
			const regex = makeRegex(pattern3, ignoreCase)
			return new IgnoreRule(origin, pattern3, negative, regex)
		}
		var throwError = (message, Ctor) => {
			throw new Ctor(message)
		}
		var checkPath = (path, originalPath, doThrow) => {
			if (!isString(path)) {
				return doThrow(`path must be a string, but got \`${originalPath}\``, TypeError)
			}
			if (!path) {
				return doThrow(`path must not be empty`, TypeError)
			}
			if (checkPath.isNotRelative(path)) {
				const r = "`path.relative()`d"
				return doThrow(`path should be a ${r} string, but got "${originalPath}"`, RangeError)
			}
			return true
		}
		var isNotRelative = (path) => REGEX_TEST_INVALID_PATH.test(path)
		checkPath.isNotRelative = isNotRelative
		checkPath.convert = (p) => p
		var Ignore = class {
			constructor({ ignorecase = true, ignoreCase = ignorecase, allowRelativePaths = false } = {}) {
				define2(this, KEY_IGNORE, true)
				this._rules = []
				this._ignoreCase = ignoreCase
				this._allowRelativePaths = allowRelativePaths
				this._initCache()
			}
			_initCache() {
				this._ignoreCache = /* @__PURE__ */ Object.create(null)
				this._testCache = /* @__PURE__ */ Object.create(null)
			}
			_addPattern(pattern3) {
				if (pattern3 && pattern3[KEY_IGNORE]) {
					this._rules = this._rules.concat(pattern3._rules)
					this._added = true
					return
				}
				if (checkPattern(pattern3)) {
					const rule = createRule(pattern3, this._ignoreCase)
					this._added = true
					this._rules.push(rule)
				}
			}
			// @param {Array<string> | string | Ignore} pattern
			add(pattern3) {
				this._added = false
				makeArray(isString(pattern3) ? splitPattern(pattern3) : pattern3).forEach(
					this._addPattern,
					this
				)
				if (this._added) {
					this._initCache()
				}
				return this
			}
			// legacy
			addPattern(pattern3) {
				return this.add(pattern3)
			}
			//          |           ignored : unignored
			// negative |   0:0   |   0:1   |   1:0   |   1:1
			// -------- | ------- | ------- | ------- | --------
			//     0    |  TEST   |  TEST   |  SKIP   |    X
			//     1    |  TESTIF |  SKIP   |  TEST   |    X
			// - SKIP: always skip
			// - TEST: always test
			// - TESTIF: only test if checkUnignored
			// - X: that never happen
			// @param {boolean} whether should check if the path is unignored,
			//   setting `checkUnignored` to `false` could reduce additional
			//   path matching.
			// @returns {TestResult} true if a file is ignored
			_testOne(path, checkUnignored) {
				let ignored = false
				let unignored = false
				for (const rule of this._rules) {
					const { negative } = rule
					if (
						(unignored === negative && ignored !== unignored) ||
						(negative && !ignored && !unignored && !checkUnignored)
					) {
						continue
					}
					const matched = rule.regex.test(path)
					if (matched) {
						ignored = !negative
						unignored = negative
					}
				}
				return {
					ignored,
					unignored,
				}
			}
			// @returns {TestResult}
			_test(originalPath, cache2, checkUnignored, slices) {
				const path = originalPath && checkPath.convert(originalPath)
				checkPath(path, originalPath, this._allowRelativePaths ? RETURN_FALSE : throwError)
				return this._t(path, cache2, checkUnignored, slices)
			}
			_t(path, cache2, checkUnignored, slices) {
				if (path in cache2) {
					return cache2[path]
				}
				if (!slices) {
					slices = path.split(SLASH)
				}
				slices.pop()
				if (!slices.length) {
					return (cache2[path] = this._testOne(path, checkUnignored))
				}
				const parent = this._t(slices.join(SLASH) + SLASH, cache2, checkUnignored, slices)
				return (cache2[path] = parent.ignored ? parent : this._testOne(path, checkUnignored))
			}
			ignores(path) {
				return this._test(path, this._ignoreCache, false).ignored
			}
			createFilter() {
				return (path) => !this.ignores(path)
			}
			filter(paths) {
				return makeArray(paths).filter(this.createFilter())
			}
			// @returns {TestResult}
			test(path) {
				return this._test(path, this._testCache, true)
			}
		}
		var factory = (options) => new Ignore(options)
		var isPathValid = (path) => checkPath(path && checkPath.convert(path), path, RETURN_FALSE)
		factory.isPathValid = isPathValid
		factory.default = factory
		module2.exports = factory
		if (
			// Detect `process` so that it can run in browsers.
			typeof process !== "undefined" &&
			((process.env && process.env.IGNORE_TEST_WIN32) || process.platform === "win32")
		) {
			const makePosix = (str) =>
				/^\\\\\?\\/.test(str) || /["<>|\u0000-\u001F]+/u.test(str) ? str : str.replace(/\\/g, "/")
			checkPath.convert = makePosix
			const REGIX_IS_WINDOWS_PATH_ABSOLUTE = /^[a-z]:\//i
			checkPath.isNotRelative = (path) =>
				REGIX_IS_WINDOWS_PATH_ABSOLUTE.test(path) || isNotRelative(path)
		}
	},
})

// ../../../node_modules/.pnpm/clean-git-ref@2.0.1/node_modules/clean-git-ref/lib/index.js
var require_lib3 = __commonJS({
	"../../../node_modules/.pnpm/clean-git-ref@2.0.1/node_modules/clean-git-ref/lib/index.js"(
		exports2,
		module2
	) {
		"use strict"
		function escapeRegExp(string) {
			return string.replace(/[.*+?^${}()|[\]\\]/g, "\\$&")
		}
		function replaceAll(str, search, replacement) {
			search = search instanceof RegExp ? search : new RegExp(escapeRegExp(search), "g")
			return str.replace(search, replacement)
		}
		var CleanGitRef = {
			clean: function clean(value) {
				if (typeof value !== "string") {
					throw new Error("Expected a string, received: " + value)
				}
				value = replaceAll(value, "./", "/")
				value = replaceAll(value, "..", ".")
				value = replaceAll(value, " ", "-")
				value = replaceAll(value, /^[~^:?*\\\-]/g, "")
				value = replaceAll(value, /[~^:?*\\]/g, "-")
				value = replaceAll(value, /[~^:?*\\\-]$/g, "")
				value = replaceAll(value, "@{", "-")
				value = replaceAll(value, /\.$/g, "")
				value = replaceAll(value, /\/$/g, "")
				value = replaceAll(value, /\.lock$/g, "")
				return value
			},
		}
		module2.exports = CleanGitRef
	},
})

// ../../../lix/packages/client/vendored/diff3/onp.js
function onp_default(a_, b_) {
	var a = a_,
		b = b_,
		m = a.length,
		n = b.length,
		reverse = false,
		offset = m + 1,
		path = [],
		pathposi = []
	var tmp1, tmp2
	var init2 = function () {
		if (m >= n) {
			tmp1 = a
			tmp2 = m
			a = b
			b = tmp1
			m = n
			n = tmp2
			reverse = true
			offset = m + 1
		}
	}
	var P = function (startX, startY, endX, endY, r) {
		return {
			startX,
			startY,
			endX,
			endY,
			r,
		}
	}
	var snake = function (k, p, pp) {
		var r, x, y, startX, startY
		if (p > pp) {
			r = path[k - 1 + offset]
		} else {
			r = path[k + 1 + offset]
		}
		startY = y = Math.max(p, pp)
		startX = x = y - k
		while (x < m && y < n && a[x] === b[y]) {
			++x
			++y
		}
		if (startX == x && startY == y) {
			path[k + offset] = r
		} else {
			path[k + offset] = pathposi.length
			pathposi[pathposi.length] = new P(startX, startY, x, y, r)
		}
		return y
	}
	init2()
	return {
		compose: function () {
			var delta, size, fp, p, r, i, k, lastStartX, lastStartY, result
			delta = n - m
			size = m + n + 3
			fp = {}
			for (i = 0; i < size; ++i) {
				fp[i] = -1
				path[i] = -1
			}
			p = -1
			do {
				++p
				for (k = -p; k <= delta - 1; ++k) {
					fp[k + offset] = snake(k, fp[k - 1 + offset] + 1, fp[k + 1 + offset])
				}
				for (k = delta + p; k >= delta + 1; --k) {
					fp[k + offset] = snake(k, fp[k - 1 + offset] + 1, fp[k + 1 + offset])
				}
				fp[delta + offset] = snake(delta, fp[delta - 1 + offset] + 1, fp[delta + 1 + offset])
			} while (fp[delta + offset] !== n)
			r = path[delta + offset]
			lastStartX = m
			lastStartY = n
			result = []
			while (r !== -1) {
				let elem = pathposi[r]
				if (m != elem.endX || n != elem.endY) {
					result.push({
						file1: [
							reverse ? elem.endY : elem.endX,
							reverse ? lastStartY - elem.endY : lastStartX - elem.endX,
						],
						file2: [
							reverse ? elem.endX : elem.endY,
							reverse ? lastStartX - elem.endX : lastStartY - elem.endY,
						],
					})
				}
				lastStartX = elem.startX
				lastStartY = elem.startY
				r = pathposi[r].r
			}
			if (lastStartX != 0 || lastStartY != 0) {
				result.push({
					file1: [0, reverse ? lastStartY : lastStartX],
					file2: [0, reverse ? lastStartX : lastStartY],
				})
			}
			result.reverse()
			return result
		},
	}
}
var init_onp = __esm({
	"../../../lix/packages/client/vendored/diff3/onp.js"() {
		"use strict"
	},
})

// ../../../lix/packages/client/vendored/diff3/diff3.js
function diff3MergeIndices(a, o, b) {
	var i
	var m1 = new onp_default(o, a).compose()
	var m2 = new onp_default(o, b).compose()
	var hunks = []
	function addHunk(h, side2) {
		hunks.push([h.file1[0], side2, h.file1[1], h.file2[0], h.file2[1]])
	}
	for (i = 0; i < m1.length; i++) {
		addHunk(m1[i], 0)
	}
	for (i = 0; i < m2.length; i++) {
		addHunk(m2[i], 2)
	}
	hunks.sort(function (x, y) {
		return x[0] - y[0]
	})
	var result = []
	var commonOffset = 0
	function copyCommon(targetOffset) {
		if (targetOffset > commonOffset) {
			result.push([1, commonOffset, targetOffset - commonOffset])
			commonOffset = targetOffset
		}
	}
	for (var hunkIndex = 0; hunkIndex < hunks.length; hunkIndex++) {
		var firstHunkIndex = hunkIndex
		var hunk = hunks[hunkIndex]
		var regionLhs = hunk[0]
		var regionRhs = regionLhs + hunk[2]
		while (hunkIndex < hunks.length - 1) {
			var maybeOverlapping = hunks[hunkIndex + 1]
			var maybeLhs = maybeOverlapping[0]
			if (maybeLhs > regionRhs) break
			regionRhs = Math.max(regionRhs, maybeLhs + maybeOverlapping[2])
			hunkIndex++
		}
		copyCommon(regionLhs)
		if (firstHunkIndex == hunkIndex) {
			if (hunk[4] > 0) {
				result.push([hunk[1], hunk[3], hunk[4]])
			}
		} else {
			var regions = {
				0: [a.length, -1, o.length, -1],
				2: [b.length, -1, o.length, -1],
			}
			for (i = firstHunkIndex; i <= hunkIndex; i++) {
				hunk = hunks[i]
				var side = hunk[1]
				var r = regions[side]
				var oLhs = hunk[0]
				var oRhs = oLhs + hunk[2]
				var abLhs = hunk[3]
				var abRhs = abLhs + hunk[4]
				r[0] = Math.min(abLhs, r[0])
				r[1] = Math.max(abRhs, r[1])
				r[2] = Math.min(oLhs, r[2])
				r[3] = Math.max(oRhs, r[3])
			}
			var aLhs = regions[0][0] + (regionLhs - regions[0][2])
			var aRhs = regions[0][1] + (regionRhs - regions[0][3])
			var bLhs = regions[2][0] + (regionLhs - regions[2][2])
			var bRhs = regions[2][1] + (regionRhs - regions[2][3])
			result.push([-1, aLhs, aRhs - aLhs, regionLhs, regionRhs - regionLhs, bLhs, bRhs - bLhs])
		}
		commonOffset = regionRhs
	}
	copyCommon(o.length)
	return result
}
function diff3Merge(a, o, b) {
	var result = []
	var files = [a, o, b]
	var indices = diff3MergeIndices(a, o, b)
	var okLines = []
	function flushOk() {
		if (okLines.length) {
			result.push({
				ok: okLines,
			})
		}
		okLines = []
	}
	function pushOk(xs) {
		for (const x_ of xs) {
			okLines.push(x_)
		}
	}
	function isTrueConflict(rec) {
		if (rec[2] != rec[6]) return true
		var aoff = rec[1]
		var boff = rec[5]
		for (var j = 0; j < rec[2]; j++) {
			if (a[j + aoff] != b[j + boff]) return true
		}
		return false
	}
	for (var x of indices) {
		var side = x[0]
		if (side == -1) {
			if (!isTrueConflict(x)) {
				pushOk(files[0].slice(x[1], x[1] + x[2]))
			} else {
				flushOk()
				result.push({
					conflict: {
						a: a.slice(x[1], x[1] + x[2]),
						aIndex: x[1],
						o: o.slice(x[3], x[3] + x[4]),
						oIndex: x[3],
						b: b.slice(x[5], x[5] + x[6]),
						bIndex: x[5],
					},
				})
			}
		} else {
			pushOk(files[side].slice(x[1], x[1] + x[2]))
		}
	}
	flushOk()
	return result
}
var diff3_default
var init_diff3 = __esm({
	"../../../lix/packages/client/vendored/diff3/diff3.js"() {
		"use strict"
		init_onp()
		diff3_default = diff3Merge
	},
})

// ../../../lix/packages/client/vendored/isomorphic-git/index.js
function compareStrings(a, b) {
	return -(a < b) || +(a > b)
}
function comparePath(a, b) {
	return compareStrings(a.path, b.path)
}
function normalizeMode(mode) {
	let type = mode > 0 ? mode >> 12 : 0
	if (type !== 4 && type !== 8 && type !== 10 && type !== 14) {
		type = 8
	}
	let permissions = mode & 511
	if (permissions & 73) {
		permissions = 493
	} else {
		permissions = 420
	}
	if (type !== 8) permissions = 0
	return (type << 12) + permissions
}
function SecondsNanoseconds(givenSeconds, givenNanoseconds, milliseconds, date) {
	if (givenSeconds !== void 0 && givenNanoseconds !== void 0) {
		return [givenSeconds, givenNanoseconds]
	}
	if (milliseconds === void 0) {
		milliseconds = date.valueOf()
	}
	const seconds = Math.floor(milliseconds / 1e3)
	const nanoseconds = (milliseconds - seconds * 1e3) * 1e6
	return [seconds, nanoseconds]
}
function normalizeStats(e) {
	const [ctimeSeconds, ctimeNanoseconds] = SecondsNanoseconds(
		e.ctimeSeconds,
		e.ctimeNanoseconds,
		e.ctimeMs,
		e.ctime
	)
	const [mtimeSeconds, mtimeNanoseconds] = SecondsNanoseconds(
		e.mtimeSeconds,
		e.mtimeNanoseconds,
		e.mtimeMs,
		e.mtime
	)
	return {
		ctimeSeconds: ctimeSeconds % MAX_UINT32,
		ctimeNanoseconds: ctimeNanoseconds % MAX_UINT32,
		mtimeSeconds: mtimeSeconds % MAX_UINT32,
		mtimeNanoseconds: mtimeNanoseconds % MAX_UINT32,
		dev: e.dev % MAX_UINT32,
		ino: e.ino % MAX_UINT32,
		mode: normalizeMode(e.mode % MAX_UINT32),
		uid: e.uid % MAX_UINT32,
		gid: e.gid % MAX_UINT32,
		// size of -1 happens over a BrowserFS HTTP Backend that doesn't serve Content-Length headers
		// (like the Karma webserver) because BrowserFS HTTP Backend uses HTTP HEAD requests to do fs.stat
		size: e.size > -1 ? e.size % MAX_UINT32 : 0,
	}
}
function toHex(buffer) {
	let hex = ""
	for (const byte of new Uint8Array(buffer)) {
		if (byte < 16) hex += "0"
		hex += byte.toString(16)
	}
	return hex
}
async function shasum(buffer) {
	if (supportsSubtleSHA1 === null) {
		supportsSubtleSHA1 = await testSubtleSHA1()
	}
	return supportsSubtleSHA1 ? subtleSHA1(buffer) : shasumSync(buffer)
}
function shasumSync(buffer) {
	return new import_sha12.default().update(buffer).digest("hex")
}
async function subtleSHA1(buffer) {
	const hash2 = await crypto.subtle.digest("SHA-1", buffer)
	return toHex(hash2)
}
async function testSubtleSHA1() {
	try {
		const hash2 = await subtleSHA1(new Uint8Array([]))
		if (hash2 === "da39a3ee5e6b4b0d3255bfef95601890afd80709") return true
	} catch (_) {}
	return false
}
function parseCacheEntryFlags(bits) {
	return {
		assumeValid: Boolean(bits & 32768),
		extended: Boolean(bits & 16384),
		stage: (bits & 12288) >> 12,
		nameLength: bits & 4095,
	}
}
function renderCacheEntryFlags(entry) {
	const flags = entry.flags
	flags.extended = false
	flags.nameLength = Math.min(Buffer.from(entry.path).length, 4095)
	return (
		(flags.assumeValid ? 32768 : 0) +
		(flags.extended ? 16384 : 0) +
		((flags.stage & 3) << 12) +
		(flags.nameLength & 4095)
	)
}
function compareStats(entry, stats) {
	const e = normalizeStats(entry)
	const s = normalizeStats(stats)
	const staleness =
		e.mode !== s.mode ||
		e.mtimeSeconds !== s.mtimeSeconds ||
		e.ctimeSeconds !== s.ctimeSeconds ||
		e.uid !== s.uid ||
		e.gid !== s.gid ||
		e.ino !== s.ino ||
		e.size !== s.size
	return staleness
}
function createCache() {
	return {
		map: /* @__PURE__ */ new Map(),
		stats: /* @__PURE__ */ new Map(),
	}
}
async function updateCachedIndexFile(fs2, filepath, cache2) {
	const stat = await fs2.lstat(filepath)
	const rawIndexFile = await fs2.read(filepath)
	const index2 = await GitIndex.from(rawIndexFile)
	cache2.map.set(filepath, index2)
	cache2.stats.set(filepath, stat)
}
async function isIndexStale(fs2, filepath, cache2) {
	const savedStats = cache2.stats.get(filepath)
	if (savedStats === void 0) return true
	const currStats = await fs2.lstat(filepath)
	if (savedStats === null) return false
	if (currStats === null) return false
	return compareStats(savedStats, currStats)
}
function basename(path) {
	const last = Math.max(path.lastIndexOf("/"), path.lastIndexOf("\\"))
	if (last > -1) {
		path = path.slice(last + 1)
	}
	return path
}
function dirname(path) {
	const last = Math.max(path.lastIndexOf("/"), path.lastIndexOf("\\"))
	if (last === -1) return "."
	if (last === 0) return "/"
	return path.slice(0, last)
}
function flatFileListToDirectoryStructure(files) {
	const inodes = /* @__PURE__ */ new Map()
	const mkdir = function (name) {
		if (!inodes.has(name)) {
			const dir = {
				type: "tree",
				fullpath: name,
				basename: basename(name),
				metadata: {},
				children: [],
			}
			inodes.set(name, dir)
			dir.parent = mkdir(dirname(name))
			if (dir.parent && dir.parent !== dir) dir.parent.children.push(dir)
		}
		return inodes.get(name)
	}
	const mkfile = function (name, metadata) {
		if (!inodes.has(name)) {
			const file = {
				type: "blob",
				fullpath: name,
				basename: basename(name),
				metadata,
				// This recursively generates any missing parent folders.
				parent: mkdir(dirname(name)),
				children: [],
			}
			if (file.parent) file.parent.children.push(file)
			inodes.set(name, file)
		}
		return inodes.get(name)
	}
	mkdir(".")
	for (const file of files) {
		mkfile(file.path, file)
	}
	return inodes
}
function mode2type(mode) {
	switch (mode) {
		case 16384:
			return "tree"
		case 33188:
			return "blob"
		case 33261:
			return "blob"
		case 40960:
			return "blob"
		case 57344:
			return "commit"
	}
	throw new InternalError(`Unexpected GitTree entry mode: ${mode.toString(8)}`)
}
function STAGE() {
	const o = /* @__PURE__ */ Object.create(null)
	Object.defineProperty(o, GitWalkSymbol, {
		value: function ({ fs: fs2, gitdir, cache: cache2 }) {
			return new GitWalkerIndex({ fs: fs2, gitdir, cache: cache2 })
		},
	})
	Object.freeze(o)
	return o
}
function compareRefNames(a, b) {
	const _a = a.replace(/\^\{\}$/, "")
	const _b = b.replace(/\^\{\}$/, "")
	const tmp = -(_a < _b) || +(_a > _b)
	if (tmp === 0) {
		return a.endsWith("^{}") ? 1 : -1
	}
	return tmp
}
function normalizePath(path) {
	let normalizedPath = memo.get(path)
	if (!normalizedPath) {
		normalizedPath = normalizePathInternal(path)
		memo.set(path, normalizedPath)
	}
	return normalizedPath
}
function normalizePathInternal(path) {
	path = path
		.split("/./")
		.join("/")
		.replace(/\/{2,}/g, "/")
	if (path === "/.") return "/"
	if (path === "./") return "."
	if (path.startsWith("./")) path = path.slice(2)
	if (path.endsWith("/.")) path = path.slice(0, -2)
	if (path.length > 1 && path.endsWith("/")) path = path.slice(0, -1)
	if (path === "") return "."
	return path
}
function join(...parts) {
	return normalizePath(parts.map(normalizePath).join("/"))
}
async function acquireLock(ref, callback) {
	if (lock$1 === void 0) lock$1 = new import_async_lock.default()
	return lock$1.acquire(ref, callback)
}
function compareTreeEntryPath(a, b) {
	return compareStrings(appendSlashIfDir(a), appendSlashIfDir(b))
}
function appendSlashIfDir(entry) {
	return entry.mode === "040000" ? entry.path + "/" : entry.path
}
function mode2type$1(mode) {
	switch (mode) {
		case "040000":
			return "tree"
		case "100644":
			return "blob"
		case "100755":
			return "blob"
		case "120000":
			return "blob"
		case "160000":
			return "commit"
	}
	throw new InternalError(`Unexpected GitTree entry mode: ${mode}`)
}
function parseBuffer(buffer) {
	const _entries = []
	let cursor = 0
	while (cursor < buffer.length) {
		const space = buffer.indexOf(32, cursor)
		if (space === -1) {
			throw new InternalError(
				`GitTree: Error parsing buffer at byte location ${cursor}: Could not find the next space character.`
			)
		}
		const nullchar = buffer.indexOf(0, cursor)
		if (nullchar === -1) {
			throw new InternalError(
				`GitTree: Error parsing buffer at byte location ${cursor}: Could not find the next null character.`
			)
		}
		let mode = buffer.slice(cursor, space).toString("utf8")
		if (mode === "40000") mode = "040000"
		const type = mode2type$1(mode)
		const path = buffer.slice(space + 1, nullchar).toString("utf8")
		if (path.includes("\\") || path.includes("/")) {
			throw new UnsafeFilepathError(path)
		}
		const oid = buffer.slice(nullchar + 1, nullchar + 21).toString("hex")
		cursor = nullchar + 21
		_entries.push({ mode, path, oid, type })
	}
	return _entries
}
function limitModeToAllowed(mode) {
	if (typeof mode === "number") {
		mode = mode.toString(8)
	}
	if (mode.match(/^0?4.*/)) return "040000"
	if (mode.match(/^1006.*/)) return "100644"
	if (mode.match(/^1007.*/)) return "100755"
	if (mode.match(/^120.*/)) return "120000"
	if (mode.match(/^160.*/)) return "160000"
	throw new InternalError(`Could not understand file mode: ${mode}`)
}
function nudgeIntoShape(entry) {
	if (!entry.oid && entry.sha) {
		entry.oid = entry.sha
	}
	entry.mode = limitModeToAllowed(entry.mode)
	if (!entry.type) {
		entry.type = mode2type$1(entry.mode)
	}
	return entry
}
async function readObjectLoose({ fs: fs2, gitdir, oid }) {
	const source = `objects/${oid.slice(0, 2)}/${oid.slice(2)}`
	const file = await fs2.read(`${gitdir}/${source}`)
	if (!file) {
		return null
	}
	return { object: file, format: "deflated", source }
}
function applyDelta(delta, source) {
	const reader = new BufferCursor(delta)
	const sourceSize = readVarIntLE(reader)
	if (sourceSize !== source.byteLength) {
		throw new InternalError(
			`applyDelta expected source buffer to be ${sourceSize} bytes but the provided buffer was ${source.length} bytes`
		)
	}
	const targetSize = readVarIntLE(reader)
	let target
	const firstOp = readOp(reader, source)
	if (firstOp.byteLength === targetSize) {
		target = firstOp
	} else {
		target = Buffer.alloc(targetSize)
		const writer = new BufferCursor(target)
		writer.copy(firstOp)
		while (!reader.eof()) {
			writer.copy(readOp(reader, source))
		}
		const tell = writer.tell()
		if (targetSize !== tell) {
			throw new InternalError(
				`applyDelta expected target buffer to be ${targetSize} bytes but the resulting buffer was ${tell} bytes`
			)
		}
	}
	return target
}
function readVarIntLE(reader) {
	let result = 0
	let shift = 0
	let byte = null
	do {
		byte = reader.readUInt8()
		result |= (byte & 127) << shift
		shift += 7
	} while (byte & 128)
	return result
}
function readCompactLE(reader, flags, size) {
	let result = 0
	let shift = 0
	while (size--) {
		if (flags & 1) {
			result |= reader.readUInt8() << shift
		}
		flags >>= 1
		shift += 8
	}
	return result
}
function readOp(reader, source) {
	const byte = reader.readUInt8()
	const COPY = 128
	const OFFS = 15
	const SIZE = 112
	if (byte & COPY) {
		const offset = readCompactLE(reader, byte & OFFS, 4)
		let size = readCompactLE(reader, (byte & SIZE) >> 4, 3)
		if (size === 0) size = 65536
		return source.slice(offset, offset + size)
	} else {
		return reader.slice(byte)
	}
}
function fromValue(value) {
	let queue = [value]
	return {
		next() {
			return Promise.resolve({ done: queue.length === 0, value: queue.pop() })
		},
		return() {
			queue = []
			return {}
		},
		[Symbol.asyncIterator]() {
			return this
		},
	}
}
function getIterator(iterable) {
	if (iterable[Symbol.asyncIterator]) {
		return iterable[Symbol.asyncIterator]()
	}
	if (iterable[Symbol.iterator]) {
		return iterable[Symbol.iterator]()
	}
	if (iterable.next) {
		return iterable
	}
	return fromValue(iterable)
}
function lengthBuffers(buffers) {
	return buffers.reduce((acc, buffer) => acc + buffer.length, 0)
}
async function listpack(stream, onData) {
	const reader = new StreamReader(stream)
	let PACK = await reader.read(4)
	PACK = PACK.toString("utf8")
	if (PACK !== "PACK") {
		throw new InternalError(`Invalid PACK header '${PACK}'`)
	}
	let version3 = await reader.read(4)
	version3 = version3.readUInt32BE(0)
	if (version3 !== 2) {
		throw new InternalError(`Invalid packfile version: ${version3}`)
	}
	let numObjects = await reader.read(4)
	numObjects = numObjects.readUInt32BE(0)
	if (numObjects < 1) return
	while (!reader.eof() && numObjects--) {
		const offset = reader.tell()
		const { type, length, ofs, reference } = await parseHeader(reader)
		const inflator = new import_pako.default.Inflate()
		while (!inflator.result) {
			const chunk = await reader.chunk()
			if (!chunk) break
			inflator.push(chunk, false)
			if (inflator.err) {
				throw new InternalError(`Pako error: ${inflator.msg}`)
			}
			if (inflator.result) {
				if (inflator.result.length !== length) {
					throw new InternalError(`Inflated object size is different from that stated in packfile.`)
				}
				await reader.undo()
				await reader.read(chunk.length - inflator.strm.avail_in)
				const end = reader.tell()
				await onData({
					data: inflator.result,
					type,
					num: numObjects,
					offset,
					end,
					reference,
					ofs,
				})
			}
		}
	}
}
async function parseHeader(reader) {
	let byte = await reader.byte()
	const type = (byte >> 4) & 7
	let length = byte & 15
	if (byte & 128) {
		let shift = 4
		do {
			byte = await reader.byte()
			length |= (byte & 127) << shift
			shift += 7
		} while (byte & 128)
	}
	let ofs
	let reference
	if (type === 6) {
		let shift = 0
		ofs = 0
		const bytes = []
		do {
			byte = await reader.byte()
			ofs |= (byte & 127) << shift
			shift += 7
			bytes.push(byte)
		} while (byte & 128)
		reference = Buffer.from(bytes)
	}
	if (type === 7) {
		const buf = await reader.read(20)
		reference = buf
	}
	return { type, length, ofs, reference }
}
async function inflate(buffer) {
	if (supportsDecompressionStream === null) {
		supportsDecompressionStream = testDecompressionStream()
	}
	return supportsDecompressionStream ? browserInflate(buffer) : import_pako.default.inflate(buffer)
}
async function browserInflate(buffer) {
	const ds = new DecompressionStream("deflate")
	const d = new Blob([buffer]).stream().pipeThrough(ds)
	return new Uint8Array(await new Response(d).arrayBuffer())
}
function testDecompressionStream() {
	try {
		const ds = new DecompressionStream("deflate")
		if (ds) return true
	} catch (_) {}
	return false
}
function decodeVarInt(reader) {
	const bytes = []
	let byte = 0
	let multibyte = 0
	do {
		byte = reader.readUInt8()
		const lastSeven = byte & 127
		bytes.push(lastSeven)
		multibyte = byte & 128
	} while (multibyte)
	return bytes.reduce((a, b) => ((a + 1) << 7) | b, -1)
}
function otherVarIntDecode(reader, startWith) {
	let result = startWith
	let shift = 4
	let byte = null
	do {
		byte = reader.readUInt8()
		result |= (byte & 127) << shift
		shift += 7
	} while (byte & 128)
	return result
}
async function loadPackIndex({ fs: fs2, filename, getExternalRefDelta, emitter, emitterPrefix }) {
	const idx = await fs2.read(filename)
	return GitPackIndex.fromIdx({ idx, getExternalRefDelta })
}
function readPackIndex({
	fs: fs2,
	cache: cache2,
	filename,
	getExternalRefDelta,
	emitter,
	emitterPrefix,
}) {
	if (!cache2[PackfileCache]) cache2[PackfileCache] = /* @__PURE__ */ new Map()
	let p = cache2[PackfileCache].get(filename)
	if (!p) {
		p = loadPackIndex({
			fs: fs2,
			filename,
			getExternalRefDelta,
			emitter,
			emitterPrefix,
		})
		cache2[PackfileCache].set(filename, p)
	}
	return p
}
async function readObjectPacked({
	fs: fs2,
	cache: cache2,
	gitdir,
	oid,
	format = "content",
	getExternalRefDelta,
}) {
	let list = await fs2.readdir(join(gitdir, "objects/pack"))
	list = list.filter((x) => x.endsWith(".idx"))
	for (const filename of list) {
		const indexFile = `${gitdir}/objects/pack/${filename}`
		const p = await readPackIndex({
			fs: fs2,
			cache: cache2,
			filename: indexFile,
			getExternalRefDelta,
		})
		if (p.error) throw new InternalError(p.error)
		if (p.offsets.has(oid)) {
			if (!p.pack) {
				const packFile = indexFile.replace(/idx$/, "pack")
				p.pack = fs2.read(packFile)
			}
			const result = await p.read({ oid, getExternalRefDelta })
			result.format = "content"
			result.source = `objects/pack/${filename.replace(/idx$/, "pack")}`
			return result
		}
	}
	return null
}
async function _readObject({ fs: fs2, cache: cache2, gitdir, oid, format = "content" }) {
	const getExternalRefDelta = (oid2) => _readObject({ fs: fs2, cache: cache2, gitdir, oid: oid2 })
	let result
	if (oid === "4b825dc642cb6eb9a060e54bf8d69288fbee4904") {
		result = { format: "wrapped", object: Buffer.from(`tree 0\0`) }
	}
	if (!result) {
		result = await readObjectLoose({ fs: fs2, gitdir, oid })
	}
	if (!result) {
		result = await readObjectPacked({
			fs: fs2,
			cache: cache2,
			gitdir,
			oid,
			getExternalRefDelta,
		})
	}
	if (!result) {
		throw new NotFoundError(oid)
	}
	if (format === "deflated") {
		return result
	}
	if (result.format === "deflated") {
		result.object = Buffer.from(await inflate(result.object))
		result.format = "wrapped"
	}
	if (result.format === "wrapped") {
		if (format === "wrapped" && result.format === "wrapped") {
			return result
		}
		const sha = await shasum(result.object)
		if (sha !== oid) {
			throw new InternalError(`SHA check failed! Expected ${oid}, computed ${sha}`)
		}
		const { object, type } = GitObject.unwrap(result.object)
		result.type = type
		result.object = object
		result.format = "content"
	}
	if (result.format === "content") {
		if (format === "content") return result
		return
	}
	throw new InternalError(`invalid format "${result.format}"`)
}
function formatAuthor({ name, email, timestamp, timezoneOffset }) {
	timezoneOffset = formatTimezoneOffset(timezoneOffset)
	return `${name} <${email}> ${timestamp} ${timezoneOffset}`
}
function formatTimezoneOffset(minutes) {
	const sign = simpleSign(negateExceptForZero(minutes))
	minutes = Math.abs(minutes)
	const hours = Math.floor(minutes / 60)
	minutes -= hours * 60
	let strHours = String(hours)
	let strMinutes = String(minutes)
	if (strHours.length < 2) strHours = "0" + strHours
	if (strMinutes.length < 2) strMinutes = "0" + strMinutes
	return (sign === -1 ? "-" : "+") + strHours + strMinutes
}
function simpleSign(n) {
	return Math.sign(n) || (Object.is(n, -0) ? -1 : 1)
}
function negateExceptForZero(n) {
	return n === 0 ? n : -n
}
function normalizeNewlines(str) {
	str = str.replace(/\r/g, "")
	str = str.replace(/^\n+/, "")
	str = str.replace(/\n+$/, "") + "\n"
	return str
}
function parseAuthor(author) {
	const [, name, email, timestamp, offset] = author.match(/^(.*) <(.*)> (.*) (.*)$/)
	return {
		name,
		email,
		timestamp: Number(timestamp),
		timezoneOffset: parseTimezoneOffset(offset),
	}
}
function parseTimezoneOffset(offset) {
	let [, sign, hours, minutes] = offset.match(/(\+|-)(\d\d)(\d\d)/)
	minutes = (sign === "+" ? 1 : -1) * (Number(hours) * 60 + Number(minutes))
	return negateExceptForZero$1(minutes)
}
function negateExceptForZero$1(n) {
	return n === 0 ? n : -n
}
function indent(str) {
	return (
		str
			.trim()
			.split("\n")
			.map((x) => " " + x)
			.join("\n") + "\n"
	)
}
function outdent(str) {
	return str
		.split("\n")
		.map((x) => x.replace(/^ /, ""))
		.join("\n")
}
async function resolveTree({ fs: fs2, cache: cache2, gitdir, oid }) {
	if (oid === "4b825dc642cb6eb9a060e54bf8d69288fbee4904") {
		return { tree: GitTree.from([]), oid }
	}
	const { type, object } = await _readObject({ fs: fs2, cache: cache2, gitdir, oid })
	if (type === "tag") {
		oid = GitAnnotatedTag.from(object).parse().object
		return resolveTree({ fs: fs2, cache: cache2, gitdir, oid })
	}
	if (type === "commit") {
		oid = GitCommit.from(object).parse().tree
		return resolveTree({ fs: fs2, cache: cache2, gitdir, oid })
	}
	if (type !== "tree") {
		throw new ObjectTypeError(oid, type, "tree")
	}
	return { tree: GitTree.from(object), oid }
}
function TREE({ ref = "HEAD" } = {}) {
	const o = /* @__PURE__ */ Object.create(null)
	Object.defineProperty(o, GitWalkSymbol, {
		value: function ({ fs: fs2, gitdir, cache: cache2 }) {
			return new GitWalkerRepo({ fs: fs2, gitdir, ref, cache: cache2 })
		},
	})
	Object.freeze(o)
	return o
}
function WORKDIR() {
	const o = /* @__PURE__ */ Object.create(null)
	Object.defineProperty(o, GitWalkSymbol, {
		value: function ({ fs: fs2, dir, gitdir, cache: cache2 }) {
			return new GitWalkerFs({ fs: fs2, dir, gitdir, cache: cache2 })
		},
	})
	Object.freeze(o)
	return o
}
function arrayRange(start, end) {
	const length = end - start
	return Array.from({ length }, (_, i) => start + i)
}
function* unionOfIterators(sets) {
	const min = new RunningMinimum()
	let minimum
	const heads = []
	const numsets = sets.length
	for (let i = 0; i < numsets; i++) {
		heads[i] = sets[i].next().value
		if (heads[i] !== void 0) {
			min.consider(heads[i])
		}
	}
	if (min.value === null) return
	while (true) {
		const result = []
		minimum = min.value
		min.reset()
		for (let i = 0; i < numsets; i++) {
			if (heads[i] !== void 0 && heads[i] === minimum) {
				result[i] = heads[i]
				heads[i] = sets[i].next().value
			} else {
				result[i] = null
			}
			if (heads[i] !== void 0) {
				min.consider(heads[i])
			}
		}
		yield result
		if (min.value === null) return
	}
}
async function _walk({
	fs: fs2,
	cache: cache2,
	dir,
	gitdir,
	trees,
	// @ts-ignore
	map = async (_, entry) => entry,
	// The default reducer is a flatmap that filters out undefineds.
	reduce = async (parent, children2) => {
		const flatten = flat(children2)
		if (parent !== void 0) flatten.unshift(parent)
		return flatten
	},
	// The default iterate function walks all children concurrently
	iterate = (walk3, children2) => Promise.all([...children2].map(walk3)),
}) {
	const walkers = trees.map((proxy) =>
		proxy[GitWalkSymbol]({ fs: fs2, dir, gitdir, cache: cache2 })
	)
	const root = new Array(walkers.length).fill(".")
	const range = arrayRange(0, walkers.length)
	const unionWalkerFromReaddir = async (entries) => {
		range.map((i) => {
			entries[i] = entries[i] && new walkers[i].ConstructEntry(entries[i])
		})
		const subdirs = await Promise.all(
			range.map((i) => (entries[i] ? walkers[i].readdir(entries[i]) : []))
		)
		const iterators = subdirs
			.map((array) => (array === null ? [] : array))
			.map((array) => array[Symbol.iterator]())
		return {
			entries,
			children: unionOfIterators(iterators),
		}
	}
	const walk3 = async (root2) => {
		const { entries, children: children2 } = await unionWalkerFromReaddir(root2)
		const fullpath = entries.find((entry) => entry && entry._fullpath)._fullpath
		const parent = await map(fullpath, entries)
		if (parent !== null) {
			let walkedChildren = await iterate(walk3, children2)
			walkedChildren = walkedChildren.filter((x) => x !== void 0)
			return reduce(parent, walkedChildren)
		}
	}
	return walk3(root)
}
async function rmRecursive(fs2, filepath) {
	const entries = await fs2.readdir(filepath)
	if (entries == undefined) {
		await fs2.rm(filepath)
	} else if (entries.length) {
		await Promise.all(
			entries.map((entry) => {
				const subpath = join(filepath, entry)
				return fs2.lstat(subpath).then((stat) => {
					if (!stat) return
					return stat.isDirectory() ? rmRecursive(fs2, subpath) : fs2.rm(subpath)
				})
			})
		).then(() => fs2.rmdir(filepath))
	} else {
		await fs2.rmdir(filepath)
	}
}
function isPromiseLike(obj) {
	return isObject(obj) && isFunction(obj.then) && isFunction(obj.catch)
}
function isObject(obj) {
	return obj && typeof obj === "object"
}
function isFunction(obj) {
	return typeof obj === "function"
}
function isPromiseFs(fs2) {
	const test = (targetFs) => {
		try {
			return targetFs.readFile().catch((e) => e)
		} catch (e) {
			return e
		}
	}
	return isPromiseLike(test(fs2))
}
function bindFs(target, fs2) {
	if (isPromiseFs(fs2)) {
		for (const command of commands) {
			target[`_${command}`] = fs2[command].bind(fs2)
		}
	} else {
		for (const command of commands) {
			target[`_${command}`] = (0, import_pify.default)(fs2[command].bind(fs2))
		}
	}
	if (isPromiseFs(fs2)) {
		if (fs2.rm) target._rm = fs2.rm.bind(fs2)
		else if (fs2.rmdir.length > 1) target._rm = fs2.rmdir.bind(fs2)
		else target._rm = rmRecursive.bind(null, target)
	} else {
		if (fs2.rm) target._rm = (0, import_pify.default)(fs2.rm.bind(fs2))
		else if (fs2.rmdir.length > 2) target._rm = (0, import_pify.default)(fs2.rmdir.bind(fs2))
		else target._rm = rmRecursive.bind(null, target)
	}
}
function assertParameter(name, value) {
	if (value === void 0) {
		throw new MissingParameterError(name)
	}
}
async function modified(entry, base) {
	if (!entry && !base) return false
	if (entry && !base) return true
	if (!entry && base) return true
	if ((await entry.type()) === "tree" && (await base.type()) === "tree") {
		return false
	}
	if (
		(await entry.type()) === (await base.type()) &&
		(await entry.mode()) === (await base.mode()) &&
		(await entry.oid()) === (await base.oid())
	) {
		return false
	}
	return true
}
async function abortMerge({
	fs: _fs,
	dir,
	gitdir = join(dir, ".git"),
	commit: commit3 = "HEAD",
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", _fs)
		assertParameter("dir", dir)
		assertParameter("gitdir", gitdir)
		const fs2 = new FileSystem(_fs)
		const trees = [TREE({ ref: commit3 }), WORKDIR(), STAGE()]
		let unmergedPaths = []
		await GitIndexManager.acquire({ fs: fs2, gitdir, cache: cache2 }, async function (index2) {
			unmergedPaths = index2.unmergedPaths
		})
		const results = await _walk({
			fs: fs2,
			cache: cache2,
			dir,
			gitdir,
			trees,
			map: async function (path, [head, workdir, index2]) {
				const staged = !(await modified(workdir, index2))
				const unmerged = unmergedPaths.includes(path)
				const unmodified = !(await modified(index2, head))
				if (staged || unmerged) {
					return head
						? {
								path,
								mode: await head.mode(),
								oid: await head.oid(),
								type: await head.type(),
								content: await head.content(),
						  }
						: void 0
				}
				if (unmodified) return false
				else throw new IndexResetError(path)
			},
		})
		await GitIndexManager.acquire({ fs: fs2, gitdir, cache: cache2 }, async function (index2) {
			for (const entry of results) {
				if (entry === false) continue
				if (!entry) {
					await fs2.rmdir(`${dir}/${entry.path}`, { recursive: true })
					index2.delete({ filepath: entry.path })
					continue
				}
				if (entry.type === "blob") {
					const content = new TextDecoder().decode(entry.content)
					await fs2.write(`${dir}/${entry.path}`, content, { mode: entry.mode })
					index2.insert({
						filepath: entry.path,
						oid: entry.oid,
						stage: 0,
					})
				}
			}
		})
	} catch (err) {
		err.caller = "git.abortMerge"
		throw err
	}
}
async function writeObjectLoose({ fs: fs2, gitdir, object, format, oid }) {
	if (format !== "deflated") {
		throw new InternalError("GitObjectStoreLoose expects objects to write to be in deflated format")
	}
	const source = `objects/${oid.slice(0, 2)}/${oid.slice(2)}`
	const filepath = `${gitdir}/${source}`
	if (!(await fs2.exists(filepath))) await fs2.write(filepath, object)
}
async function deflate(buffer) {
	if (supportsCompressionStream === null) {
		supportsCompressionStream = testCompressionStream()
	}
	return supportsCompressionStream ? browserDeflate(buffer) : import_pako.default.deflate(buffer)
}
async function browserDeflate(buffer) {
	const cs = new CompressionStream("deflate")
	const c = new Blob([buffer]).stream().pipeThrough(cs)
	return new Uint8Array(await new Response(c).arrayBuffer())
}
function testCompressionStream() {
	try {
		const cs = new CompressionStream("deflate")
		cs.writable.close()
		const stream = new Blob([]).stream()
		stream.cancel()
		return true
	} catch (_) {
		return false
	}
}
async function _writeObject({
	fs: fs2,
	gitdir,
	type,
	object,
	format = "content",
	oid = void 0,
	dryRun = false,
}) {
	if (format !== "deflated") {
		if (format !== "wrapped") {
			object = GitObject.wrap({ type, object })
		}
		oid = await shasum(object)
		object = Buffer.from(await deflate(object))
	}
	if (!dryRun) {
		await writeObjectLoose({ fs: fs2, gitdir, object, format: "deflated", oid })
	}
	return oid
}
function posixifyPathBuffer(buffer) {
	let idx
	while (~(idx = buffer.indexOf(92))) buffer[idx] = 47
	return buffer
}
async function add({
	fs: _fs,
	dir,
	gitdir = join(dir, ".git"),
	filepath,
	cache: cache2 = {},
	force = false,
	parallel = true,
}) {
	try {
		assertParameter("fs", _fs)
		assertParameter("dir", dir)
		assertParameter("gitdir", gitdir)
		assertParameter("filepath", filepath)
		const fs2 = new FileSystem(_fs)
		await GitIndexManager.acquire({ fs: fs2, gitdir, cache: cache2 }, async (index2) => {
			return addToIndex({
				dir,
				gitdir,
				fs: fs2,
				filepath,
				index: index2,
				force,
				parallel,
			})
		})
	} catch (err) {
		err.caller = "git.add"
		throw err
	}
}
async function addToIndex({ dir, gitdir, fs: fs2, filepath, index: index2, force, parallel }) {
	filepath = Array.isArray(filepath) ? filepath : [filepath]
	const promises = filepath.map(async (currentFilepath) => {
		if (!force) {
			const ignored = await GitIgnoreManager.isIgnored({
				fs: fs2,
				dir,
				gitdir,
				filepath: currentFilepath,
			})
			if (ignored) return
		}
		const stats = await fs2.lstat(join(dir, currentFilepath))
		if (!stats) throw new NotFoundError(currentFilepath)
		if (stats.isDirectory()) {
			const children2 = await fs2.readdir(join(dir, currentFilepath))
			if (parallel) {
				const promises2 = children2.map((child) =>
					addToIndex({
						dir,
						gitdir,
						fs: fs2,
						filepath: [join(currentFilepath, child)],
						index: index2,
						force,
						parallel,
					})
				)
				await Promise.all(promises2)
			} else {
				for (const child of children2) {
					await addToIndex({
						dir,
						gitdir,
						fs: fs2,
						filepath: [join(currentFilepath, child)],
						index: index2,
						force,
						parallel,
					})
				}
			}
		} else {
			const object = stats.isSymbolicLink()
				? await fs2.readlink(join(dir, currentFilepath)).then(posixifyPathBuffer)
				: await fs2.read(join(dir, currentFilepath))
			if (object === null) throw new NotFoundError(currentFilepath)
			const oid = await _writeObject({ fs: fs2, gitdir, type: "blob", object })
			index2.insert({ filepath: currentFilepath, stats, oid })
		}
	})
	const settledPromises = await Promise.allSettled(promises)
	const rejectedPromises = settledPromises
		.filter((settle) => settle.status === "rejected")
		.map((settle) => settle.reason)
	if (rejectedPromises.length > 1) {
		throw new MultipleGitError(rejectedPromises)
	}
	if (rejectedPromises.length === 1) {
		throw rejectedPromises[0]
	}
	const fulfilledPromises = settledPromises
		.filter((settle) => settle.status === "fulfilled" && settle.value)
		.map((settle) => settle.value)
	return fulfilledPromises
}
async function _commit({
	fs: fs2,
	cache: cache2,
	onSign,
	gitdir,
	message,
	author,
	committer,
	signingKey,
	dryRun = false,
	noUpdateBranch = false,
	ref,
	parent,
	tree,
}) {
	if (!ref) {
		ref = await GitRefManager.resolve({
			fs: fs2,
			gitdir,
			ref: "HEAD",
			depth: 2,
		})
	}
	return GitIndexManager.acquire(
		{ fs: fs2, gitdir, cache: cache2, allowUnmerged: false },
		async function (index2) {
			const inodes = flatFileListToDirectoryStructure(index2.entries)
			const inode = inodes.get(".")
			if (!tree) {
				tree = await constructTree({ fs: fs2, gitdir, inode, dryRun })
			}
			if (!parent) {
				try {
					parent = [
						await GitRefManager.resolve({
							fs: fs2,
							gitdir,
							ref,
						}),
					]
				} catch (err) {
					parent = []
				}
			} else {
				parent = await Promise.all(
					parent.map((p) => {
						return GitRefManager.resolve({ fs: fs2, gitdir, ref: p })
					})
				)
			}
			let comm = GitCommit.from({
				tree,
				parent,
				author,
				committer,
				message,
			})
			if (signingKey) {
				comm = await GitCommit.sign(comm, onSign, signingKey)
			}
			const oid = await _writeObject({
				fs: fs2,
				gitdir,
				type: "commit",
				object: comm.toObject(),
				dryRun,
			})
			if (!noUpdateBranch && !dryRun) {
				await GitRefManager.writeRef({
					fs: fs2,
					gitdir,
					ref,
					value: oid,
				})
			}
			return oid
		}
	)
}
async function constructTree({ fs: fs2, gitdir, inode, dryRun }) {
	const children2 = inode.children
	for (const inode2 of children2) {
		if (inode2.type === "tree") {
			inode2.metadata.mode = "040000"
			inode2.metadata.oid = await constructTree({ fs: fs2, gitdir, inode: inode2, dryRun })
		}
	}
	const entries = children2.map((inode2) => ({
		mode: inode2.metadata.mode,
		path: inode2.basename,
		oid: inode2.metadata.oid,
		type: inode2.type,
	}))
	const tree = GitTree.from(entries)
	const oid = await _writeObject({
		fs: fs2,
		gitdir,
		type: "tree",
		object: tree.toObject(),
		dryRun,
	})
	return oid
}
async function resolveFilepath({ fs: fs2, cache: cache2, gitdir, oid, filepath }) {
	if (filepath.startsWith("/")) {
		throw new InvalidFilepathError("leading-slash")
	} else if (filepath.endsWith("/")) {
		throw new InvalidFilepathError("trailing-slash")
	}
	const _oid = oid
	const result = await resolveTree({ fs: fs2, cache: cache2, gitdir, oid })
	const tree = result.tree
	if (filepath === "") {
		oid = result.oid
	} else {
		const pathArray = filepath.split("/")
		oid = await _resolveFilepath({
			fs: fs2,
			cache: cache2,
			gitdir,
			tree,
			pathArray,
			oid: _oid,
			filepath,
		})
	}
	return oid
}
async function _resolveFilepath({
	fs: fs2,
	cache: cache2,
	gitdir,
	tree,
	pathArray,
	oid,
	filepath,
}) {
	const name = pathArray.shift()
	for (const entry of tree) {
		if (entry.path === name) {
			if (pathArray.length === 0) {
				return entry.oid
			} else {
				const { type, object } = await _readObject({
					fs: fs2,
					cache: cache2,
					gitdir,
					oid: entry.oid,
				})
				if (type !== "tree") {
					throw new ObjectTypeError(oid, type, "tree", filepath)
				}
				tree = GitTree.from(object)
				return _resolveFilepath({
					fs: fs2,
					cache: cache2,
					gitdir,
					tree,
					pathArray,
					oid,
					filepath,
				})
			}
		}
	}
	throw new NotFoundError(`file or directory found at "${oid}:${filepath}"`)
}
async function _readTree({ fs: fs2, cache: cache2, gitdir, oid, filepath = void 0 }) {
	if (filepath !== void 0) {
		oid = await resolveFilepath({ fs: fs2, cache: cache2, gitdir, oid, filepath })
	}
	const { tree, oid: treeOid } = await resolveTree({ fs: fs2, cache: cache2, gitdir, oid })
	const result = {
		oid: treeOid,
		tree: tree.entries(),
	}
	return result
}
async function _writeTree({ fs: fs2, gitdir, tree }) {
	const object = GitTree.from(tree).toObject()
	const oid = await _writeObject({
		fs: fs2,
		gitdir,
		type: "tree",
		object,
		format: "content",
	})
	return oid
}
async function _addNote({
	fs: fs2,
	cache: cache2,
	onSign,
	gitdir,
	ref,
	oid,
	note,
	force,
	author,
	committer,
	signingKey,
}) {
	let parent
	try {
		parent = await GitRefManager.resolve({ gitdir, fs: fs2, ref })
	} catch (err) {
		if (!(err instanceof NotFoundError)) {
			throw err
		}
	}
	const result = await _readTree({
		fs: fs2,
		cache: cache2,
		gitdir,
		oid: parent || "4b825dc642cb6eb9a060e54bf8d69288fbee4904",
	})
	let tree = result.tree
	if (force) {
		tree = tree.filter((entry) => entry.path !== oid)
	} else {
		for (const entry of tree) {
			if (entry.path === oid) {
				throw new AlreadyExistsError("note", oid)
			}
		}
	}
	if (typeof note === "string") {
		note = Buffer.from(note, "utf8")
	}
	const noteOid = await _writeObject({
		fs: fs2,
		gitdir,
		type: "blob",
		object: note,
		format: "content",
	})
	tree.push({ mode: "100644", path: oid, oid: noteOid, type: "blob" })
	const treeOid = await _writeTree({
		fs: fs2,
		gitdir,
		tree,
	})
	const commitOid = await _commit({
		fs: fs2,
		cache: cache2,
		onSign,
		gitdir,
		ref,
		tree: treeOid,
		parent: parent && [parent],
		message: `Note added by 'isomorphic-git addNote'
`,
		author,
		committer,
		signingKey,
	})
	return commitOid
}
async function _getConfig({ fs: fs2, gitdir, path }) {
	const config = await GitConfigManager.get({ fs: fs2, gitdir })
	return config.get(path)
}
async function normalizeAuthorObject({ fs: fs2, gitdir, author = {} }) {
	let { name, email, timestamp, timezoneOffset } = author
	name = name || (await _getConfig({ fs: fs2, gitdir, path: "user.name" }))
	email = email || (await _getConfig({ fs: fs2, gitdir, path: "user.email" })) || ""
	if (name === void 0) {
		return void 0
	}
	timestamp = timestamp != undefined ? timestamp : Math.floor(Date.now() / 1e3)
	timezoneOffset =
		timezoneOffset != undefined ? timezoneOffset : new Date(timestamp * 1e3).getTimezoneOffset()
	return { name, email, timestamp, timezoneOffset }
}
async function normalizeCommitterObject({ fs: fs2, gitdir, author, committer }) {
	committer = Object.assign({}, committer || author)
	if (author) {
		committer.timestamp = committer.timestamp || author.timestamp
		committer.timezoneOffset = committer.timezoneOffset || author.timezoneOffset
	}
	committer = await normalizeAuthorObject({ fs: fs2, gitdir, author: committer })
	return committer
}
async function addNote({
	fs: _fs,
	onSign,
	dir,
	gitdir = join(dir, ".git"),
	ref = "refs/notes/commits",
	oid,
	note,
	force,
	author: _author,
	committer: _committer,
	signingKey,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", _fs)
		assertParameter("gitdir", gitdir)
		assertParameter("oid", oid)
		assertParameter("note", note)
		if (signingKey) {
			assertParameter("onSign", onSign)
		}
		const fs2 = new FileSystem(_fs)
		const author = await normalizeAuthorObject({ fs: fs2, gitdir, author: _author })
		if (!author) throw new MissingNameError("author")
		const committer = await normalizeCommitterObject({
			fs: fs2,
			gitdir,
			author,
			committer: _committer,
		})
		if (!committer) throw new MissingNameError("committer")
		return await _addNote({
			fs: new FileSystem(fs2),
			cache: cache2,
			onSign,
			gitdir,
			ref,
			oid,
			note,
			force,
			author,
			committer,
			signingKey,
		})
	} catch (err) {
		err.caller = "git.addNote"
		throw err
	}
}
async function _addRemote({ fs: fs2, gitdir, remote, url, force }) {
	if (remote !== import_clean_git_ref.default.clean(remote)) {
		throw new InvalidRefNameError(remote, import_clean_git_ref.default.clean(remote))
	}
	const config = await GitConfigManager.get({ fs: fs2, gitdir })
	if (!force) {
		const remoteNames = await config.getSubsections("remote")
		if (remoteNames.includes(remote) && url !== (await config.get(`remote.${remote}.url`))) {
			throw new AlreadyExistsError("remote", remote)
		}
	}
	await config.set(`remote.${remote}.url`, url)
	await config.set(`remote.${remote}.fetch`, `+refs/heads/*:refs/remotes/${remote}/*`)
	await GitConfigManager.save({ fs: fs2, gitdir, config })
}
async function addRemote({ fs: fs2, dir, gitdir = join(dir, ".git"), remote, url, force = false }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("remote", remote)
		assertParameter("url", url)
		return await _addRemote({
			fs: new FileSystem(fs2),
			gitdir,
			remote,
			url,
			force,
		})
	} catch (err) {
		err.caller = "git.addRemote"
		throw err
	}
}
async function _annotatedTag({
	fs: fs2,
	cache: cache2,
	onSign,
	gitdir,
	ref,
	tagger,
	message = ref,
	gpgsig,
	object,
	signingKey,
	force = false,
}) {
	ref = ref.startsWith("refs/tags/") ? ref : `refs/tags/${ref}`
	if (!force && (await GitRefManager.exists({ fs: fs2, gitdir, ref }))) {
		throw new AlreadyExistsError("tag", ref)
	}
	const oid = await GitRefManager.resolve({
		fs: fs2,
		gitdir,
		ref: object || "HEAD",
	})
	const { type } = await _readObject({ fs: fs2, cache: cache2, gitdir, oid })
	let tagObject = GitAnnotatedTag.from({
		object: oid,
		type,
		tag: ref.replace("refs/tags/", ""),
		tagger,
		message,
		gpgsig,
	})
	if (signingKey) {
		tagObject = await GitAnnotatedTag.sign(tagObject, onSign, signingKey)
	}
	const value = await _writeObject({
		fs: fs2,
		gitdir,
		type: "tag",
		object: tagObject.toObject(),
	})
	await GitRefManager.writeRef({ fs: fs2, gitdir, ref, value })
}
async function annotatedTag({
	fs: _fs,
	onSign,
	dir,
	gitdir = join(dir, ".git"),
	ref,
	tagger: _tagger,
	message = ref,
	gpgsig,
	object,
	signingKey,
	force = false,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", _fs)
		assertParameter("gitdir", gitdir)
		assertParameter("ref", ref)
		if (signingKey) {
			assertParameter("onSign", onSign)
		}
		const fs2 = new FileSystem(_fs)
		const tagger = await normalizeAuthorObject({ fs: fs2, gitdir, author: _tagger })
		if (!tagger) throw new MissingNameError("tagger")
		return await _annotatedTag({
			fs: fs2,
			cache: cache2,
			onSign,
			gitdir,
			ref,
			tagger,
			message,
			gpgsig,
			object,
			signingKey,
			force,
		})
	} catch (err) {
		err.caller = "git.annotatedTag"
		throw err
	}
}
async function _branch({
	fs: fs2,
	gitdir,
	ref,
	object,
	checkout: checkout3 = false,
	force = false,
}) {
	if (ref !== import_clean_git_ref.default.clean(ref)) {
		throw new InvalidRefNameError(ref, import_clean_git_ref.default.clean(ref))
	}
	const fullref = `refs/heads/${ref}`
	if (!force) {
		const exist = await GitRefManager.exists({ fs: fs2, gitdir, ref: fullref })
		if (exist) {
			throw new AlreadyExistsError("branch", ref, false)
		}
	}
	let oid
	try {
		oid = await GitRefManager.resolve({ fs: fs2, gitdir, ref: object || "HEAD" })
	} catch (e) {}
	if (oid) {
		await GitRefManager.writeRef({ fs: fs2, gitdir, ref: fullref, value: oid })
	}
	if (checkout3) {
		await GitRefManager.writeSymbolicRef({
			fs: fs2,
			gitdir,
			ref: "HEAD",
			value: fullref,
		})
	}
}
async function branch({
	fs: fs2,
	dir,
	gitdir = join(dir, ".git"),
	ref,
	object,
	checkout: checkout3 = false,
	force = false,
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("ref", ref)
		return await _branch({
			fs: new FileSystem(fs2),
			gitdir,
			ref,
			object,
			checkout: checkout3,
			force,
		})
	} catch (err) {
		err.caller = "git.branch"
		throw err
	}
}
async function _checkout({
	fs: fs2,
	cache: cache2,
	onProgress,
	dir,
	gitdir,
	remote,
	ref,
	filepaths,
	noCheckout,
	noUpdateHead,
	dryRun,
	force,
	track = true,
}) {
	let oid
	try {
		oid = await GitRefManager.resolve({ fs: fs2, gitdir, ref })
	} catch (err) {
		if (ref === "HEAD") throw err
		const remoteRef = `${remote}/${ref}`
		oid = await GitRefManager.resolve({
			fs: fs2,
			gitdir,
			ref: remoteRef,
		})
		if (track) {
			const config = await GitConfigManager.get({ fs: fs2, gitdir })
			await config.set(`branch.${ref}.remote`, remote)
			await config.set(`branch.${ref}.merge`, `refs/heads/${ref}`)
			await GitConfigManager.save({ fs: fs2, gitdir, config })
		}
		await GitRefManager.writeRef({
			fs: fs2,
			gitdir,
			ref: `refs/heads/${ref}`,
			value: oid,
		})
	}
	if (!noCheckout) {
		let ops
		try {
			ops = await analyze({
				fs: fs2,
				cache: cache2,
				onProgress,
				dir,
				gitdir,
				ref,
				force,
				filepaths,
			})
		} catch (err) {
			if (err instanceof NotFoundError && err.data.what === oid) {
				throw new CommitNotFetchedError(ref, oid)
			} else {
				throw err
			}
		}
		const conflicts = ops
			.filter(([method]) => method === "conflict")
			.map(([method, fullpath]) => fullpath)
		if (conflicts.length > 0) {
			throw new CheckoutConflictError(conflicts)
		}
		const errors = ops
			.filter(([method]) => method === "error")
			.map(([method, fullpath]) => fullpath)
		if (errors.length > 0) {
			throw new InternalError(errors.join(", "))
		}
		if (dryRun) {
			return
		}
		let count = 0
		const total = ops.length
		await GitIndexManager.acquire({ fs: fs2, gitdir, cache: cache2 }, async function (index2) {
			await Promise.all(
				ops
					.filter(([method]) => method === "delete" || method === "delete-index")
					.map(async function ([method, fullpath]) {
						const filepath = `${dir}/${fullpath}`
						if (method === "delete") {
							await fs2.rm(filepath)
						}
						index2.delete({ filepath: fullpath })
						if (onProgress) {
							await onProgress({
								phase: "Updating workdir",
								loaded: ++count,
								total,
							})
						}
					})
			)
		})
		await GitIndexManager.acquire({ fs: fs2, gitdir, cache: cache2 }, async function (index2) {
			for (const [method, fullpath] of ops) {
				if (method === "rmdir" || method === "rmdir-index") {
					const filepath = `${dir}/${fullpath}`
					try {
						if (method === "rmdir-index") {
							index2.delete({ filepath: fullpath })
						}
						await fs2.rmdir(filepath)
						if (onProgress) {
							await onProgress({
								phase: "Updating workdir",
								loaded: ++count,
								total,
							})
						}
					} catch (e) {
						if (e.code === "ENOTEMPTY") {
							console.log(`Did not delete ${fullpath} because directory is not empty`)
						} else {
							throw e
						}
					}
				}
			}
		})
		await Promise.all(
			ops
				.filter(([method]) => method === "mkdir" || method === "mkdir-index")
				.map(async function ([_, fullpath]) {
					const filepath = `${dir}/${fullpath}`
					await fs2.mkdir(filepath)
					if (onProgress) {
						await onProgress({
							phase: "Updating workdir",
							loaded: ++count,
							total,
						})
					}
				})
		)
		await GitIndexManager.acquire({ fs: fs2, gitdir, cache: cache2 }, async function (index2) {
			await Promise.all(
				ops
					.filter(
						([method]) =>
							method === "create" ||
							method === "create-index" ||
							method === "update" ||
							method === "mkdir-index"
					)
					.map(async function ([method, fullpath, oid2, mode, chmod]) {
						const filepath = `${dir}/${fullpath}`
						try {
							if (method !== "create-index" && method !== "mkdir-index") {
								const { object } = await _readObject({ fs: fs2, cache: cache2, gitdir, oid: oid2 })
								if (chmod) {
									await fs2.rm(filepath)
								}
								if (mode === 33188) {
									await fs2.write(filepath, object)
								} else if (mode === 33261) {
									await fs2.write(filepath, object, { mode: 511 })
								} else if (mode === 40960) {
									await fs2.writelink(filepath, object)
								} else {
									throw new InternalError(
										`Invalid mode 0o${mode.toString(8)} detected in blob ${oid2}`
									)
								}
							}
							const stats = await fs2.lstat(filepath)
							if (mode === 33261) {
								stats.mode = 493
							}
							if (method === "mkdir-index") {
								stats.mode = 57344
							}
							index2.insert({
								filepath: fullpath,
								stats,
								oid: oid2,
							})
							if (onProgress) {
								await onProgress({
									phase: "Updating workdir",
									loaded: ++count,
									total,
								})
							}
						} catch (e) {
							console.log(e)
						}
					})
			)
		})
	}
	if (!noUpdateHead) {
		const fullRef = await GitRefManager.expand({ fs: fs2, gitdir, ref })
		if (fullRef.startsWith("refs/heads")) {
			await GitRefManager.writeSymbolicRef({
				fs: fs2,
				gitdir,
				ref: "HEAD",
				value: fullRef,
			})
		} else {
			await GitRefManager.writeRef({ fs: fs2, gitdir, ref: "HEAD", value: oid })
		}
	}
}
async function analyze({ fs: fs2, cache: cache2, onProgress, dir, gitdir, ref, force, filepaths }) {
	let count = 0
	return _walk({
		fs: fs2,
		cache: cache2,
		dir,
		gitdir,
		trees: [TREE({ ref }), WORKDIR(), STAGE()],
		map: async function (fullpath, [commit3, workdir, stage]) {
			if (fullpath === ".") return
			if (filepaths && !filepaths.some((base) => worthWalking(fullpath, base))) {
				return null
			}
			if (onProgress) {
				await onProgress({ phase: "Analyzing workdir", loaded: ++count })
			}
			const key = [!!stage, !!commit3, !!workdir].map(Number).join("")
			switch (key) {
				case "000":
					return
				case "001":
					if (force && filepaths && filepaths.includes(fullpath)) {
						return ["delete", fullpath]
					}
					return
				case "010": {
					switch (await commit3.type()) {
						case "tree": {
							return ["mkdir", fullpath]
						}
						case "blob": {
							return ["create", fullpath, await commit3.oid(), await commit3.mode()]
						}
						case "commit": {
							return ["mkdir-index", fullpath, await commit3.oid(), await commit3.mode()]
						}
						default: {
							return ["error", `new entry Unhandled type ${await commit3.type()}`]
						}
					}
				}
				case "011": {
					switch (`${await commit3.type()}-${await workdir.type()}`) {
						case "tree-tree": {
							return
						}
						case "tree-blob":
						case "blob-tree": {
							return ["conflict", fullpath]
						}
						case "blob-blob": {
							if ((await commit3.oid()) !== (await workdir.oid())) {
								if (force) {
									return [
										"update",
										fullpath,
										await commit3.oid(),
										await commit3.mode(),
										(await commit3.mode()) !== (await workdir.mode()),
									]
								} else {
									return ["conflict", fullpath]
								}
							} else {
								if ((await commit3.mode()) !== (await workdir.mode())) {
									if (force) {
										return ["update", fullpath, await commit3.oid(), await commit3.mode(), true]
									} else {
										return ["conflict", fullpath]
									}
								} else {
									return ["create-index", fullpath, await commit3.oid(), await commit3.mode()]
								}
							}
						}
						case "commit-tree": {
							return
						}
						case "commit-blob": {
							return ["conflict", fullpath]
						}
						default: {
							return ["error", `new entry Unhandled type ${commit3.type}`]
						}
					}
				}
				case "100": {
					return ["delete-index", fullpath]
				}
				case "101": {
					switch (await stage.type()) {
						case "tree": {
							return ["rmdir", fullpath]
						}
						case "blob": {
							if ((await stage.oid()) !== (await workdir.oid())) {
								if (force) {
									return ["delete", fullpath]
								} else {
									return ["conflict", fullpath]
								}
							} else {
								return ["delete", fullpath]
							}
						}
						case "commit": {
							return ["rmdir-index", fullpath]
						}
						default: {
							return ["error", `delete entry Unhandled type ${await stage.type()}`]
						}
					}
				}
				case "110":
				case "111": {
					switch (`${await stage.type()}-${await commit3.type()}`) {
						case "tree-tree": {
							return
						}
						case "blob-blob": {
							if (
								(await stage.oid()) === (await commit3.oid()) &&
								(await stage.mode()) === (await commit3.mode()) &&
								!force
							) {
								return
							}
							if (workdir) {
								if (
									(await workdir.oid()) !== (await stage.oid()) &&
									(await workdir.oid()) !== (await commit3.oid())
								) {
									if (force) {
										return [
											"update",
											fullpath,
											await commit3.oid(),
											await commit3.mode(),
											(await commit3.mode()) !== (await workdir.mode()),
										]
									} else {
										return ["conflict", fullpath]
									}
								}
							} else if (force) {
								return [
									"update",
									fullpath,
									await commit3.oid(),
									await commit3.mode(),
									(await commit3.mode()) !== (await stage.mode()),
								]
							}
							if ((await commit3.mode()) !== (await stage.mode())) {
								return ["update", fullpath, await commit3.oid(), await commit3.mode(), true]
							}
							if ((await commit3.oid()) !== (await stage.oid())) {
								return ["update", fullpath, await commit3.oid(), await commit3.mode(), false]
							} else {
								return
							}
						}
						case "tree-blob": {
							return ["update-dir-to-blob", fullpath, await commit3.oid()]
						}
						case "blob-tree": {
							return ["update-blob-to-tree", fullpath]
						}
						case "commit-commit": {
							return ["mkdir-index", fullpath, await commit3.oid(), await commit3.mode()]
						}
						default: {
							return [
								"error",
								`update entry Unhandled type ${await stage.type()}-${await commit3.type()}`,
							]
						}
					}
				}
			}
		},
		// Modify the default flat mapping
		reduce: async function (parent, children2) {
			children2 = flat(children2)
			if (!parent) {
				return children2
			} else if (parent && parent[0] === "rmdir") {
				children2.push(parent)
				return children2
			} else {
				children2.unshift(parent)
				return children2
			}
		},
	})
}
async function checkout({
	fs: fs2,
	onProgress,
	dir,
	gitdir = join(dir, ".git"),
	remote = "origin",
	ref: _ref,
	filepaths,
	noCheckout = false,
	noUpdateHead = _ref === void 0,
	dryRun = false,
	force = false,
	track = true,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("dir", dir)
		assertParameter("gitdir", gitdir)
		const ref = _ref || "HEAD"
		return await _checkout({
			fs: new FileSystem(fs2),
			cache: cache2,
			onProgress,
			dir,
			gitdir,
			remote,
			ref,
			filepaths,
			noCheckout,
			noUpdateHead,
			dryRun,
			force,
			track,
		})
	} catch (err) {
		err.caller = "git.checkout"
		throw err
	}
}
function abbreviateRef(ref) {
	const match = abbreviateRx.exec(ref)
	if (match) {
		if (match[1] === "remotes/" && ref.endsWith("/HEAD")) {
			return match[2].slice(0, -5)
		} else {
			return match[2]
		}
	}
	return ref
}
async function _currentBranch({ fs: fs2, gitdir, fullname = false, test = false }) {
	const ref = await GitRefManager.resolve({
		fs: fs2,
		gitdir,
		ref: "HEAD",
		depth: 2,
	})
	if (test) {
		try {
			await GitRefManager.resolve({ fs: fs2, gitdir, ref })
		} catch (_) {
			return
		}
	}
	if (!ref.startsWith("refs/")) return
	return fullname ? ref : abbreviateRef(ref)
}
function translateSSHtoHTTP(url) {
	url = url.replace(/^git@([^:]+):/, "https://$1/")
	url = url.replace(/^ssh:\/\//, "https://")
	return url
}
function calculateBasicAuthHeader({ username = "", password = "" }) {
	return `Basic ${Buffer.from(`${username}:${password}`).toString("base64")}`
}
async function forAwait(iterable, cb) {
	const iter = getIterator(iterable)
	while (true) {
		const { value, done } = await iter.next()
		if (value) await cb(value)
		if (done) break
	}
	if (iter.return) iter.return()
}
async function collect(iterable) {
	let size = 0
	const buffers = []
	await forAwait(iterable, (value) => {
		buffers.push(value)
		size += value.byteLength
	})
	const result = new Uint8Array(size)
	let nextIndex = 0
	for (const buffer of buffers) {
		result.set(buffer, nextIndex)
		nextIndex += buffer.byteLength
	}
	return result
}
function extractAuthFromUrl(url) {
	let userpass = url.match(/^https?:\/\/([^/]+)@/)
	if (userpass == undefined) return { url, auth: {} }
	userpass = userpass[1]
	const [username, password] = userpass.split(":")
	url = url.replace(`${userpass}@`, "")
	return { url, auth: { username, password } }
}
function padHex(b, n) {
	const s = n.toString(16)
	return "0".repeat(b - s.length) + s
}
async function parseCapabilitiesV2(read) {
	const capabilities2 = {}
	let line
	while (true) {
		line = await read()
		if (line === true) break
		if (line === null) continue
		line = line.toString("utf8").replace(/\n$/, "")
		const i = line.indexOf("=")
		if (i > -1) {
			const key = line.slice(0, i)
			const value = line.slice(i + 1)
			capabilities2[key] = value
		} else {
			capabilities2[line] = true
		}
	}
	return { protocolVersion: 2, capabilities2 }
}
async function parseRefsAdResponse(stream, { service }) {
	const capabilities = /* @__PURE__ */ new Set()
	const refs = /* @__PURE__ */ new Map()
	const symrefs = /* @__PURE__ */ new Map()
	const read = GitPktLine.streamReader(stream)
	let lineOne = await read()
	while (lineOne === null) lineOne = await read()
	if (lineOne === true) throw new EmptyServerResponseError()
	if (lineOne.includes("version 2")) {
		return parseCapabilitiesV2(read)
	}
	if (lineOne.toString("utf8").replace(/\n$/, "") !== `# service=${service}`) {
		throw new ParseError(`# service=${service}\\n`, lineOne.toString("utf8"))
	}
	let lineTwo = await read()
	while (lineTwo === null) lineTwo = await read()
	if (lineTwo === true) return { capabilities, refs, symrefs }
	lineTwo = lineTwo.toString("utf8")
	if (lineTwo.includes("version 2")) {
		return parseCapabilitiesV2(read)
	}
	const [firstRef, capabilitiesLine] = splitAndAssert(lineTwo, "\0", "\\x00")
	capabilitiesLine.split(" ").map((x) => capabilities.add(x))
	const [ref, name] = splitAndAssert(firstRef, " ", " ")
	refs.set(name, ref)
	while (true) {
		const line = await read()
		if (line === true) break
		if (line !== null) {
			const [ref2, name2] = splitAndAssert(line.toString("utf8"), " ", " ")
			refs.set(name2, ref2)
		}
	}
	for (const cap of capabilities) {
		if (cap.startsWith("symref=")) {
			const m = cap.match(/symref=([^:]+):(.*)/)
			if (m.length === 3) {
				symrefs.set(m[1], m[2])
			}
		}
	}
	return { protocolVersion: 1, capabilities, refs, symrefs }
}
function splitAndAssert(line, sep, expected) {
	const split = line.trim().split(sep)
	if (split.length !== 2) {
		throw new ParseError(`Two strings separated by '${expected}'`, line.toString("utf8"))
	}
	return split
}
function parseRemoteUrl({ url }) {
	if (url.startsWith("git@")) {
		return {
			transport: "ssh",
			address: url,
		}
	}
	const matches = url.match(/(\w+)(:\/\/|::)(.*)/)
	if (matches === null) return
	if (matches[2] === "://") {
		return {
			transport: matches[1],
			address: matches[0],
		}
	}
	if (matches[2] === "::") {
		return {
			transport: matches[1],
			address: matches[3],
		}
	}
}
async function hasObjectLoose({ fs: fs2, gitdir, oid }) {
	const source = `objects/${oid.slice(0, 2)}/${oid.slice(2)}`
	return fs2.exists(`${gitdir}/${source}`)
}
async function hasObjectPacked({ fs: fs2, cache: cache2, gitdir, oid, getExternalRefDelta }) {
	let list = await fs2.readdir(join(gitdir, "objects/pack"))
	list = list.filter((x) => x.endsWith(".idx"))
	for (const filename of list) {
		const indexFile = `${gitdir}/objects/pack/${filename}`
		const p = await readPackIndex({
			fs: fs2,
			cache: cache2,
			filename: indexFile,
			getExternalRefDelta,
		})
		if (p.error) throw new InternalError(p.error)
		if (p.offsets.has(oid)) {
			return true
		}
	}
	return false
}
async function hasObject({ fs: fs2, cache: cache2, gitdir, oid, format = "content" }) {
	const getExternalRefDelta = (oid2) => _readObject({ fs: fs2, cache: cache2, gitdir, oid: oid2 })
	let result = await hasObjectLoose({ fs: fs2, gitdir, oid })
	if (!result) {
		result = await hasObjectPacked({
			fs: fs2,
			cache: cache2,
			gitdir,
			oid,
			getExternalRefDelta,
		})
	}
	return result
}
function emptyPackfile(pack) {
	const pheader = "5041434b"
	const version3 = "00000002"
	const obCount = "00000000"
	const header = pheader + version3 + obCount
	return pack.slice(0, 12).toString("hex") === header
}
function filterCapabilities(server, client) {
	const serverNames = server.map((cap) => cap.split("=", 1)[0])
	return client.filter((cap) => {
		const name = cap.split("=", 1)[0]
		return serverNames.includes(name)
	})
}
function findSplit(str) {
	const r = str.indexOf("\r")
	const n = str.indexOf("\n")
	if (r === -1 && n === -1) return -1
	if (r === -1) return n + 1
	if (n === -1) return r + 1
	if (n === r + 1) return n + 1
	return Math.min(r, n) + 1
}
function splitLines(input) {
	const output = new FIFO()
	let tmp = ""
	;(async () => {
		await forAwait(input, (chunk) => {
			chunk = chunk.toString("utf8")
			tmp += chunk
			while (true) {
				const i = findSplit(tmp)
				if (i === -1) break
				output.write(tmp.slice(0, i))
				tmp = tmp.slice(i)
			}
		})
		if (tmp.length > 0) {
			output.write(tmp)
		}
		output.end()
	})()
	return output
}
async function parseUploadPackResponse(stream) {
	const { packetlines, packfile, progress } = GitSideBand.demux(stream)
	const shallows = []
	const unshallows = []
	const acks = []
	let nak = false
	let done = false
	return new Promise((resolve, reject) => {
		forAwait(packetlines, (data) => {
			const line = data.toString("utf8").trim()
			if (line.startsWith("shallow")) {
				const oid = line.slice(-41).trim()
				if (oid.length !== 40) {
					reject(new InvalidOidError(oid))
				}
				shallows.push(oid)
			} else if (line.startsWith("unshallow")) {
				const oid = line.slice(-41).trim()
				if (oid.length !== 40) {
					reject(new InvalidOidError(oid))
				}
				unshallows.push(oid)
			} else if (line.startsWith("ACK")) {
				const [, oid, status3] = line.split(" ")
				acks.push({ oid, status: status3 })
				if (!status3) done = true
			} else if (line.startsWith("NAK")) {
				nak = true
				done = true
			} else {
				done = true
				nak = true
			}
			if (done) {
				stream.error
					? reject(stream.error)
					: resolve({ shallows, unshallows, acks, nak, packfile, progress })
			}
		}).finally(() => {
			if (!done) {
				stream.error
					? reject(stream.error)
					: resolve({ shallows, unshallows, acks, nak, packfile, progress })
			}
		})
	})
}
function writeUploadPackRequest({
	capabilities = [],
	wants = [],
	haves = [],
	shallows = [],
	depth = null,
	since = null,
	exclude = [],
}) {
	const packstream = []
	wants = [...new Set(wants)]
	let firstLineCapabilities = ` ${capabilities.join(" ")}`
	for (const oid of wants) {
		packstream.push(
			GitPktLine.encode(`want ${oid}${firstLineCapabilities}
`)
		)
		firstLineCapabilities = ""
	}
	for (const oid of shallows) {
		packstream.push(
			GitPktLine.encode(`shallow ${oid}
`)
		)
	}
	if (depth !== null) {
		packstream.push(
			GitPktLine.encode(`deepen ${depth}
`)
		)
	}
	if (since !== null) {
		packstream.push(
			GitPktLine.encode(`deepen-since ${Math.floor(since.valueOf() / 1e3)}
`)
		)
	}
	for (const oid of exclude) {
		packstream.push(
			GitPktLine.encode(`deepen-not ${oid}
`)
		)
	}
	packstream.push(GitPktLine.flush())
	for (const oid of haves) {
		packstream.push(
			GitPktLine.encode(`have ${oid}
`)
		)
	}
	packstream.push(
		GitPktLine.encode(`done
`)
	)
	return packstream
}
async function _fetch({
	fs: fs2,
	cache: cache2,
	http,
	onProgress,
	onMessage,
	onAuth,
	onAuthSuccess,
	onAuthFailure,
	gitdir,
	ref: _ref,
	remoteRef: _remoteRef,
	remote: _remote,
	url: _url,
	corsProxy,
	depth = null,
	since = null,
	exclude = [],
	relative = false,
	tags = false,
	singleBranch = false,
	headers = {},
	prune = false,
	pruneTags = false,
}) {
	const ref = _ref || (await _currentBranch({ fs: fs2, gitdir, test: true }))
	const config = await GitConfigManager.get({ fs: fs2, gitdir })
	const remote = _remote || (ref && (await config.get(`branch.${ref}.remote`))) || "origin"
	const url = _url || (await config.get(`remote.${remote}.url`))
	if (typeof url === "undefined") {
		throw new MissingParameterError("remote OR url")
	}
	const remoteRef =
		_remoteRef || (ref && (await config.get(`branch.${ref}.merge`))) || _ref || "HEAD"
	if (corsProxy === void 0) {
		corsProxy = await config.get("http.corsProxy")
	}
	const GitRemoteHTTP2 = GitRemoteManager.getRemoteHelperFor({ url })
	const remoteHTTP = await GitRemoteHTTP2.discover({
		http,
		onAuth,
		onAuthSuccess,
		onAuthFailure,
		corsProxy,
		service: "git-upload-pack",
		url,
		headers,
		protocolVersion: 1,
	})
	const auth = remoteHTTP.auth
	const remoteRefs = remoteHTTP.refs
	if (remoteRefs.size === 0) {
		return {
			defaultBranch: null,
			fetchHead: null,
			fetchHeadDescription: null,
		}
	}
	if (depth !== null && !remoteHTTP.capabilities.has("shallow")) {
		throw new RemoteCapabilityError("shallow", "depth")
	}
	if (since !== null && !remoteHTTP.capabilities.has("deepen-since")) {
		throw new RemoteCapabilityError("deepen-since", "since")
	}
	if (exclude.length > 0 && !remoteHTTP.capabilities.has("deepen-not")) {
		throw new RemoteCapabilityError("deepen-not", "exclude")
	}
	if (relative === true && !remoteHTTP.capabilities.has("deepen-relative")) {
		throw new RemoteCapabilityError("deepen-relative", "relative")
	}
	const { oid, fullref } = GitRefManager.resolveAgainstMap({
		ref: remoteRef,
		map: remoteRefs,
	})
	for (const remoteRef2 of remoteRefs.keys()) {
		if (
			remoteRef2 === fullref ||
			remoteRef2 === "HEAD" ||
			remoteRef2.startsWith("refs/heads/") ||
			(tags && remoteRef2.startsWith("refs/tags/"))
		) {
			continue
		}
		remoteRefs.delete(remoteRef2)
	}
	const capabilities = filterCapabilities(
		[...remoteHTTP.capabilities],
		[
			"multi_ack_detailed",
			"no-done",
			"side-band-64k",
			// Note: I removed 'thin-pack' option since our code doesn't "fatten" packfiles,
			// which is necessary for compatibility with git. It was the cause of mysterious
			// 'fatal: pack has [x] unresolved deltas' errors that plagued us for some time.
			// isomorphic-git is perfectly happy with thin packfiles in .git/objects/pack but
			// canonical git it turns out is NOT.
			"ofs-delta",
			`agent=${pkg.agent}`,
		]
	)
	if (relative) capabilities.push("deepen-relative")
	const wants = singleBranch ? [oid] : remoteRefs.values()
	const haveRefs = singleBranch
		? [ref]
		: await GitRefManager.listRefs({
				fs: fs2,
				gitdir,
				filepath: `refs`,
		  })
	let haves = []
	for (let ref2 of haveRefs) {
		try {
			ref2 = await GitRefManager.expand({ fs: fs2, gitdir, ref: ref2 })
			const oid2 = await GitRefManager.resolve({ fs: fs2, gitdir, ref: ref2 })
			if (await hasObject({ fs: fs2, cache: cache2, gitdir, oid: oid2 })) {
				haves.push(oid2)
			}
		} catch (err) {}
	}
	haves = [...new Set(haves)]
	const oids = await GitShallowManager.read({ fs: fs2, gitdir })
	const shallows = remoteHTTP.capabilities.has("shallow") ? [...oids] : []
	const packstream = writeUploadPackRequest({
		capabilities,
		wants,
		haves,
		shallows,
		depth,
		since,
		exclude,
	})
	const packbuffer = Buffer.from(await collect(packstream))
	const raw = await GitRemoteHTTP2.connect({
		http,
		onProgress,
		corsProxy,
		service: "git-upload-pack",
		url,
		auth,
		body: [packbuffer],
		headers,
	})
	const response = await parseUploadPackResponse(raw.body)
	if (raw.headers) {
		response.headers = raw.headers
	}
	for (const oid2 of response.shallows) {
		if (!oids.has(oid2)) {
			try {
				const { object } = await _readObject({ fs: fs2, cache: cache2, gitdir, oid: oid2 })
				const commit3 = new GitCommit(object)
				const hasParents = await Promise.all(
					commit3
						.headers()
						.parent.map((oid3) => hasObject({ fs: fs2, cache: cache2, gitdir, oid: oid3 }))
				)
				const haveAllParents = hasParents.length === 0 || hasParents.every((has) => has)
				if (!haveAllParents) {
					oids.add(oid2)
				}
			} catch (err) {
				oids.add(oid2)
			}
		}
	}
	for (const oid2 of response.unshallows) {
		oids.delete(oid2)
	}
	await GitShallowManager.write({ fs: fs2, gitdir, oids })
	if (singleBranch) {
		const refs = /* @__PURE__ */ new Map([[fullref, oid]])
		const symrefs = /* @__PURE__ */ new Map()
		let bail = 10
		let key = fullref
		while (bail--) {
			const value = remoteHTTP.symrefs.get(key)
			if (value === void 0) break
			symrefs.set(key, value)
			key = value
		}
		const realRef = remoteRefs.get(key)
		if (realRef) {
			refs.set(key, realRef)
		}
		const { pruned } = await GitRefManager.updateRemoteRefs({
			fs: fs2,
			gitdir,
			remote,
			refs,
			symrefs,
			tags,
			prune,
		})
		if (prune) {
			response.pruned = pruned
		}
	} else {
		const { pruned } = await GitRefManager.updateRemoteRefs({
			fs: fs2,
			gitdir,
			remote,
			refs: remoteRefs,
			symrefs: remoteHTTP.symrefs,
			tags,
			prune,
			pruneTags,
		})
		if (prune) {
			response.pruned = pruned
		}
	}
	response.HEAD = remoteHTTP.symrefs.get("HEAD")
	if (response.HEAD === void 0) {
		const { oid: oid2 } = GitRefManager.resolveAgainstMap({
			ref: "HEAD",
			map: remoteRefs,
		})
		for (const [key, value] of remoteRefs.entries()) {
			if (key !== "HEAD" && value === oid2) {
				response.HEAD = key
				break
			}
		}
	}
	const noun = fullref.startsWith("refs/tags") ? "tag" : "branch"
	response.FETCH_HEAD = {
		oid,
		description: `${noun} '${abbreviateRef(fullref)}' of ${url}`,
	}
	if (onProgress || onMessage) {
		const lines = splitLines(response.progress)
		forAwait(lines, async (line) => {
			if (onMessage) await onMessage(line)
			if (onProgress) {
				const matches = line.match(/([^:]*).*\((\d+?)\/(\d+?)\)/)
				if (matches) {
					await onProgress({
						phase: matches[1].trim(),
						loaded: parseInt(matches[2], 10),
						total: parseInt(matches[3], 10),
					})
				}
			}
		})
	}
	const packfile = Buffer.from(await collect(response.packfile))
	if (raw.body.error) throw raw.body.error
	const packfileSha = packfile.slice(-20).toString("hex")
	const res = {
		defaultBranch: response.HEAD,
		fetchHead: response.FETCH_HEAD.oid,
		fetchHeadDescription: response.FETCH_HEAD.description,
	}
	if (response.headers) {
		res.headers = response.headers
	}
	if (prune) {
		res.pruned = response.pruned
	}
	if (packfileSha !== "" && !emptyPackfile(packfile)) {
		res.packfile = `objects/pack/pack-${packfileSha}.pack`
		const fullpath = join(gitdir, res.packfile)
		await fs2.write(fullpath, packfile)
		const getExternalRefDelta = (oid2) => _readObject({ fs: fs2, cache: cache2, gitdir, oid: oid2 })
		const idx = await GitPackIndex.fromPack({
			pack: packfile,
			getExternalRefDelta,
			onProgress,
		})
		await fs2.write(fullpath.replace(/\.pack$/, ".idx"), await idx.toBuffer())
	}
	return res
}
async function _init({
	fs: fs2,
	bare = false,
	dir,
	gitdir = bare ? dir : join(dir, ".git"),
	defaultBranch = "master",
}) {
	if (await fs2.exists(gitdir + "/config")) return
	let folders = ["hooks", "info", "objects/info", "objects/pack", "refs/heads", "refs/tags"]
	folders = folders.map((dir2) => gitdir + "/" + dir2)
	for (const folder of folders) {
		await fs2.mkdir(folder)
	}
	await fs2.write(
		gitdir + "/config",
		`[core]
	repositoryformatversion = 0
	filemode = false
	bare = ${bare}
` +
			(bare ? "" : "	logallrefupdates = true\n") +
			"	symlinks = false\n	ignorecase = true\n"
	)
	await fs2.write(
		gitdir + "/HEAD",
		`ref: refs/heads/${defaultBranch}
`
	)
}
async function _clone({
	fs: fs2,
	cache: cache2,
	http,
	onProgress,
	onMessage,
	onAuth,
	onAuthSuccess,
	onAuthFailure,
	dir,
	gitdir,
	url,
	corsProxy,
	ref,
	remote,
	depth,
	since,
	exclude,
	relative,
	singleBranch,
	noCheckout,
	noTags,
	headers,
}) {
	try {
		await _init({ fs: fs2, gitdir })
		await _addRemote({ fs: fs2, gitdir, remote, url, force: false })
		if (corsProxy) {
			const config = await GitConfigManager.get({ fs: fs2, gitdir })
			await config.set(`http.corsProxy`, corsProxy)
			await GitConfigManager.save({ fs: fs2, gitdir, config })
		}
		const { defaultBranch, fetchHead } = await _fetch({
			fs: fs2,
			cache: cache2,
			http,
			onProgress,
			onMessage,
			onAuth,
			onAuthSuccess,
			onAuthFailure,
			gitdir,
			ref,
			remote,
			corsProxy,
			depth,
			since,
			exclude,
			relative,
			singleBranch,
			headers,
			tags: !noTags,
		})
		if (fetchHead === null) return
		ref = ref || defaultBranch
		ref = ref.replace("refs/heads/", "")
		await _checkout({
			fs: fs2,
			cache: cache2,
			onProgress,
			dir,
			gitdir,
			ref,
			remote,
			noCheckout,
		})
	} catch (err) {
		await fs2.rmdir(gitdir, { recursive: true, maxRetries: 10 }).catch(() => void 0)
		throw err
	}
}
async function clone({
	fs: fs2,
	http,
	onProgress,
	onMessage,
	onAuth,
	onAuthSuccess,
	onAuthFailure,
	dir,
	gitdir = join(dir, ".git"),
	url,
	corsProxy = void 0,
	ref = void 0,
	remote = "origin",
	depth = void 0,
	since = void 0,
	exclude = [],
	relative = false,
	singleBranch = false,
	noCheckout = false,
	noTags = false,
	headers = {},
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("http", http)
		assertParameter("gitdir", gitdir)
		if (!noCheckout) {
			assertParameter("dir", dir)
		}
		assertParameter("url", url)
		return await _clone({
			fs: new FileSystem(fs2),
			cache: cache2,
			http,
			onProgress,
			onMessage,
			onAuth,
			onAuthSuccess,
			onAuthFailure,
			dir,
			gitdir,
			url,
			corsProxy,
			ref,
			remote,
			depth,
			since,
			exclude,
			relative,
			singleBranch,
			noCheckout,
			noTags,
			headers,
		})
	} catch (err) {
		err.caller = "git.clone"
		throw err
	}
}
async function commit({
	fs: _fs,
	onSign,
	dir,
	gitdir = join(dir, ".git"),
	message,
	author: _author,
	committer: _committer,
	signingKey,
	dryRun = false,
	noUpdateBranch = false,
	ref,
	parent,
	tree,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", _fs)
		assertParameter("message", message)
		if (signingKey) {
			assertParameter("onSign", onSign)
		}
		const fs2 = new FileSystem(_fs)
		const author = await normalizeAuthorObject({ fs: fs2, gitdir, author: _author })
		if (!author) throw new MissingNameError("author")
		const committer = await normalizeCommitterObject({
			fs: fs2,
			gitdir,
			author,
			committer: _committer,
		})
		if (!committer) throw new MissingNameError("committer")
		return await _commit({
			fs: fs2,
			cache: cache2,
			onSign,
			gitdir,
			message,
			author,
			committer,
			signingKey,
			dryRun,
			noUpdateBranch,
			ref,
			parent,
			tree,
		})
	} catch (err) {
		err.caller = "git.commit"
		throw err
	}
}
async function currentBranch({
	fs: fs2,
	dir,
	gitdir = join(dir, ".git"),
	fullname = false,
	test = false,
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		return await _currentBranch({
			fs: new FileSystem(fs2),
			gitdir,
			fullname,
			test,
		})
	} catch (err) {
		err.caller = "git.currentBranch"
		throw err
	}
}
async function _deleteBranch({ fs: fs2, gitdir, ref }) {
	ref = ref.startsWith("refs/heads/") ? ref : `refs/heads/${ref}`
	const exist = await GitRefManager.exists({ fs: fs2, gitdir, ref })
	if (!exist) {
		throw new NotFoundError(ref)
	}
	const fullRef = await GitRefManager.expand({ fs: fs2, gitdir, ref })
	const currentRef = await _currentBranch({ fs: fs2, gitdir, fullname: true })
	if (fullRef === currentRef) {
		const value = await GitRefManager.resolve({ fs: fs2, gitdir, ref: fullRef })
		await GitRefManager.writeRef({ fs: fs2, gitdir, ref: "HEAD", value })
	}
	await GitRefManager.deleteRef({ fs: fs2, gitdir, ref: fullRef })
}
async function deleteBranch({ fs: fs2, dir, gitdir = join(dir, ".git"), ref }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("ref", ref)
		return await _deleteBranch({
			fs: new FileSystem(fs2),
			gitdir,
			ref,
		})
	} catch (err) {
		err.caller = "git.deleteBranch"
		throw err
	}
}
async function deleteRef({ fs: fs2, dir, gitdir = join(dir, ".git"), ref }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("ref", ref)
		await GitRefManager.deleteRef({ fs: new FileSystem(fs2), gitdir, ref })
	} catch (err) {
		err.caller = "git.deleteRef"
		throw err
	}
}
async function _deleteRemote({ fs: fs2, gitdir, remote }) {
	const config = await GitConfigManager.get({ fs: fs2, gitdir })
	await config.deleteSection("remote", remote)
	await GitConfigManager.save({ fs: fs2, gitdir, config })
}
async function deleteRemote({ fs: fs2, dir, gitdir = join(dir, ".git"), remote }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("remote", remote)
		return await _deleteRemote({
			fs: new FileSystem(fs2),
			gitdir,
			remote,
		})
	} catch (err) {
		err.caller = "git.deleteRemote"
		throw err
	}
}
async function _deleteTag({ fs: fs2, gitdir, ref }) {
	ref = ref.startsWith("refs/tags/") ? ref : `refs/tags/${ref}`
	await GitRefManager.deleteRef({ fs: fs2, gitdir, ref })
}
async function deleteTag({ fs: fs2, dir, gitdir = join(dir, ".git"), ref }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("ref", ref)
		return await _deleteTag({
			fs: new FileSystem(fs2),
			gitdir,
			ref,
		})
	} catch (err) {
		err.caller = "git.deleteTag"
		throw err
	}
}
async function expandOidLoose({ fs: fs2, gitdir, oid: short }) {
	const prefix = short.slice(0, 2)
	const objectsSuffixes = await fs2.readdir(`${gitdir}/objects/${prefix}`)
	return objectsSuffixes
		.map((suffix) => `${prefix}${suffix}`)
		.filter((_oid) => _oid.startsWith(short))
}
async function expandOidPacked({
	fs: fs2,
	cache: cache2,
	gitdir,
	oid: short,
	getExternalRefDelta,
}) {
	const results = []
	let list = await fs2.readdir(join(gitdir, "objects/pack"))
	list = list.filter((x) => x.endsWith(".idx"))
	for (const filename of list) {
		const indexFile = `${gitdir}/objects/pack/${filename}`
		const p = await readPackIndex({
			fs: fs2,
			cache: cache2,
			filename: indexFile,
			getExternalRefDelta,
		})
		if (p.error) throw new InternalError(p.error)
		for (const oid of p.offsets.keys()) {
			if (oid.startsWith(short)) results.push(oid)
		}
	}
	return results
}
async function _expandOid({ fs: fs2, cache: cache2, gitdir, oid: short }) {
	const getExternalRefDelta = (oid) => _readObject({ fs: fs2, cache: cache2, gitdir, oid })
	const results = await expandOidLoose({ fs: fs2, gitdir, oid: short })
	const packedOids = await expandOidPacked({
		fs: fs2,
		cache: cache2,
		gitdir,
		oid: short,
		getExternalRefDelta,
	})
	for (const packedOid of packedOids) {
		if (!results.includes(packedOid)) {
			results.push(packedOid)
		}
	}
	if (results.length === 1) {
		return results[0]
	}
	if (results.length > 1) {
		throw new AmbiguousError("oids", short, results)
	}
	throw new NotFoundError(`an object matching "${short}"`)
}
async function expandOid({ fs: fs2, dir, gitdir = join(dir, ".git"), oid, cache: cache2 = {} }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("oid", oid)
		return await _expandOid({
			fs: new FileSystem(fs2),
			cache: cache2,
			gitdir,
			oid,
		})
	} catch (err) {
		err.caller = "git.expandOid"
		throw err
	}
}
async function expandRef({ fs: fs2, dir, gitdir = join(dir, ".git"), ref }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("ref", ref)
		return await GitRefManager.expand({
			fs: new FileSystem(fs2),
			gitdir,
			ref,
		})
	} catch (err) {
		err.caller = "git.expandRef"
		throw err
	}
}
async function _findMergeBase({ fs: fs2, cache: cache2, gitdir, oids }) {
	const visits = {}
	const passes = oids.length
	let heads = oids.map((oid, index2) => ({ index: index2, oid }))
	while (heads.length) {
		const result = /* @__PURE__ */ new Set()
		for (const { oid, index: index2 } of heads) {
			if (!visits[oid]) visits[oid] = /* @__PURE__ */ new Set()
			visits[oid].add(index2)
			if (visits[oid].size === passes) {
				result.add(oid)
			}
		}
		if (result.size > 0) {
			return [...result]
		}
		const newheads = /* @__PURE__ */ new Map()
		for (const { oid, index: index2 } of heads) {
			try {
				const { object } = await _readObject({ fs: fs2, cache: cache2, gitdir, oid })
				const commit3 = GitCommit.from(object)
				const { parent } = commit3.parseHeaders()
				for (const oid2 of parent) {
					if (!visits[oid2] || !visits[oid2].has(index2)) {
						newheads.set(oid2 + ":" + index2, { oid: oid2, index: index2 })
					}
				}
			} catch (err) {}
		}
		heads = [...newheads.values()]
	}
	return []
}
function mergeFile({ branches, contents }) {
	const ourName = branches[1]
	const theirName = branches[2]
	const baseContent = contents[0]
	const ourContent = contents[1]
	const theirContent = contents[2]
	const ours = ourContent.match(LINEBREAKS)
	const base = baseContent.match(LINEBREAKS)
	const theirs = theirContent.match(LINEBREAKS)
	const result = diff3_default(ours, base, theirs)
	const markerSize = 7
	let mergedText = ""
	let cleanMerge = true
	for (const item of result) {
		if (item.ok) {
			mergedText += item.ok.join("")
		}
		if (item.conflict) {
			cleanMerge = false
			mergedText += `${"<".repeat(markerSize)} ${ourName}
`
			mergedText += item.conflict.a.join("")
			mergedText += `${"=".repeat(markerSize)}
`
			mergedText += item.conflict.b.join("")
			mergedText += `${">".repeat(markerSize)} ${theirName}
`
		}
	}
	return { cleanMerge, mergedText }
}
async function mergeTree({
	fs: fs2,
	cache: cache2,
	dir,
	gitdir = join(dir, ".git"),
	index: index2,
	ourOid,
	baseOid,
	theirOid,
	ourName = "ours",
	baseName = "base",
	theirName = "theirs",
	dryRun = false,
	abortOnConflict = true,
	mergeDriver,
}) {
	const ourTree = TREE({ ref: ourOid })
	const baseTree = TREE({ ref: baseOid })
	const theirTree = TREE({ ref: theirOid })
	const unmergedFiles = []
	const bothModified = []
	const deleteByUs = []
	const deleteByTheirs = []
	const results = await _walk({
		fs: fs2,
		cache: cache2,
		dir,
		gitdir,
		trees: [ourTree, baseTree, theirTree],
		map: async function (filepath, [ours, base, theirs]) {
			const path = basename(filepath)
			const ourChange = await modified(ours, base)
			const theirChange = await modified(theirs, base)
			switch (`${ourChange}-${theirChange}`) {
				case "false-false": {
					return {
						mode: await base.mode(),
						path,
						oid: await base.oid(),
						type: await base.type(),
					}
				}
				case "false-true": {
					return theirs
						? {
								mode: await theirs.mode(),
								path,
								oid: await theirs.oid(),
								type: await theirs.type(),
						  }
						: void 0
				}
				case "true-false": {
					return ours
						? {
								mode: await ours.mode(),
								path,
								oid: await ours.oid(),
								type: await ours.type(),
						  }
						: void 0
				}
				case "true-true": {
					if (
						ours &&
						base &&
						theirs &&
						(await ours.type()) === "blob" &&
						(await base.type()) === "blob" &&
						(await theirs.type()) === "blob"
					) {
						return mergeBlobs({
							fs: fs2,
							gitdir,
							path,
							ours,
							base,
							theirs,
							ourName,
							baseName,
							theirName,
							mergeDriver,
						}).then(async (r) => {
							if (!r.cleanMerge) {
								unmergedFiles.push(filepath)
								bothModified.push(filepath)
								if (!abortOnConflict) {
									const baseOid2 = await base.oid()
									const ourOid2 = await ours.oid()
									const theirOid2 = await theirs.oid()
									index2.delete({ filepath })
									index2.insert({ filepath, oid: baseOid2, stage: 1 })
									index2.insert({ filepath, oid: ourOid2, stage: 2 })
									index2.insert({ filepath, oid: theirOid2, stage: 3 })
								}
							} else if (!abortOnConflict) {
								index2.insert({ filepath, oid: r.mergeResult.oid, stage: 0 })
							}
							return r.mergeResult
						})
					}
					if (
						base &&
						!ours &&
						theirs &&
						(await base.type()) === "blob" &&
						(await theirs.type()) === "blob"
					) {
						unmergedFiles.push(filepath)
						deleteByUs.push(filepath)
						if (!abortOnConflict) {
							const baseOid2 = await base.oid()
							const theirOid2 = await theirs.oid()
							index2.delete({ filepath })
							index2.insert({ filepath, oid: baseOid2, stage: 1 })
							index2.insert({ filepath, oid: theirOid2, stage: 3 })
						}
						return {
							mode: await theirs.mode(),
							oid: await theirs.oid(),
							type: "blob",
							path,
						}
					}
					if (
						base &&
						ours &&
						!theirs &&
						(await base.type()) === "blob" &&
						(await ours.type()) === "blob"
					) {
						unmergedFiles.push(filepath)
						deleteByTheirs.push(filepath)
						if (!abortOnConflict) {
							const baseOid2 = await base.oid()
							const ourOid2 = await ours.oid()
							index2.delete({ filepath })
							index2.insert({ filepath, oid: baseOid2, stage: 1 })
							index2.insert({ filepath, oid: ourOid2, stage: 2 })
						}
						return {
							mode: await ours.mode(),
							oid: await ours.oid(),
							type: "blob",
							path,
						}
					}
					if (base && !ours && !theirs && (await base.type()) === "blob") {
						return void 0
					}
					throw new MergeNotSupportedError()
				}
			}
		},
		/**
		 * @param {TreeEntry} [parent]
		 * @param {Array<TreeEntry>} children
		 */
		reduce:
			unmergedFiles.length !== 0 && (!dir || abortOnConflict)
				? void 0
				: async (parent, children2) => {
						const entries = children2.filter(Boolean)
						if (!parent) return
						if (parent && parent.type === "tree" && entries.length === 0) return
						if (entries.length > 0) {
							const tree = new GitTree(entries)
							const object = tree.toObject()
							const oid = await _writeObject({
								fs: fs2,
								gitdir,
								type: "tree",
								object,
								dryRun,
							})
							parent.oid = oid
						}
						return parent
				  },
	})
	if (unmergedFiles.length !== 0) {
		if (dir && !abortOnConflict) {
			await _walk({
				fs: fs2,
				cache: cache2,
				dir,
				gitdir,
				trees: [TREE({ ref: results.oid })],
				map: async function (filepath, [entry]) {
					const path = `${dir}/${filepath}`
					if ((await entry.type()) === "blob") {
						const mode = await entry.mode()
						const content = new TextDecoder().decode(await entry.content())
						await fs2.write(path, content, { mode })
					}
					return true
				},
			})
		}
		return new MergeConflictError(unmergedFiles, bothModified, deleteByUs, deleteByTheirs)
	}
	return results.oid
}
async function mergeBlobs({
	fs: fs2,
	gitdir,
	path,
	ours,
	base,
	theirs,
	ourName,
	theirName,
	baseName,
	dryRun,
	mergeDriver = mergeFile,
}) {
	const type = "blob"
	const mode = (await base.mode()) === (await ours.mode()) ? await theirs.mode() : await ours.mode()
	if ((await ours.oid()) === (await theirs.oid())) {
		return {
			cleanMerge: true,
			mergeResult: { mode, path, oid: await ours.oid(), type },
		}
	}
	if ((await ours.oid()) === (await base.oid())) {
		return {
			cleanMerge: true,
			mergeResult: { mode, path, oid: await theirs.oid(), type },
		}
	}
	if ((await theirs.oid()) === (await base.oid())) {
		return {
			cleanMerge: true,
			mergeResult: { mode, path, oid: await ours.oid(), type },
		}
	}
	const ourContent = Buffer.from(await ours.content()).toString("utf8")
	const baseContent = Buffer.from(await base.content()).toString("utf8")
	const theirContent = Buffer.from(await theirs.content()).toString("utf8")
	const { mergedText, cleanMerge } = await mergeDriver({
		branches: [baseName, ourName, theirName],
		contents: [baseContent, ourContent, theirContent],
		path,
	})
	const oid = await _writeObject({
		fs: fs2,
		gitdir,
		type: "blob",
		object: Buffer.from(mergedText, "utf8"),
		dryRun,
	})
	return { cleanMerge, mergeResult: { mode, path, oid, type } }
}
async function _merge({
	fs: fs2,
	cache: cache2,
	dir,
	gitdir,
	ours,
	theirs,
	fastForward: fastForward2 = true,
	fastForwardOnly = false,
	dryRun = false,
	noUpdateBranch = false,
	abortOnConflict = true,
	message,
	author,
	committer,
	signingKey,
	onSign,
	mergeDriver,
}) {
	if (ours === void 0) {
		ours = await _currentBranch({ fs: fs2, gitdir, fullname: true })
	}
	ours = await GitRefManager.expand({
		fs: fs2,
		gitdir,
		ref: ours,
	})
	theirs = await GitRefManager.expand({
		fs: fs2,
		gitdir,
		ref: theirs,
	})
	const ourOid = await GitRefManager.resolve({
		fs: fs2,
		gitdir,
		ref: ours,
	})
	const theirOid = await GitRefManager.resolve({
		fs: fs2,
		gitdir,
		ref: theirs,
	})
	const baseOids = await _findMergeBase({
		fs: fs2,
		cache: cache2,
		gitdir,
		oids: [ourOid, theirOid],
	})
	if (baseOids.length !== 1) {
		throw new MergeNotSupportedError()
	}
	const baseOid = baseOids[0]
	if (baseOid === theirOid) {
		return {
			oid: ourOid,
			alreadyMerged: true,
		}
	}
	if (fastForward2 && baseOid === ourOid) {
		if (!dryRun && !noUpdateBranch) {
			await GitRefManager.writeRef({ fs: fs2, gitdir, ref: ours, value: theirOid })
		}
		return {
			oid: theirOid,
			fastForward: true,
		}
	} else {
		if (fastForwardOnly) {
			throw new FastForwardError()
		}
		const tree = await GitIndexManager.acquire(
			{ fs: fs2, gitdir, cache: cache2, allowUnmerged: false },
			async (index2) => {
				return mergeTree({
					fs: fs2,
					cache: cache2,
					dir,
					gitdir,
					index: index2,
					ourOid,
					theirOid,
					baseOid,
					ourName: abbreviateRef(ours),
					baseName: "base",
					theirName: abbreviateRef(theirs),
					dryRun,
					abortOnConflict,
					mergeDriver,
				})
			}
		)
		if (tree instanceof MergeConflictError) throw tree
		if (!message) {
			message = `Merge branch '${abbreviateRef(theirs)}' into ${abbreviateRef(ours)}`
		}
		const oid = await _commit({
			fs: fs2,
			cache: cache2,
			gitdir,
			message,
			ref: ours,
			tree,
			parent: [ourOid, theirOid],
			author,
			committer,
			signingKey,
			onSign,
			dryRun,
			noUpdateBranch,
		})
		return {
			oid,
			tree,
			mergeCommit: true,
		}
	}
}
async function _pull({
	fs: fs2,
	cache: cache2,
	http,
	onProgress,
	onMessage,
	onAuth,
	onAuthSuccess,
	onAuthFailure,
	dir,
	gitdir,
	ref,
	url,
	remote,
	remoteRef,
	prune,
	pruneTags,
	fastForward: fastForward2,
	fastForwardOnly,
	corsProxy,
	singleBranch,
	headers,
	author,
	committer,
	signingKey,
}) {
	try {
		if (!ref) {
			const head = await _currentBranch({ fs: fs2, gitdir })
			if (!head) {
				throw new MissingParameterError("ref")
			}
			ref = head
		}
		const { fetchHead, fetchHeadDescription } = await _fetch({
			fs: fs2,
			cache: cache2,
			http,
			onProgress,
			onMessage,
			onAuth,
			onAuthSuccess,
			onAuthFailure,
			gitdir,
			corsProxy,
			ref,
			url,
			remote,
			remoteRef,
			singleBranch,
			headers,
			prune,
			pruneTags,
		})
		await _merge({
			fs: fs2,
			cache: cache2,
			gitdir,
			ours: ref,
			theirs: fetchHead,
			fastForward: fastForward2,
			fastForwardOnly,
			message: `Merge ${fetchHeadDescription}`,
			author,
			committer,
			signingKey,
			dryRun: false,
			noUpdateBranch: false,
		})
		await _checkout({
			fs: fs2,
			cache: cache2,
			onProgress,
			dir,
			gitdir,
			ref,
			remote,
			noCheckout: false,
		})
	} catch (err) {
		err.caller = "git.pull"
		throw err
	}
}
async function fastForward({
	fs: fs2,
	http,
	onProgress,
	onMessage,
	onAuth,
	onAuthSuccess,
	onAuthFailure,
	dir,
	gitdir = join(dir, ".git"),
	ref,
	url,
	remote,
	remoteRef,
	corsProxy,
	singleBranch,
	headers = {},
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("http", http)
		assertParameter("gitdir", gitdir)
		const thisWillNotBeUsed = {
			name: "",
			email: "",
			timestamp: Date.now(),
			timezoneOffset: 0,
		}
		return await _pull({
			fs: new FileSystem(fs2),
			cache: cache2,
			http,
			onProgress,
			onMessage,
			onAuth,
			onAuthSuccess,
			onAuthFailure,
			dir,
			gitdir,
			ref,
			url,
			remote,
			remoteRef,
			fastForwardOnly: true,
			corsProxy,
			singleBranch,
			headers,
			author: thisWillNotBeUsed,
			committer: thisWillNotBeUsed,
		})
	} catch (err) {
		err.caller = "git.fastForward"
		throw err
	}
}
async function fetch2({
	fs: fs2,
	http,
	onProgress,
	onMessage,
	onAuth,
	onAuthSuccess,
	onAuthFailure,
	dir,
	gitdir = join(dir, ".git"),
	ref,
	remote,
	remoteRef,
	url,
	corsProxy,
	depth = null,
	since = null,
	exclude = [],
	relative = false,
	tags = false,
	singleBranch = false,
	headers = {},
	prune = false,
	pruneTags = false,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("http", http)
		assertParameter("gitdir", gitdir)
		return await _fetch({
			fs: new FileSystem(fs2),
			cache: cache2,
			http,
			onProgress,
			onMessage,
			onAuth,
			onAuthSuccess,
			onAuthFailure,
			gitdir,
			ref,
			remote,
			remoteRef,
			url,
			corsProxy,
			depth,
			since,
			exclude,
			relative,
			tags,
			singleBranch,
			headers,
			prune,
			pruneTags,
		})
	} catch (err) {
		err.caller = "git.fetch"
		throw err
	}
}
async function findMergeBase({
	fs: fs2,
	dir,
	gitdir = join(dir, ".git"),
	oids,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("oids", oids)
		return await _findMergeBase({
			fs: new FileSystem(fs2),
			cache: cache2,
			gitdir,
			oids,
		})
	} catch (err) {
		err.caller = "git.findMergeBase"
		throw err
	}
}
async function _findRoot({ fs: fs2, filepath }) {
	if (await fs2.exists(join(filepath, ".git"))) {
		return filepath
	} else {
		const parent = dirname(filepath)
		if (parent === filepath) {
			throw new NotFoundError(`git root for ${filepath}`)
		}
		return _findRoot({ fs: fs2, filepath: parent })
	}
}
async function findRoot({ fs: fs2, filepath }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("filepath", filepath)
		return await _findRoot({ fs: new FileSystem(fs2), filepath })
	} catch (err) {
		err.caller = "git.findRoot"
		throw err
	}
}
async function getConfig({ fs: fs2, dir, gitdir = join(dir, ".git"), path }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("path", path)
		return await _getConfig({
			fs: new FileSystem(fs2),
			gitdir,
			path,
		})
	} catch (err) {
		err.caller = "git.getConfig"
		throw err
	}
}
async function _getConfigAll({ fs: fs2, gitdir, path }) {
	const config = await GitConfigManager.get({ fs: fs2, gitdir })
	return config.getall(path)
}
async function getConfigAll({ fs: fs2, dir, gitdir = join(dir, ".git"), path }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("path", path)
		return await _getConfigAll({
			fs: new FileSystem(fs2),
			gitdir,
			path,
		})
	} catch (err) {
		err.caller = "git.getConfigAll"
		throw err
	}
}
async function getRemoteInfo({
	http,
	onAuth,
	onAuthSuccess,
	onAuthFailure,
	corsProxy,
	url,
	headers = {},
	forPush = false,
}) {
	try {
		assertParameter("http", http)
		assertParameter("url", url)
		const GitRemoteHTTP2 = GitRemoteManager.getRemoteHelperFor({ url })
		const remote = await GitRemoteHTTP2.discover({
			http,
			onAuth,
			onAuthSuccess,
			onAuthFailure,
			corsProxy,
			service: forPush ? "git-receive-pack" : "git-upload-pack",
			url,
			headers,
			protocolVersion: 1,
		})
		const result = {
			capabilities: [...remote.capabilities],
		}
		for (const [ref, oid] of remote.refs) {
			const parts = ref.split("/")
			const last = parts.pop()
			let o = result
			for (const part of parts) {
				o[part] = o[part] || {}
				o = o[part]
			}
			o[last] = oid
		}
		for (const [symref, ref] of remote.symrefs) {
			const parts = symref.split("/")
			const last = parts.pop()
			let o = result
			for (const part of parts) {
				o[part] = o[part] || {}
				o = o[part]
			}
			o[last] = ref
		}
		return result
	} catch (err) {
		err.caller = "git.getRemoteInfo"
		throw err
	}
}
function formatInfoRefs(remote, prefix, symrefs, peelTags) {
	const refs = []
	for (const [key, value] of remote.refs) {
		if (prefix && !key.startsWith(prefix)) continue
		if (key.endsWith("^{}")) {
			if (peelTags) {
				const _key = key.replace("^{}", "")
				const last = refs.at(-1)
				const r = last.ref === _key ? last : refs.find((x) => x.ref === _key)
				if (r === void 0) {
					throw new Error("I did not expect this to happen")
				}
				r.peeled = value
			}
			continue
		}
		const ref = { ref: key, oid: value }
		if (symrefs && remote.symrefs.has(key)) {
			ref.target = remote.symrefs.get(key)
		}
		refs.push(ref)
	}
	return refs
}
async function getRemoteInfo2({
	http,
	onAuth,
	onAuthSuccess,
	onAuthFailure,
	corsProxy,
	url,
	headers = {},
	forPush = false,
	protocolVersion = 2,
}) {
	try {
		assertParameter("http", http)
		assertParameter("url", url)
		const GitRemoteHTTP2 = GitRemoteManager.getRemoteHelperFor({ url })
		const remote = await GitRemoteHTTP2.discover({
			http,
			onAuth,
			onAuthSuccess,
			onAuthFailure,
			corsProxy,
			service: forPush ? "git-receive-pack" : "git-upload-pack",
			url,
			headers,
			protocolVersion,
		})
		if (remote.protocolVersion === 2) {
			return {
				protocolVersion: remote.protocolVersion,
				capabilities: remote.capabilities2,
			}
		}
		const capabilities = {}
		for (const cap of remote.capabilities) {
			const [key, value] = cap.split("=")
			if (value) {
				capabilities[key] = value
			} else {
				capabilities[key] = true
			}
		}
		return {
			protocolVersion: 1,
			capabilities,
			refs: formatInfoRefs(remote, void 0, true, true),
		}
	} catch (err) {
		err.caller = "git.getRemoteInfo2"
		throw err
	}
}
async function hashObject({ type, object, format = "content", oid = void 0 }) {
	if (format !== "deflated") {
		if (format !== "wrapped") {
			object = GitObject.wrap({ type, object })
		}
		oid = await shasum(object)
	}
	return { oid, object }
}
async function hashBlob({ object }) {
	try {
		assertParameter("object", object)
		if (typeof object === "string") {
			object = Buffer.from(object, "utf8")
		} else {
			object = Buffer.from(object)
		}
		const type = "blob"
		const { oid, object: _object } = await hashObject({
			type: "blob",
			format: "content",
			object,
		})
		return { oid, type, object: new Uint8Array(_object), format: "wrapped" }
	} catch (err) {
		err.caller = "git.hashBlob"
		throw err
	}
}
async function _indexPack({ fs: fs2, cache: cache2, onProgress, dir, gitdir, filepath }) {
	try {
		filepath = join(dir, filepath)
		const pack = await fs2.read(filepath)
		const getExternalRefDelta = (oid) => _readObject({ fs: fs2, cache: cache2, gitdir, oid })
		const idx = await GitPackIndex.fromPack({
			pack,
			getExternalRefDelta,
			onProgress,
		})
		await fs2.write(filepath.replace(/\.pack$/, ".idx"), await idx.toBuffer())
		return {
			oids: [...idx.hashes],
		}
	} catch (err) {
		err.caller = "git.indexPack"
		throw err
	}
}
async function indexPack({
	fs: fs2,
	onProgress,
	dir,
	gitdir = join(dir, ".git"),
	filepath,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("dir", dir)
		assertParameter("gitdir", dir)
		assertParameter("filepath", filepath)
		return await _indexPack({
			fs: new FileSystem(fs2),
			cache: cache2,
			onProgress,
			dir,
			gitdir,
			filepath,
		})
	} catch (err) {
		err.caller = "git.indexPack"
		throw err
	}
}
async function init({
	fs: fs2,
	bare = false,
	dir,
	gitdir = bare ? dir : join(dir, ".git"),
	defaultBranch = "master",
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		if (!bare) {
			assertParameter("dir", dir)
		}
		return await _init({
			fs: new FileSystem(fs2),
			bare,
			dir,
			gitdir,
			defaultBranch,
		})
	} catch (err) {
		err.caller = "git.init"
		throw err
	}
}
async function _isDescendent({ fs: fs2, cache: cache2, gitdir, oid, ancestor, depth }) {
	const shallows = await GitShallowManager.read({ fs: fs2, gitdir })
	if (!oid) {
		throw new MissingParameterError("oid")
	}
	if (!ancestor) {
		throw new MissingParameterError("ancestor")
	}
	if (oid === ancestor) return false
	const queue = [oid]
	const visited = /* @__PURE__ */ new Set()
	let searchdepth = 0
	while (queue.length) {
		if (searchdepth++ === depth) {
			throw new MaxDepthError(depth)
		}
		const oid2 = queue.shift()
		const { type, object } = await _readObject({
			fs: fs2,
			cache: cache2,
			gitdir,
			oid: oid2,
		})
		if (type !== "commit") {
			throw new ObjectTypeError(oid2, type, "commit")
		}
		const commit3 = GitCommit.from(object).parse()
		for (const parent of commit3.parent) {
			if (parent === ancestor) return true
		}
		if (!shallows.has(oid2)) {
			for (const parent of commit3.parent) {
				if (!visited.has(parent)) {
					queue.push(parent)
					visited.add(parent)
				}
			}
		}
	}
	return false
}
async function isDescendent({
	fs: fs2,
	dir,
	gitdir = join(dir, ".git"),
	oid,
	ancestor,
	depth = -1,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("oid", oid)
		assertParameter("ancestor", ancestor)
		return await _isDescendent({
			fs: new FileSystem(fs2),
			cache: cache2,
			gitdir,
			oid,
			ancestor,
			depth,
		})
	} catch (err) {
		err.caller = "git.isDescendent"
		throw err
	}
}
async function isIgnored({ fs: fs2, dir, gitdir = join(dir, ".git"), filepath }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("dir", dir)
		assertParameter("gitdir", gitdir)
		assertParameter("filepath", filepath)
		return GitIgnoreManager.isIgnored({
			fs: new FileSystem(fs2),
			dir,
			gitdir,
			filepath,
		})
	} catch (err) {
		err.caller = "git.isIgnored"
		throw err
	}
}
async function listBranches({ fs: fs2, dir, gitdir = join(dir, ".git"), remote }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		return GitRefManager.listBranches({
			fs: new FileSystem(fs2),
			gitdir,
			remote,
		})
	} catch (err) {
		err.caller = "git.listBranches"
		throw err
	}
}
async function _listFiles({ fs: fs2, gitdir, ref, cache: cache2 }) {
	if (ref) {
		const oid = await GitRefManager.resolve({ gitdir, fs: fs2, ref })
		const filenames = []
		await accumulateFilesFromOid({
			fs: fs2,
			cache: cache2,
			gitdir,
			oid,
			filenames,
			prefix: "",
		})
		return filenames
	} else {
		return GitIndexManager.acquire({ fs: fs2, gitdir, cache: cache2 }, async function (index2) {
			return index2.entries.map((x) => x.path)
		})
	}
}
async function accumulateFilesFromOid({ fs: fs2, cache: cache2, gitdir, oid, filenames, prefix }) {
	const { tree } = await _readTree({ fs: fs2, cache: cache2, gitdir, oid })
	for (const entry of tree) {
		if (entry.type === "tree") {
			await accumulateFilesFromOid({
				fs: fs2,
				cache: cache2,
				gitdir,
				oid: entry.oid,
				filenames,
				prefix: join(prefix, entry.path),
			})
		} else {
			filenames.push(join(prefix, entry.path))
		}
	}
}
async function listFiles({ fs: fs2, dir, gitdir = join(dir, ".git"), ref, cache: cache2 = {} }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		return await _listFiles({
			fs: new FileSystem(fs2),
			cache: cache2,
			gitdir,
			ref,
		})
	} catch (err) {
		err.caller = "git.listFiles"
		throw err
	}
}
async function _listNotes({ fs: fs2, cache: cache2, gitdir, ref }) {
	let parent
	try {
		parent = await GitRefManager.resolve({ gitdir, fs: fs2, ref })
	} catch (err) {
		if (err instanceof NotFoundError) {
			return []
		}
	}
	const result = await _readTree({
		fs: fs2,
		cache: cache2,
		gitdir,
		oid: parent,
	})
	const notes = result.tree.map((entry) => ({
		target: entry.path,
		note: entry.oid,
	}))
	return notes
}
async function listNotes({
	fs: fs2,
	dir,
	gitdir = join(dir, ".git"),
	ref = "refs/notes/commits",
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("ref", ref)
		return await _listNotes({
			fs: new FileSystem(fs2),
			cache: cache2,
			gitdir,
			ref,
		})
	} catch (err) {
		err.caller = "git.listNotes"
		throw err
	}
}
async function _listRemotes({ fs: fs2, gitdir }) {
	const config = await GitConfigManager.get({ fs: fs2, gitdir })
	const remoteNames = await config.getSubsections("remote")
	const remotes = Promise.all(
		remoteNames.map(async (remote) => {
			const url = await config.get(`remote.${remote}.url`)
			return { remote, url }
		})
	)
	return remotes
}
async function listRemotes({ fs: fs2, dir, gitdir = join(dir, ".git") }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		return await _listRemotes({
			fs: new FileSystem(fs2),
			gitdir,
		})
	} catch (err) {
		err.caller = "git.listRemotes"
		throw err
	}
}
async function parseListRefsResponse(stream) {
	const read = GitPktLine.streamReader(stream)
	const refs = []
	let line
	while (true) {
		line = await read()
		if (line === true) break
		if (line === null) continue
		line = line.toString("utf8").replace(/\n$/, "")
		const [oid, ref, ...attrs] = line.split(" ")
		const r = { ref, oid }
		for (const attr of attrs) {
			const [name, value] = attr.split(":")
			if (name === "symref-target") {
				r.target = value
			} else if (name === "peeled") {
				r.peeled = value
			}
		}
		refs.push(r)
	}
	return refs
}
async function writeListRefsRequest({ prefix, symrefs, peelTags }) {
	const packstream = []
	packstream.push(GitPktLine.encode("command=ls-refs\n"))
	packstream.push(
		GitPktLine.encode(`agent=${pkg.agent}
`)
	)
	if (peelTags || symrefs || prefix) {
		packstream.push(GitPktLine.delim())
	}
	if (peelTags) packstream.push(GitPktLine.encode("peel"))
	if (symrefs) packstream.push(GitPktLine.encode("symrefs"))
	if (prefix) packstream.push(GitPktLine.encode(`ref-prefix ${prefix}`))
	packstream.push(GitPktLine.flush())
	return packstream
}
async function listServerRefs({
	http,
	onAuth,
	onAuthSuccess,
	onAuthFailure,
	corsProxy,
	url,
	headers = {},
	forPush = false,
	protocolVersion = 2,
	prefix,
	symrefs,
	peelTags,
}) {
	try {
		assertParameter("http", http)
		assertParameter("url", url)
		const remote = await GitRemoteHTTP.discover({
			http,
			onAuth,
			onAuthSuccess,
			onAuthFailure,
			corsProxy,
			service: forPush ? "git-receive-pack" : "git-upload-pack",
			url,
			headers,
			protocolVersion,
		})
		if (remote.protocolVersion === 1) {
			return formatInfoRefs(remote, prefix, symrefs, peelTags)
		}
		const body = await writeListRefsRequest({ prefix, symrefs, peelTags })
		const res = await GitRemoteHTTP.connect({
			http,
			auth: remote.auth,
			headers,
			corsProxy,
			service: forPush ? "git-receive-pack" : "git-upload-pack",
			url,
			body,
		})
		return parseListRefsResponse(res.body)
	} catch (err) {
		err.caller = "git.listServerRefs"
		throw err
	}
}
async function listTags({ fs: fs2, dir, gitdir = join(dir, ".git") }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		return GitRefManager.listTags({ fs: new FileSystem(fs2), gitdir })
	} catch (err) {
		err.caller = "git.listTags"
		throw err
	}
}
async function resolveCommit({ fs: fs2, cache: cache2, gitdir, oid }) {
	const { type, object } = await _readObject({ fs: fs2, cache: cache2, gitdir, oid })
	if (type === "tag") {
		oid = GitAnnotatedTag.from(object).parse().object
		return resolveCommit({ fs: fs2, cache: cache2, gitdir, oid })
	}
	if (type !== "commit") {
		throw new ObjectTypeError(oid, type, "commit")
	}
	return { commit: GitCommit.from(object), oid }
}
async function _readCommit({ fs: fs2, cache: cache2, gitdir, oid }) {
	const { commit: commit3, oid: commitOid } = await resolveCommit({
		fs: fs2,
		cache: cache2,
		gitdir,
		oid,
	})
	const result = {
		oid: commitOid,
		commit: commit3.parse(),
		payload: commit3.withoutSignature(),
	}
	return result
}
function compareAge(a, b) {
	return a.committer.timestamp - b.committer.timestamp
}
async function resolveFileIdInTree({ fs: fs2, cache: cache2, gitdir, oid, fileId }) {
	if (fileId === EMPTY_OID) return
	const _oid = oid
	let filepath
	const result = await resolveTree({ fs: fs2, cache: cache2, gitdir, oid })
	const tree = result.tree
	if (fileId === result.oid) {
		filepath = result.path
	} else {
		filepath = await _resolveFileId({
			fs: fs2,
			cache: cache2,
			gitdir,
			tree,
			fileId,
			oid: _oid,
		})
		if (Array.isArray(filepath)) {
			if (filepath.length === 0) filepath = void 0
			else if (filepath.length === 1) filepath = filepath[0]
		}
	}
	return filepath
}
async function _resolveFileId({
	fs: fs2,
	cache: cache2,
	gitdir,
	tree,
	fileId,
	oid,
	filepaths = [],
	parentPath = "",
}) {
	const walks = tree.entries().map(function (entry) {
		let result
		if (entry.oid === fileId) {
			result = join(parentPath, entry.path)
			filepaths.push(result)
		} else if (entry.type === "tree") {
			result = _readObject({
				fs: fs2,
				cache: cache2,
				gitdir,
				oid: entry.oid,
			}).then(function ({ object }) {
				return _resolveFileId({
					fs: fs2,
					cache: cache2,
					gitdir,
					tree: GitTree.from(object),
					fileId,
					oid,
					filepaths,
					parentPath: join(parentPath, entry.path),
				})
			})
		}
		return result
	})
	await Promise.all(walks)
	return filepaths
}
async function _log({
	fs: fs2,
	cache: cache2,
	gitdir,
	filepath,
	ref,
	depth,
	since,
	force,
	follow,
}) {
	const sinceTimestamp = typeof since === "undefined" ? void 0 : Math.floor(since.valueOf() / 1e3)
	const commits = []
	const shallowCommits = await GitShallowManager.read({ fs: fs2, gitdir })
	const oid = await GitRefManager.resolve({ fs: fs2, gitdir, ref })
	const tips = [await _readCommit({ fs: fs2, cache: cache2, gitdir, oid })]
	let lastFileOid
	let lastCommit
	let isOk
	function endCommit(commit3) {
		if (isOk && filepath) commits.push(commit3)
	}
	while (tips.length > 0) {
		const commit3 = tips.pop()
		if (sinceTimestamp !== void 0 && commit3.commit.committer.timestamp <= sinceTimestamp) {
			break
		}
		if (filepath) {
			let vFileOid
			try {
				vFileOid = await resolveFilepath({
					fs: fs2,
					cache: cache2,
					gitdir,
					oid: commit3.commit.tree,
					filepath,
				})
				if (lastCommit && lastFileOid !== vFileOid) {
					commits.push(lastCommit)
				}
				lastFileOid = vFileOid
				lastCommit = commit3
				isOk = true
			} catch (e) {
				if (e instanceof NotFoundError) {
					let found = follow && lastFileOid
					if (found) {
						found = await resolveFileIdInTree({
							fs: fs2,
							cache: cache2,
							gitdir,
							oid: commit3.commit.tree,
							fileId: lastFileOid,
						})
						if (found) {
							if (Array.isArray(found)) {
								if (lastCommit) {
									const lastFound = await resolveFileIdInTree({
										fs: fs2,
										cache: cache2,
										gitdir,
										oid: lastCommit.commit.tree,
										fileId: lastFileOid,
									})
									if (Array.isArray(lastFound)) {
										found = found.filter((p) => !lastFound.includes(p))
										if (found.length === 1) {
											found = found[0]
											filepath = found
											if (lastCommit) commits.push(lastCommit)
										} else {
											found = false
											if (lastCommit) commits.push(lastCommit)
											break
										}
									}
								}
							} else {
								filepath = found
								if (lastCommit) commits.push(lastCommit)
							}
						}
					}
					if (!found) {
						if (isOk && lastFileOid) {
							commits.push(lastCommit)
							if (!force) break
						}
						if (!force && !follow) throw e
					}
					lastCommit = commit3
					isOk = false
				} else throw e
			}
		} else {
			commits.push(commit3)
		}
		if (depth !== void 0 && commits.length === depth) {
			endCommit(commit3)
			break
		}
		if (!shallowCommits.has(commit3.oid)) {
			for (const oid2 of commit3.commit.parent) {
				const commit4 = await _readCommit({ fs: fs2, cache: cache2, gitdir, oid: oid2 })
				if (!tips.map((commit5) => commit5.oid).includes(commit4.oid)) {
					tips.push(commit4)
				}
			}
		}
		if (tips.length === 0) {
			endCommit(commit3)
		}
		tips.sort((a, b) => compareAge(a.commit, b.commit))
	}
	return commits
}
async function log({
	fs: fs2,
	dir,
	gitdir = join(dir, ".git"),
	filepath,
	ref = "HEAD",
	depth,
	since,
	// Date
	force,
	follow,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("ref", ref)
		return await _log({
			fs: new FileSystem(fs2),
			cache: cache2,
			gitdir,
			filepath,
			ref,
			depth,
			since,
			force,
			follow,
		})
	} catch (err) {
		err.caller = "git.log"
		throw err
	}
}
async function merge({
	fs: _fs,
	onSign,
	dir,
	gitdir = join(dir, ".git"),
	ours,
	theirs,
	fastForward: fastForward2 = true,
	fastForwardOnly = false,
	dryRun = false,
	noUpdateBranch = false,
	abortOnConflict = true,
	message,
	author: _author,
	committer: _committer,
	signingKey,
	cache: cache2 = {},
	mergeDriver,
}) {
	try {
		assertParameter("fs", _fs)
		if (signingKey) {
			assertParameter("onSign", onSign)
		}
		const fs2 = new FileSystem(_fs)
		const author = await normalizeAuthorObject({ fs: fs2, gitdir, author: _author })
		if (!author && (!fastForwardOnly || !fastForward2)) {
			throw new MissingNameError("author")
		}
		const committer = await normalizeCommitterObject({
			fs: fs2,
			gitdir,
			author,
			committer: _committer,
		})
		if (!committer && (!fastForwardOnly || !fastForward2)) {
			throw new MissingNameError("committer")
		}
		return await _merge({
			fs: fs2,
			cache: cache2,
			dir,
			gitdir,
			ours,
			theirs,
			fastForward: fastForward2,
			fastForwardOnly,
			dryRun,
			noUpdateBranch,
			abortOnConflict,
			message,
			author,
			committer,
			signingKey,
			onSign,
			mergeDriver,
		})
	} catch (err) {
		err.caller = "git.merge"
		throw err
	}
}
async function _pack({ fs: fs2, cache: cache2, dir, gitdir = join(dir, ".git"), oids }) {
	const hash2 = new import_sha12.default()
	const outputStream = []
	function write(chunk, enc) {
		const buff = Buffer.from(chunk, enc)
		outputStream.push(buff)
		hash2.update(buff)
	}
	async function writeObject2({ stype, object }) {
		const type = types[stype]
		let length = object.length
		let multibyte = length > 15 ? 128 : 0
		const lastFour = length & 15
		length = length >>> 4
		let byte = (multibyte | type | lastFour).toString(16)
		write(byte, "hex")
		while (multibyte) {
			multibyte = length > 127 ? 128 : 0
			byte = multibyte | (length & 127)
			write(padHex(2, byte), "hex")
			length = length >>> 7
		}
		write(Buffer.from(await deflate(object)))
	}
	write("PACK")
	write("00000002", "hex")
	write(padHex(8, oids.length), "hex")
	for (const oid of oids) {
		const { type, object } = await _readObject({ fs: fs2, cache: cache2, gitdir, oid })
		await writeObject2({ write, object, stype: type })
	}
	const digest = hash2.digest()
	outputStream.push(digest)
	return outputStream
}
async function _packObjects({ fs: fs2, cache: cache2, gitdir, oids, write }) {
	const buffers = await _pack({ fs: fs2, cache: cache2, gitdir, oids })
	const packfile = Buffer.from(await collect(buffers))
	const packfileSha = packfile.slice(-20).toString("hex")
	const filename = `pack-${packfileSha}.pack`
	if (write) {
		await fs2.write(join(gitdir, `objects/pack/${filename}`), packfile)
		return { filename }
	}
	return {
		filename,
		packfile: new Uint8Array(packfile),
	}
}
async function packObjects({
	fs: fs2,
	dir,
	gitdir = join(dir, ".git"),
	oids,
	write = false,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("oids", oids)
		return await _packObjects({
			fs: new FileSystem(fs2),
			cache: cache2,
			gitdir,
			oids,
			write,
		})
	} catch (err) {
		err.caller = "git.packObjects"
		throw err
	}
}
async function pull({
	fs: _fs,
	http,
	onProgress,
	onMessage,
	onAuth,
	onAuthSuccess,
	onAuthFailure,
	dir,
	gitdir = join(dir, ".git"),
	ref,
	url,
	remote,
	remoteRef,
	prune = false,
	pruneTags = false,
	fastForward: fastForward2 = true,
	fastForwardOnly = false,
	corsProxy,
	singleBranch,
	headers = {},
	author: _author,
	committer: _committer,
	signingKey,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", _fs)
		assertParameter("gitdir", gitdir)
		const fs2 = new FileSystem(_fs)
		const author = await normalizeAuthorObject({ fs: fs2, gitdir, author: _author })
		if (!author) throw new MissingNameError("author")
		const committer = await normalizeCommitterObject({
			fs: fs2,
			gitdir,
			author,
			committer: _committer,
		})
		if (!committer) throw new MissingNameError("committer")
		return await _pull({
			fs: fs2,
			cache: cache2,
			http,
			onProgress,
			onMessage,
			onAuth,
			onAuthSuccess,
			onAuthFailure,
			dir,
			gitdir,
			ref,
			url,
			remote,
			remoteRef,
			fastForward: fastForward2,
			fastForwardOnly,
			corsProxy,
			singleBranch,
			headers,
			author,
			committer,
			signingKey,
			prune,
			pruneTags,
		})
	} catch (err) {
		err.caller = "git.pull"
		throw err
	}
}
async function listCommitsAndTags({
	fs: fs2,
	cache: cache2,
	dir,
	gitdir = join(dir, ".git"),
	start,
	finish,
}) {
	const shallows = await GitShallowManager.read({ fs: fs2, gitdir })
	const startingSet = /* @__PURE__ */ new Set()
	const finishingSet = /* @__PURE__ */ new Set()
	for (const ref of start) {
		startingSet.add(await GitRefManager.resolve({ fs: fs2, gitdir, ref }))
	}
	for (const ref of finish) {
		try {
			const oid = await GitRefManager.resolve({ fs: fs2, gitdir, ref })
			finishingSet.add(oid)
		} catch (err) {}
	}
	const visited = /* @__PURE__ */ new Set()
	async function walk3(oid) {
		visited.add(oid)
		const { type, object } = await _readObject({ fs: fs2, cache: cache2, gitdir, oid })
		if (type === "tag") {
			const tag2 = GitAnnotatedTag.from(object)
			const commit3 = tag2.headers().object
			return walk3(commit3)
		}
		if (type !== "commit") {
			throw new ObjectTypeError(oid, type, "commit")
		}
		if (!shallows.has(oid)) {
			const commit3 = GitCommit.from(object)
			const parents = commit3.headers().parent
			for (oid of parents) {
				if (!finishingSet.has(oid) && !visited.has(oid)) {
					await walk3(oid)
				}
			}
		}
	}
	for (const oid of startingSet) {
		await walk3(oid)
	}
	return visited
}
async function listObjects({ fs: fs2, cache: cache2, dir, gitdir = join(dir, ".git"), oids }) {
	const visited = /* @__PURE__ */ new Set()
	async function walk3(oid) {
		if (visited.has(oid)) return
		visited.add(oid)
		const { type, object } = await _readObject({ fs: fs2, cache: cache2, gitdir, oid })
		if (type === "tag") {
			const tag2 = GitAnnotatedTag.from(object)
			const obj = tag2.headers().object
			await walk3(obj)
		} else if (type === "commit") {
			const commit3 = GitCommit.from(object)
			const tree = commit3.headers().tree
			await walk3(tree)
		} else if (type === "tree") {
			const tree = GitTree.from(object)
			for (const entry of tree) {
				if (entry.type === "blob") {
					visited.add(entry.oid)
				}
				if (entry.type === "tree") {
					await walk3(entry.oid)
				}
			}
		}
	}
	for (const oid of oids) {
		await walk3(oid)
	}
	return visited
}
async function parseReceivePackResponse(packfile) {
	const result = {}
	let response = ""
	const read = GitPktLine.streamReader(packfile)
	let line = await read()
	while (line !== true) {
		if (line !== null) response += line.toString("utf8") + "\n"
		line = await read()
	}
	const lines = response.toString("utf8").split("\n")
	line = lines.shift()
	if (!line.startsWith("unpack ")) {
		throw new ParseError('unpack ok" or "unpack [error message]', line)
	}
	result.ok = line === "unpack ok"
	if (!result.ok) {
		result.error = line.slice("unpack ".length)
	}
	result.refs = {}
	for (const line2 of lines) {
		if (line2.trim() === "") continue
		const status3 = line2.slice(0, 2)
		const refAndMessage = line2.slice(3)
		let space = refAndMessage.indexOf(" ")
		if (space === -1) space = refAndMessage.length
		const ref = refAndMessage.slice(0, space)
		const error = refAndMessage.slice(space + 1)
		result.refs[ref] = {
			ok: status3 === "ok",
			error,
		}
	}
	return result
}
async function writeReceivePackRequest({ capabilities = [], triplets = [] }) {
	const packstream = []
	let capsFirstLine = `\0 ${capabilities.join(" ")}`
	for (const trip of triplets) {
		packstream.push(
			GitPktLine.encode(
				`${trip.oldoid} ${trip.oid} ${trip.fullRef}${capsFirstLine}
`
			)
		)
		capsFirstLine = ""
	}
	packstream.push(GitPktLine.flush())
	return packstream
}
async function _push({
	fs: fs2,
	cache: cache2,
	http,
	onProgress,
	onMessage,
	onAuth,
	onAuthSuccess,
	onAuthFailure,
	gitdir,
	ref: _ref,
	remoteRef: _remoteRef,
	remote,
	url: _url,
	force = false,
	delete: _delete = false,
	corsProxy,
	headers = {},
}) {
	const ref = _ref || (await _currentBranch({ fs: fs2, gitdir }))
	if (typeof ref === "undefined") {
		throw new MissingParameterError("ref")
	}
	const config = await GitConfigManager.get({ fs: fs2, gitdir })
	remote =
		remote ||
		(await config.get(`branch.${ref}.pushRemote`)) ||
		(await config.get("remote.pushDefault")) ||
		(await config.get(`branch.${ref}.remote`)) ||
		"origin"
	const url =
		_url ||
		(await config.get(`remote.${remote}.pushurl`)) ||
		(await config.get(`remote.${remote}.url`))
	if (typeof url === "undefined") {
		throw new MissingParameterError("remote OR url")
	}
	const remoteRef = _remoteRef || (await config.get(`branch.${ref}.merge`))
	if (typeof url === "undefined") {
		throw new MissingParameterError("remoteRef")
	}
	if (corsProxy === void 0) {
		corsProxy = await config.get("http.corsProxy")
	}
	const fullRef = await GitRefManager.expand({ fs: fs2, gitdir, ref })
	const oid = _delete
		? "0000000000000000000000000000000000000000"
		: await GitRefManager.resolve({ fs: fs2, gitdir, ref: fullRef })
	const GitRemoteHTTP2 = GitRemoteManager.getRemoteHelperFor({ url })
	const httpRemote = await GitRemoteHTTP2.discover({
		http,
		onAuth,
		onAuthSuccess,
		onAuthFailure,
		corsProxy,
		service: "git-receive-pack",
		url,
		headers,
		protocolVersion: 1,
	})
	const auth = httpRemote.auth
	let fullRemoteRef
	if (!remoteRef) {
		fullRemoteRef = fullRef
	} else {
		try {
			fullRemoteRef = await GitRefManager.expandAgainstMap({
				ref: remoteRef,
				map: httpRemote.refs,
			})
		} catch (err) {
			if (err instanceof NotFoundError) {
				fullRemoteRef = remoteRef.startsWith("refs/") ? remoteRef : `refs/heads/${remoteRef}`
			} else {
				throw err
			}
		}
	}
	const oldoid = httpRemote.refs.get(fullRemoteRef) || "0000000000000000000000000000000000000000"
	const thinPack = !httpRemote.capabilities.has("no-thin")
	let objects = /* @__PURE__ */ new Set()
	if (!_delete) {
		const finish = [...httpRemote.refs.values()]
		let skipObjects = /* @__PURE__ */ new Set()
		if (oldoid !== "0000000000000000000000000000000000000000") {
			const mergebase = await _findMergeBase({
				fs: fs2,
				cache: cache2,
				gitdir,
				oids: [oid, oldoid],
			})
			for (const oid2 of mergebase) finish.push(oid2)
			if (thinPack) {
				skipObjects = await listObjects({ fs: fs2, cache: cache2, gitdir, oids: mergebase })
			}
		}
		if (!finish.includes(oid)) {
			const commits = await listCommitsAndTags({
				fs: fs2,
				cache: cache2,
				gitdir,
				start: [oid],
				finish,
			})
			objects = await listObjects({ fs: fs2, cache: cache2, gitdir, oids: commits })
		}
		if (thinPack) {
			try {
				const ref2 = await GitRefManager.resolve({
					fs: fs2,
					gitdir,
					ref: `refs/remotes/${remote}/HEAD`,
					depth: 2,
				})
				const { oid: oid2 } = await GitRefManager.resolveAgainstMap({
					ref: ref2.replace(`refs/remotes/${remote}/`, ""),
					fullref: ref2,
					map: httpRemote.refs,
				})
				const oids = [oid2]
				for (const oid3 of await listObjects({ fs: fs2, cache: cache2, gitdir, oids })) {
					skipObjects.add(oid3)
				}
			} catch (e) {}
			for (const oid2 of skipObjects) {
				objects.delete(oid2)
			}
		}
		if (oid === oldoid) force = true
		if (!force) {
			if (
				fullRef.startsWith("refs/tags") &&
				oldoid !== "0000000000000000000000000000000000000000"
			) {
				throw new PushRejectedError("tag-exists")
			}
			if (
				oid !== "0000000000000000000000000000000000000000" &&
				oldoid !== "0000000000000000000000000000000000000000" &&
				!(await _isDescendent({
					fs: fs2,
					cache: cache2,
					gitdir,
					oid,
					ancestor: oldoid,
					depth: -1,
				}))
			) {
				throw new PushRejectedError("not-fast-forward")
			}
		}
	}
	const capabilities = filterCapabilities(
		[...httpRemote.capabilities],
		["report-status", "side-band-64k", `agent=${pkg.agent}`]
	)
	const packstream1 = await writeReceivePackRequest({
		capabilities,
		triplets: [{ oldoid, oid, fullRef: fullRemoteRef }],
	})
	const packstream2 = _delete
		? []
		: await _pack({
				fs: fs2,
				cache: cache2,
				gitdir,
				oids: [...objects],
		  })
	const res = await GitRemoteHTTP2.connect({
		http,
		onProgress,
		corsProxy,
		service: "git-receive-pack",
		url,
		auth,
		headers,
		body: [...packstream1, ...packstream2],
	})
	const { packfile, progress } = await GitSideBand.demux(res.body)
	if (onMessage) {
		const lines = splitLines(progress)
		forAwait(lines, async (line) => {
			await onMessage(line)
		})
	}
	const result = await parseReceivePackResponse(packfile)
	if (res.headers) {
		result.headers = res.headers
	}
	if (remote && result.ok && result.refs[fullRemoteRef].ok) {
		const ref2 = `refs/remotes/${remote}/${fullRemoteRef.replace("refs/heads", "")}`
		if (_delete) {
			await GitRefManager.deleteRef({ fs: fs2, gitdir, ref: ref2 })
		} else {
			await GitRefManager.writeRef({ fs: fs2, gitdir, ref: ref2, value: oid })
		}
	}
	if (result.ok && Object.values(result.refs).every((result2) => result2.ok)) {
		return result
	} else {
		const prettyDetails = Object.entries(result.refs)
			.filter(([k, v]) => !v.ok)
			.map(
				([k, v]) => `
  - ${k}: ${v.error}`
			)
			.join("")
		throw new GitPushError(prettyDetails, result)
	}
}
async function push({
	fs: fs2,
	http,
	onProgress,
	onMessage,
	onAuth,
	onAuthSuccess,
	onAuthFailure,
	dir,
	gitdir = join(dir, ".git"),
	ref,
	remoteRef,
	remote = "origin",
	url,
	force = false,
	delete: _delete = false,
	corsProxy,
	headers = {},
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("http", http)
		assertParameter("gitdir", gitdir)
		return await _push({
			fs: new FileSystem(fs2),
			cache: cache2,
			http,
			onProgress,
			onMessage,
			onAuth,
			onAuthSuccess,
			onAuthFailure,
			gitdir,
			ref,
			remoteRef,
			remote,
			url,
			force,
			delete: _delete,
			corsProxy,
			headers,
		})
	} catch (err) {
		err.caller = "git.push"
		throw err
	}
}
async function resolveBlob({ fs: fs2, cache: cache2, gitdir, oid }) {
	const { type, object } = await _readObject({ fs: fs2, cache: cache2, gitdir, oid })
	if (type === "tag") {
		oid = GitAnnotatedTag.from(object).parse().object
		return resolveBlob({ fs: fs2, cache: cache2, gitdir, oid })
	}
	if (type !== "blob") {
		throw new ObjectTypeError(oid, type, "blob")
	}
	return { oid, blob: new Uint8Array(object) }
}
async function _readBlob({ fs: fs2, cache: cache2, gitdir, oid, filepath = void 0 }) {
	if (filepath !== void 0) {
		oid = await resolveFilepath({ fs: fs2, cache: cache2, gitdir, oid, filepath })
	}
	const blob = await resolveBlob({
		fs: fs2,
		cache: cache2,
		gitdir,
		oid,
	})
	return blob
}
async function readBlob({
	fs: fs2,
	dir,
	gitdir = join(dir, ".git"),
	oid,
	filepath,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("oid", oid)
		return await _readBlob({
			fs: new FileSystem(fs2),
			cache: cache2,
			gitdir,
			oid,
			filepath,
		})
	} catch (err) {
		err.caller = "git.readBlob"
		throw err
	}
}
async function readCommit({ fs: fs2, dir, gitdir = join(dir, ".git"), oid, cache: cache2 = {} }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("oid", oid)
		return await _readCommit({
			fs: new FileSystem(fs2),
			cache: cache2,
			gitdir,
			oid,
		})
	} catch (err) {
		err.caller = "git.readCommit"
		throw err
	}
}
async function _readNote({ fs: fs2, cache: cache2, gitdir, ref = "refs/notes/commits", oid }) {
	const parent = await GitRefManager.resolve({ gitdir, fs: fs2, ref })
	const { blob } = await _readBlob({
		fs: fs2,
		cache: cache2,
		gitdir,
		oid: parent,
		filepath: oid,
	})
	return blob
}
async function readNote({
	fs: fs2,
	dir,
	gitdir = join(dir, ".git"),
	ref = "refs/notes/commits",
	oid,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("ref", ref)
		assertParameter("oid", oid)
		return await _readNote({
			fs: new FileSystem(fs2),
			cache: cache2,
			gitdir,
			ref,
			oid,
		})
	} catch (err) {
		err.caller = "git.readNote"
		throw err
	}
}
async function readObject({
	fs: _fs,
	dir,
	gitdir = join(dir, ".git"),
	oid,
	format = "parsed",
	filepath = void 0,
	encoding = void 0,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", _fs)
		assertParameter("gitdir", gitdir)
		assertParameter("oid", oid)
		const fs2 = new FileSystem(_fs)
		if (filepath !== void 0) {
			oid = await resolveFilepath({
				fs: fs2,
				cache: cache2,
				gitdir,
				oid,
				filepath,
			})
		}
		const _format = format === "parsed" ? "content" : format
		const result = await _readObject({
			fs: fs2,
			cache: cache2,
			gitdir,
			oid,
			format: _format,
		})
		result.oid = oid
		if (format === "parsed") {
			result.format = "parsed"
			switch (result.type) {
				case "commit":
					result.object = GitCommit.from(result.object).parse()
					break
				case "tree":
					result.object = GitTree.from(result.object).entries()
					break
				case "blob":
					if (encoding) {
						result.object = result.object.toString(encoding)
					} else {
						result.object = new Uint8Array(result.object)
						result.format = "content"
					}
					break
				case "tag":
					result.object = GitAnnotatedTag.from(result.object).parse()
					break
				default:
					throw new ObjectTypeError(result.oid, result.type, "blob|commit|tag|tree")
			}
		} else if (result.format === "deflated" || result.format === "wrapped") {
			result.type = result.format
		}
		return result
	} catch (err) {
		err.caller = "git.readObject"
		throw err
	}
}
async function _readTag({ fs: fs2, cache: cache2, gitdir, oid }) {
	const { type, object } = await _readObject({
		fs: fs2,
		cache: cache2,
		gitdir,
		oid,
		format: "content",
	})
	if (type !== "tag") {
		throw new ObjectTypeError(oid, type, "tag")
	}
	const tag2 = GitAnnotatedTag.from(object)
	const result = {
		oid,
		tag: tag2.parse(),
		payload: tag2.payload(),
	}
	return result
}
async function readTag({ fs: fs2, dir, gitdir = join(dir, ".git"), oid, cache: cache2 = {} }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("oid", oid)
		return await _readTag({
			fs: new FileSystem(fs2),
			cache: cache2,
			gitdir,
			oid,
		})
	} catch (err) {
		err.caller = "git.readTag"
		throw err
	}
}
async function readTree({
	fs: fs2,
	dir,
	gitdir = join(dir, ".git"),
	oid,
	filepath = void 0,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("oid", oid)
		return await _readTree({
			fs: new FileSystem(fs2),
			cache: cache2,
			gitdir,
			oid,
			filepath,
		})
	} catch (err) {
		err.caller = "git.readTree"
		throw err
	}
}
async function remove({ fs: _fs, dir, gitdir = join(dir, ".git"), filepath, cache: cache2 = {} }) {
	try {
		assertParameter("fs", _fs)
		assertParameter("gitdir", gitdir)
		assertParameter("filepath", filepath)
		await GitIndexManager.acquire(
			{ fs: new FileSystem(_fs), gitdir, cache: cache2 },
			async function (index2) {
				index2.delete({ filepath })
			}
		)
	} catch (err) {
		err.caller = "git.remove"
		throw err
	}
}
async function _removeNote({
	fs: fs2,
	cache: cache2,
	onSign,
	gitdir,
	ref = "refs/notes/commits",
	oid,
	author,
	committer,
	signingKey,
}) {
	let parent
	try {
		parent = await GitRefManager.resolve({ gitdir, fs: fs2, ref })
	} catch (err) {
		if (!(err instanceof NotFoundError)) {
			throw err
		}
	}
	const result = await _readTree({
		fs: fs2,
		gitdir,
		oid: parent || "4b825dc642cb6eb9a060e54bf8d69288fbee4904",
	})
	let tree = result.tree
	tree = tree.filter((entry) => entry.path !== oid)
	const treeOid = await _writeTree({
		fs: fs2,
		gitdir,
		tree,
	})
	const commitOid = await _commit({
		fs: fs2,
		cache: cache2,
		onSign,
		gitdir,
		ref,
		tree: treeOid,
		parent: parent && [parent],
		message: `Note removed by 'isomorphic-git removeNote'
`,
		author,
		committer,
		signingKey,
	})
	return commitOid
}
async function removeNote({
	fs: _fs,
	onSign,
	dir,
	gitdir = join(dir, ".git"),
	ref = "refs/notes/commits",
	oid,
	author: _author,
	committer: _committer,
	signingKey,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", _fs)
		assertParameter("gitdir", gitdir)
		assertParameter("oid", oid)
		const fs2 = new FileSystem(_fs)
		const author = await normalizeAuthorObject({ fs: fs2, gitdir, author: _author })
		if (!author) throw new MissingNameError("author")
		const committer = await normalizeCommitterObject({
			fs: fs2,
			gitdir,
			author,
			committer: _committer,
		})
		if (!committer) throw new MissingNameError("committer")
		return await _removeNote({
			fs: fs2,
			cache: cache2,
			onSign,
			gitdir,
			ref,
			oid,
			author,
			committer,
			signingKey,
		})
	} catch (err) {
		err.caller = "git.removeNote"
		throw err
	}
}
async function _renameBranch({ fs: fs2, gitdir, oldref, ref, checkout: checkout3 = false }) {
	if (ref !== import_clean_git_ref.default.clean(ref)) {
		throw new InvalidRefNameError(ref, import_clean_git_ref.default.clean(ref))
	}
	if (oldref !== import_clean_git_ref.default.clean(oldref)) {
		throw new InvalidRefNameError(oldref, import_clean_git_ref.default.clean(oldref))
	}
	const fulloldref = `refs/heads/${oldref}`
	const fullnewref = `refs/heads/${ref}`
	const newexist = await GitRefManager.exists({ fs: fs2, gitdir, ref: fullnewref })
	if (newexist) {
		throw new AlreadyExistsError("branch", ref, false)
	}
	const value = await GitRefManager.resolve({
		fs: fs2,
		gitdir,
		ref: fulloldref,
		depth: 1,
	})
	await GitRefManager.writeRef({ fs: fs2, gitdir, ref: fullnewref, value })
	await GitRefManager.deleteRef({ fs: fs2, gitdir, ref: fulloldref })
	const fullCurrentBranchRef = await _currentBranch({
		fs: fs2,
		gitdir,
		fullname: true,
	})
	const isCurrentBranch = fullCurrentBranchRef === fulloldref
	if (checkout3 || isCurrentBranch) {
		await GitRefManager.writeSymbolicRef({
			fs: fs2,
			gitdir,
			ref: "HEAD",
			value: fullnewref,
		})
	}
}
async function renameBranch({
	fs: fs2,
	dir,
	gitdir = join(dir, ".git"),
	ref,
	oldref,
	checkout: checkout3 = false,
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("ref", ref)
		assertParameter("oldref", oldref)
		return await _renameBranch({
			fs: new FileSystem(fs2),
			gitdir,
			ref,
			oldref,
			checkout: checkout3,
		})
	} catch (err) {
		err.caller = "git.renameBranch"
		throw err
	}
}
async function hashObject$1({ gitdir, type, object }) {
	return shasum(GitObject.wrap({ type, object }))
}
async function resetIndex({
	fs: _fs,
	dir,
	gitdir = join(dir, ".git"),
	filepath,
	ref,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", _fs)
		assertParameter("gitdir", gitdir)
		assertParameter("filepath", filepath)
		const fs2 = new FileSystem(_fs)
		let oid
		let workdirOid
		try {
			oid = await GitRefManager.resolve({ fs: fs2, gitdir, ref: ref || "HEAD" })
		} catch (e) {
			if (ref) {
				throw e
			}
		}
		if (oid) {
			try {
				oid = await resolveFilepath({
					fs: fs2,
					cache: cache2,
					gitdir,
					oid,
					filepath,
				})
			} catch (e) {
				oid = null
			}
		}
		let stats = {
			ctime: /* @__PURE__ */ new Date(0),
			mtime: /* @__PURE__ */ new Date(0),
			dev: 0,
			ino: 0,
			mode: 0,
			uid: 0,
			gid: 0,
			size: 0,
		}
		const object = dir && (await fs2.read(join(dir, filepath)))
		if (object) {
			workdirOid = await hashObject$1({
				gitdir,
				type: "blob",
				object,
			})
			if (oid === workdirOid) {
				stats = await fs2.lstat(join(dir, filepath))
			}
		}
		await GitIndexManager.acquire({ fs: fs2, gitdir, cache: cache2 }, async function (index2) {
			index2.delete({ filepath })
			if (oid) {
				index2.insert({ filepath, stats, oid })
			}
		})
	} catch (err) {
		err.caller = "git.reset"
		throw err
	}
}
async function resolveRef({ fs: fs2, dir, gitdir = join(dir, ".git"), ref, depth }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("ref", ref)
		const oid = await GitRefManager.resolve({
			fs: new FileSystem(fs2),
			gitdir,
			ref,
			depth,
		})
		return oid
	} catch (err) {
		err.caller = "git.resolveRef"
		throw err
	}
}
async function setConfig({
	fs: _fs,
	dir,
	gitdir = join(dir, ".git"),
	path,
	value,
	append = false,
}) {
	try {
		assertParameter("fs", _fs)
		assertParameter("gitdir", gitdir)
		assertParameter("path", path)
		const fs2 = new FileSystem(_fs)
		const config = await GitConfigManager.get({ fs: fs2, gitdir })
		if (append) {
			await config.append(path, value)
		} else {
			await config.set(path, value)
		}
		await GitConfigManager.save({ fs: fs2, gitdir, config })
	} catch (err) {
		err.caller = "git.setConfig"
		throw err
	}
}
async function status({ fs: _fs, dir, gitdir = join(dir, ".git"), filepath, cache: cache2 = {} }) {
	try {
		assertParameter("fs", _fs)
		assertParameter("gitdir", gitdir)
		assertParameter("filepath", filepath)
		const fs2 = new FileSystem(_fs)
		const ignored = await GitIgnoreManager.isIgnored({
			fs: fs2,
			gitdir,
			dir,
			filepath,
		})
		if (ignored) {
			return "ignored"
		}
		const headTree = await getHeadTree({ fs: fs2, cache: cache2, gitdir })
		const treeOid = await getOidAtPath({
			fs: fs2,
			cache: cache2,
			gitdir,
			tree: headTree,
			path: filepath,
		})
		const indexEntry = await GitIndexManager.acquire(
			{ fs: fs2, gitdir, cache: cache2 },
			async function (index2) {
				for (const entry of index2) {
					if (entry.path === filepath) return entry
				}
				return null
			}
		)
		const stats = await fs2.lstat(join(dir, filepath))
		const H = treeOid !== null
		const I = indexEntry !== null
		const W = stats !== null
		const getWorkdirOid = async () => {
			if (I && !compareStats(indexEntry, stats)) {
				return indexEntry.oid
			} else {
				const object = await fs2.read(join(dir, filepath))
				const workdirOid = await hashObject$1({
					gitdir,
					type: "blob",
					object,
				})
				if (I && indexEntry.oid === workdirOid && stats.size !== -1) {
					GitIndexManager.acquire({ fs: fs2, gitdir, cache: cache2 }, async function (index2) {
						index2.insert({ filepath, stats, oid: workdirOid })
					})
				}
				return workdirOid
			}
		}
		if (!H && !W && !I) return "absent"
		if (!H && !W && I) return "*absent"
		if (!H && W && !I) return "*added"
		if (!H && W && I) {
			const workdirOid = await getWorkdirOid()
			return workdirOid === indexEntry.oid ? "added" : "*added"
		}
		if (H && !W && !I) return "deleted"
		if (H && !W && I) {
			return treeOid === indexEntry.oid ? "*deleted" : "*deleted"
		}
		if (H && W && !I) {
			const workdirOid = await getWorkdirOid()
			return workdirOid === treeOid ? "*undeleted" : "*undeletemodified"
		}
		if (H && W && I) {
			const workdirOid = await getWorkdirOid()
			if (workdirOid === treeOid) {
				return workdirOid === indexEntry.oid ? "unmodified" : "*unmodified"
			} else {
				return workdirOid === indexEntry.oid ? "modified" : "*modified"
			}
		}
	} catch (err) {
		err.caller = "git.status"
		throw err
	}
}
async function getOidAtPath({ fs: fs2, cache: cache2, gitdir, tree, path }) {
	if (typeof path === "string") path = path.split("/")
	const dirname2 = path.shift()
	for (const entry of tree) {
		if (entry.path === dirname2) {
			if (path.length === 0) {
				return entry.oid
			}
			const { type, object } = await _readObject({
				fs: fs2,
				cache: cache2,
				gitdir,
				oid: entry.oid,
			})
			if (type === "tree") {
				const tree2 = GitTree.from(object)
				return getOidAtPath({ fs: fs2, cache: cache2, gitdir, tree: tree2, path })
			}
			if (type === "blob") {
				throw new ObjectTypeError(entry.oid, type, "blob", path.join("/"))
			}
		}
	}
	return null
}
async function getHeadTree({ fs: fs2, cache: cache2, gitdir }) {
	let oid
	try {
		oid = await GitRefManager.resolve({ fs: fs2, gitdir, ref: "HEAD" })
	} catch (e) {
		if (e instanceof NotFoundError) {
			return []
		}
	}
	const { tree } = await _readTree({ fs: fs2, cache: cache2, gitdir, oid })
	return tree
}
async function statusMatrix({
	fs: _fs,
	dir,
	gitdir = join(dir, ".git"),
	ref = "HEAD",
	filepaths = ["."],
	filter,
	cache: cache2 = {},
	ignored: shouldIgnore = false,
}) {
	try {
		assertParameter("fs", _fs)
		assertParameter("gitdir", gitdir)
		assertParameter("ref", ref)
		const fs2 = new FileSystem(_fs)
		return await _walk({
			fs: fs2,
			cache: cache2,
			dir,
			gitdir,
			trees: [TREE({ ref }), WORKDIR(), STAGE()],
			map: async function (filepath, [head, workdir, stage]) {
				if (!head && !stage && workdir && !shouldIgnore) {
					const isIgnored3 = await GitIgnoreManager.isIgnored({
						fs: fs2,
						dir,
						filepath,
					})
					if (isIgnored3) {
						return null
					}
				}
				if (!filepaths.some((base) => worthWalking(filepath, base))) {
					return null
				}
				if (filter && !filter(filepath)) return
				const [headType, workdirType, stageType] = await Promise.all([
					head && head.type(),
					workdir && workdir.type(),
					stage && stage.type(),
				])
				const isBlob = [headType, workdirType, stageType].includes("blob")
				if ((headType === "tree" || headType === "special") && !isBlob) return
				if (headType === "commit") return null
				if ((workdirType === "tree" || workdirType === "special") && !isBlob) return
				if (stageType === "commit") return null
				if ((stageType === "tree" || stageType === "special") && !isBlob) return
				const headOid = headType === "blob" ? await head.oid() : void 0
				const stageOid = stageType === "blob" ? await stage.oid() : void 0
				let workdirOid
				if (headType !== "blob" && workdirType === "blob" && stageType !== "blob") {
					workdirOid = "42"
				} else if (workdirType === "blob") {
					workdirOid = await workdir.oid()
				}
				const entry = [void 0, headOid, workdirOid, stageOid]
				const result = entry.map((value) => entry.indexOf(value))
				result.shift()
				return [filepath, ...result]
			},
		})
	} catch (err) {
		err.caller = "git.statusMatrix"
		throw err
	}
}
async function tag({ fs: _fs, dir, gitdir = join(dir, ".git"), ref, object, force = false }) {
	try {
		assertParameter("fs", _fs)
		assertParameter("gitdir", gitdir)
		assertParameter("ref", ref)
		const fs2 = new FileSystem(_fs)
		if (ref === void 0) {
			throw new MissingParameterError("ref")
		}
		ref = ref.startsWith("refs/tags/") ? ref : `refs/tags/${ref}`
		const value = await GitRefManager.resolve({
			fs: fs2,
			gitdir,
			ref: object || "HEAD",
		})
		if (!force && (await GitRefManager.exists({ fs: fs2, gitdir, ref }))) {
			throw new AlreadyExistsError("tag", ref)
		}
		await GitRefManager.writeRef({ fs: fs2, gitdir, ref, value })
	} catch (err) {
		err.caller = "git.tag"
		throw err
	}
}
async function updateIndex({
	fs: _fs,
	dir,
	gitdir = join(dir, ".git"),
	cache: cache2 = {},
	filepath,
	oid,
	mode,
	add: add3,
	remove: remove3,
	force,
}) {
	try {
		assertParameter("fs", _fs)
		assertParameter("gitdir", gitdir)
		assertParameter("filepath", filepath)
		const fs2 = new FileSystem(_fs)
		if (remove3) {
			return await GitIndexManager.acquire(
				{ fs: fs2, gitdir, cache: cache2 },
				async function (index2) {
					let fileStats2
					if (!force) {
						fileStats2 = await fs2.lstat(join(dir, filepath))
						if (fileStats2) {
							if (fileStats2.isDirectory()) {
								throw new InvalidFilepathError("directory")
							}
							return
						}
					}
					if (index2.has({ filepath })) {
						index2.delete({
							filepath,
						})
					}
				}
			)
		}
		let fileStats
		if (!oid) {
			fileStats = await fs2.lstat(join(dir, filepath))
			if (!fileStats) {
				throw new NotFoundError(`file at "${filepath}" on disk and "remove" not set`)
			}
			if (fileStats.isDirectory()) {
				throw new InvalidFilepathError("directory")
			}
		}
		return await GitIndexManager.acquire(
			{ fs: fs2, gitdir, cache: cache2 },
			async function (index2) {
				if (!add3 && !index2.has({ filepath })) {
					throw new NotFoundError(`file at "${filepath}" in index and "add" not set`)
				}
				let stats = {
					ctime: /* @__PURE__ */ new Date(0),
					mtime: /* @__PURE__ */ new Date(0),
					dev: 0,
					ino: 0,
					mode,
					uid: 0,
					gid: 0,
					size: 0,
				}
				if (!oid) {
					stats = fileStats
					const object = stats.isSymbolicLink()
						? await fs2.readlink(join(dir, filepath))
						: await fs2.read(join(dir, filepath))
					oid = await _writeObject({
						fs: fs2,
						gitdir,
						type: "blob",
						format: "content",
						object,
					})
				}
				index2.insert({
					filepath,
					oid,
					stats,
				})
				return oid
			}
		)
	} catch (err) {
		err.caller = "git.updateIndex"
		throw err
	}
}
function version2() {
	try {
		return pkg.version
	} catch (err) {
		err.caller = "git.version"
		throw err
	}
}
async function walk({
	fs: fs2,
	dir,
	gitdir = join(dir, ".git"),
	trees,
	map,
	reduce,
	iterate,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("trees", trees)
		return await _walk({
			fs: new FileSystem(fs2),
			cache: cache2,
			dir,
			gitdir,
			trees,
			map,
			reduce,
			iterate,
		})
	} catch (err) {
		err.caller = "git.walk"
		throw err
	}
}
async function writeBlob({ fs: fs2, dir, gitdir = join(dir, ".git"), blob }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("blob", blob)
		return await _writeObject({
			fs: new FileSystem(fs2),
			gitdir,
			type: "blob",
			object: blob,
			format: "content",
		})
	} catch (err) {
		err.caller = "git.writeBlob"
		throw err
	}
}
async function _writeCommit({ fs: fs2, gitdir, commit: commit3 }) {
	const object = GitCommit.from(commit3).toObject()
	const oid = await _writeObject({
		fs: fs2,
		gitdir,
		type: "commit",
		object,
		format: "content",
	})
	return oid
}
async function writeCommit({ fs: fs2, dir, gitdir = join(dir, ".git"), commit: commit3 }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("commit", commit3)
		return await _writeCommit({
			fs: new FileSystem(fs2),
			gitdir,
			commit: commit3,
		})
	} catch (err) {
		err.caller = "git.writeCommit"
		throw err
	}
}
async function writeObject({
	fs: _fs,
	dir,
	gitdir = join(dir, ".git"),
	type,
	object,
	format = "parsed",
	oid,
	encoding = void 0,
}) {
	try {
		const fs2 = new FileSystem(_fs)
		if (format === "parsed") {
			switch (type) {
				case "commit":
					object = GitCommit.from(object).toObject()
					break
				case "tree":
					object = GitTree.from(object).toObject()
					break
				case "blob":
					object = Buffer.from(object, encoding)
					break
				case "tag":
					object = GitAnnotatedTag.from(object).toObject()
					break
				default:
					throw new ObjectTypeError(oid || "", type, "blob|commit|tag|tree")
			}
			format = "content"
		}
		oid = await _writeObject({
			fs: fs2,
			gitdir,
			type,
			object,
			oid,
			format,
		})
		return oid
	} catch (err) {
		err.caller = "git.writeObject"
		throw err
	}
}
async function writeRef({
	fs: _fs,
	dir,
	gitdir = join(dir, ".git"),
	ref,
	value,
	force = false,
	symbolic = false,
}) {
	try {
		assertParameter("fs", _fs)
		assertParameter("gitdir", gitdir)
		assertParameter("ref", ref)
		assertParameter("value", value)
		const fs2 = new FileSystem(_fs)
		if (ref !== import_clean_git_ref.default.clean(ref)) {
			throw new InvalidRefNameError(ref, import_clean_git_ref.default.clean(ref))
		}
		if (!force && (await GitRefManager.exists({ fs: fs2, gitdir, ref }))) {
			throw new AlreadyExistsError("ref", ref)
		}
		if (symbolic) {
			await GitRefManager.writeSymbolicRef({
				fs: fs2,
				gitdir,
				ref,
				value,
			})
		} else {
			value = await GitRefManager.resolve({
				fs: fs2,
				gitdir,
				ref: value,
			})
			await GitRefManager.writeRef({
				fs: fs2,
				gitdir,
				ref,
				value,
			})
		}
	} catch (err) {
		err.caller = "git.writeRef"
		throw err
	}
}
async function _writeTag({ fs: fs2, gitdir, tag: tag2 }) {
	const object = GitAnnotatedTag.from(tag2).toObject()
	const oid = await _writeObject({
		fs: fs2,
		gitdir,
		type: "tag",
		object,
		format: "content",
	})
	return oid
}
async function writeTag({ fs: fs2, dir, gitdir = join(dir, ".git"), tag: tag2 }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("tag", tag2)
		return await _writeTag({
			fs: new FileSystem(fs2),
			gitdir,
			tag: tag2,
		})
	} catch (err) {
		err.caller = "git.writeTag"
		throw err
	}
}
async function writeTree({ fs: fs2, dir, gitdir = join(dir, ".git"), tree }) {
	try {
		assertParameter("fs", fs2)
		assertParameter("gitdir", gitdir)
		assertParameter("tree", tree)
		return await _writeTree({
			fs: new FileSystem(fs2),
			gitdir,
			tree,
		})
	} catch (err) {
		err.caller = "git.writeTree"
		throw err
	}
}
async function writeRefsAdResponse({ capabilities, refs, symrefs }) {
	const stream = []
	let syms = ""
	for (const [key, value] of Object.entries(symrefs)) {
		syms += `symref=${key}:${value} `
	}
	let caps = `\0${[...capabilities].join(" ")} ${syms}agent=${pkg.agent}`
	for (const [key, value] of Object.entries(refs)) {
		stream.push(
			GitPktLine.encode(`${value} ${key}${caps}
`)
		)
		caps = ""
	}
	stream.push(GitPktLine.flush())
	return stream
}
async function uploadPack({ fs: fs2, dir, gitdir = join(dir, ".git"), advertiseRefs = false }) {
	try {
		if (advertiseRefs) {
			const capabilities = [
				"thin-pack",
				"side-band",
				"side-band-64k",
				"shallow",
				"deepen-since",
				"deepen-not",
				"allow-tip-sha1-in-want",
				"allow-reachable-sha1-in-want",
			]
			let keys = await GitRefManager.listRefs({
				fs: fs2,
				gitdir,
				filepath: "refs",
			})
			keys = keys.map((ref) => `refs/${ref}`)
			const refs = {}
			keys.unshift("HEAD")
			for (const key of keys) {
				refs[key] = await GitRefManager.resolve({ fs: fs2, gitdir, ref: key })
			}
			const symrefs = {}
			symrefs.HEAD = await GitRefManager.resolve({
				fs: fs2,
				gitdir,
				ref: "HEAD",
				depth: 2,
			})
			return writeRefsAdResponse({
				capabilities,
				refs,
				symrefs,
			})
		}
	} catch (err) {
		err.caller = "git.uploadPack"
		throw err
	}
}
function fromEntries(map) {
	const o = {}
	for (const [key, value] of map) {
		o[key] = value
	}
	return o
}
function fromNodeStream(stream) {
	const asyncIterator = Object.getOwnPropertyDescriptor(stream, Symbol.asyncIterator)
	if (asyncIterator && asyncIterator.enumerable) {
		return stream
	}
	let ended = false
	const queue = []
	let defer = {}
	stream.on("data", (chunk) => {
		queue.push(chunk)
		if (defer.resolve) {
			defer.resolve({ value: queue.shift(), done: false })
			defer = {}
		}
	})
	stream.on("error", (err) => {
		if (defer.reject) {
			defer.reject(err)
			defer = {}
		}
	})
	stream.on("end", () => {
		ended = true
		if (defer.resolve) {
			defer.resolve({ done: true })
			defer = {}
		}
	})
	return {
		next() {
			return new Promise((resolve, reject) => {
				if (queue.length === 0 && ended) {
					return resolve({ done: true })
				} else if (queue.length > 0) {
					return resolve({ value: queue.shift(), done: false })
				} else if (queue.length === 0 && !ended) {
					defer = { resolve, reject }
				}
			})
		},
		return() {
			stream.removeAllListeners()
			if (stream.destroy) stream.destroy()
		},
		[Symbol.asyncIterator]() {
			return this
		},
	}
}
function fromStream(stream) {
	if (stream[Symbol.asyncIterator]) return stream
	const reader = stream.getReader()
	return {
		next() {
			return reader.read()
		},
		return() {
			reader.releaseLock()
			return {}
		},
		[Symbol.asyncIterator]() {
			return this
		},
	}
}
function isBinary(buffer) {
	const MAX_XDIFF_SIZE = 1024 * 1024 * 1023
	if (buffer.length > MAX_XDIFF_SIZE) return true
	return buffer.slice(0, 8e3).includes(0)
}
async function sleep(ms) {
	return new Promise((resolve, reject) => setTimeout(resolve, ms))
}
async function parseUploadPackRequest(stream) {
	const read = GitPktLine.streamReader(stream)
	let done = false
	let capabilities = null
	const wants = []
	const haves = []
	const shallows = []
	let depth
	let since
	const exclude = []
	let relative = false
	while (!done) {
		const line = await read()
		if (line === true) break
		if (line === null) continue
		const [key, value, ...rest] = line.toString("utf8").trim().split(" ")
		if (!capabilities) capabilities = rest
		switch (key) {
			case "want":
				wants.push(value)
				break
			case "have":
				haves.push(value)
				break
			case "shallow":
				shallows.push(value)
				break
			case "deepen":
				depth = parseInt(value)
				break
			case "deepen-since":
				since = parseInt(value)
				break
			case "deepen-not":
				exclude.push(value)
				break
			case "deepen-relative":
				relative = true
				break
			case "done":
				done = true
				break
		}
	}
	return {
		capabilities,
		wants,
		haves,
		shallows,
		depth,
		since,
		exclude,
		relative,
		done,
	}
}
var import_async_lock,
	import_sha12,
	import_crc_32,
	import_pako,
	import_pify,
	import_ignore,
	import_clean_git_ref,
	BaseError,
	UnmergedPathsError,
	InternalError,
	UnsafeFilepathError,
	BufferCursor,
	MAX_UINT32,
	supportsSubtleSHA1,
	GitIndex,
	lock,
	IndexCache,
	GitIndexManager,
	GitWalkerIndex,
	GitWalkSymbol,
	NotFoundError,
	ObjectTypeError,
	InvalidOidError,
	NoRefspecError,
	GitPackedRefs,
	GitRefSpec,
	GitRefSpecSet,
	memo,
	num,
	bool,
	schema,
	SECTION_LINE_REGEX,
	SECTION_REGEX,
	VARIABLE_LINE_REGEX,
	VARIABLE_NAME_REGEX,
	VARIABLE_VALUE_COMMENT_REGEX,
	extractSectionLine,
	extractVariableLine,
	removeComments,
	hasOddNumberOfQuotes,
	removeQuotes,
	lower,
	getPath,
	normalizePath$1,
	findLastIndex,
	GitConfig,
	GitConfigManager,
	refpaths,
	GIT_FILES,
	lock$1,
	GitRefManager,
	GitTree,
	GitObject,
	StreamReader,
	supportsDecompressionStream,
	GitPackIndex,
	PackfileCache,
	AlreadyExistsError,
	AmbiguousError,
	CheckoutConflictError,
	CommitNotFetchedError,
	EmptyServerResponseError,
	FastForwardError,
	GitPushError,
	HttpError,
	InvalidFilepathError,
	InvalidRefNameError,
	MaxDepthError,
	MergeNotSupportedError,
	MergeConflictError,
	MissingNameError,
	MissingParameterError,
	MultipleGitError,
	ParseError,
	PushRejectedError,
	RemoteCapabilityError,
	SmartHttpError,
	UnknownTransportError,
	UrlParseError,
	UserCanceledError,
	IndexResetError,
	Errors,
	GitAnnotatedTag,
	GitCommit,
	GitWalkerRepo,
	GitWalkerFs,
	flat,
	RunningMinimum,
	commands,
	FileSystem,
	GitIgnoreManager,
	supportsCompressionStream,
	worthWalking,
	abbreviateRx,
	GitPktLine,
	corsProxify,
	updateHeaders,
	stringifyBody,
	GitRemoteHTTP,
	GitRemoteManager,
	lock$2,
	GitShallowManager,
	pkg,
	FIFO,
	GitSideBand,
	LINEBREAKS,
	EMPTY_OID,
	types,
	deepget,
	DeepMap,
	index,
	isomorphic_git_default
var init_isomorphic_git = __esm({
	"../../../lix/packages/client/vendored/isomorphic-git/index.js"() {
		"use strict"
		import_async_lock = __toESM(require_async_lock(), 1)
		import_sha12 = __toESM(require_sha1(), 1)
		import_crc_32 = __toESM(require_crc32(), 1)
		import_pako = __toESM(require_pako(), 1)
		import_pify = __toESM(require_pify(), 1)
		import_ignore = __toESM(require_ignore(), 1)
		import_clean_git_ref = __toESM(require_lib3(), 1)
		init_diff3()
		BaseError = class _BaseError extends Error {
			constructor(message) {
				super(message)
				this.caller = ""
			}
			toJSON() {
				return {
					code: this.code,
					data: this.data,
					caller: this.caller,
					message: this.message,
					stack: this.stack,
				}
			}
			fromJSON(json) {
				const e = new _BaseError(json.message)
				e.code = json.code
				e.data = json.data
				e.caller = json.caller
				e.stack = json.stack
				return e
			}
			get isIsomorphicGitError() {
				return true
			}
		}
		UnmergedPathsError = class _UnmergedPathsError extends BaseError {
			/**
			 * @param {Array<string>} filepaths
			 */
			constructor(filepaths) {
				super(
					`Modifying the index is not possible because you have unmerged files: ${filepaths.toString}. Fix them up in the work tree, and then use 'git add/rm as appropriate to mark resolution and make a commit.`
				)
				this.code = this.name = _UnmergedPathsError.code
				this.data = { filepaths }
			}
		}
		UnmergedPathsError.code = "UnmergedPathsError"
		InternalError = class _InternalError extends BaseError {
			/**
			 * @param {string} message
			 */
			constructor(message) {
				super(
					`An internal error caused this command to fail. Please file a bug report at https://github.com/isomorphic-git/isomorphic-git/issues with this error message: ${message}`
				)
				this.code = this.name = _InternalError.code
				this.data = { message }
			}
		}
		InternalError.code = "InternalError"
		UnsafeFilepathError = class _UnsafeFilepathError extends BaseError {
			/**
			 * @param {string} filepath
			 */
			constructor(filepath) {
				super(`The filepath "${filepath}" contains unsafe character sequences`)
				this.code = this.name = _UnsafeFilepathError.code
				this.data = { filepath }
			}
		}
		UnsafeFilepathError.code = "UnsafeFilepathError"
		BufferCursor = class {
			constructor(buffer) {
				this.buffer = buffer
				this._start = 0
			}
			eof() {
				return this._start >= this.buffer.length
			}
			tell() {
				return this._start
			}
			seek(n) {
				this._start = n
			}
			slice(n) {
				const r = this.buffer.slice(this._start, this._start + n)
				this._start += n
				return r
			}
			toString(enc, length) {
				const r = this.buffer.toString(enc, this._start, this._start + length)
				this._start += length
				return r
			}
			write(value, length, enc) {
				const r = this.buffer.write(value, this._start, length, enc)
				this._start += length
				return r
			}
			copy(source, start, end) {
				const r = source.copy(this.buffer, this._start, start, end)
				this._start += r
				return r
			}
			readUInt8() {
				const r = this.buffer.readUInt8(this._start)
				this._start += 1
				return r
			}
			writeUInt8(value) {
				const r = this.buffer.writeUInt8(value, this._start)
				this._start += 1
				return r
			}
			readUInt16BE() {
				const r = this.buffer.readUInt16BE(this._start)
				this._start += 2
				return r
			}
			writeUInt16BE(value) {
				const r = this.buffer.writeUInt16BE(value, this._start)
				this._start += 2
				return r
			}
			readUInt32BE() {
				const r = this.buffer.readUInt32BE(this._start)
				this._start += 4
				return r
			}
			writeUInt32BE(value) {
				const r = this.buffer.writeUInt32BE(value, this._start)
				this._start += 4
				return r
			}
		}
		MAX_UINT32 = 2 ** 32
		supportsSubtleSHA1 = null
		GitIndex = class _GitIndex {
			/*::
       _entries: Map<string, CacheEntry>
       _dirty: boolean // Used to determine if index needs to be saved to filesystem
       */
			constructor(entries, unmergedPaths) {
				this._dirty = false
				this._unmergedPaths = unmergedPaths || /* @__PURE__ */ new Set()
				this._entries = entries || /* @__PURE__ */ new Map()
			}
			_addEntry(entry) {
				if (entry.flags.stage === 0) {
					entry.stages = [entry]
					this._entries.set(entry.path, entry)
					this._unmergedPaths.delete(entry.path)
				} else {
					let existingEntry = this._entries.get(entry.path)
					if (!existingEntry) {
						this._entries.set(entry.path, entry)
						existingEntry = entry
					}
					existingEntry.stages[entry.flags.stage] = entry
					this._unmergedPaths.add(entry.path)
				}
			}
			static async from(buffer) {
				if (Buffer.isBuffer(buffer)) {
					return _GitIndex.fromBuffer(buffer)
				} else if (buffer === null) {
					return new _GitIndex(null)
				} else {
					throw new InternalError("invalid type passed to GitIndex.from")
				}
			}
			static async fromBuffer(buffer) {
				if (buffer.length === 0) {
					throw new InternalError("Index file is empty (.git/index)")
				}
				const index2 = new _GitIndex()
				const reader = new BufferCursor(buffer)
				const magic = reader.toString("utf8", 4)
				if (magic !== "DIRC") {
					throw new InternalError(`Invalid dircache magic file number: ${magic}`)
				}
				const shaComputed = await shasum(buffer.slice(0, -20))
				const shaClaimed = buffer.slice(-20).toString("hex")
				if (shaClaimed !== shaComputed) {
					throw new InternalError(
						`Invalid checksum in GitIndex buffer: expected ${shaClaimed} but saw ${shaComputed}`
					)
				}
				const version3 = reader.readUInt32BE()
				if (version3 !== 2) {
					throw new InternalError(`Unsupported dircache version: ${version3}`)
				}
				const numEntries = reader.readUInt32BE()
				let i = 0
				while (!reader.eof() && i < numEntries) {
					const entry = {}
					entry.ctimeSeconds = reader.readUInt32BE()
					entry.ctimeNanoseconds = reader.readUInt32BE()
					entry.mtimeSeconds = reader.readUInt32BE()
					entry.mtimeNanoseconds = reader.readUInt32BE()
					entry.dev = reader.readUInt32BE()
					entry.ino = reader.readUInt32BE()
					entry.mode = reader.readUInt32BE()
					entry.uid = reader.readUInt32BE()
					entry.gid = reader.readUInt32BE()
					entry.size = reader.readUInt32BE()
					entry.oid = reader.slice(20).toString("hex")
					const flags = reader.readUInt16BE()
					entry.flags = parseCacheEntryFlags(flags)
					const pathlength = buffer.indexOf(0, reader.tell() + 1) - reader.tell()
					if (pathlength < 1) {
						throw new InternalError(`Got a path length of: ${pathlength}`)
					}
					entry.path = reader.toString("utf8", pathlength)
					if (entry.path.includes("..\\") || entry.path.includes("../")) {
						throw new UnsafeFilepathError(entry.path)
					}
					let padding = 8 - ((reader.tell() - 12) % 8)
					if (padding === 0) padding = 8
					while (padding--) {
						const tmp = reader.readUInt8()
						if (tmp !== 0) {
							throw new InternalError(
								`Expected 1-8 null characters but got '${tmp}' after ${entry.path}`
							)
						} else if (reader.eof()) {
							throw new InternalError("Unexpected end of file")
						}
					}
					entry.stages = []
					index2._addEntry(entry)
					i++
				}
				return index2
			}
			get unmergedPaths() {
				return [...this._unmergedPaths]
			}
			get entries() {
				return [...this._entries.values()].sort(comparePath)
			}
			get entriesMap() {
				return this._entries
			}
			get entriesFlat() {
				return [...this.entries].flatMap((entry) => {
					return entry.stages.length > 1 ? entry.stages.filter((x) => x) : entry
				})
			}
			*[Symbol.iterator]() {
				for (const entry of this.entries) {
					yield entry
				}
			}
			insert({ filepath, stats, oid, stage = 0 }) {
				if (!stats) {
					stats = {
						ctimeSeconds: 0,
						ctimeNanoseconds: 0,
						mtimeSeconds: 0,
						mtimeNanoseconds: 0,
						dev: 0,
						ino: 0,
						mode: 0,
						uid: 0,
						gid: 0,
						size: 0,
					}
				}
				stats = normalizeStats(stats)
				const bfilepath = Buffer.from(filepath)
				const entry = {
					ctimeSeconds: stats.ctimeSeconds,
					ctimeNanoseconds: stats.ctimeNanoseconds,
					mtimeSeconds: stats.mtimeSeconds,
					mtimeNanoseconds: stats.mtimeNanoseconds,
					dev: stats.dev,
					ino: stats.ino,
					// We provide a fallback value for `mode` here because not all fs
					// implementations assign it, but we use it in GitTree.
					// '100644' is for a "regular non-executable file"
					mode: stats.mode || 33188,
					uid: stats.uid,
					gid: stats.gid,
					size: stats.size,
					path: filepath,
					oid,
					flags: {
						assumeValid: false,
						extended: false,
						stage,
						nameLength: bfilepath.length < 4095 ? bfilepath.length : 4095,
					},
					stages: [],
				}
				this._addEntry(entry)
				this._dirty = true
			}
			delete({ filepath }) {
				if (this._entries.has(filepath)) {
					this._entries.delete(filepath)
				} else {
					for (const key of this._entries.keys()) {
						if (key.startsWith(filepath + "/")) {
							this._entries.delete(key)
						}
					}
				}
				if (this._unmergedPaths.has(filepath)) {
					this._unmergedPaths.delete(filepath)
				}
				this._dirty = true
			}
			clear() {
				this._entries.clear()
				this._dirty = true
			}
			has({ filepath }) {
				return this._entries.has(filepath)
			}
			render() {
				return this.entries
					.map((entry) => `${entry.mode.toString(8)} ${entry.oid}    ${entry.path}`)
					.join("\n")
			}
			static async _entryToBuffer(entry) {
				const bpath = Buffer.from(entry.path)
				const length = Math.ceil((62 + bpath.length + 1) / 8) * 8
				const written = Buffer.alloc(length)
				const writer = new BufferCursor(written)
				const stat = normalizeStats(entry)
				writer.writeUInt32BE(stat.ctimeSeconds)
				writer.writeUInt32BE(stat.ctimeNanoseconds)
				writer.writeUInt32BE(stat.mtimeSeconds)
				writer.writeUInt32BE(stat.mtimeNanoseconds)
				writer.writeUInt32BE(stat.dev)
				writer.writeUInt32BE(stat.ino)
				writer.writeUInt32BE(stat.mode)
				writer.writeUInt32BE(stat.uid)
				writer.writeUInt32BE(stat.gid)
				writer.writeUInt32BE(stat.size)
				writer.write(entry.oid, 20, "hex")
				writer.writeUInt16BE(renderCacheEntryFlags(entry))
				writer.write(entry.path, bpath.length, "utf8")
				return written
			}
			async toObject() {
				const header = Buffer.alloc(12)
				const writer = new BufferCursor(header)
				writer.write("DIRC", 4, "utf8")
				writer.writeUInt32BE(2)
				writer.writeUInt32BE(this.entriesFlat.length)
				let entryBuffers = []
				for (const entry of this.entries) {
					entryBuffers.push(_GitIndex._entryToBuffer(entry))
					if (entry.stages.length > 1) {
						for (const stage of entry.stages) {
							if (stage && stage !== entry) {
								entryBuffers.push(_GitIndex._entryToBuffer(stage))
							}
						}
					}
				}
				entryBuffers = await Promise.all(entryBuffers)
				const body = Buffer.concat(entryBuffers)
				const main = Buffer.concat([header, body])
				const sum = await shasum(main)
				return Buffer.concat([main, Buffer.from(sum, "hex")])
			}
		}
		lock = null
		IndexCache = Symbol("IndexCache")
		GitIndexManager = class {
			/**
			 *
			 * @param {object} opts
			 * @param {import('../models/FileSystem.js').FileSystem} opts.fs
			 * @param {string} opts.gitdir
			 * @param {object} opts.cache
			 * @param {bool} opts.allowUnmerged
			 * @param {function(GitIndex): any} closure
			 */
			static async acquire({ fs: fs2, gitdir, cache: cache2, allowUnmerged = true }, closure) {
				if (!cache2[IndexCache]) cache2[IndexCache] = createCache()
				const filepath = `${gitdir}/index`
				if (lock === null) lock = new import_async_lock.default({ maxPending: Infinity })
				let result
				let unmergedPaths = []
				await lock.acquire(filepath, async () => {
					if (await isIndexStale(fs2, filepath, cache2[IndexCache])) {
						await updateCachedIndexFile(fs2, filepath, cache2[IndexCache])
					}
					const index2 = cache2[IndexCache].map.get(filepath)
					unmergedPaths = index2.unmergedPaths
					if (unmergedPaths.length && !allowUnmerged) throw new UnmergedPathsError(unmergedPaths)
					result = await closure(index2)
					if (index2._dirty) {
						const buffer = await index2.toObject()
						await fs2.write(filepath, buffer)
						cache2[IndexCache].stats.set(filepath, await fs2.lstat(filepath))
						index2._dirty = false
					}
				})
				return result
			}
		}
		GitWalkerIndex = class {
			constructor({ fs: fs2, gitdir, cache: cache2 }) {
				this.treePromise = GitIndexManager.acquire(
					{ fs: fs2, gitdir, cache: cache2 },
					async function (index2) {
						return flatFileListToDirectoryStructure(index2.entries)
					}
				)
				const walker = this
				this.ConstructEntry = class StageEntry {
					constructor(fullpath) {
						this._fullpath = fullpath
						this._type = false
						this._mode = false
						this._stat = false
						this._oid = false
					}
					async type() {
						return walker.type(this)
					}
					async mode() {
						return walker.mode(this)
					}
					async stat() {
						return walker.stat(this)
					}
					async content() {
						return walker.content(this)
					}
					async oid() {
						return walker.oid(this)
					}
				}
			}
			async readdir(entry) {
				const filepath = entry._fullpath
				const tree = await this.treePromise
				const inode = tree.get(filepath)
				if (!inode) return null
				if (inode.type === "blob") return null
				if (inode.type !== "tree") {
					throw new Error(`ENOTDIR: not a directory, scandir '${filepath}'`)
				}
				const names = inode.children.map((inode2) => inode2.fullpath)
				names.sort(compareStrings)
				return names
			}
			async type(entry) {
				if (entry._type === false) {
					await entry.stat()
				}
				return entry._type
			}
			async mode(entry) {
				if (entry._mode === false) {
					await entry.stat()
				}
				return entry._mode
			}
			async stat(entry) {
				if (entry._stat === false) {
					const tree = await this.treePromise
					const inode = tree.get(entry._fullpath)
					if (!inode) {
						throw new Error(`ENOENT: no such file or directory, lstat '${entry._fullpath}'`)
					}
					const stats = inode.type === "tree" ? {} : normalizeStats(inode.metadata)
					entry._type = inode.type === "tree" ? "tree" : mode2type(stats.mode)
					entry._mode = stats.mode
					if (inode.type === "tree") {
						entry._stat = void 0
					} else {
						entry._stat = stats
					}
				}
				return entry._stat
			}
			async content(_entry) {}
			async oid(entry) {
				if (entry._oid === false) {
					const tree = await this.treePromise
					const inode = tree.get(entry._fullpath)
					entry._oid = inode.metadata.oid
				}
				return entry._oid
			}
		}
		GitWalkSymbol = Symbol("GitWalkSymbol")
		NotFoundError = class _NotFoundError extends BaseError {
			/**
			 * @param {string} what
			 */
			constructor(what) {
				super(`Could not find ${what}.`)
				this.code = this.name = _NotFoundError.code
				this.data = { what }
			}
		}
		NotFoundError.code = "NotFoundError"
		ObjectTypeError = class _ObjectTypeError extends BaseError {
			/**
			 * @param {string} oid
			 * @param {'blob'|'commit'|'tag'|'tree'} actual
			 * @param {'blob'|'commit'|'tag'|'tree'} expected
			 * @param {string} [filepath]
			 */
			constructor(oid, actual, expected, filepath) {
				super(
					`Object ${oid} ${
						filepath ? `at ${filepath}` : ""
					}was anticipated to be a ${expected} but it is a ${actual}.`
				)
				this.code = this.name = _ObjectTypeError.code
				this.data = { oid, actual, expected, filepath }
			}
		}
		ObjectTypeError.code = "ObjectTypeError"
		InvalidOidError = class _InvalidOidError extends BaseError {
			/**
			 * @param {string} value
			 */
			constructor(value) {
				super(`Expected a 40-char hex object id but saw "${value}".`)
				this.code = this.name = _InvalidOidError.code
				this.data = { value }
			}
		}
		InvalidOidError.code = "InvalidOidError"
		NoRefspecError = class _NoRefspecError extends BaseError {
			/**
			 * @param {string} remote
			 */
			constructor(remote) {
				super(`Could not find a fetch refspec for remote "${remote}". Make sure the config file has an entry like the following:
[remote "${remote}"]
	fetch = +refs/heads/*:refs/remotes/origin/*
`)
				this.code = this.name = _NoRefspecError.code
				this.data = { remote }
			}
		}
		NoRefspecError.code = "NoRefspecError"
		GitPackedRefs = class _GitPackedRefs {
			constructor(text) {
				this.refs = /* @__PURE__ */ new Map()
				this.parsedConfig = []
				if (text) {
					let key = null
					this.parsedConfig = text
						.trim()
						.split("\n")
						.map((line) => {
							if (/^\s*#/.test(line)) {
								return { line, comment: true }
							}
							const i = line.indexOf(" ")
							if (line.startsWith("^")) {
								const value = line.slice(1)
								this.refs.set(key + "^{}", value)
								return { line, ref: key, peeled: value }
							} else {
								const value = line.slice(0, i)
								key = line.slice(i + 1)
								this.refs.set(key, value)
								return { line, ref: key, oid: value }
							}
						})
				}
				return this
			}
			static from(text) {
				return new _GitPackedRefs(text)
			}
			delete(ref) {
				this.parsedConfig = this.parsedConfig.filter((entry) => entry.ref !== ref)
				this.refs.delete(ref)
			}
			toString() {
				return this.parsedConfig.map(({ line }) => line).join("\n") + "\n"
			}
		}
		GitRefSpec = class _GitRefSpec {
			constructor({ remotePath, localPath, force, matchPrefix }) {
				Object.assign(this, {
					remotePath,
					localPath,
					force,
					matchPrefix,
				})
			}
			static from(refspec) {
				const [forceMatch, remotePath, remoteGlobMatch, localPath, localGlobMatch] = refspec
					.match(/^(\+?)(.*?)(\*?):(.*?)(\*?)$/)
					.slice(1)
				const force = forceMatch === "+"
				const remoteIsGlob = remoteGlobMatch === "*"
				const localIsGlob = localGlobMatch === "*"
				if (remoteIsGlob !== localIsGlob) {
					throw new InternalError("Invalid refspec")
				}
				return new _GitRefSpec({
					remotePath,
					localPath,
					force,
					matchPrefix: remoteIsGlob,
				})
			}
			translate(remoteBranch) {
				if (this.matchPrefix) {
					if (remoteBranch.startsWith(this.remotePath)) {
						return this.localPath + remoteBranch.replace(this.remotePath, "")
					}
				} else {
					if (remoteBranch === this.remotePath) return this.localPath
				}
				return null
			}
			reverseTranslate(localBranch) {
				if (this.matchPrefix) {
					if (localBranch.startsWith(this.localPath)) {
						return this.remotePath + localBranch.replace(this.localPath, "")
					}
				} else {
					if (localBranch === this.localPath) return this.remotePath
				}
				return null
			}
		}
		GitRefSpecSet = class _GitRefSpecSet {
			constructor(rules = []) {
				this.rules = rules
			}
			static from(refspecs) {
				const rules = []
				for (const refspec of refspecs) {
					rules.push(GitRefSpec.from(refspec))
				}
				return new _GitRefSpecSet(rules)
			}
			add(refspec) {
				const rule = GitRefSpec.from(refspec)
				this.rules.push(rule)
			}
			translate(remoteRefs) {
				const result = []
				for (const rule of this.rules) {
					for (const remoteRef of remoteRefs) {
						const localRef = rule.translate(remoteRef)
						if (localRef) {
							result.push([remoteRef, localRef])
						}
					}
				}
				return result
			}
			translateOne(remoteRef) {
				let result = null
				for (const rule of this.rules) {
					const localRef = rule.translate(remoteRef)
					if (localRef) {
						result = localRef
					}
				}
				return result
			}
			localNamespaces() {
				return this.rules
					.filter((rule) => rule.matchPrefix)
					.map((rule) => rule.localPath.replace(/\/$/, ""))
			}
		}
		memo = /* @__PURE__ */ new Map()
		num = (val) => {
			val = val.toLowerCase()
			let n = parseInt(val)
			if (val.endsWith("k")) n *= 1024
			if (val.endsWith("m")) n *= 1024 * 1024
			if (val.endsWith("g")) n *= 1024 * 1024 * 1024
			return n
		}
		bool = (val) => {
			val = val.trim().toLowerCase()
			if (val === "true" || val === "yes" || val === "on") return true
			if (val === "false" || val === "no" || val === "off") return false
			throw Error(`Expected 'true', 'false', 'yes', 'no', 'on', or 'off', but got ${val}`)
		}
		schema = {
			core: {
				filemode: bool,
				bare: bool,
				logallrefupdates: bool,
				symlinks: bool,
				ignorecase: bool,
				bigFileThreshold: num,
			},
		}
		SECTION_LINE_REGEX = /^\[([A-Za-z0-9-.]+)(?: "(.*)")?\]$/
		SECTION_REGEX = /^[A-Za-z0-9-.]+$/
		VARIABLE_LINE_REGEX = /^([A-Za-z][A-Za-z-]*)(?: *= *(.*))?$/
		VARIABLE_NAME_REGEX = /^[A-Za-z][A-Za-z-]*$/
		VARIABLE_VALUE_COMMENT_REGEX = /^(.*?)( *[#;].*)$/
		extractSectionLine = (line) => {
			const matches = SECTION_LINE_REGEX.exec(line)
			if (matches != undefined) {
				const [section, subsection] = matches.slice(1)
				return [section, subsection]
			}
			return null
		}
		extractVariableLine = (line) => {
			const matches = VARIABLE_LINE_REGEX.exec(line)
			if (matches != undefined) {
				const [name, rawValue = "true"] = matches.slice(1)
				const valueWithoutComments = removeComments(rawValue)
				const valueWithoutQuotes = removeQuotes(valueWithoutComments)
				return [name, valueWithoutQuotes]
			}
			return null
		}
		removeComments = (rawValue) => {
			const commentMatches = VARIABLE_VALUE_COMMENT_REGEX.exec(rawValue)
			if (commentMatches == undefined) {
				return rawValue
			}
			const [valueWithoutComment, comment] = commentMatches.slice(1)
			if (hasOddNumberOfQuotes(valueWithoutComment) && hasOddNumberOfQuotes(comment)) {
				return `${valueWithoutComment}${comment}`
			}
			return valueWithoutComment
		}
		hasOddNumberOfQuotes = (text) => {
			const numberOfQuotes = (text.match(/(?:^|[^\\])"/g) || []).length
			return numberOfQuotes % 2 !== 0
		}
		removeQuotes = (text) => {
			return text.split("").reduce((newText, c, idx, text2) => {
				const isQuote = c === '"' && text2[idx - 1] !== "\\"
				const isEscapeForQuote = c === "\\" && text2[idx + 1] === '"'
				if (isQuote || isEscapeForQuote) {
					return newText
				}
				return newText + c
			}, "")
		}
		lower = (text) => {
			return text != undefined ? text.toLowerCase() : null
		}
		getPath = (section, subsection, name) => {
			return [lower(section), subsection, lower(name)].filter((a) => a != undefined).join(".")
		}
		normalizePath$1 = (path) => {
			const pathSegments = path.split(".")
			const section = pathSegments.shift()
			const name = pathSegments.pop()
			const subsection = pathSegments.length ? pathSegments.join(".") : void 0
			return {
				section,
				subsection,
				name,
				path: getPath(section, subsection, name),
				sectionPath: getPath(section, subsection, null),
			}
		}
		findLastIndex = (array, callback) => {
			return array.reduce((lastIndex, item, index2) => {
				return callback(item) ? index2 : lastIndex
			}, -1)
		}
		GitConfig = class _GitConfig {
			constructor(text) {
				let section = null
				let subsection = null
				this.parsedConfig = text.split("\n").map((line) => {
					let name = null
					let value = null
					const trimmedLine = line.trim()
					const extractedSection = extractSectionLine(trimmedLine)
					const isSection = extractedSection != undefined
					if (isSection) {
						;[section, subsection] = extractedSection
					} else {
						const extractedVariable = extractVariableLine(trimmedLine)
						const isVariable = extractedVariable != undefined
						if (isVariable) {
							;[name, value] = extractedVariable
						}
					}
					const path = getPath(section, subsection, name)
					return { line, isSection, section, subsection, name, value, path }
				})
			}
			static from(text) {
				return new _GitConfig(text)
			}
			async get(path, getall = false) {
				const normalizedPath = normalizePath$1(path).path
				const allValues = this.parsedConfig
					.filter((config) => config.path === normalizedPath)
					.map(({ section, name, value }) => {
						const fn = schema[section] && schema[section][name]
						return fn ? fn(value) : value
					})
				return getall ? allValues : allValues.pop()
			}
			async getall(path) {
				return this.get(path, true)
			}
			async getSubsections(section) {
				return this.parsedConfig
					.filter((config) => config.section === section && config.isSection)
					.map((config) => config.subsection)
			}
			async deleteSection(section, subsection) {
				this.parsedConfig = this.parsedConfig.filter(
					(config) => !(config.section === section && config.subsection === subsection)
				)
			}
			async append(path, value) {
				return this.set(path, value, true)
			}
			async set(path, value, append = false) {
				const {
					section,
					subsection,
					name,
					path: normalizedPath,
					sectionPath,
				} = normalizePath$1(path)
				const configIndex = findLastIndex(
					this.parsedConfig,
					(config) => config.path === normalizedPath
				)
				if (value == undefined) {
					if (configIndex !== -1) {
						this.parsedConfig.splice(configIndex, 1)
					}
				} else {
					if (configIndex !== -1) {
						const config = this.parsedConfig[configIndex]
						const modifiedConfig = Object.assign({}, config, {
							name,
							value,
							modified: true,
						})
						if (append) {
							this.parsedConfig.splice(configIndex + 1, 0, modifiedConfig)
						} else {
							this.parsedConfig[configIndex] = modifiedConfig
						}
					} else {
						const sectionIndex = this.parsedConfig.findIndex(
							(config) => config.path === sectionPath
						)
						const newConfig = {
							section,
							subsection,
							name,
							value,
							modified: true,
							path: normalizedPath,
						}
						if (SECTION_REGEX.test(section) && VARIABLE_NAME_REGEX.test(name)) {
							if (sectionIndex >= 0) {
								this.parsedConfig.splice(sectionIndex + 1, 0, newConfig)
							} else {
								const newSection = {
									section,
									subsection,
									modified: true,
									path: sectionPath,
								}
								this.parsedConfig.push(newSection, newConfig)
							}
						}
					}
				}
			}
			toString() {
				return this.parsedConfig
					.map(({ line, section, subsection, name, value, modified: modified2 = false }) => {
						if (!modified2) {
							return line
						}
						if (name != undefined && value != undefined) {
							if (typeof value === "string" && /[#;]/.test(value)) {
								return `	${name} = "${value}"`
							}
							return `	${name} = ${value}`
						}
						if (subsection != undefined) {
							return `[${section} "${subsection}"]`
						}
						return `[${section}]`
					})
					.join("\n")
			}
		}
		GitConfigManager = class {
			static async get({ fs: fs2, gitdir }) {
				const text = await fs2.read(`${gitdir}/config`, { encoding: "utf8" })
				return GitConfig.from(text)
			}
			static async save({ fs: fs2, gitdir, config }) {
				await fs2.write(`${gitdir}/config`, config.toString(), {
					encoding: "utf8",
				})
			}
		}
		refpaths = (ref) => [
			`${ref}`,
			`refs/${ref}`,
			`refs/tags/${ref}`,
			`refs/heads/${ref}`,
			`refs/remotes/${ref}`,
			`refs/remotes/${ref}/HEAD`,
		]
		GIT_FILES = ["config", "description", "index", "shallow", "commondir"]
		GitRefManager = class _GitRefManager {
			static async updateRemoteRefs({
				fs: fs2,
				gitdir,
				remote,
				refs,
				symrefs,
				tags,
				refspecs = void 0,
				prune = false,
				pruneTags = false,
			}) {
				for (const value of refs.values()) {
					if (!value.match(/[0-9a-f]{40}/)) {
						throw new InvalidOidError(value)
					}
				}
				const config = await GitConfigManager.get({ fs: fs2, gitdir })
				if (!refspecs) {
					refspecs = await config.getall(`remote.${remote}.fetch`)
					if (refspecs.length === 0) {
						throw new NoRefspecError(remote)
					}
					refspecs.unshift(`+HEAD:refs/remotes/${remote}/HEAD`)
				}
				const refspec = GitRefSpecSet.from(refspecs)
				const actualRefsToWrite = /* @__PURE__ */ new Map()
				if (pruneTags) {
					const tags2 = await _GitRefManager.listRefs({
						fs: fs2,
						gitdir,
						filepath: "refs/tags",
					})
					await _GitRefManager.deleteRefs({
						fs: fs2,
						gitdir,
						refs: tags2.map((tag2) => `refs/tags/${tag2}`),
					})
				}
				if (tags) {
					for (const serverRef of refs.keys()) {
						if (
							serverRef.startsWith("refs/tags") &&
							!serverRef.endsWith("^{}") &&
							!(await _GitRefManager.exists({ fs: fs2, gitdir, ref: serverRef }))
						) {
							const oid = refs.get(serverRef)
							actualRefsToWrite.set(serverRef, oid)
						}
					}
				}
				const refTranslations = refspec.translate([...refs.keys()])
				for (const [serverRef, translatedRef] of refTranslations) {
					const value = refs.get(serverRef)
					actualRefsToWrite.set(translatedRef, value)
				}
				const symrefTranslations = refspec.translate([...symrefs.keys()])
				for (const [serverRef, translatedRef] of symrefTranslations) {
					const value = symrefs.get(serverRef)
					const symtarget = refspec.translateOne(value)
					if (symtarget) {
						actualRefsToWrite.set(translatedRef, `ref: ${symtarget}`)
					}
				}
				const pruned = []
				if (prune) {
					for (const filepath of refspec.localNamespaces()) {
						const refs2 = (
							await _GitRefManager.listRefs({
								fs: fs2,
								gitdir,
								filepath,
							})
						).map((file) => `${filepath}/${file}`)
						for (const ref of refs2) {
							if (!actualRefsToWrite.has(ref)) {
								pruned.push(ref)
							}
						}
					}
					if (pruned.length > 0) {
						await _GitRefManager.deleteRefs({ fs: fs2, gitdir, refs: pruned })
					}
				}
				for (const [key, value] of actualRefsToWrite) {
					await acquireLock(key, async () =>
						fs2.write(
							join(gitdir, key),
							`${value.trim()}
`,
							"utf8"
						)
					)
				}
				return { pruned }
			}
			// TODO: make this less crude?
			static async writeRef({ fs: fs2, gitdir, ref, value }) {
				if (!value.match(/[0-9a-f]{40}/)) {
					throw new InvalidOidError(value)
				}
				await acquireLock(ref, async () =>
					fs2.write(
						join(gitdir, ref),
						`${value.trim()}
`,
						"utf8"
					)
				)
			}
			static async writeSymbolicRef({ fs: fs2, gitdir, ref, value }) {
				await acquireLock(ref, async () =>
					fs2.write(
						join(gitdir, ref),
						`ref: ${value.trim()}
`,
						"utf8"
					)
				)
			}
			static async deleteRef({ fs: fs2, gitdir, ref }) {
				return _GitRefManager.deleteRefs({ fs: fs2, gitdir, refs: [ref] })
			}
			static async deleteRefs({ fs: fs2, gitdir, refs }) {
				await Promise.all(refs.map((ref) => fs2.rm(join(gitdir, ref))))
				let text = await acquireLock("packed-refs", async () =>
					fs2.read(`${gitdir}/packed-refs`, { encoding: "utf8" })
				)
				const packed = GitPackedRefs.from(text)
				const beforeSize = packed.refs.size
				for (const ref of refs) {
					if (packed.refs.has(ref)) {
						packed.delete(ref)
					}
				}
				if (packed.refs.size < beforeSize) {
					text = packed.toString()
					await acquireLock("packed-refs", async () =>
						fs2.write(`${gitdir}/packed-refs`, text, { encoding: "utf8" })
					)
				}
			}
			/**
			 * @param {object} args
			 * @param {import('../models/FileSystem.js').FileSystem} args.fs
			 * @param {string} args.gitdir
			 * @param {string} args.ref
			 * @param {number} [args.depth]
			 * @returns {Promise<string>}
			 */
			static async resolve({ fs: fs2, gitdir, ref, depth = void 0 }) {
				if (depth !== void 0) {
					depth--
					if (depth === -1) {
						return ref
					}
				}
				if (ref.startsWith("ref: ")) {
					ref = ref.slice("ref: ".length)
					return _GitRefManager.resolve({ fs: fs2, gitdir, ref, depth })
				}
				if (ref.length === 40 && /[0-9a-f]{40}/.test(ref)) {
					return ref
				}
				const packedMap = await _GitRefManager.packedRefs({ fs: fs2, gitdir })
				const allpaths = refpaths(ref).filter((p) => !GIT_FILES.includes(p))
				for (const ref2 of allpaths) {
					const sha = await acquireLock(
						ref2,
						async () =>
							(await fs2.read(`${gitdir}/${ref2}`, { encoding: "utf8" })) || packedMap.get(ref2)
					)
					if (sha) {
						return _GitRefManager.resolve({ fs: fs2, gitdir, ref: sha.trim(), depth })
					}
				}
				throw new NotFoundError(ref)
			}
			static async exists({ fs: fs2, gitdir, ref }) {
				try {
					await _GitRefManager.expand({ fs: fs2, gitdir, ref })
					return true
				} catch (err) {
					return false
				}
			}
			static async expand({ fs: fs2, gitdir, ref }) {
				if (ref.length === 40 && /[0-9a-f]{40}/.test(ref)) {
					return ref
				}
				const packedMap = await _GitRefManager.packedRefs({ fs: fs2, gitdir })
				const allpaths = refpaths(ref)
				for (const ref2 of allpaths) {
					const refExists = await acquireLock(ref2, async () => fs2.exists(`${gitdir}/${ref2}`))
					if (refExists) return ref2
					if (packedMap.has(ref2)) return ref2
				}
				throw new NotFoundError(ref)
			}
			static async expandAgainstMap({ ref, map }) {
				const allpaths = refpaths(ref)
				for (const ref2 of allpaths) {
					if (await map.has(ref2)) return ref2
				}
				throw new NotFoundError(ref)
			}
			static resolveAgainstMap({ ref, fullref = ref, depth = void 0, map }) {
				if (depth !== void 0) {
					depth--
					if (depth === -1) {
						return { fullref, oid: ref }
					}
				}
				if (ref.startsWith("ref: ")) {
					ref = ref.slice("ref: ".length)
					return _GitRefManager.resolveAgainstMap({ ref, fullref, depth, map })
				}
				if (ref.length === 40 && /[0-9a-f]{40}/.test(ref)) {
					return { fullref, oid: ref }
				}
				const allpaths = refpaths(ref)
				for (const ref2 of allpaths) {
					const sha = map.get(ref2)
					if (sha) {
						return _GitRefManager.resolveAgainstMap({
							ref: sha.trim(),
							fullref: ref2,
							depth,
							map,
						})
					}
				}
				throw new NotFoundError(ref)
			}
			static async packedRefs({ fs: fs2, gitdir }) {
				const text = await acquireLock("packed-refs", async () =>
					fs2.read(`${gitdir}/packed-refs`, { encoding: "utf8" })
				)
				const packed = GitPackedRefs.from(text)
				return packed.refs
			}
			// List all the refs that match the `filepath` prefix
			static async listRefs({ fs: fs2, gitdir, filepath }) {
				const packedMap = _GitRefManager.packedRefs({ fs: fs2, gitdir })
				let files = null
				try {
					files = await fs2.readdirDeep(`${gitdir}/${filepath}`)
					files = files.map((x) => x.replace(`${gitdir}/${filepath}/`, ""))
				} catch (err) {
					files = []
				}
				for (let key of (await packedMap).keys()) {
					if (key.startsWith(filepath)) {
						key = key.replace(filepath + "/", "")
						if (!files.includes(key)) {
							files.push(key)
						}
					}
				}
				files.sort(compareRefNames)
				return files
			}
			static async listBranches({ fs: fs2, gitdir, remote }) {
				if (remote) {
					return _GitRefManager.listRefs({
						fs: fs2,
						gitdir,
						filepath: `refs/remotes/${remote}`,
					})
				} else {
					return _GitRefManager.listRefs({ fs: fs2, gitdir, filepath: `refs/heads` })
				}
			}
			static async listTags({ fs: fs2, gitdir }) {
				const tags = await _GitRefManager.listRefs({
					fs: fs2,
					gitdir,
					filepath: `refs/tags`,
				})
				return tags.filter((x) => !x.endsWith("^{}"))
			}
		}
		GitTree = class _GitTree {
			constructor(entries) {
				if (Buffer.isBuffer(entries)) {
					this._entries = parseBuffer(entries)
				} else if (Array.isArray(entries)) {
					this._entries = entries.map(nudgeIntoShape)
				} else {
					throw new InternalError("invalid type passed to GitTree constructor")
				}
				this._entries.sort(comparePath)
			}
			static from(tree) {
				return new _GitTree(tree)
			}
			render() {
				return this._entries
					.map((entry) => `${entry.mode} ${entry.type} ${entry.oid}    ${entry.path}`)
					.join("\n")
			}
			toObject() {
				const entries = [...this._entries]
				entries.sort(compareTreeEntryPath)
				return Buffer.concat(
					entries.map((entry) => {
						const mode = Buffer.from(entry.mode.replace(/^0/, ""))
						const space = Buffer.from(" ")
						const path = Buffer.from(entry.path, "utf8")
						const nullchar = Buffer.from([0])
						const oid = Buffer.from(entry.oid, "hex")
						return Buffer.concat([mode, space, path, nullchar, oid])
					})
				)
			}
			/**
			 * @returns {TreeEntry[]}
			 */
			entries() {
				return this._entries
			}
			*[Symbol.iterator]() {
				for (const entry of this._entries) {
					yield entry
				}
			}
		}
		GitObject = class {
			static wrap({ type, object }) {
				return Buffer.concat([
					Buffer.from(`${type} ${object.byteLength.toString()}\0`),
					Buffer.from(object),
				])
			}
			static unwrap(buffer) {
				const s = buffer.indexOf(32)
				const i = buffer.indexOf(0)
				const type = buffer.slice(0, s).toString("utf8")
				const length = buffer.slice(s + 1, i).toString("utf8")
				const actualLength = buffer.length - (i + 1)
				if (parseInt(length) !== actualLength) {
					throw new InternalError(
						`Length mismatch: expected ${length} bytes but got ${actualLength} instead.`
					)
				}
				return {
					type,
					object: Buffer.from(buffer.slice(i + 1)),
				}
			}
		}
		StreamReader = class {
			constructor(stream) {
				this.stream = getIterator(stream)
				this.buffer = null
				this.cursor = 0
				this.undoCursor = 0
				this.started = false
				this._ended = false
				this._discardedBytes = 0
			}
			eof() {
				return this._ended && this.cursor === this.buffer.length
			}
			tell() {
				return this._discardedBytes + this.cursor
			}
			async byte() {
				if (this.eof()) return
				if (!this.started) await this._init()
				if (this.cursor === this.buffer.length) {
					await this._loadnext()
					if (this._ended) return
				}
				this._moveCursor(1)
				return this.buffer[this.undoCursor]
			}
			async chunk() {
				if (this.eof()) return
				if (!this.started) await this._init()
				if (this.cursor === this.buffer.length) {
					await this._loadnext()
					if (this._ended) return
				}
				this._moveCursor(this.buffer.length)
				return this.buffer.slice(this.undoCursor, this.cursor)
			}
			async read(n) {
				if (this.eof()) return
				if (!this.started) await this._init()
				if (this.cursor + n > this.buffer.length) {
					this._trim()
					await this._accumulate(n)
				}
				this._moveCursor(n)
				return this.buffer.slice(this.undoCursor, this.cursor)
			}
			async skip(n) {
				if (this.eof()) return
				if (!this.started) await this._init()
				if (this.cursor + n > this.buffer.length) {
					this._trim()
					await this._accumulate(n)
				}
				this._moveCursor(n)
			}
			async undo() {
				this.cursor = this.undoCursor
			}
			async _next() {
				this.started = true
				let { done, value } = await this.stream.next()
				if (done) {
					this._ended = true
					if (!value) return Buffer.alloc(0)
				}
				if (value) {
					value = Buffer.from(value)
				}
				return value
			}
			_trim() {
				this.buffer = this.buffer.slice(this.undoCursor)
				this.cursor -= this.undoCursor
				this._discardedBytes += this.undoCursor
				this.undoCursor = 0
			}
			_moveCursor(n) {
				this.undoCursor = this.cursor
				this.cursor += n
				if (this.cursor > this.buffer.length) {
					this.cursor = this.buffer.length
				}
			}
			async _accumulate(n) {
				if (this._ended) return
				const buffers = [this.buffer]
				while (this.cursor + n > lengthBuffers(buffers)) {
					const nextbuffer = await this._next()
					if (this._ended) break
					buffers.push(nextbuffer)
				}
				this.buffer = Buffer.concat(buffers)
			}
			async _loadnext() {
				this._discardedBytes += this.buffer.length
				this.undoCursor = 0
				this.cursor = 0
				this.buffer = await this._next()
			}
			async _init() {
				this.buffer = await this._next()
			}
		}
		supportsDecompressionStream = false
		GitPackIndex = class _GitPackIndex {
			constructor(stuff) {
				Object.assign(this, stuff)
				this.offsetCache = {}
			}
			static async fromIdx({ idx, getExternalRefDelta }) {
				const reader = new BufferCursor(idx)
				const magic = reader.slice(4).toString("hex")
				if (magic !== "ff744f63") {
					return
				}
				const version3 = reader.readUInt32BE()
				if (version3 !== 2) {
					throw new InternalError(
						`Unable to read version ${version3} packfile IDX. (Only version 2 supported)`
					)
				}
				if (idx.byteLength > 2048 * 1024 * 1024) {
					throw new InternalError(
						`To keep implementation simple, I haven't implemented the layer 5 feature needed to support packfiles > 2GB in size.`
					)
				}
				reader.seek(reader.tell() + 4 * 255)
				const size = reader.readUInt32BE()
				const hashes = []
				for (let i = 0; i < size; i++) {
					const hash2 = reader.slice(20).toString("hex")
					hashes[i] = hash2
				}
				reader.seek(reader.tell() + 4 * size)
				const offsets = /* @__PURE__ */ new Map()
				for (let i = 0; i < size; i++) {
					offsets.set(hashes[i], reader.readUInt32BE())
				}
				const packfileSha = reader.slice(20).toString("hex")
				return new _GitPackIndex({
					hashes,
					crcs: {},
					offsets,
					packfileSha,
					getExternalRefDelta,
				})
			}
			static async fromPack({ pack, getExternalRefDelta, onProgress }) {
				const listpackTypes = {
					1: "commit",
					2: "tree",
					3: "blob",
					4: "tag",
					6: "ofs-delta",
					7: "ref-delta",
				}
				const offsetToObject = {}
				const packfileSha = pack.slice(-20).toString("hex")
				const hashes = []
				const crcs = {}
				const offsets = /* @__PURE__ */ new Map()
				let totalObjectCount = null
				let lastPercent = null
				await listpack([pack], async ({ data, type, reference, offset, num: num2 }) => {
					if (totalObjectCount === null) totalObjectCount = num2
					const percent = Math.floor(((totalObjectCount - num2) * 100) / totalObjectCount)
					if (percent !== lastPercent && onProgress) {
						await onProgress({
							phase: "Receiving objects",
							loaded: totalObjectCount - num2,
							total: totalObjectCount,
						})
					}
					lastPercent = percent
					type = listpackTypes[type]
					if (["commit", "tree", "blob", "tag"].includes(type)) {
						offsetToObject[offset] = {
							type,
							offset,
						}
					} else if (type === "ofs-delta") {
						offsetToObject[offset] = {
							type,
							offset,
						}
					} else if (type === "ref-delta") {
						offsetToObject[offset] = {
							type,
							offset,
						}
					}
				})
				const offsetArray = Object.keys(offsetToObject).map(Number)
				for (const [i, start] of offsetArray.entries()) {
					const end = i + 1 === offsetArray.length ? pack.byteLength - 20 : offsetArray[i + 1]
					const o = offsetToObject[start]
					const crc = import_crc_32.default.buf(pack.slice(start, end)) >>> 0
					o.end = end
					o.crc = crc
				}
				const p = new _GitPackIndex({
					pack: Promise.resolve(pack),
					packfileSha,
					crcs,
					hashes,
					offsets,
					getExternalRefDelta,
				})
				lastPercent = null
				let count = 0
				const objectsByDepth = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
				for (let offset in offsetToObject) {
					offset = Number(offset)
					const percent = Math.floor((count * 100) / totalObjectCount)
					if (percent !== lastPercent && onProgress) {
						await onProgress({
							phase: "Resolving deltas",
							loaded: count,
							total: totalObjectCount,
						})
					}
					count++
					lastPercent = percent
					const o = offsetToObject[offset]
					if (o.oid) continue
					try {
						p.readDepth = 0
						p.externalReadDepth = 0
						const { type, object } = await p.readSlice({ start: offset })
						objectsByDepth[p.readDepth] += 1
						const oid = await shasum(GitObject.wrap({ type, object }))
						o.oid = oid
						hashes.push(oid)
						offsets.set(oid, offset)
						crcs[oid] = o.crc
					} catch (err) {
						continue
					}
				}
				hashes.sort()
				return p
			}
			async toBuffer() {
				const buffers = []
				const write = (str, encoding) => {
					buffers.push(Buffer.from(str, encoding))
				}
				write("ff744f63", "hex")
				write("00000002", "hex")
				const fanoutBuffer = new BufferCursor(Buffer.alloc(256 * 4))
				for (let i = 0; i < 256; i++) {
					let count = 0
					for (const hash2 of this.hashes) {
						if (parseInt(hash2.slice(0, 2), 16) <= i) count++
					}
					fanoutBuffer.writeUInt32BE(count)
				}
				buffers.push(fanoutBuffer.buffer)
				for (const hash2 of this.hashes) {
					write(hash2, "hex")
				}
				const crcsBuffer = new BufferCursor(Buffer.alloc(this.hashes.length * 4))
				for (const hash2 of this.hashes) {
					crcsBuffer.writeUInt32BE(this.crcs[hash2])
				}
				buffers.push(crcsBuffer.buffer)
				const offsetsBuffer = new BufferCursor(Buffer.alloc(this.hashes.length * 4))
				for (const hash2 of this.hashes) {
					offsetsBuffer.writeUInt32BE(this.offsets.get(hash2))
				}
				buffers.push(offsetsBuffer.buffer)
				write(this.packfileSha, "hex")
				const totalBuffer = Buffer.concat(buffers)
				const sha = await shasum(totalBuffer)
				const shaBuffer = Buffer.alloc(20)
				shaBuffer.write(sha, "hex")
				return Buffer.concat([totalBuffer, shaBuffer])
			}
			async load({ pack }) {
				this.pack = pack
			}
			async unload() {
				this.pack = null
			}
			async read({ oid }) {
				if (!this.offsets.get(oid)) {
					if (this.getExternalRefDelta) {
						this.externalReadDepth++
						return this.getExternalRefDelta(oid)
					} else {
						throw new InternalError(`Could not read object ${oid} from packfile`)
					}
				}
				const start = this.offsets.get(oid)
				return this.readSlice({ start })
			}
			async readSlice({ start }) {
				if (this.offsetCache[start]) {
					return Object.assign({}, this.offsetCache[start])
				}
				this.readDepth++
				const types2 = {
					16: "commit",
					32: "tree",
					48: "blob",
					64: "tag",
					96: "ofs_delta",
					112: "ref_delta",
				}
				if (!this.pack) {
					throw new InternalError(
						"Tried to read from a GitPackIndex with no packfile loaded into memory"
					)
				}
				const raw = (await this.pack).slice(start)
				const reader = new BufferCursor(raw)
				const byte = reader.readUInt8()
				const btype = byte & 112
				let type = types2[btype]
				if (type === void 0) {
					throw new InternalError("Unrecognized type: 0b" + btype.toString(2))
				}
				const lastFour = byte & 15
				let length = lastFour
				const multibyte = byte & 128
				if (multibyte) {
					length = otherVarIntDecode(reader, lastFour)
				}
				let base = null
				let object = null
				if (type === "ofs_delta") {
					const offset = decodeVarInt(reader)
					const baseOffset = start - offset
					;({ object: base, type } = await this.readSlice({ start: baseOffset }))
				}
				if (type === "ref_delta") {
					const oid = reader.slice(20).toString("hex")
					;({ object: base, type } = await this.read({ oid }))
				}
				const buffer = raw.slice(reader.tell())
				object = Buffer.from(await inflate(buffer))
				if (object.byteLength !== length) {
					throw new InternalError(
						`Packfile told us object would have length ${length} but it had length ${object.byteLength}`
					)
				}
				if (base) {
					object = Buffer.from(applyDelta(object, base))
				}
				if (this.readDepth > 3) {
					this.offsetCache[start] = { type, object }
				}
				return { type, format: "content", object }
			}
		}
		PackfileCache = Symbol("PackfileCache")
		AlreadyExistsError = class _AlreadyExistsError extends BaseError {
			/**
			 * @param {'note'|'remote'|'tag'|'branch'} noun
			 * @param {string} where
			 * @param {boolean} canForce
			 */
			constructor(noun, where, canForce = true) {
				super(
					`Failed to create ${noun} at ${where} because it already exists.${
						canForce ? ` (Hint: use 'force: true' parameter to overwrite existing ${noun}.)` : ""
					}`
				)
				this.code = this.name = _AlreadyExistsError.code
				this.data = { noun, where, canForce }
			}
		}
		AlreadyExistsError.code = "AlreadyExistsError"
		AmbiguousError = class _AmbiguousError extends BaseError {
			/**
			 * @param {'oids'|'refs'} nouns
			 * @param {string} short
			 * @param {string[]} matches
			 */
			constructor(nouns, short, matches) {
				super(
					`Found multiple ${nouns} matching "${short}" (${matches.join(
						", "
					)}). Use a longer abbreviation length to disambiguate them.`
				)
				this.code = this.name = _AmbiguousError.code
				this.data = { nouns, short, matches }
			}
		}
		AmbiguousError.code = "AmbiguousError"
		CheckoutConflictError = class _CheckoutConflictError extends BaseError {
			/**
			 * @param {string[]} filepaths
			 */
			constructor(filepaths) {
				super(
					`Your local changes to the following files would be overwritten by checkout: ${filepaths.join(
						", "
					)}`
				)
				this.code = this.name = _CheckoutConflictError.code
				this.data = { filepaths }
			}
		}
		CheckoutConflictError.code = "CheckoutConflictError"
		CommitNotFetchedError = class _CommitNotFetchedError extends BaseError {
			/**
			 * @param {string} ref
			 * @param {string} oid
			 */
			constructor(ref, oid) {
				super(
					`Failed to checkout "${ref}" because commit ${oid} is not available locally. Do a git fetch to make the branch available locally.`
				)
				this.code = this.name = _CommitNotFetchedError.code
				this.data = { ref, oid }
			}
		}
		CommitNotFetchedError.code = "CommitNotFetchedError"
		EmptyServerResponseError = class _EmptyServerResponseError extends BaseError {
			constructor() {
				super(`Empty response from git server.`)
				this.code = this.name = _EmptyServerResponseError.code
				this.data = {}
			}
		}
		EmptyServerResponseError.code = "EmptyServerResponseError"
		FastForwardError = class _FastForwardError extends BaseError {
			constructor() {
				super(`A simple fast-forward merge was not possible.`)
				this.code = this.name = _FastForwardError.code
				this.data = {}
			}
		}
		FastForwardError.code = "FastForwardError"
		GitPushError = class _GitPushError extends BaseError {
			/**
			 * @param {string} prettyDetails
			 * @param {PushResult} result
			 */
			constructor(prettyDetails, result) {
				super(`One or more branches were not updated: ${prettyDetails}`)
				this.code = this.name = _GitPushError.code
				this.data = { prettyDetails, result }
			}
		}
		GitPushError.code = "GitPushError"
		HttpError = class _HttpError extends BaseError {
			/**
			 * @param {number} statusCode
			 * @param {string} statusMessage
			 * @param {string} response
			 */
			constructor(statusCode, statusMessage, response) {
				super(`HTTP Error: ${statusCode} ${statusMessage}`)
				this.code = this.name = _HttpError.code
				this.data = { statusCode, statusMessage, response }
			}
		}
		HttpError.code = "HttpError"
		InvalidFilepathError = class _InvalidFilepathError extends BaseError {
			/**
			 * @param {'leading-slash'|'trailing-slash'|'directory'} [reason]
			 */
			constructor(reason) {
				let message = "invalid filepath"
				if (reason === "leading-slash" || reason === "trailing-slash") {
					message = `"filepath" parameter should not include leading or trailing directory separators because these can cause problems on some platforms.`
				} else if (reason === "directory") {
					message = `"filepath" should not be a directory.`
				}
				super(message)
				this.code = this.name = _InvalidFilepathError.code
				this.data = { reason }
			}
		}
		InvalidFilepathError.code = "InvalidFilepathError"
		InvalidRefNameError = class _InvalidRefNameError extends BaseError {
			/**
			 * @param {string} ref
			 * @param {string} suggestion
			 * @param {boolean} canForce
			 */
			constructor(ref, suggestion) {
				super(
					`"${ref}" would be an invalid git reference. (Hint: a valid alternative would be "${suggestion}".)`
				)
				this.code = this.name = _InvalidRefNameError.code
				this.data = { ref, suggestion }
			}
		}
		InvalidRefNameError.code = "InvalidRefNameError"
		MaxDepthError = class _MaxDepthError extends BaseError {
			/**
			 * @param {number} depth
			 */
			constructor(depth) {
				super(`Maximum search depth of ${depth} exceeded.`)
				this.code = this.name = _MaxDepthError.code
				this.data = { depth }
			}
		}
		MaxDepthError.code = "MaxDepthError"
		MergeNotSupportedError = class _MergeNotSupportedError extends BaseError {
			constructor() {
				super(`Merges with conflicts are not supported yet.`)
				this.code = this.name = _MergeNotSupportedError.code
				this.data = {}
			}
		}
		MergeNotSupportedError.code = "MergeNotSupportedError"
		MergeConflictError = class _MergeConflictError extends BaseError {
			/**
			 * @param {Array<string>} filepaths
			 * @param {Array<string>} bothModified
			 * @param {Array<string>} deleteByUs
			 * @param {Array<string>} deleteByTheirs
			 */
			constructor(filepaths, bothModified, deleteByUs, deleteByTheirs) {
				super(
					`Automatic merge failed with one or more merge conflicts in the following files: ${filepaths.toString()}. Fix conflicts then commit the result.`
				)
				this.code = this.name = _MergeConflictError.code
				this.data = { filepaths, bothModified, deleteByUs, deleteByTheirs }
			}
		}
		MergeConflictError.code = "MergeConflictError"
		MissingNameError = class _MissingNameError extends BaseError {
			/**
			 * @param {'author'|'committer'|'tagger'} role
			 */
			constructor(role) {
				super(`No name was provided for ${role} in the argument or in the .git/config file.`)
				this.code = this.name = _MissingNameError.code
				this.data = { role }
			}
		}
		MissingNameError.code = "MissingNameError"
		MissingParameterError = class _MissingParameterError extends BaseError {
			/**
			 * @param {string} parameter
			 */
			constructor(parameter) {
				super(`The function requires a "${parameter}" parameter but none was provided.`)
				this.code = this.name = _MissingParameterError.code
				this.data = { parameter }
			}
		}
		MissingParameterError.code = "MissingParameterError"
		MultipleGitError = class _MultipleGitError extends BaseError {
			/**
			 * @param {Error[]} errors
			 * @param {string} message
			 */
			constructor(errors) {
				super(
					`There are multiple errors that were thrown by the method. Please refer to the "errors" property to see more`
				)
				this.code = this.name = _MultipleGitError.code
				this.data = { errors }
				this.errors = errors
			}
		}
		MultipleGitError.code = "MultipleGitError"
		ParseError = class _ParseError extends BaseError {
			/**
			 * @param {string} expected
			 * @param {string} actual
			 */
			constructor(expected, actual) {
				super(`Expected "${expected}" but received "${actual}".`)
				this.code = this.name = _ParseError.code
				this.data = { expected, actual }
			}
		}
		ParseError.code = "ParseError"
		PushRejectedError = class _PushRejectedError extends BaseError {
			/**
			 * @param {'not-fast-forward'|'tag-exists'} reason
			 */
			constructor(reason) {
				let message = ""
				if (reason === "not-fast-forward") {
					message = " because it was not a simple fast-forward"
				} else if (reason === "tag-exists") {
					message = " because tag already exists"
				}
				super(`Push rejected${message}. Use "force: true" to override.`)
				this.code = this.name = _PushRejectedError.code
				this.data = { reason }
			}
		}
		PushRejectedError.code = "PushRejectedError"
		RemoteCapabilityError = class _RemoteCapabilityError extends BaseError {
			/**
			 * @param {'shallow'|'deepen-since'|'deepen-not'|'deepen-relative'} capability
			 * @param {'depth'|'since'|'exclude'|'relative'} parameter
			 */
			constructor(capability, parameter) {
				super(
					`Remote does not support the "${capability}" so the "${parameter}" parameter cannot be used.`
				)
				this.code = this.name = _RemoteCapabilityError.code
				this.data = { capability, parameter }
			}
		}
		RemoteCapabilityError.code = "RemoteCapabilityError"
		SmartHttpError = class _SmartHttpError extends BaseError {
			/**
			 * @param {string} preview
			 * @param {string} response
			 */
			constructor(preview, response) {
				super(
					`Remote did not reply using the "smart" HTTP protocol. Expected "001e# service=git-upload-pack" but received: ${preview}`
				)
				this.code = this.name = _SmartHttpError.code
				this.data = { preview, response }
			}
		}
		SmartHttpError.code = "SmartHttpError"
		UnknownTransportError = class _UnknownTransportError extends BaseError {
			/**
			 * @param {string} url
			 * @param {string} transport
			 * @param {string} [suggestion]
			 */
			constructor(url, transport, suggestion) {
				super(`Git remote "${url}" uses an unrecognized transport protocol: "${transport}"`)
				this.code = this.name = _UnknownTransportError.code
				this.data = { url, transport, suggestion }
			}
		}
		UnknownTransportError.code = "UnknownTransportError"
		UrlParseError = class _UrlParseError extends BaseError {
			/**
			 * @param {string} url
			 */
			constructor(url) {
				super(`Cannot parse remote URL: "${url}"`)
				this.code = this.name = _UrlParseError.code
				this.data = { url }
			}
		}
		UrlParseError.code = "UrlParseError"
		UserCanceledError = class _UserCanceledError extends BaseError {
			constructor() {
				super(`The operation was canceled.`)
				this.code = this.name = _UserCanceledError.code
				this.data = {}
			}
		}
		UserCanceledError.code = "UserCanceledError"
		IndexResetError = class _IndexResetError extends BaseError {
			/**
			 * @param {Array<string>} filepaths
			 */
			constructor(filepath) {
				super(
					`Could not merge index: Entry for '${filepath}' is not up to date. Either reset the index entry to HEAD, or stage your unstaged changes.`
				)
				this.code = this.name = _IndexResetError.code
				this.data = { filepath }
			}
		}
		IndexResetError.code = "IndexResetError"
		Errors = /* @__PURE__ */ Object.freeze({
			__proto__: null,
			AlreadyExistsError,
			AmbiguousError,
			CheckoutConflictError,
			CommitNotFetchedError,
			EmptyServerResponseError,
			FastForwardError,
			GitPushError,
			HttpError,
			InternalError,
			InvalidFilepathError,
			InvalidOidError,
			InvalidRefNameError,
			MaxDepthError,
			MergeNotSupportedError,
			MergeConflictError,
			MissingNameError,
			MissingParameterError,
			MultipleGitError,
			NoRefspecError,
			NotFoundError,
			ObjectTypeError,
			ParseError,
			PushRejectedError,
			RemoteCapabilityError,
			SmartHttpError,
			UnknownTransportError,
			UnsafeFilepathError,
			UrlParseError,
			UserCanceledError,
			UnmergedPathsError,
			IndexResetError,
		})
		GitAnnotatedTag = class _GitAnnotatedTag {
			constructor(tag2) {
				if (typeof tag2 === "string") {
					this._tag = tag2
				} else if (Buffer.isBuffer(tag2)) {
					this._tag = tag2.toString("utf8")
				} else if (typeof tag2 === "object") {
					this._tag = _GitAnnotatedTag.render(tag2)
				} else {
					throw new InternalError("invalid type passed to GitAnnotatedTag constructor")
				}
			}
			static from(tag2) {
				return new _GitAnnotatedTag(tag2)
			}
			static render(obj) {
				return `object ${obj.object}
type ${obj.type}
tag ${obj.tag}
tagger ${formatAuthor(obj.tagger)}

${obj.message}
${obj.gpgsig ? obj.gpgsig : ""}`
			}
			justHeaders() {
				return this._tag.slice(0, this._tag.indexOf("\n\n"))
			}
			message() {
				const tag2 = this.withoutSignature()
				return tag2.slice(tag2.indexOf("\n\n") + 2)
			}
			parse() {
				return Object.assign(this.headers(), {
					message: this.message(),
					gpgsig: this.gpgsig(),
				})
			}
			render() {
				return this._tag
			}
			headers() {
				const headers = this.justHeaders().split("\n")
				const hs = []
				for (const h of headers) {
					if (h[0] === " ") {
						hs[hs.length - 1] += "\n" + h.slice(1)
					} else {
						hs.push(h)
					}
				}
				const obj = {}
				for (const h of hs) {
					const key = h.slice(0, h.indexOf(" "))
					const value = h.slice(h.indexOf(" ") + 1)
					if (Array.isArray(obj[key])) {
						obj[key].push(value)
					} else {
						obj[key] = value
					}
				}
				if (obj.tagger) {
					obj.tagger = parseAuthor(obj.tagger)
				}
				if (obj.committer) {
					obj.committer = parseAuthor(obj.committer)
				}
				return obj
			}
			withoutSignature() {
				const tag2 = normalizeNewlines(this._tag)
				if (!tag2.includes("\n-----BEGIN PGP SIGNATURE-----")) return tag2
				return tag2.slice(0, tag2.lastIndexOf("\n-----BEGIN PGP SIGNATURE-----"))
			}
			gpgsig() {
				if (!this._tag.includes("\n-----BEGIN PGP SIGNATURE-----")) return
				const signature = this._tag.slice(
					this._tag.indexOf("-----BEGIN PGP SIGNATURE-----"),
					this._tag.indexOf("-----END PGP SIGNATURE-----") + "-----END PGP SIGNATURE-----".length
				)
				return normalizeNewlines(signature)
			}
			payload() {
				return this.withoutSignature() + "\n"
			}
			toObject() {
				return Buffer.from(this._tag, "utf8")
			}
			static async sign(tag2, sign, secretKey) {
				const payload = tag2.payload()
				let { signature } = await sign({ payload, secretKey })
				signature = normalizeNewlines(signature)
				const signedTag = payload + signature
				return _GitAnnotatedTag.from(signedTag)
			}
		}
		GitCommit = class _GitCommit {
			constructor(commit3) {
				if (typeof commit3 === "string") {
					this._commit = commit3
				} else if (Buffer.isBuffer(commit3)) {
					this._commit = commit3.toString("utf8")
				} else if (typeof commit3 === "object") {
					this._commit = _GitCommit.render(commit3)
				} else {
					throw new InternalError("invalid type passed to GitCommit constructor")
				}
			}
			static fromPayloadSignature({ payload, signature }) {
				const headers = _GitCommit.justHeaders(payload)
				const message = _GitCommit.justMessage(payload)
				const commit3 = normalizeNewlines(headers + "\ngpgsig" + indent(signature) + "\n" + message)
				return new _GitCommit(commit3)
			}
			static from(commit3) {
				return new _GitCommit(commit3)
			}
			toObject() {
				return Buffer.from(this._commit, "utf8")
			}
			// Todo: allow setting the headers and message
			headers() {
				return this.parseHeaders()
			}
			// Todo: allow setting the headers and message
			message() {
				return _GitCommit.justMessage(this._commit)
			}
			parse() {
				return Object.assign({ message: this.message() }, this.headers())
			}
			static justMessage(commit3) {
				return normalizeNewlines(commit3.slice(commit3.indexOf("\n\n") + 2))
			}
			static justHeaders(commit3) {
				return commit3.slice(0, commit3.indexOf("\n\n"))
			}
			parseHeaders() {
				const headers = _GitCommit.justHeaders(this._commit).split("\n")
				const hs = []
				for (const h of headers) {
					if (h[0] === " ") {
						hs[hs.length - 1] += "\n" + h.slice(1)
					} else {
						hs.push(h)
					}
				}
				const obj = {
					parent: [],
				}
				for (const h of hs) {
					const key = h.slice(0, h.indexOf(" "))
					const value = h.slice(h.indexOf(" ") + 1)
					if (Array.isArray(obj[key])) {
						obj[key].push(value)
					} else {
						obj[key] = value
					}
				}
				if (obj.author) {
					obj.author = parseAuthor(obj.author)
				}
				if (obj.committer) {
					obj.committer = parseAuthor(obj.committer)
				}
				return obj
			}
			static renderHeaders(obj) {
				let headers = ""
				if (obj.tree) {
					headers += `tree ${obj.tree}
`
				} else {
					headers += `tree 4b825dc642cb6eb9a060e54bf8d69288fbee4904
`
				}
				if (obj.parent) {
					if (obj.parent.length === void 0) {
						throw new InternalError(`commit 'parent' property should be an array`)
					}
					for (const p of obj.parent) {
						headers += `parent ${p}
`
					}
				}
				const author = obj.author
				headers += `author ${formatAuthor(author)}
`
				const committer = obj.committer || obj.author
				headers += `committer ${formatAuthor(committer)}
`
				if (obj.gpgsig) {
					headers += "gpgsig" + indent(obj.gpgsig)
				}
				return headers
			}
			static render(obj) {
				return _GitCommit.renderHeaders(obj) + "\n" + normalizeNewlines(obj.message)
			}
			render() {
				return this._commit
			}
			withoutSignature() {
				const commit3 = normalizeNewlines(this._commit)
				if (!commit3.includes("\ngpgsig")) return commit3
				const headers = commit3.slice(0, commit3.indexOf("\ngpgsig"))
				const message = commit3.slice(
					commit3.indexOf("-----END PGP SIGNATURE-----\n") + "-----END PGP SIGNATURE-----\n".length
				)
				return normalizeNewlines(headers + "\n" + message)
			}
			isolateSignature() {
				const signature = this._commit.slice(
					this._commit.indexOf("-----BEGIN PGP SIGNATURE-----"),
					this._commit.indexOf("-----END PGP SIGNATURE-----") + "-----END PGP SIGNATURE-----".length
				)
				return outdent(signature)
			}
			static async sign(commit3, sign, secretKey) {
				const payload = commit3.withoutSignature()
				const message = _GitCommit.justMessage(commit3._commit)
				let { signature } = await sign({ payload, secretKey })
				signature = normalizeNewlines(signature)
				const headers = _GitCommit.justHeaders(commit3._commit)
				const signedCommit = headers + "\ngpgsig" + indent(signature) + "\n" + message
				return _GitCommit.from(signedCommit)
			}
		}
		GitWalkerRepo = class {
			constructor({ fs: fs2, gitdir, ref, cache: cache2 }) {
				this.fs = fs2
				this.cache = cache2
				this.gitdir = gitdir
				this.mapPromise = (async () => {
					const map = /* @__PURE__ */ new Map()
					let oid
					try {
						oid = await GitRefManager.resolve({ fs: fs2, gitdir, ref })
					} catch (e) {
						if (e instanceof NotFoundError) {
							oid = "4b825dc642cb6eb9a060e54bf8d69288fbee4904"
						}
					}
					const tree = await resolveTree({ fs: fs2, cache: this.cache, gitdir, oid })
					tree.type = "tree"
					tree.mode = "40000"
					map.set(".", tree)
					return map
				})()
				const walker = this
				this.ConstructEntry = class TreeEntry {
					constructor(fullpath) {
						this._fullpath = fullpath
						this._type = false
						this._mode = false
						this._stat = false
						this._content = false
						this._oid = false
					}
					async type() {
						return walker.type(this)
					}
					async mode() {
						return walker.mode(this)
					}
					async stat() {
						return walker.stat(this)
					}
					async content() {
						return walker.content(this)
					}
					async oid() {
						return walker.oid(this)
					}
				}
			}
			async readdir(entry) {
				const filepath = entry._fullpath
				const { fs: fs2, cache: cache2, gitdir } = this
				const map = await this.mapPromise
				const obj = map.get(filepath)
				if (!obj) throw new Error(`No obj for ${filepath}`)
				const oid = obj.oid
				if (!oid) throw new Error(`No oid for obj ${JSON.stringify(obj)}`)
				if (obj.type !== "tree") {
					return null
				}
				const { type, object } = await _readObject({ fs: fs2, cache: cache2, gitdir, oid })
				if (type !== obj.type) {
					throw new ObjectTypeError(oid, type, obj.type)
				}
				const tree = GitTree.from(object)
				for (const entry2 of tree) {
					map.set(join(filepath, entry2.path), entry2)
				}
				return tree.entries().map((entry2) => join(filepath, entry2.path))
			}
			async type(entry) {
				if (entry._type === false) {
					const map = await this.mapPromise
					const { type } = map.get(entry._fullpath)
					entry._type = type
				}
				return entry._type
			}
			async mode(entry) {
				if (entry._mode === false) {
					const map = await this.mapPromise
					const { mode } = map.get(entry._fullpath)
					entry._mode = normalizeMode(parseInt(mode, 8))
				}
				return entry._mode
			}
			async stat(_entry) {}
			async content(entry) {
				if (entry._content === false) {
					const map = await this.mapPromise
					const { fs: fs2, cache: cache2, gitdir } = this
					const obj = map.get(entry._fullpath)
					const oid = obj.oid
					const { type, object } = await _readObject({ fs: fs2, cache: cache2, gitdir, oid })
					if (type !== "blob") {
						entry._content = void 0
					} else {
						entry._content = new Uint8Array(object)
					}
				}
				return entry._content
			}
			async oid(entry) {
				if (entry._oid === false) {
					const map = await this.mapPromise
					const obj = map.get(entry._fullpath)
					entry._oid = obj.oid
				}
				return entry._oid
			}
		}
		GitWalkerFs = class {
			constructor({ fs: fs2, dir, gitdir, cache: cache2 }) {
				this.fs = fs2
				this.cache = cache2
				this.dir = dir
				this.gitdir = gitdir
				const walker = this
				this.ConstructEntry = class WorkdirEntry {
					constructor(fullpath) {
						this._fullpath = fullpath
						this._type = false
						this._mode = false
						this._stat = false
						this._content = false
						this._oid = false
					}
					async type() {
						return walker.type(this)
					}
					async mode() {
						return walker.mode(this)
					}
					async stat() {
						return walker.stat(this)
					}
					async content() {
						return walker.content(this)
					}
					async oid() {
						return walker.oid(this)
					}
				}
			}
			async readdir(entry) {
				const filepath = entry._fullpath
				const { fs: fs2, dir } = this
				const names = await fs2.readdir(join(dir, filepath))
				if (names === null) return null
				return names.map((name) => join(filepath, name))
			}
			async type(entry) {
				if (entry._type === false) {
					await entry.stat()
				}
				return entry._type
			}
			async mode(entry) {
				if (entry._mode === false) {
					await entry.stat()
				}
				return entry._mode
			}
			async stat(entry) {
				if (entry._stat === false) {
					const { fs: fs2, dir } = this
					let stat = await fs2.lstat(`${dir}/${entry._fullpath}`)
					if (!stat) {
						throw new Error(`ENOENT: no such file or directory, lstat '${entry._fullpath}'`)
					}
					let type = stat.isDirectory() ? "tree" : "blob"
					if (type === "blob" && !stat.isFile() && !stat.isSymbolicLink()) {
						type = "special"
					}
					entry._type = type
					stat = normalizeStats(stat)
					entry._mode = stat.mode
					if (stat.size === -1 && entry._actualSize) {
						stat.size = entry._actualSize
					}
					entry._stat = stat
				}
				return entry._stat
			}
			async content(entry) {
				if (entry._content === false) {
					const { fs: fs2, dir } = this
					if ((await entry.type()) === "tree") {
						entry._content = void 0
					} else {
						const content = await fs2.read(`${dir}/${entry._fullpath}`)
						entry._actualSize = content.length
						if (entry._stat && entry._stat.size === -1) {
							entry._stat.size = entry._actualSize
						}
						entry._content = new Uint8Array(content)
					}
				}
				return entry._content
			}
			async oid(entry) {
				if (entry._oid === false) {
					const { fs: fs2, gitdir, cache: cache2 } = this
					let oid
					await GitIndexManager.acquire(
						{ fs: fs2, gitdir, cache: cache2 },
						async function (index2) {
							const stage = index2.entriesMap.get(entry._fullpath)
							const stats = await entry.stat()
							if (!stage || compareStats(stats, stage)) {
								const content = await entry.content()
								if (content === void 0) {
									oid = void 0
								} else {
									oid = await shasum(
										GitObject.wrap({ type: "blob", object: await entry.content() })
									)
									if (
										stage &&
										oid === stage.oid &&
										stats.mode === stage.mode &&
										compareStats(stats, stage)
									) {
										index2.insert({
											filepath: entry._fullpath,
											stats,
											oid,
										})
									}
								}
							} else {
								oid = stage.oid
							}
						}
					)
					entry._oid = oid
				}
				return entry._oid
			}
		}
		flat =
			typeof Array.prototype.flat === "undefined"
				? (entries) => entries.reduce((acc, x) => acc.concat(x), [])
				: (entries) => entries.flat()
		RunningMinimum = class {
			constructor() {
				this.value = null
			}
			consider(value) {
				if (value === null || value === void 0) return
				if (this.value === null) {
					this.value = value
				} else if (value < this.value) {
					this.value = value
				}
			}
			reset() {
				this.value = null
			}
		}
		commands = [
			"readFile",
			"writeFile",
			"mkdir",
			"rmdir",
			"unlink",
			"stat",
			"lstat",
			"readdir",
			"readlink",
			"symlink",
		]
		FileSystem = class {
			constructor(fs2) {
				if (typeof fs2._original_unwrapped_fs !== "undefined") return fs2
				const promises = Object.getOwnPropertyDescriptor(fs2, "promises")
				if (promises && promises.enumerable) {
					bindFs(this, fs2.promises)
				} else {
					bindFs(this, fs2)
				}
				this._original_unwrapped_fs = fs2
			}
			/**
			 * Return true if a file exists, false if it doesn't exist.
			 * Rethrows errors that aren't related to file existence.
			 */
			async exists(filepath, options = {}) {
				try {
					await this._stat(filepath)
					return true
				} catch (err) {
					if (err.code === "ENOENT" || err.code === "ENOTDIR") {
						return false
					} else {
						console.log('Unhandled error in "FileSystem.exists()" function', err)
						throw err
					}
				}
			}
			/**
			 * Return the contents of a file if it exists, otherwise returns null.
			 *
			 * @param {string} filepath
			 * @param {object} [options]
			 *
			 * @returns {Promise<Buffer|string|null>}
			 */
			async read(filepath, options = {}) {
				try {
					let buffer = await this._readFile(filepath, options)
					if (typeof buffer !== "string") {
						buffer = Buffer.from(buffer)
					}
					return buffer
				} catch (err) {
					return null
				}
			}
			/**
			 * Write a file (creating missing directories if need be) without throwing errors.
			 *
			 * @param {string} filepath
			 * @param {Buffer|Uint8Array|string} contents
			 * @param {object|string} [options]
			 */
			async write(filepath, contents, options = {}) {
				try {
					await this._writeFile(filepath, contents, options)
					return
				} catch (err) {
					await this.mkdir(dirname(filepath))
					await this._writeFile(filepath, contents, options)
				}
			}
			/**
			 * Make a directory (or series of nested directories) without throwing an error if it already exists.
			 */
			async mkdir(filepath, _selfCall = false) {
				try {
					await this._mkdir(filepath)
					return
				} catch (err) {
					if (err === null) return
					if (err.code === "EEXIST") return
					if (_selfCall) throw err
					if (err.code === "ENOENT") {
						const parent = dirname(filepath)
						if (parent === "." || parent === "/" || parent === filepath) throw err
						await this.mkdir(parent)
						await this.mkdir(filepath, true)
					}
				}
			}
			/**
			 * Delete a file without throwing an error if it is already deleted.
			 */
			async rm(filepath) {
				try {
					await this._unlink(filepath)
				} catch (err) {
					if (err.code !== "ENOENT") throw err
				}
			}
			/**
			 * Delete a directory without throwing an error if it is already deleted.
			 */
			async rmdir(filepath, opts) {
				try {
					if (opts && opts.recursive) {
						await this._rm(filepath, opts)
					} else {
						await this._rmdir(filepath)
					}
				} catch (err) {
					if (err.code !== "ENOENT") throw err
				}
			}
			/**
			 * Read a directory without throwing an error is the directory doesn't exist
			 */
			async readdir(filepath) {
				try {
					const names = await this._readdir(filepath)
					names.sort(compareStrings)
					return names
				} catch (err) {
					if (err.code === "ENOTDIR") return null
					return []
				}
			}
			/**
			 * Return a flast list of all the files nested inside a directory
			 *
			 * Based on an elegant concurrent recursive solution from SO
			 * https://stackoverflow.com/a/45130990/2168416
			 */
			async readdirDeep(dir) {
				const subdirs = await this._readdir(dir)
				const files = await Promise.all(
					subdirs.map(async (subdir) => {
						const res = dir + "/" + subdir
						return (await this._stat(res)).isDirectory() ? this.readdirDeep(res) : res
					})
				)
				return files.reduce((a, f) => a.concat(f), [])
			}
			/**
			 * Return the Stats of a file/symlink if it exists, otherwise returns null.
			 * Rethrows errors that aren't related to file existence.
			 */
			async lstat(filename) {
				try {
					const stats = await this._lstat(filename)
					return stats
				} catch (err) {
					if (err.code === "ENOENT") {
						return null
					}
					throw err
				}
			}
			/**
			 * Reads the contents of a symlink if it exists, otherwise returns null.
			 * Rethrows errors that aren't related to file existence.
			 */
			async readlink(filename, opts = { encoding: "buffer" }) {
				try {
					const link = await this._readlink(filename, opts)
					return Buffer.isBuffer(link) ? link : Buffer.from(link)
				} catch (err) {
					if (err.code === "ENOENT") {
						return null
					}
					throw err
				}
			}
			/**
			 * Write the contents of buffer to a symlink.
			 */
			async writelink(filename, buffer) {
				return this._symlink(buffer.toString("utf8"), filename)
			}
		}
		GitIgnoreManager = class {
			static async isIgnored({ fs: fs2, dir, gitdir = join(dir, ".git"), filepath }) {
				if (basename(filepath) === ".git") return true
				if (filepath === ".") return false
				let excludes = ""
				const excludesFile = join(gitdir, "info", "exclude")
				if (await fs2.exists(excludesFile)) {
					excludes = await fs2.read(excludesFile, "utf8")
				}
				const pairs = [
					{
						gitignore: join(dir, ".gitignore"),
						filepath,
					},
				]
				const pieces = filepath.split("/").filter(Boolean)
				for (let i = 1; i < pieces.length; i++) {
					const folder = pieces.slice(0, i).join("/")
					const file = pieces.slice(i).join("/")
					pairs.push({
						gitignore: join(dir, folder, ".gitignore"),
						filepath: file,
					})
				}
				let ignoredStatus = false
				for (const p of pairs) {
					let file
					try {
						file = await fs2.read(p.gitignore, "utf8")
					} catch (err) {
						if (err.code === "NOENT") continue
					}
					const ign = (0, import_ignore.default)().add(excludes)
					ign.add(file)
					const parentdir = dirname(p.filepath)
					if (parentdir !== "." && ign.ignores(parentdir)) return true
					if (ignoredStatus) {
						ignoredStatus = !ign.test(p.filepath).unignored
					} else {
						ignoredStatus = ign.test(p.filepath).ignored
					}
				}
				return ignoredStatus
			}
		}
		supportsCompressionStream = null
		worthWalking = (filepath, root) => {
			if (
				filepath === "." ||
				root == undefined ||
				root.length === 0 ||
				root === "." ||
				root === filepath
			) {
				return true
			}
			if (root.length > filepath.length) {
				return root.startsWith(filepath + "/")
			} else {
				return filepath.startsWith(root + "/")
			}
		}
		abbreviateRx = new RegExp("^refs/(heads/|tags/|remotes/)?(.*)")
		GitPktLine = class {
			static flush() {
				return Buffer.from("0000", "utf8")
			}
			static delim() {
				return Buffer.from("0001", "utf8")
			}
			static encode(line) {
				if (typeof line === "string") {
					line = Buffer.from(line)
				}
				const length = line.length + 4
				const hexlength = padHex(4, length)
				return Buffer.concat([Buffer.from(hexlength, "utf8"), line])
			}
			static streamReader(stream) {
				const reader = new StreamReader(stream)
				return async function read() {
					try {
						let length = await reader.read(4)
						if (length == undefined) return true
						length = parseInt(length.toString("utf8"), 16)
						if (length === 0) return null
						if (length === 1) return null
						const buffer = await reader.read(length - 4)
						if (buffer == undefined) return true
						return buffer
					} catch (err) {
						stream.error = err
						return true
					}
				}
			}
		}
		corsProxify = (corsProxy, url) =>
			corsProxy.endsWith("?")
				? `${corsProxy}${url}`
				: `${corsProxy}/${url.replace(/^https?:\/\//, "")}`
		updateHeaders = (headers, auth) => {
			if (auth.username || auth.password) {
				headers.Authorization = calculateBasicAuthHeader(auth)
			}
			if (auth.headers) {
				Object.assign(headers, auth.headers)
			}
		}
		stringifyBody = async (res) => {
			try {
				const data = Buffer.from(await collect(res.body))
				const response = data.toString("utf8")
				const preview = response.length < 256 ? response : response.slice(0, 256) + "..."
				return { preview, response, data }
			} catch (e) {
				return {}
			}
		}
		GitRemoteHTTP = class {
			static async capabilities() {
				return ["discover", "connect"]
			}
			/**
			 * @param {Object} args
			 * @param {HttpClient} args.http
			 * @param {ProgressCallback} [args.onProgress]
			 * @param {AuthCallback} [args.onAuth]
			 * @param {AuthFailureCallback} [args.onAuthFailure]
			 * @param {AuthSuccessCallback} [args.onAuthSuccess]
			 * @param {string} [args.corsProxy]
			 * @param {string} args.service
			 * @param {string} args.url
			 * @param {Object<string, string>} args.headers
			 * @param {1 | 2} args.protocolVersion - Git Protocol Version
			 */
			static async discover({
				http,
				onProgress,
				onAuth,
				onAuthSuccess,
				onAuthFailure,
				corsProxy,
				service,
				url: _origUrl,
				headers,
				protocolVersion,
			}) {
				let { url, auth } = extractAuthFromUrl(_origUrl)
				const proxifiedURL = corsProxy ? corsProxify(corsProxy, url) : url
				if (auth.username || auth.password) {
					headers.Authorization = calculateBasicAuthHeader(auth)
				}
				if (protocolVersion === 2) {
					headers["Git-Protocol"] = "version=2"
				}
				let res
				let tryAgain
				let providedAuthBefore = false
				do {
					res = await http.request({
						onProgress,
						method: "GET",
						url: `${proxifiedURL}/info/refs?service=${service}`,
						headers,
					})
					tryAgain = false
					if (res.statusCode === 401 || res.statusCode === 203) {
						const getAuth = providedAuthBefore ? onAuthFailure : onAuth
						if (getAuth) {
							auth = await getAuth(url, {
								...auth,
								headers: { ...headers },
							})
							if (auth && auth.cancel) {
								throw new UserCanceledError()
							} else if (auth) {
								updateHeaders(headers, auth)
								providedAuthBefore = true
								tryAgain = true
							}
						}
					} else if (res.statusCode === 200 && providedAuthBefore && onAuthSuccess) {
						await onAuthSuccess(url, auth)
					}
				} while (tryAgain)
				if (res.statusCode !== 200) {
					const { response } = await stringifyBody(res)
					throw new HttpError(res.statusCode, res.statusMessage, response)
				}
				if (res.headers["content-type"] === `application/x-${service}-advertisement`) {
					const remoteHTTP = await parseRefsAdResponse(res.body, { service })
					remoteHTTP.auth = auth
					return remoteHTTP
				} else {
					const { preview, response, data } = await stringifyBody(res)
					try {
						const remoteHTTP = await parseRefsAdResponse([data], { service })
						remoteHTTP.auth = auth
						return remoteHTTP
					} catch (e) {
						throw new SmartHttpError(preview, response)
					}
				}
			}
			/**
			 * @param {Object} args
			 * @param {HttpClient} args.http
			 * @param {ProgressCallback} [args.onProgress]
			 * @param {string} [args.corsProxy]
			 * @param {string} args.service
			 * @param {string} args.url
			 * @param {Object<string, string>} [args.headers]
			 * @param {any} args.body
			 * @param {any} args.auth
			 */
			static async connect({ http, onProgress, corsProxy, service, url, auth, body, headers }) {
				const urlAuth = extractAuthFromUrl(url)
				if (urlAuth) url = urlAuth.url
				if (corsProxy) url = corsProxify(corsProxy, url)
				headers["content-type"] = `application/x-${service}-request`
				headers.accept = `application/x-${service}-result`
				updateHeaders(headers, auth)
				const res = await http.request({
					onProgress,
					method: "POST",
					url: `${url}/${service}`,
					body,
					headers,
				})
				if (res.statusCode !== 200) {
					const { response } = stringifyBody(res)
					throw new HttpError(res.statusCode, res.statusMessage, response)
				}
				return res
			}
		}
		GitRemoteManager = class {
			static getRemoteHelperFor({ url }) {
				const remoteHelpers = /* @__PURE__ */ new Map()
				remoteHelpers.set("http", GitRemoteHTTP)
				remoteHelpers.set("https", GitRemoteHTTP)
				const parts = parseRemoteUrl({ url })
				if (!parts) {
					throw new UrlParseError(url)
				}
				if (remoteHelpers.has(parts.transport)) {
					return remoteHelpers.get(parts.transport)
				}
				throw new UnknownTransportError(
					url,
					parts.transport,
					parts.transport === "ssh" ? translateSSHtoHTTP(url) : void 0
				)
			}
		}
		lock$2 = null
		GitShallowManager = class {
			static async read({ fs: fs2, gitdir }) {
				if (lock$2 === null) lock$2 = new import_async_lock.default()
				const filepath = join(gitdir, "shallow")
				const oids = /* @__PURE__ */ new Set()
				await lock$2.acquire(filepath, async function () {
					const text = await fs2.read(filepath, { encoding: "utf8" })
					if (text === null) return oids
					if (text.trim() === "") return oids
					text
						.trim()
						.split("\n")
						.map((oid) => oids.add(oid))
				})
				return oids
			}
			static async write({ fs: fs2, gitdir, oids }) {
				if (lock$2 === null) lock$2 = new import_async_lock.default()
				const filepath = join(gitdir, "shallow")
				if (oids.size > 0) {
					const text = [...oids].join("\n") + "\n"
					await lock$2.acquire(filepath, async function () {
						await fs2.write(filepath, text, {
							encoding: "utf8",
						})
					})
				} else {
					await lock$2.acquire(filepath, async function () {
						await fs2.rm(filepath)
					})
				}
			}
		}
		pkg = {
			name: "isomorphic-git",
			version: "0.0.0-development",
			agent: "git/isomorphic-git@0.0.0-development",
		}
		FIFO = class {
			constructor() {
				this._queue = []
			}
			write(chunk) {
				if (this._ended) {
					throw Error("You cannot write to a FIFO that has already been ended!")
				}
				if (this._waiting) {
					const resolve = this._waiting
					this._waiting = null
					resolve({ value: chunk })
				} else {
					this._queue.push(chunk)
				}
			}
			end() {
				this._ended = true
				if (this._waiting) {
					const resolve = this._waiting
					this._waiting = null
					resolve({ done: true })
				}
			}
			destroy(err) {
				this.error = err
				this.end()
			}
			async next() {
				if (this._queue.length > 0) {
					return { value: this._queue.shift() }
				}
				if (this._ended) {
					return { done: true }
				}
				if (this._waiting) {
					throw Error("You cannot call read until the previous call to read has returned!")
				}
				return new Promise((resolve) => {
					this._waiting = resolve
				})
			}
		}
		GitSideBand = class {
			static demux(input) {
				const read = GitPktLine.streamReader(input)
				const packetlines = new FIFO()
				const packfile = new FIFO()
				const progress = new FIFO()
				const nextBit = async function () {
					const line = await read()
					if (line === null) return nextBit()
					if (line === true) {
						packetlines.end()
						progress.end()
						input.error ? packfile.destroy(input.error) : packfile.end()
						return
					}
					switch (line[0]) {
						case 1: {
							packfile.write(line.slice(1))
							break
						}
						case 2: {
							progress.write(line.slice(1))
							break
						}
						case 3: {
							const error = line.slice(1)
							progress.write(error)
							packetlines.end()
							progress.end()
							packfile.destroy(new Error(error.toString("utf8")))
							return
						}
						default: {
							packetlines.write(line)
						}
					}
					nextBit()
				}
				nextBit()
				return {
					packetlines,
					packfile,
					progress,
				}
			}
			// static mux ({
			//   protocol, // 'side-band' or 'side-band-64k'
			//   packetlines,
			//   packfile,
			//   progress,
			//   error
			// }) {
			//   const MAX_PACKET_LENGTH = protocol === 'side-band-64k' ? 999 : 65519
			//   let output = new PassThrough()
			//   packetlines.on('data', data => {
			//     if (data === null) {
			//       output.write(GitPktLine.flush())
			//     } else {
			//       output.write(GitPktLine.encode(data))
			//     }
			//   })
			//   let packfileWasEmpty = true
			//   let packfileEnded = false
			//   let progressEnded = false
			//   let errorEnded = false
			//   let goodbye = Buffer.concat([
			//     GitPktLine.encode(Buffer.from('010A', 'hex')),
			//     GitPktLine.flush()
			//   ])
			//   packfile
			//     .on('data', data => {
			//       packfileWasEmpty = false
			//       const buffers = splitBuffer(data, MAX_PACKET_LENGTH)
			//       for (const buffer of buffers) {
			//         output.write(
			//           GitPktLine.encode(Buffer.concat([Buffer.from('01', 'hex'), buffer]))
			//         )
			//       }
			//     })
			//     .on('end', () => {
			//       packfileEnded = true
			//       if (!packfileWasEmpty) output.write(goodbye)
			//       if (progressEnded && errorEnded) output.end()
			//     })
			//   progress
			//     .on('data', data => {
			//       const buffers = splitBuffer(data, MAX_PACKET_LENGTH)
			//       for (const buffer of buffers) {
			//         output.write(
			//           GitPktLine.encode(Buffer.concat([Buffer.from('02', 'hex'), buffer]))
			//         )
			//       }
			//     })
			//     .on('end', () => {
			//       progressEnded = true
			//       if (packfileEnded && errorEnded) output.end()
			//     })
			//   error
			//     .on('data', data => {
			//       const buffers = splitBuffer(data, MAX_PACKET_LENGTH)
			//       for (const buffer of buffers) {
			//         output.write(
			//           GitPktLine.encode(Buffer.concat([Buffer.from('03', 'hex'), buffer]))
			//         )
			//       }
			//     })
			//     .on('end', () => {
			//       errorEnded = true
			//       if (progressEnded && packfileEnded) output.end()
			//     })
			//   return output
			// }
		}
		LINEBREAKS = /^.*(\r?\n|$)/gm
		EMPTY_OID = "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391"
		types = {
			commit: 16,
			tree: 32,
			blob: 48,
			tag: 64,
			ofs_delta: 96,
			ref_delta: 112,
		}
		deepget = (keys, map) => {
			for (const key of keys) {
				if (!map.has(key)) map.set(key, /* @__PURE__ */ new Map())
				map = map.get(key)
			}
			return map
		}
		DeepMap = class {
			constructor() {
				this._root = /* @__PURE__ */ new Map()
			}
			set(keys, value) {
				const lastKey = keys.pop()
				const lastMap = deepget(keys, this._root)
				lastMap.set(lastKey, value)
			}
			get(keys) {
				const lastKey = keys.pop()
				const lastMap = deepget(keys, this._root)
				return lastMap.get(lastKey)
			}
			has(keys) {
				const lastKey = keys.pop()
				const lastMap = deepget(keys, this._root)
				return lastMap.has(lastKey)
			}
		}
		index = {
			Errors,
			STAGE,
			TREE,
			WORKDIR,
			add,
			abortMerge,
			addNote,
			addRemote,
			annotatedTag,
			branch,
			checkout,
			clone,
			commit,
			getConfig,
			getConfigAll,
			setConfig,
			currentBranch,
			deleteBranch,
			deleteRef,
			deleteRemote,
			deleteTag,
			expandOid,
			expandRef,
			fastForward,
			fetch: fetch2,
			findMergeBase,
			findRoot,
			getRemoteInfo,
			getRemoteInfo2,
			hashBlob,
			indexPack,
			init,
			isDescendent,
			isIgnored,
			listBranches,
			listFiles,
			listNotes,
			listRemotes,
			listServerRefs,
			listTags,
			log,
			merge,
			packObjects,
			pull,
			push,
			readBlob,
			readCommit,
			readNote,
			readObject,
			readTag,
			readTree,
			remove,
			removeNote,
			renameBranch,
			resetIndex,
			updateIndex,
			resolveRef,
			status,
			statusMatrix,
			tag,
			version: version2,
			walk,
			_walk,
			writeBlob,
			writeCommit,
			writeObject,
			writeRef,
			writeTag,
			writeTree,
			_listObjects: listObjects,
			_pack,
			_uploadPack: uploadPack,
			_GitConfigManager: GitConfigManager,
			_GitIgnoreManager: GitIgnoreManager,
			_GitIndexManager: GitIndexManager,
			_GitRefManager: GitRefManager,
			_GitRemoteHTTP: GitRemoteHTTP,
			_GitRemoteManager: GitRemoteManager,
			_GitShallowManager: GitShallowManager,
			_FileSystem: FileSystem,
			_GitAnnotatedTag: GitAnnotatedTag,
			_GitCommit: GitCommit,
			_GitConfig: GitConfig,
			_GitIndex: GitIndex,
			_GitObject: GitObject,
			_GitPackIndex: GitPackIndex,
			_GitPktLine: GitPktLine,
			_GitRefSpec: GitRefSpec,
			_GitRefSpecSet: GitRefSpecSet,
			_GitSideBand: GitSideBand,
			_GitTree: GitTree,
			_GitWalkerFs: GitWalkerFs,
			_GitWalkerIndex: GitWalkerIndex,
			_GitWalkerRepo: GitWalkerRepo,
			_RunningMinimum: RunningMinimum,
			_expandOid,
			_expandOidLoose: expandOidLoose,
			_expandOidPacked: expandOidPacked,
			_hasObject: hasObject,
			_hasObjectLoose: hasObjectLoose,
			_hasObjectPacked: hasObjectPacked,
			_hashObject: hashObject,
			_readObject,
			_readObjectLoose: readObjectLoose,
			_readObjectPacked: readObjectPacked,
			_readPackIndex: readPackIndex,
			_writeObject,
			_writeObjectLoose: writeObjectLoose,
			_BufferCursor: BufferCursor,
			_DeepMap: DeepMap,
			_FIFO: FIFO,
			_StreamReader: StreamReader,
			_abbreviateRef: abbreviateRef,
			_applyDelta: applyDelta,
			_arrayRange: arrayRange,
			_assertParameter: assertParameter,
			// _asyncIteratorToStream,
			_basename: basename,
			_calculateBasicAuthHeader: calculateBasicAuthHeader,
			_collect: collect,
			_compareAge: compareAge,
			_comparePath: comparePath,
			_compareRefNames: compareRefNames,
			_compareStats: compareStats,
			_compareStrings: compareStrings,
			_compareTreeEntryPath: compareTreeEntryPath,
			_deflate: deflate,
			_dirname: dirname,
			_emptyPackfile: emptyPackfile,
			_extractAuthFromUrl: extractAuthFromUrl,
			_filterCapabilities: filterCapabilities,
			_flat: flat,
			_flatFileListToDirectoryStructure: flatFileListToDirectoryStructure,
			_forAwait: forAwait,
			_formatAuthor: formatAuthor,
			_formatInfoRefs: formatInfoRefs,
			_fromEntries: fromEntries,
			_fromNodeStream: fromNodeStream,
			_fromStream: fromStream,
			_fromValue: fromValue,
			_getIterator: getIterator,
			_listpack: listpack,
			_utils_hashObject: hashObject$1,
			_indent: indent,
			_inflate: inflate,
			_isBinary: isBinary,
			_join: join,
			_mergeFile: mergeFile,
			_mergeTree: mergeTree,
			_mode2type: mode2type,
			_modified: modified,
			_normalizeAuthorObject: normalizeAuthorObject,
			_normalizeCommitterObject: normalizeCommitterObject,
			_normalizeMode: normalizeMode,
			_normalizeNewlines: normalizeNewlines,
			_normalizePath: normalizePath,
			_normalizeStats: normalizeStats,
			_outdent: outdent,
			_padHex: padHex,
			_parseAuthor: parseAuthor,
			_pkg: pkg,
			_posixifyPathBuffer: posixifyPathBuffer,
			_resolveBlob: resolveBlob,
			_resolveCommit: resolveCommit,
			_resolveFileIdInTree: resolveFileIdInTree,
			_resolveFilepath: resolveFilepath,
			_resolveTree: resolveTree,
			_rmRecursive: rmRecursive,
			_shasum: shasum,
			_sleep: sleep,
			_splitLines: splitLines,
			// _symbols,
			_toHex: toHex,
			_translateSSHtoHTTP: translateSSHtoHTTP,
			_unionOfIterators: unionOfIterators,
			_worthWalking: worthWalking,
			_parseCapabilitiesV2: parseCapabilitiesV2,
			_parseListRefsResponse: parseListRefsResponse,
			_parseReceivePackResponse: parseReceivePackResponse,
			_parseRefsAdResponse: parseRefsAdResponse,
			_parseUploadPackRequest: parseUploadPackRequest,
			_parseUploadPackResponse: parseUploadPackResponse,
			_writeListRefsRequest: writeListRefsRequest,
			_writeReceivePackRequest: writeReceivePackRequest,
			_writeRefsAdResponse: writeRefsAdResponse,
			_writeUploadPackRequest: writeUploadPackRequest,
		}
		isomorphic_git_default = index
	},
})

// ../../../lix/packages/fs/dist/errors/FilesystemError.js
var FilesystemError
var init_FilesystemError = __esm({
	"../../../lix/packages/fs/dist/errors/FilesystemError.js"() {
		"use strict"
		FilesystemError = class extends Error {
			code
			path
			syscall
			target
			constructor(code, path, syscall, target) {
				let message
				switch (code) {
					case "ENOENT":
						message = `${code}: No such file or directory, ${syscall} '${path}'`
						break
					case "ENOTDIR":
						message = `${code}: Not a directory, ${syscall} '${path}'`
						break
					case "EISDIR":
						message = `${code}: Illegal operation on a directory, ${syscall} '${path}'`
						break
					case "ENOTEMPTY":
						message = `${code}: Directory not empty, ${syscall} '${path}'`
						break
					case "EEXIST":
						message = `${code}: File exists, ${syscall} '${path}' -> '${target}'`
						break
					case "EINVAL":
						message = `${code}: Invaid argument, ${syscall} '${path}'`
						break
					default:
						message = `Unknown error with code "${code}", '${syscall}' on '${path}'`
				}
				super(message)
				this.name = "FilesystemError"
				this.code = code
				this.path = path
				this.syscall = syscall
				this.target = target
			}
		}
	},
})

// ../../../lix/packages/fs/dist/utilities/helpers.js
function assertIsAbsolutePath(path) {
	if ((path.startsWith("/") || /^[A-Za-z]:[\\/]/.test(path)) === false) {
		{
			throw new Error(`Path is not absolute: ${path}. All paths should be absolute to avoid bugs.`)
		}
	}
}
function normalizePath2(path, { trailingSlash, leadingSlash } = {}) {
	path = path.replace(/^\.\//, "/")
	if (path === "\\" || path === "" || path === "/" || path === "." || path === "//.") {
		return "/"
	}
	if (path.length <= 1) {
		return path
	}
	const hadTrailingSlash = path.at(-1) === "/" || path.at(-1) === "\\"
	const addleadingSlash = leadingSlash === "always" || path[0] === "/" || path[0] === "\\"
	const segs = path.split(/[/\\]+/)
	const stack = []
	for (const seg of segs) {
		if (seg === "..") {
			stack.pop()
		} else if (seg && seg !== ".") {
			stack.push(seg)
		}
	}
	if (trailingSlash !== "strip" && (hadTrailingSlash || trailingSlash === "always")) {
		stack.push("")
	}
	return addleadingSlash ? "/" + stack.join("/") : stack.join("/")
}
function getDirname(path) {
	const dirname2 = path
		.split("/")
		.filter((x) => x)
		.slice(0, -1)
		.join("/")
	return normalizePath2(dirname2, { leadingSlash: "always", trailingSlash: "always" }) ?? path
}
function getBasename(path) {
	return (
		path
			.split("/")
			.filter((x) => x)
			.at(-1) ?? ""
	)
}
var init_helpers = __esm({
	"../../../lix/packages/fs/dist/utilities/helpers.js"() {
		"use strict"
	},
})

// ../../../lix/packages/fs/dist/memoryFs.js
function toSnapshot(fs2) {
	return {
		fsMap: Object.fromEntries(
			[...fs2._state.fsMap].map(([path, content]) => {
				let serializedContent
				if (content instanceof Set) {
					serializedContent = [...content].sort()
				} else if (content.placeholder) {
					serializedContent = content
				} else {
					serializedContent = content.toString("base64")
				}
				return [
					path,
					// requires node buffers, but no web standard method exists
					serializedContent,
					// Alternative to try:
					// onst binaryData = new Uint8Array([255, 116, 79, 99 /*...*/]);
					// const base64Encoded = btoa(String.fromCharCode.apply(null, binaryData));
					// // Decode Base64 back to binary
					// const decodedBinaryString = atob(base64Encoded);
					// const decodedBinaryData = new Uint8Array([...decodedBinaryString].map(char => char.charCodeAt(0)));
					// this breaks packfile binary data but could be fixed in future btoa(unescape(encodeURIComponent(new TextDecoder().decode(content)))),
				]
			})
		),
		fsStats: Object.fromEntries(
			[...fs2._state.fsStats].map(([path, fsStat]) => {
				return [
					path,
					{
						...fsStat,
						ino: void 0,
						isFile: void 0,
						isDirectory: void 0,
						isSymbolicLink: void 0,
					},
				]
			})
		),
	}
}
function fromSnapshot(fs2, snapshot, { pathPrefix = "" } = {}) {
	fs2._state.lastIno = 1
	fs2._state.fsMap = new Map(
		// @ts-ignore FIXME: no idea what ts wants me to do here the error message is ridiculous
		Object.entries(snapshot.fsMap).map(([path, content]) => {
			if (typeof content === "string") {
				const data = Buffer.from(content, "base64")
				return [pathPrefix + path, data]
			} else if (content?.placeholder) {
				return [pathPrefix + path, content]
			}
			return [pathPrefix + path, new Set(content)]
		})
	)
	fs2._state.fsStats = new Map(
		Object.entries(snapshot.fsStats).map(([path, rawStat]) => {
			const serializedStat = rawStat
			const statsObj = {
				...serializedStat,
				ino: fs2._state.lastIno++,
				isFile: () => serializedStat._kind === 0,
				isDirectory: () => serializedStat._kind === 1,
				isSymbolicLink: () => serializedStat._kind === 2,
			}
			return [pathPrefix + path, statsObj]
		})
	)
	if (pathPrefix) {
		const prefixParts = pathPrefix.split("/")
		const rootStat = fs2._state.fsStats.get(pathPrefix + "/")
		for (let i = 1; i < prefixParts.length; i++) {
			const path = prefixParts.slice(0, i).join("/") + "/"
			fs2._state.fsMap.set(path, /* @__PURE__ */ new Set([prefixParts[i]]))
			fs2._state.fsStats.set(path, rootStat)
		}
	}
}
function createNodeishMemoryFs() {
	const state = {
		lastIno: 1,
		fsMap: /* @__PURE__ */ new Map(),
		fsStats: /* @__PURE__ */ new Map(),
	}
	state.fsMap.set("/", /* @__PURE__ */ new Set())
	newStatEntry({
		path: "/",
		stats: state.fsStats,
		kind: 1,
		modeBits: 493,
	})
	const listeners = /* @__PURE__ */ new Set()
	async function stat(path) {
		path = normalizePath2(path, { trailingSlash: "always", leadingSlash: "always" })
		const stats = state.fsStats.get(path)
		if (stats === void 0) {
			throw new FilesystemError("ENOENT", path, "stat")
		}
		if (stats.symlinkTarget) {
			return stat(stats.symlinkTarget)
		}
		return Object.assign({}, stats)
	}
	async function lstat(path) {
		path = normalizePath2(path, { trailingSlash: "always", leadingSlash: "always" })
		const stats = state.fsStats.get(path)
		if (stats === void 0) {
			throw new FilesystemError("ENOENT", path, "lstat")
		}
		if (!stats.symlinkTarget) {
			return stat(path)
		}
		return Object.assign({}, stats)
	}
	return {
		_state: state,
		_createPlaceholder: async function (path, options) {
			path = normalizePath2(path, { trailingSlash: "always", leadingSlash: "always" })
			const dirName = getDirname(path)
			const baseName = getBasename(path)
			let parentDir = state.fsMap.get(dirName)
			if (!(parentDir instanceof Set)) {
				await this.mkdir(dirName, { recursive: true })
				parentDir = state.fsMap.get(dirName)
				if (!(parentDir instanceof Set)) {
					throw new FilesystemError("ENOENT", path, "writeFile")
				}
			}
			parentDir.add(baseName)
			const isSymbolicLink = options.mode === 12e4
			newStatEntry({
				path,
				stats: state.fsStats,
				kind: isSymbolicLink ? 2 : 0,
				modeBits: options.mode,
				oid: options.oid,
				rootHash: options.rootHash,
			})
			state.fsMap.set(path, { placeholder: true })
		},
		_isPlaceholder: function (path) {
			path = normalizePath2(path, { trailingSlash: "always", leadingSlash: "always" })
			const entry = state.fsMap.get(path)
			if (entry && "placeholder" in entry) {
				return true
			}
			return false
		},
		writeFile: async function (path, data, options) {
			path = normalizePath2(path, { trailingSlash: "always", leadingSlash: "always" })
			const dirName = getDirname(path)
			const baseName = getBasename(path)
			const parentDir = state.fsMap.get(dirName)
			if (!(parentDir instanceof Set)) throw new FilesystemError("ENOENT", path, "writeFile")
			let inodeData
			if (typeof data === "string") {
				inodeData = Buffer.from(new TextEncoder().encode(data))
			} else if (!(data instanceof Uint8Array)) {
				throw new FilesystemError(
					'The "data" argument must be of type string/Uint8Array',
					data,
					"readFile"
				)
			} else if (!Buffer.isBuffer(data)) {
				inodeData = Buffer.from(data)
			} else {
				inodeData = data
			}
			parentDir.add(baseName)
			newStatEntry({
				path,
				stats: state.fsStats,
				kind: 0,
				modeBits: options?.mode ?? 420,
			})
			state.fsMap.set(path, inodeData)
			for (const listener of listeners) {
				listener({ eventType: "rename", filename: dirName + baseName })
			}
		},
		// @ts-expect-error
		//   Typescript can't derive that the return type is either
		//   a string or a Uint8Array based on the options.
		readFile: async function (path, options) {
			const decoder = new TextDecoder()
			path = normalizePath2(path, { trailingSlash: "always", leadingSlash: "always" })
			const file = state.fsMap.get(path)
			if (file instanceof Set) throw new FilesystemError("EISDIR", path, "readFile")
			if (file === void 0) throw new FilesystemError("ENOENT", path, "readFile")
			if ("placeholder" in file) throw new FilesystemError("EPLACEHOLDER", path, "readFile")
			if (!(options?.encoding || typeof options === "string")) return file
			return decoder.decode(file)
		},
		readdir: async function (path) {
			path = normalizePath2(path, { trailingSlash: "always", leadingSlash: "always" })
			const dir = state.fsMap.get(path)
			if (dir instanceof Set) return [...dir.keys()]
			if (dir === void 0) throw new FilesystemError("ENOENT", path, "readdir")
			throw new FilesystemError("ENOTDIR", path, "readdir")
		},
		mkdir: async function mkdir(path, options) {
			path = normalizePath2(path, { trailingSlash: "always", leadingSlash: "always" })
			const dirName = getDirname(path)
			const baseName = getBasename(path)
			const parentDir = state.fsMap.get(dirName)
			if (typeof parentDir === "string" || (parentDir && "palceholder" in parentDir)) {
				throw new FilesystemError("ENOTDIR", path, "mkdir")
			}
			if (parentDir && parentDir instanceof Set) {
				if (state.fsMap.has(path)) {
					if (!options?.recursive) {
						throw new FilesystemError("EEXIST", path, "mkdir")
					} else {
						return void 0
					}
				}
				parentDir.add(baseName)
				newStatEntry({
					path,
					stats: state.fsStats,
					kind: 1,
					modeBits: 493,
				})
				state.fsMap.set(path, /* @__PURE__ */ new Set())
				for (const listener of listeners) {
					listener({ eventType: "rename", filename: dirName + baseName })
				}
				return path
			} else if (options?.recursive) {
				const parent = getDirname(path)
				const parentRes = await mkdir(parent, options)
				await mkdir(path, { recursive: false }).catch(() => {})
				return parentRes
			}
			throw new FilesystemError("ENOENT", path, "mkdir")
		},
		rm: async function rm(path, options) {
			path = normalizePath2(path, { trailingSlash: "always", leadingSlash: "always" })
			const dirName = getDirname(path)
			const baseName = getBasename(path)
			const target = state.fsMap.get(path)
			const targetStats = state.fsStats.get(path)
			const parentDir = state.fsMap.get(dirName)
			if (parentDir === void 0 || targetStats === void 0)
				throw new FilesystemError("ENOENT", path, "rm")
			if (parentDir instanceof Uint8Array || "placeholder" in parentDir) {
				throw new FilesystemError("ENOTDIR", path, "rm")
			}
			if (
				target instanceof Uint8Array ||
				(target && "placeholder" in target) ||
				targetStats.isSymbolicLink()
			) {
				parentDir.delete(baseName)
				state.fsStats.delete(path)
				state.fsMap.delete(path)
				for (const listener of listeners) {
					listener({ eventType: "rename", filename: dirName + baseName })
				}
				return
			}
			if (target instanceof Set && options?.recursive) {
				await Promise.all(
					[...target.keys()].map(async (child) => {
						await rm(`${path}/${child}`, { recursive: true })
					})
				)
				parentDir.delete(baseName)
				state.fsStats.delete(path)
				state.fsMap.delete(path)
				for (const listener of listeners) {
					listener({ eventType: "rename", filename: dirName + baseName })
				}
				return
			}
			throw new FilesystemError("EISDIR", path, "rm")
		},
		/**
		 *
		 * @throws {"ENOENT" | WatchAbortedError} // TODO: move to lix error classes FileDoesNotExistError
		 */
		watch: function (path, options) {
			path = normalizePath2(path, { trailingSlash: "always", leadingSlash: "always" })
			const watchName = getBasename(path)
			const watchDir = getDirname(path)
			const watchPath = watchName === "/" ? watchDir : watchDir + watchName
			if (options?.persistent || options?.encoding) {
				throw new Error("Some watch opptions not implemented, only 'recursive' allowed")
			}
			const queue = []
			let handleNext
			let rejecteNext
			let changeEvent = new Promise((resolve, reject) => {
				handleNext = resolve
				rejecteNext = reject
			})
			const listener = ({ eventType, filename }) => {
				const event = {
					eventType,
					filename,
				}
				if (event.filename === null) {
					throw new Error("Internal watcher error: missing filename")
				}
				const changeName = getBasename(event.filename)
				const changeDir = getDirname(event.filename)
				if (event.filename === watchPath) {
					event.filename = changeName
					queue.push(event)
					setTimeout(() => handleNext(void 0), 0)
				} else if (changeDir === `${watchPath}/`) {
					event.filename = event.filename.replace(`${watchPath}/`, "") || changeName
					queue.push(event)
					setTimeout(() => handleNext(void 0), 0)
				} else if (options?.recursive && event.filename.startsWith(watchPath)) {
					event.filename = event.filename.replace(`${watchPath}/`, "") || changeName
					queue.push(event)
					setTimeout(() => handleNext(void 0), 0)
				}
			}
			listeners.add(listener)
			if (options?.signal) {
				options.signal.addEventListener(
					"abort",
					() => {
						listeners.delete(listener)
						try {
							options.signal?.throwIfAborted()
						} catch (err) {
							rejecteNext(err)
						}
					},
					{ once: true }
				)
			}
			const asyncIterator = async function* () {
				while (!options?.signal?.aborted) {
					if (queue.length > 0) {
						yield queue.shift()
					} else {
						await changeEvent
						changeEvent = new Promise((resolve, reject) => {
							handleNext = resolve
							rejecteNext = reject
						})
					}
				}
			}
			return asyncIterator()
		},
		rmdir: async function (path) {
			path = normalizePath2(path, { trailingSlash: "always", leadingSlash: "always" })
			const dirName = getDirname(path)
			const baseName = getBasename(path)
			const target = state.fsMap.get(path)
			const parentDir = state.fsMap.get(dirName)
			if (parentDir === void 0 || target === void 0)
				throw new FilesystemError("ENOENT", path, "rmdir")
			if (parentDir instanceof Uint8Array || target instanceof Uint8Array)
				throw new FilesystemError("ENOTDIR", path, "rmdir")
			if ("placeholder" in parentDir || "placeholder" in target) {
				throw new FilesystemError("ENOTDIR", path, "rmdir")
			}
			if (target.size) throw new FilesystemError("ENOTEMPTY", path, "rmdir")
			parentDir.delete(baseName)
			state.fsStats.delete(path)
			state.fsMap.delete(path)
			for (const listener of listeners) {
				listener({ eventType: "rename", filename: dirName + baseName })
			}
		},
		symlink: async function (target, path) {
			path = normalizePath2(path, { trailingSlash: "always", leadingSlash: "always" })
			const rawTarget = target.startsWith("/") ? target : `${path}/../${target}`
			const targetWithTrailing = normalizePath2(rawTarget, {
				trailingSlash: "always",
				leadingSlash: "always",
			})
			const targetInode = state.fsMap.get(targetWithTrailing)
			const parentDir = state.fsMap.get(getDirname(path))
			if (state.fsMap.get(path)) {
				throw new FilesystemError("EEXIST", path, "symlink", target)
			}
			if (parentDir === void 0) {
				throw new FilesystemError("ENOENT", path, "symlink", target)
			}
			if (parentDir instanceof Uint8Array || "placeholder" in parentDir) {
				throw new FilesystemError("ENOTDIR", path, "symlink", target)
			}
			if (targetInode !== void 0) {
				state.fsMap.set(path, targetInode)
			}
			parentDir.add(getBasename(path))
			newStatEntry({
				path,
				stats: state.fsStats,
				kind: 2,
				modeBits: 511,
				target: rawTarget,
			})
		},
		unlink: async function (path) {
			path = normalizePath2(path, { trailingSlash: "always", leadingSlash: "always" })
			const targetStats = state.fsStats.get(path)
			const target = state.fsMap.get(path)
			const parentDir = state.fsMap.get(getDirname(path))
			if (parentDir === void 0 || target === void 0)
				throw new FilesystemError("ENOENT", path, "unlink")
			if (parentDir instanceof Uint8Array || "placeholder" in parentDir) {
				throw new FilesystemError("ENOTDIR", path, "unlink")
			}
			if (targetStats?.isDirectory()) {
				throw new FilesystemError("EISDIR", path, "unlink")
			}
			parentDir.delete(getBasename(path))
			state.fsStats.delete(path)
			state.fsMap.delete(path)
		},
		readlink: async function (path) {
			path = normalizePath2(path, { trailingSlash: "always", leadingSlash: "always" })
			const linkStats = await lstat(path)
			if (linkStats === void 0) {
				throw new FilesystemError("ENOENT", path, "readlink")
			}
			if (linkStats.symlinkTarget === void 0) {
				throw new FilesystemError("EINVAL", path, "readlink")
			}
			return linkStats.symlinkTarget
		},
		stat,
		lstat,
	}
	function newStatEntry({ path, stats, kind, modeBits, target, oid, rootHash }) {
		const currentTime = Date.now()
		const _kind = kind
		const targetPath = normalizePath2(path, { trailingSlash: "always", leadingSlash: "always" })
		const oldStats = stats.get(targetPath)
		const mtimeMs =
			Math.floor(currentTime / 1e3) === (oldStats?.mtimeMs && Math.floor(oldStats?.mtimeMs / 1e3))
				? currentTime + 1e3
				: currentTime
		stats.set(targetPath, {
			ctimeMs: oldStats?.ctimeMs || currentTime,
			mtimeMs,
			dev: 0,
			ino: oldStats?.ino || state.lastIno++,
			mode: (!kind ? 32768 : kind === 1 ? 16384 : 40960) | modeBits,
			uid: 0,
			gid: 0,
			size: -1,
			isFile: () => kind === 0,
			isDirectory: () => kind === 1,
			isSymbolicLink: () => kind === 2,
			// symlinkTarget is only for symlinks, and is not normalized
			symlinkTarget: target,
			_oid: oid,
			_rootHash: rootHash,
			_kind,
		})
	}
}
var init_memoryFs = __esm({
	"../../../lix/packages/fs/dist/memoryFs.js"() {
		"use strict"
		init_FilesystemError()
		init_helpers()
	},
})

// ../../../lix/packages/fs/dist/index.js
var dist_exports = {}
__export(dist_exports, {
	assertIsAbsolutePath: () => assertIsAbsolutePath,
	createNodeishMemoryFs: () => createNodeishMemoryFs,
	fromSnapshot: () => fromSnapshot,
	getBasename: () => getBasename,
	getDirname: () => getDirname,
	normalizePath: () => normalizePath2,
	toSnapshot: () => toSnapshot,
})
var init_dist = __esm({
	"../../../lix/packages/fs/dist/index.js"() {
		"use strict"
		init_memoryFs()
		init_helpers()
	},
})

// ../../../lix/packages/client/dist/git/debug/packfile.js
var packfile_exports = {}
__export(packfile_exports, {
	inflatePackResponse: () => inflatePackResponse,
	inflatePackfile: () => inflatePackfile,
})
async function inflatePackResponse(packResonseBody) {
	const bodyResponse = await isomorphic_git_default._parseUploadPackResponse([packResonseBody])
	const packfile = Buffer.from(await collect(bodyResponse.packfile))
	const packfileSha = packfile.slice(-20).toString("hex")
	if (!packfileSha) {
		return ""
	}
	return {
		acks: bodyResponse.acks,
		nak: bodyResponse.nak,
		shallows: bodyResponse.shallows,
		unshallows: bodyResponse.unshallows,
		packfilePath: `objects/pack/pack-${packfileSha}.pack`,
		...(await inflatePackfile(packfile)),
	}
}
async function inflatePackfile(packfile) {
	const getExternalRefDelta = (oid) => console.warn("trying to catch external ref", oid)
	const idx = await GitPackIndex.fromPack({
		pack: packfile,
		getExternalRefDelta,
		onProgress: void 0,
	})
	const inflatedPack = {}
	const trees = {}
	for (const hash2 of idx.hashes) {
		const object = await idx.read({ oid: hash2 })
		const typeKey = object.type + "s"
		if (!inflatedPack[typeKey]) {
			inflatedPack[typeKey] = {}
		}
		if (object.type === "tree") {
			trees[hash2] = new GitTree(object.object)
		} else if (object.type === "commit") {
			const commit3 = new GitCommit(object.object)
			inflatedPack[typeKey][hash2] = commit3.parse()
		} else if (object.type === "blob") {
			object.string = object.object.toString()
			inflatedPack[typeKey][hash2] = object
		} else {
			inflatedPack[typeKey][hash2] = object
		}
	}
	for (const commit3 of Object.values(inflatedPack.commits || {})) {
		inflatedPack.trees[commit3.tree] = extractTree(trees, commit3.tree)
	}
	inflatedPack.trees = { ...inflatedPack.trees, ...trees }
	return inflatedPack
}
function extractTree(treeEntries, treeHash) {
	const tree = treeEntries[treeHash]
	if (!tree) {
		return {}
	}
	const extractedTree = {}
	for (const entry of tree._entries) {
		if (entry.type === "tree") {
			extractedTree[entry.path] = {
				children: extractTree(treeEntries, entry.oid),
				...entry,
			}
		} else {
			extractedTree[entry.path] = entry
		}
	}
	delete treeEntries[treeHash]
	return extractedTree
}
var init_packfile = __esm({
	"../../../lix/packages/client/dist/git/debug/packfile.js"() {
		"use strict"
		init_isomorphic_git()
		if (window) {
			window.isoGit = isomorphic_git_default
		}
	},
})

// ../../../node_modules/.pnpm/@octokit+plugin-paginate-graphql@4.0.1_@octokit+core@5.2.0/node_modules/@octokit/plugin-paginate-graphql/dist-node/index.js
var require_dist_node11 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+plugin-paginate-graphql@4.0.1_@octokit+core@5.2.0/node_modules/@octokit/plugin-paginate-graphql/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			paginateGraphql: () => paginateGraphql,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var generateMessage = (path, cursorValue) =>
			`The cursor at "${path.join(
				","
			)}" did not change its value "${cursorValue}" after a page transition. Please make sure your that your query is set up correctly.`
		var MissingCursorChange = class extends Error {
			constructor(pageInfo, cursorValue) {
				super(generateMessage(pageInfo.pathInQuery, cursorValue))
				this.pageInfo = pageInfo
				this.cursorValue = cursorValue
				this.name = "MissingCursorChangeError"
				if (Error.captureStackTrace) {
					Error.captureStackTrace(this, this.constructor)
				}
			}
		}
		var MissingPageInfo = class extends Error {
			constructor(response) {
				super(
					`No pageInfo property found in response. Please make sure to specify the pageInfo in your query. Response-Data: ${JSON.stringify(
						response,
						null,
						2
					)}`
				)
				this.response = response
				this.name = "MissingPageInfo"
				if (Error.captureStackTrace) {
					Error.captureStackTrace(this, this.constructor)
				}
			}
		}
		var isObject2 = (value) => Object.prototype.toString.call(value) === "[object Object]"
		function findPaginatedResourcePath(responseData) {
			const paginatedResourcePath = deepFindPathToProperty(responseData, "pageInfo")
			if (paginatedResourcePath.length === 0) {
				throw new MissingPageInfo(responseData)
			}
			return paginatedResourcePath
		}
		var deepFindPathToProperty = (object, searchProp, path = []) => {
			for (const key of Object.keys(object)) {
				const currentPath = [...path, key]
				const currentValue = object[key]
				if (currentValue.hasOwnProperty(searchProp)) {
					return currentPath
				}
				if (isObject2(currentValue)) {
					const result = deepFindPathToProperty(currentValue, searchProp, currentPath)
					if (result.length > 0) {
						return result
					}
				}
			}
			return []
		}
		var get = (object, path) => {
			return path.reduce((current, nextProperty) => current[nextProperty], object)
		}
		var set = (object, path, mutator) => {
			const lastProperty = path.at(-1)
			const parentPath = [...path].slice(0, -1)
			const parent = get(object, parentPath)
			if (typeof mutator === "function") {
				parent[lastProperty] = mutator(parent[lastProperty])
			} else {
				parent[lastProperty] = mutator
			}
		}
		var extractPageInfos = (responseData) => {
			const pageInfoPath = findPaginatedResourcePath(responseData)
			return {
				pathInQuery: pageInfoPath,
				pageInfo: get(responseData, [...pageInfoPath, "pageInfo"]),
			}
		}
		var isForwardSearch = (givenPageInfo) => {
			return givenPageInfo.hasOwnProperty("hasNextPage")
		}
		var getCursorFrom = (pageInfo) =>
			isForwardSearch(pageInfo) ? pageInfo.endCursor : pageInfo.startCursor
		var hasAnotherPage = (pageInfo) =>
			isForwardSearch(pageInfo) ? pageInfo.hasNextPage : pageInfo.hasPreviousPage
		var createIterator = (octokit) => {
			return (query, initialParameters = {}) => {
				let nextPageExists = true
				let parameters = { ...initialParameters }
				return {
					[Symbol.asyncIterator]: () => ({
						async next() {
							if (!nextPageExists) return { done: true, value: {} }
							const response = await octokit.graphql(query, parameters)
							const pageInfoContext = extractPageInfos(response)
							const nextCursorValue = getCursorFrom(pageInfoContext.pageInfo)
							nextPageExists = hasAnotherPage(pageInfoContext.pageInfo)
							if (nextPageExists && nextCursorValue === parameters.cursor) {
								throw new MissingCursorChange(pageInfoContext, nextCursorValue)
							}
							parameters = {
								...parameters,
								cursor: nextCursorValue,
							}
							return { done: false, value: response }
						},
					}),
				}
			}
		}
		var mergeResponses = (response1, response2) => {
			if (Object.keys(response1).length === 0) {
				return Object.assign(response1, response2)
			}
			const path = findPaginatedResourcePath(response1)
			const nodesPath = [...path, "nodes"]
			const newNodes = get(response2, nodesPath)
			if (newNodes) {
				set(response1, nodesPath, (values) => {
					return [...values, ...newNodes]
				})
			}
			const edgesPath = [...path, "edges"]
			const newEdges = get(response2, edgesPath)
			if (newEdges) {
				set(response1, edgesPath, (values) => {
					return [...values, ...newEdges]
				})
			}
			const pageInfoPath = [...path, "pageInfo"]
			set(response1, pageInfoPath, get(response2, pageInfoPath))
			return response1
		}
		var createPaginate = (octokit) => {
			const iterator = createIterator(octokit)
			return async (query, initialParameters = {}) => {
				let mergedResponse = {}
				for await (const response of iterator(query, initialParameters)) {
					mergedResponse = mergeResponses(mergedResponse, response)
				}
				return mergedResponse
			}
		}
		function paginateGraphql(octokit) {
			octokit.graphql
			return {
				graphql: Object.assign(octokit.graphql, {
					paginate: Object.assign(createPaginate(octokit), {
						iterator: createIterator(octokit),
					}),
				}),
			}
		}
	},
})

// ../../../node_modules/.pnpm/bottleneck@2.19.5/node_modules/bottleneck/light.js
var require_light = __commonJS({
	"../../../node_modules/.pnpm/bottleneck@2.19.5/node_modules/bottleneck/light.js"(
		exports2,
		module2
	) {
		;(function (global2, factory) {
			typeof exports2 === "object" && typeof module2 !== "undefined"
				? (module2.exports = factory())
				: typeof define === "function" && define.amd
				? define(factory)
				: (global2.Bottleneck = factory())
		})(exports2, function () {
			"use strict"
			var commonjsGlobal =
				typeof globalThis !== "undefined"
					? globalThis
					: typeof window !== "undefined"
					? window
					: typeof global !== "undefined"
					? global
					: typeof self !== "undefined"
					? self
					: {}
			function getCjsExportFromNamespace(n) {
				return (n && n["default"]) || n
			}
			var load = function (received, defaults, onto = {}) {
				var k, ref, v
				for (k in defaults) {
					v = defaults[k]
					onto[k] = (ref = received[k]) != undefined ? ref : v
				}
				return onto
			}
			var overwrite = function (received, defaults, onto = {}) {
				var k, v
				for (k in received) {
					v = received[k]
					if (defaults[k] !== void 0) {
						onto[k] = v
					}
				}
				return onto
			}
			var parser = {
				load,
				overwrite,
			}
			var DLList
			DLList = class DLList {
				constructor(incr, decr) {
					this.incr = incr
					this.decr = decr
					this._first = null
					this._last = null
					this.length = 0
				}
				push(value) {
					var node
					this.length++
					if (typeof this.incr === "function") {
						this.incr()
					}
					node = {
						value,
						prev: this._last,
						next: null,
					}
					if (this._last != undefined) {
						this._last.next = node
						this._last = node
					} else {
						this._first = this._last = node
					}
					return void 0
				}
				shift() {
					var value
					if (this._first == undefined) {
						return
					} else {
						this.length--
						if (typeof this.decr === "function") {
							this.decr()
						}
					}
					value = this._first.value
					if ((this._first = this._first.next) != undefined) {
						this._first.prev = null
					} else {
						this._last = null
					}
					return value
				}
				first() {
					if (this._first != undefined) {
						return this._first.value
					}
				}
				getArray() {
					var node, ref, results
					node = this._first
					results = []
					while (node != undefined) {
						results.push(((ref = node), (node = node.next), ref.value))
					}
					return results
				}
				forEachShift(cb) {
					var node
					node = this.shift()
					while (node != undefined) {
						cb(node), (node = this.shift())
					}
					return void 0
				}
				debug() {
					var node, ref, ref1, ref2, results
					node = this._first
					results = []
					while (node != undefined) {
						results.push(
							((ref = node),
							(node = node.next),
							{
								value: ref.value,
								prev: (ref1 = ref.prev) != undefined ? ref1.value : void 0,
								next: (ref2 = ref.next) != undefined ? ref2.value : void 0,
							})
						)
					}
					return results
				}
			}
			var DLList_1 = DLList
			var Events
			Events = class Events {
				constructor(instance) {
					this.instance = instance
					this._events = {}
					if (
						this.instance.on != undefined ||
						this.instance.once != undefined ||
						this.instance.removeAllListeners != undefined
					) {
						throw new Error("An Emitter already exists for this object")
					}
					this.instance.on = (name, cb) => {
						return this._addListener(name, "many", cb)
					}
					this.instance.once = (name, cb) => {
						return this._addListener(name, "once", cb)
					}
					this.instance.removeAllListeners = (name = null) => {
						if (name != undefined) {
							return delete this._events[name]
						} else {
							return (this._events = {})
						}
					}
				}
				_addListener(name, status3, cb) {
					var base
					if ((base = this._events)[name] == undefined) {
						base[name] = []
					}
					this._events[name].push({ cb, status: status3 })
					return this.instance
				}
				listenerCount(name) {
					if (this._events[name] != undefined) {
						return this._events[name].length
					} else {
						return 0
					}
				}
				async trigger(name, ...args) {
					var e, promises
					try {
						if (name !== "debug") {
							this.trigger("debug", `Event triggered: ${name}`, args)
						}
						if (this._events[name] == undefined) {
							return
						}
						this._events[name] = this._events[name].filter(function (listener) {
							return listener.status !== "none"
						})
						promises = this._events[name].map(async (listener) => {
							var e2, returned
							if (listener.status === "none") {
								return
							}
							if (listener.status === "once") {
								listener.status = "none"
							}
							try {
								returned = typeof listener.cb === "function" ? listener.cb(...args) : void 0
								if (typeof (returned != undefined ? returned.then : void 0) === "function") {
									return await returned
								} else {
									return returned
								}
							} catch (error) {
								e2 = error
								{
									this.trigger("error", e2)
								}
								return null
							}
						})
						return (await Promise.all(promises)).find(function (x) {
							return x != undefined
						})
					} catch (error) {
						e = error
						{
							this.trigger("error", e)
						}
						return null
					}
				}
			}
			var Events_1 = Events
			var DLList$1, Events$1, Queues
			DLList$1 = DLList_1
			Events$1 = Events_1
			Queues = class Queues {
				constructor(num_priorities) {
					var i
					this.Events = new Events$1(this)
					this._length = 0
					this._lists = function () {
						var j, ref, results
						results = []
						for (
							i = j = 1, ref = num_priorities;
							1 <= ref ? j <= ref : j >= ref;
							i = 1 <= ref ? ++j : --j
						) {
							results.push(
								new DLList$1(
									() => {
										return this.incr()
									},
									() => {
										return this.decr()
									}
								)
							)
						}
						return results
					}.call(this)
				}
				incr() {
					if (this._length++ === 0) {
						return this.Events.trigger("leftzero")
					}
				}
				decr() {
					if (--this._length === 0) {
						return this.Events.trigger("zero")
					}
				}
				push(job) {
					return this._lists[job.options.priority].push(job)
				}
				queued(priority) {
					if (priority != undefined) {
						return this._lists[priority].length
					} else {
						return this._length
					}
				}
				shiftAll(fn) {
					return this._lists.forEach(function (list) {
						return list.forEachShift(fn)
					})
				}
				getFirst(arr = this._lists) {
					var j, len, list
					for (j = 0, len = arr.length; j < len; j++) {
						list = arr[j]
						if (list.length > 0) {
							return list
						}
					}
					return []
				}
				shiftLastFrom(priority) {
					return this.getFirst(this._lists.slice(priority).reverse()).shift()
				}
			}
			var Queues_1 = Queues
			var BottleneckError
			BottleneckError = class BottleneckError extends Error {}
			var BottleneckError_1 = BottleneckError
			var BottleneckError$1, DEFAULT_PRIORITY, Job, NUM_PRIORITIES, parser$1
			NUM_PRIORITIES = 10
			DEFAULT_PRIORITY = 5
			parser$1 = parser
			BottleneckError$1 = BottleneckError_1
			Job = class Job {
				constructor(task, args, options, jobDefaults, rejectOnDrop, Events2, _states, Promise2) {
					this.task = task
					this.args = args
					this.rejectOnDrop = rejectOnDrop
					this.Events = Events2
					this._states = _states
					this.Promise = Promise2
					this.options = parser$1.load(options, jobDefaults)
					this.options.priority = this._sanitizePriority(this.options.priority)
					if (this.options.id === jobDefaults.id) {
						this.options.id = `${this.options.id}-${this._randomIndex()}`
					}
					this.promise = new this.Promise((_resolve, _reject) => {
						this._resolve = _resolve
						this._reject = _reject
					})
					this.retryCount = 0
				}
				_sanitizePriority(priority) {
					var sProperty
					sProperty = ~~priority !== priority ? DEFAULT_PRIORITY : priority
					if (sProperty < 0) {
						return 0
					} else if (sProperty > NUM_PRIORITIES - 1) {
						return NUM_PRIORITIES - 1
					} else {
						return sProperty
					}
				}
				_randomIndex() {
					return Math.random().toString(36).slice(2)
				}
				doDrop({ error, message = "This job has been dropped by Bottleneck" } = {}) {
					if (this._states.remove(this.options.id)) {
						if (this.rejectOnDrop) {
							this._reject(error != undefined ? error : new BottleneckError$1(message))
						}
						this.Events.trigger("dropped", {
							args: this.args,
							options: this.options,
							task: this.task,
							promise: this.promise,
						})
						return true
					} else {
						return false
					}
				}
				_assertStatus(expected) {
					var status3
					status3 = this._states.jobStatus(this.options.id)
					if (!(status3 === expected || (expected === "DONE" && status3 === null))) {
						throw new BottleneckError$1(
							`Invalid job status ${status3}, expected ${expected}. Please open an issue at https://github.com/SGrondin/bottleneck/issues`
						)
					}
				}
				doReceive() {
					this._states.start(this.options.id)
					return this.Events.trigger("received", { args: this.args, options: this.options })
				}
				doQueue(reachedHWM, blocked) {
					this._assertStatus("RECEIVED")
					this._states.next(this.options.id)
					return this.Events.trigger("queued", {
						args: this.args,
						options: this.options,
						reachedHWM,
						blocked,
					})
				}
				doRun() {
					if (this.retryCount === 0) {
						this._assertStatus("QUEUED")
						this._states.next(this.options.id)
					} else {
						this._assertStatus("EXECUTING")
					}
					return this.Events.trigger("scheduled", { args: this.args, options: this.options })
				}
				async doExecute(chained, clearGlobalState, run2, free) {
					var error, eventInfo, passed
					if (this.retryCount === 0) {
						this._assertStatus("RUNNING")
						this._states.next(this.options.id)
					} else {
						this._assertStatus("EXECUTING")
					}
					eventInfo = { args: this.args, options: this.options, retryCount: this.retryCount }
					this.Events.trigger("executing", eventInfo)
					try {
						passed = await (chained != undefined
							? chained.schedule(this.options, this.task, ...this.args)
							: this.task(...this.args))
						if (clearGlobalState()) {
							this.doDone(eventInfo)
							await free(this.options, eventInfo)
							this._assertStatus("DONE")
							return this._resolve(passed)
						}
					} catch (error1) {
						error = error1
						return this._onFailure(error, eventInfo, clearGlobalState, run2, free)
					}
				}
				doExpire(clearGlobalState, run2, free) {
					var error, eventInfo
					if (this._states.jobStatus(this.options.id === "RUNNING")) {
						this._states.next(this.options.id)
					}
					this._assertStatus("EXECUTING")
					eventInfo = { args: this.args, options: this.options, retryCount: this.retryCount }
					error = new BottleneckError$1(`This job timed out after ${this.options.expiration} ms.`)
					return this._onFailure(error, eventInfo, clearGlobalState, run2, free)
				}
				async _onFailure(error, eventInfo, clearGlobalState, run2, free) {
					var retry, retryAfter
					if (clearGlobalState()) {
						retry = await this.Events.trigger("failed", error, eventInfo)
						if (retry != undefined) {
							retryAfter = ~~retry
							this.Events.trigger(
								"retry",
								`Retrying ${this.options.id} after ${retryAfter} ms`,
								eventInfo
							)
							this.retryCount++
							return run2(retryAfter)
						} else {
							this.doDone(eventInfo)
							await free(this.options, eventInfo)
							this._assertStatus("DONE")
							return this._reject(error)
						}
					}
				}
				doDone(eventInfo) {
					this._assertStatus("EXECUTING")
					this._states.next(this.options.id)
					return this.Events.trigger("done", eventInfo)
				}
			}
			var Job_1 = Job
			var BottleneckError$2, LocalDatastore, parser$2
			parser$2 = parser
			BottleneckError$2 = BottleneckError_1
			LocalDatastore = class LocalDatastore {
				constructor(instance, storeOptions, storeInstanceOptions) {
					this.instance = instance
					this.storeOptions = storeOptions
					this.clientId = this.instance._randomIndex()
					parser$2.load(storeInstanceOptions, storeInstanceOptions, this)
					this._nextRequest = this._lastReservoirRefresh = this._lastReservoirIncrease = Date.now()
					this._running = 0
					this._done = 0
					this._unblockTime = 0
					this.ready = this.Promise.resolve()
					this.clients = {}
					this._startHeartbeat()
				}
				_startHeartbeat() {
					var base
					if (
						this.heartbeat == undefined &&
						((this.storeOptions.reservoirRefreshInterval != undefined &&
							this.storeOptions.reservoirRefreshAmount != undefined) ||
							(this.storeOptions.reservoirIncreaseInterval != undefined &&
								this.storeOptions.reservoirIncreaseAmount != undefined))
					) {
						return typeof (base = this.heartbeat =
							setInterval(() => {
								var amount, incr, maximum, now, reservoir
								now = Date.now()
								if (
									this.storeOptions.reservoirRefreshInterval != undefined &&
									now >= this._lastReservoirRefresh + this.storeOptions.reservoirRefreshInterval
								) {
									this._lastReservoirRefresh = now
									this.storeOptions.reservoir = this.storeOptions.reservoirRefreshAmount
									this.instance._drainAll(this.computeCapacity())
								}
								if (
									this.storeOptions.reservoirIncreaseInterval != undefined &&
									now >= this._lastReservoirIncrease + this.storeOptions.reservoirIncreaseInterval
								) {
									;({
										reservoirIncreaseAmount: amount,
										reservoirIncreaseMaximum: maximum,
										reservoir,
									} = this.storeOptions)
									this._lastReservoirIncrease = now
									incr = maximum != undefined ? Math.min(amount, maximum - reservoir) : amount
									if (incr > 0) {
										this.storeOptions.reservoir += incr
										return this.instance._drainAll(this.computeCapacity())
									}
								}
							}, this.heartbeatInterval)).unref === "function"
							? base.unref()
							: void 0
					} else {
						return clearInterval(this.heartbeat)
					}
				}
				async __publish__(message) {
					await this.yieldLoop()
					return this.instance.Events.trigger("message", message.toString())
				}
				async __disconnect__(flush) {
					await this.yieldLoop()
					clearInterval(this.heartbeat)
					return this.Promise.resolve()
				}
				yieldLoop(t = 0) {
					return new this.Promise(function (resolve, reject) {
						return setTimeout(resolve, t)
					})
				}
				computePenalty() {
					var ref
					return (ref = this.storeOptions.penalty) != undefined
						? ref
						: 15 * this.storeOptions.minTime || 5e3
				}
				async __updateSettings__(options) {
					await this.yieldLoop()
					parser$2.overwrite(options, options, this.storeOptions)
					this._startHeartbeat()
					this.instance._drainAll(this.computeCapacity())
					return true
				}
				async __running__() {
					await this.yieldLoop()
					return this._running
				}
				async __queued__() {
					await this.yieldLoop()
					return this.instance.queued()
				}
				async __done__() {
					await this.yieldLoop()
					return this._done
				}
				async __groupCheck__(time) {
					await this.yieldLoop()
					return this._nextRequest + this.timeout < time
				}
				computeCapacity() {
					var maxConcurrent, reservoir
					;({ maxConcurrent, reservoir } = this.storeOptions)
					if (maxConcurrent != undefined && reservoir != undefined) {
						return Math.min(maxConcurrent - this._running, reservoir)
					} else if (maxConcurrent != undefined) {
						return maxConcurrent - this._running
					} else if (reservoir != undefined) {
						return reservoir
					} else {
						return null
					}
				}
				conditionsCheck(weight) {
					var capacity
					capacity = this.computeCapacity()
					return capacity == undefined || weight <= capacity
				}
				async __incrementReservoir__(incr) {
					var reservoir
					await this.yieldLoop()
					reservoir = this.storeOptions.reservoir += incr
					this.instance._drainAll(this.computeCapacity())
					return reservoir
				}
				async __currentReservoir__() {
					await this.yieldLoop()
					return this.storeOptions.reservoir
				}
				isBlocked(now) {
					return this._unblockTime >= now
				}
				check(weight, now) {
					return this.conditionsCheck(weight) && this._nextRequest - now <= 0
				}
				async __check__(weight) {
					var now
					await this.yieldLoop()
					now = Date.now()
					return this.check(weight, now)
				}
				async __register__(index2, weight, expiration) {
					var now, wait
					await this.yieldLoop()
					now = Date.now()
					if (this.conditionsCheck(weight)) {
						this._running += weight
						if (this.storeOptions.reservoir != undefined) {
							this.storeOptions.reservoir -= weight
						}
						wait = Math.max(this._nextRequest - now, 0)
						this._nextRequest = now + wait + this.storeOptions.minTime
						return {
							success: true,
							wait,
							reservoir: this.storeOptions.reservoir,
						}
					} else {
						return {
							success: false,
						}
					}
				}
				strategyIsBlock() {
					return this.storeOptions.strategy === 3
				}
				async __submit__(queueLength, weight) {
					var blocked, now, reachedHWM
					await this.yieldLoop()
					if (
						this.storeOptions.maxConcurrent != undefined &&
						weight > this.storeOptions.maxConcurrent
					) {
						throw new BottleneckError$2(
							`Impossible to add a job having a weight of ${weight} to a limiter having a maxConcurrent setting of ${this.storeOptions.maxConcurrent}`
						)
					}
					now = Date.now()
					reachedHWM =
						this.storeOptions.highWater != undefined &&
						queueLength === this.storeOptions.highWater &&
						!this.check(weight, now)
					blocked = this.strategyIsBlock() && (reachedHWM || this.isBlocked(now))
					if (blocked) {
						this._unblockTime = now + this.computePenalty()
						this._nextRequest = this._unblockTime + this.storeOptions.minTime
						this.instance._dropAllQueued()
					}
					return {
						reachedHWM,
						blocked,
						strategy: this.storeOptions.strategy,
					}
				}
				async __free__(index2, weight) {
					await this.yieldLoop()
					this._running -= weight
					this._done += weight
					this.instance._drainAll(this.computeCapacity())
					return {
						running: this._running,
					}
				}
			}
			var LocalDatastore_1 = LocalDatastore
			var BottleneckError$3, States
			BottleneckError$3 = BottleneckError_1
			States = class States {
				constructor(status1) {
					this.status = status1
					this._jobs = {}
					this.counts = this.status.map(function () {
						return 0
					})
				}
				next(id) {
					var current, next
					current = this._jobs[id]
					next = current + 1
					if (current != undefined && next < this.status.length) {
						this.counts[current]--
						this.counts[next]++
						return this._jobs[id]++
					} else if (current != undefined) {
						this.counts[current]--
						return delete this._jobs[id]
					}
				}
				start(id) {
					var initial
					initial = 0
					this._jobs[id] = initial
					return this.counts[initial]++
				}
				remove(id) {
					var current
					current = this._jobs[id]
					if (current != undefined) {
						this.counts[current]--
						delete this._jobs[id]
					}
					return current != undefined
				}
				jobStatus(id) {
					var ref
					return (ref = this.status[this._jobs[id]]) != undefined ? ref : null
				}
				statusJobs(status3) {
					var k, pos, ref, results, v
					if (status3 != undefined) {
						pos = this.status.indexOf(status3)
						if (pos < 0) {
							throw new BottleneckError$3(`status must be one of ${this.status.join(", ")}`)
						}
						ref = this._jobs
						results = []
						for (k in ref) {
							v = ref[k]
							if (v === pos) {
								results.push(k)
							}
						}
						return results
					} else {
						return Object.keys(this._jobs)
					}
				}
				statusCounts() {
					return this.counts.reduce((acc, v, i) => {
						acc[this.status[i]] = v
						return acc
					}, {})
				}
			}
			var States_1 = States
			var DLList$2, Sync
			DLList$2 = DLList_1
			Sync = class Sync {
				constructor(name, Promise2) {
					this.schedule = this.schedule.bind(this)
					this.name = name
					this.Promise = Promise2
					this._running = 0
					this._queue = new DLList$2()
				}
				isEmpty() {
					return this._queue.length === 0
				}
				async _tryToRun() {
					var args, cb, error, reject, resolve, returned, task
					if (this._running < 1 && this._queue.length > 0) {
						this._running++
						;({ task, args, resolve, reject } = this._queue.shift())
						cb = await (async function () {
							try {
								returned = await task(...args)
								return function () {
									return resolve(returned)
								}
							} catch (error1) {
								error = error1
								return function () {
									return reject(error)
								}
							}
						})()
						this._running--
						this._tryToRun()
						return cb()
					}
				}
				schedule(task, ...args) {
					var promise, reject, resolve
					resolve = reject = null
					promise = new this.Promise(function (_resolve, _reject) {
						resolve = _resolve
						return (reject = _reject)
					})
					this._queue.push({ task, args, resolve, reject })
					this._tryToRun()
					return promise
				}
			}
			var Sync_1 = Sync
			var version3 = "2.19.5"
			var version$1 = {
				version: version3,
			}
			var version$2 = /* @__PURE__ */ Object.freeze({
				version: version3,
				default: version$1,
			})
			var require$$2 = () =>
				console.log("You must import the full version of Bottleneck in order to use this feature.")
			var require$$3 = () =>
				console.log("You must import the full version of Bottleneck in order to use this feature.")
			var require$$4 = () =>
				console.log("You must import the full version of Bottleneck in order to use this feature.")
			var Events$2, Group, IORedisConnection$1, RedisConnection$1, Scripts$1, parser$3
			parser$3 = parser
			Events$2 = Events_1
			RedisConnection$1 = require$$2
			IORedisConnection$1 = require$$3
			Scripts$1 = require$$4
			Group = function () {
				class Group2 {
					constructor(limiterOptions = {}) {
						this.deleteKey = this.deleteKey.bind(this)
						this.limiterOptions = limiterOptions
						parser$3.load(this.limiterOptions, this.defaults, this)
						this.Events = new Events$2(this)
						this.instances = {}
						this.Bottleneck = Bottleneck_1
						this._startAutoCleanup()
						this.sharedConnection = this.connection != undefined
						if (this.connection == undefined) {
							if (this.limiterOptions.datastore === "redis") {
								this.connection = new RedisConnection$1(
									Object.assign({}, this.limiterOptions, { Events: this.Events })
								)
							} else if (this.limiterOptions.datastore === "ioredis") {
								this.connection = new IORedisConnection$1(
									Object.assign({}, this.limiterOptions, { Events: this.Events })
								)
							}
						}
					}
					key(key = "") {
						var ref
						return (ref = this.instances[key]) != undefined
							? ref
							: (() => {
									var limiter
									limiter = this.instances[key] = new this.Bottleneck(
										Object.assign(this.limiterOptions, {
											id: `${this.id}-${key}`,
											timeout: this.timeout,
											connection: this.connection,
										})
									)
									this.Events.trigger("created", limiter, key)
									return limiter
							  })()
					}
					async deleteKey(key = "") {
						var deleted, instance
						instance = this.instances[key]
						if (this.connection) {
							deleted = await this.connection.__runCommand__([
								"del",
								...Scripts$1.allKeys(`${this.id}-${key}`),
							])
						}
						if (instance != undefined) {
							delete this.instances[key]
							await instance.disconnect()
						}
						return instance != undefined || deleted > 0
					}
					limiters() {
						var k, ref, results, v
						ref = this.instances
						results = []
						for (k in ref) {
							v = ref[k]
							results.push({
								key: k,
								limiter: v,
							})
						}
						return results
					}
					keys() {
						return Object.keys(this.instances)
					}
					async clusterKeys() {
						var cursor, end, found, i, k, keys, len, next, start
						if (this.connection == undefined) {
							return this.Promise.resolve(this.keys())
						}
						keys = []
						cursor = null
						start = `b_${this.id}-`.length
						end = "_settings".length
						while (cursor !== 0) {
							;[next, found] = await this.connection.__runCommand__([
								"scan",
								cursor != undefined ? cursor : 0,
								"match",
								`b_${this.id}-*_settings`,
								"count",
								1e4,
							])
							cursor = ~~next
							for (i = 0, len = found.length; i < len; i++) {
								k = found[i]
								keys.push(k.slice(start, -end))
							}
						}
						return keys
					}
					_startAutoCleanup() {
						var base
						clearInterval(this.interval)
						return typeof (base = this.interval =
							setInterval(async () => {
								var e, k, ref, results, time, v
								time = Date.now()
								ref = this.instances
								results = []
								for (k in ref) {
									v = ref[k]
									try {
										if (await v._store.__groupCheck__(time)) {
											results.push(this.deleteKey(k))
										} else {
											results.push(void 0)
										}
									} catch (error) {
										e = error
										results.push(v.Events.trigger("error", e))
									}
								}
								return results
							}, this.timeout / 2)).unref === "function"
							? base.unref()
							: void 0
					}
					updateSettings(options = {}) {
						parser$3.overwrite(options, this.defaults, this)
						parser$3.overwrite(options, options, this.limiterOptions)
						if (options.timeout != undefined) {
							return this._startAutoCleanup()
						}
					}
					disconnect(flush = true) {
						var ref
						if (!this.sharedConnection) {
							return (ref = this.connection) != undefined ? ref.disconnect(flush) : void 0
						}
					}
				}
				Group2.prototype.defaults = {
					timeout: 1e3 * 60 * 5,
					connection: null,
					Promise,
					id: "group-key",
				}
				return Group2
			}.call(commonjsGlobal)
			var Group_1 = Group
			var Batcher, Events$3, parser$4
			parser$4 = parser
			Events$3 = Events_1
			Batcher = function () {
				class Batcher2 {
					constructor(options = {}) {
						this.options = options
						parser$4.load(this.options, this.defaults, this)
						this.Events = new Events$3(this)
						this._arr = []
						this._resetPromise()
						this._lastFlush = Date.now()
					}
					_resetPromise() {
						return (this._promise = new this.Promise((res, rej) => {
							return (this._resolve = res)
						}))
					}
					_flush() {
						clearTimeout(this._timeout)
						this._lastFlush = Date.now()
						this._resolve()
						this.Events.trigger("batch", this._arr)
						this._arr = []
						return this._resetPromise()
					}
					add(data) {
						var ret
						this._arr.push(data)
						ret = this._promise
						if (this._arr.length === this.maxSize) {
							this._flush()
						} else if (this.maxTime != undefined && this._arr.length === 1) {
							this._timeout = setTimeout(() => {
								return this._flush()
							}, this.maxTime)
						}
						return ret
					}
				}
				Batcher2.prototype.defaults = {
					maxTime: null,
					maxSize: null,
					Promise,
				}
				return Batcher2
			}.call(commonjsGlobal)
			var Batcher_1 = Batcher
			var require$$4$1 = () =>
				console.log("You must import the full version of Bottleneck in order to use this feature.")
			var require$$8 = getCjsExportFromNamespace(version$2)
			var Bottleneck,
				DEFAULT_PRIORITY$1,
				Events$4,
				Job$1,
				LocalDatastore$1,
				NUM_PRIORITIES$1,
				Queues$1,
				RedisDatastore$1,
				States$1,
				Sync$1,
				parser$5,
				splice = [].splice
			NUM_PRIORITIES$1 = 10
			DEFAULT_PRIORITY$1 = 5
			parser$5 = parser
			Queues$1 = Queues_1
			Job$1 = Job_1
			LocalDatastore$1 = LocalDatastore_1
			RedisDatastore$1 = require$$4$1
			Events$4 = Events_1
			States$1 = States_1
			Sync$1 = Sync_1
			Bottleneck = function () {
				class Bottleneck2 {
					constructor(options = {}, ...invalid) {
						var storeInstanceOptions, storeOptions
						this._addToQueue = this._addToQueue.bind(this)
						this._validateOptions(options, invalid)
						parser$5.load(options, this.instanceDefaults, this)
						this._queues = new Queues$1(NUM_PRIORITIES$1)
						this._scheduled = {}
						this._states = new States$1(
							["RECEIVED", "QUEUED", "RUNNING", "EXECUTING"].concat(
								this.trackDoneStatus ? ["DONE"] : []
							)
						)
						this._limiter = null
						this.Events = new Events$4(this)
						this._submitLock = new Sync$1("submit", this.Promise)
						this._registerLock = new Sync$1("register", this.Promise)
						storeOptions = parser$5.load(options, this.storeDefaults, {})
						this._store = function () {
							if (
								this.datastore === "redis" ||
								this.datastore === "ioredis" ||
								this.connection != undefined
							) {
								storeInstanceOptions = parser$5.load(options, this.redisStoreDefaults, {})
								return new RedisDatastore$1(this, storeOptions, storeInstanceOptions)
							} else if (this.datastore === "local") {
								storeInstanceOptions = parser$5.load(options, this.localStoreDefaults, {})
								return new LocalDatastore$1(this, storeOptions, storeInstanceOptions)
							} else {
								throw new Bottleneck2.prototype.BottleneckError(
									`Invalid datastore type: ${this.datastore}`
								)
							}
						}.call(this)
						this._queues.on("leftzero", () => {
							var ref
							return (ref = this._store.heartbeat) != undefined
								? typeof ref.ref === "function"
									? ref.ref()
									: void 0
								: void 0
						})
						this._queues.on("zero", () => {
							var ref
							return (ref = this._store.heartbeat) != undefined
								? typeof ref.unref === "function"
									? ref.unref()
									: void 0
								: void 0
						})
					}
					_validateOptions(options, invalid) {
						if (!(options != undefined && typeof options === "object" && invalid.length === 0)) {
							throw new Bottleneck2.prototype.BottleneckError(
								"Bottleneck v2 takes a single object argument. Refer to https://github.com/SGrondin/bottleneck#upgrading-to-v2 if you're upgrading from Bottleneck v1."
							)
						}
					}
					ready() {
						return this._store.ready
					}
					clients() {
						return this._store.clients
					}
					channel() {
						return `b_${this.id}`
					}
					channel_client() {
						return `b_${this.id}_${this._store.clientId}`
					}
					publish(message) {
						return this._store.__publish__(message)
					}
					disconnect(flush = true) {
						return this._store.__disconnect__(flush)
					}
					chain(_limiter) {
						this._limiter = _limiter
						return this
					}
					queued(priority) {
						return this._queues.queued(priority)
					}
					clusterQueued() {
						return this._store.__queued__()
					}
					empty() {
						return this.queued() === 0 && this._submitLock.isEmpty()
					}
					running() {
						return this._store.__running__()
					}
					done() {
						return this._store.__done__()
					}
					jobStatus(id) {
						return this._states.jobStatus(id)
					}
					jobs(status3) {
						return this._states.statusJobs(status3)
					}
					counts() {
						return this._states.statusCounts()
					}
					_randomIndex() {
						return Math.random().toString(36).slice(2)
					}
					check(weight = 1) {
						return this._store.__check__(weight)
					}
					_clearGlobalState(index2) {
						if (this._scheduled[index2] != undefined) {
							clearTimeout(this._scheduled[index2].expiration)
							delete this._scheduled[index2]
							return true
						} else {
							return false
						}
					}
					async _free(index2, job, options, eventInfo) {
						var e, running
						try {
							;({ running } = await this._store.__free__(index2, options.weight))
							this.Events.trigger("debug", `Freed ${options.id}`, eventInfo)
							if (running === 0 && this.empty()) {
								return this.Events.trigger("idle")
							}
						} catch (error1) {
							e = error1
							return this.Events.trigger("error", e)
						}
					}
					_run(index2, job, wait) {
						var clearGlobalState, free, run2
						job.doRun()
						clearGlobalState = this._clearGlobalState.bind(this, index2)
						run2 = this._run.bind(this, index2, job)
						free = this._free.bind(this, index2, job)
						return (this._scheduled[index2] = {
							timeout: setTimeout(() => {
								return job.doExecute(this._limiter, clearGlobalState, run2, free)
							}, wait),
							expiration:
								job.options.expiration != undefined
									? setTimeout(function () {
											return job.doExpire(clearGlobalState, run2, free)
									  }, wait + job.options.expiration)
									: void 0,
							job,
						})
					}
					_drainOne(capacity) {
						return this._registerLock.schedule(() => {
							var args, index2, next, options, queue
							if (this.queued() === 0) {
								return this.Promise.resolve(null)
							}
							queue = this._queues.getFirst()
							;({ options, args } = next = queue.first())
							if (capacity != undefined && options.weight > capacity) {
								return this.Promise.resolve(null)
							}
							this.Events.trigger("debug", `Draining ${options.id}`, { args, options })
							index2 = this._randomIndex()
							return this._store
								.__register__(index2, options.weight, options.expiration)
								.then(({ success, wait, reservoir }) => {
									var empty
									this.Events.trigger("debug", `Drained ${options.id}`, { success, args, options })
									if (success) {
										queue.shift()
										empty = this.empty()
										if (empty) {
											this.Events.trigger("empty")
										}
										if (reservoir === 0) {
											this.Events.trigger("depleted", empty)
										}
										this._run(index2, next, wait)
										return this.Promise.resolve(options.weight)
									} else {
										return this.Promise.resolve(null)
									}
								})
						})
					}
					_drainAll(capacity, total = 0) {
						return this._drainOne(capacity)
							.then((drained) => {
								var newCapacity
								if (drained != undefined) {
									newCapacity = capacity != undefined ? capacity - drained : capacity
									return this._drainAll(newCapacity, total + drained)
								} else {
									return this.Promise.resolve(total)
								}
							})
							.catch((e) => {
								return this.Events.trigger("error", e)
							})
					}
					_dropAllQueued(message) {
						return this._queues.shiftAll(function (job) {
							return job.doDrop({ message })
						})
					}
					stop(options = {}) {
						var done, waitForExecuting
						options = parser$5.load(options, this.stopDefaults)
						waitForExecuting = (at) => {
							var finished
							finished = () => {
								var counts
								counts = this._states.counts
								return counts[0] + counts[1] + counts[2] + counts[3] === at
							}
							return new this.Promise((resolve, reject) => {
								if (finished()) {
									return resolve()
								} else {
									return this.on("done", () => {
										if (finished()) {
											this.removeAllListeners("done")
											return resolve()
										}
									})
								}
							})
						}
						done = options.dropWaitingJobs
							? ((this._run = function (index2, next) {
									return next.doDrop({
										message: options.dropErrorMessage,
									})
							  }),
							  (this._drainOne = () => {
									return this.Promise.resolve(null)
							  }),
							  this._registerLock.schedule(() => {
									return this._submitLock.schedule(() => {
										var k, ref, v
										ref = this._scheduled
										for (k in ref) {
											v = ref[k]
											if (this.jobStatus(v.job.options.id) === "RUNNING") {
												clearTimeout(v.timeout)
												clearTimeout(v.expiration)
												v.job.doDrop({
													message: options.dropErrorMessage,
												})
											}
										}
										this._dropAllQueued(options.dropErrorMessage)
										return waitForExecuting(0)
									})
							  }))
							: this.schedule(
									{
										priority: NUM_PRIORITIES$1 - 1,
										weight: 0,
									},
									() => {
										return waitForExecuting(1)
									}
							  )
						this._receive = function (job) {
							return job._reject(
								new Bottleneck2.prototype.BottleneckError(options.enqueueErrorMessage)
							)
						}
						this.stop = () => {
							return this.Promise.reject(
								new Bottleneck2.prototype.BottleneckError("stop() has already been called")
							)
						}
						return done
					}
					async _addToQueue(job) {
						var args, blocked, error, options, reachedHWM, shifted, strategy
						;({ args, options } = job)
						try {
							;({ reachedHWM, blocked, strategy } = await this._store.__submit__(
								this.queued(),
								options.weight
							))
						} catch (error1) {
							error = error1
							this.Events.trigger("debug", `Could not queue ${options.id}`, {
								args,
								options,
								error,
							})
							job.doDrop({ error })
							return false
						}
						if (blocked) {
							job.doDrop()
							return true
						} else if (reachedHWM) {
							shifted =
								strategy === Bottleneck2.prototype.strategy.LEAK
									? this._queues.shiftLastFrom(options.priority)
									: strategy === Bottleneck2.prototype.strategy.OVERFLOW_PRIORITY
									? this._queues.shiftLastFrom(options.priority + 1)
									: strategy === Bottleneck2.prototype.strategy.OVERFLOW
									? job
									: void 0
							if (shifted != undefined) {
								shifted.doDrop()
							}
							if (shifted == undefined || strategy === Bottleneck2.prototype.strategy.OVERFLOW) {
								if (shifted == undefined) {
									job.doDrop()
								}
								return reachedHWM
							}
						}
						job.doQueue(reachedHWM, blocked)
						this._queues.push(job)
						await this._drainAll()
						return reachedHWM
					}
					_receive(job) {
						if (this._states.jobStatus(job.options.id) != undefined) {
							job._reject(
								new Bottleneck2.prototype.BottleneckError(
									`A job with the same id already exists (id=${job.options.id})`
								)
							)
							return false
						} else {
							job.doReceive()
							return this._submitLock.schedule(this._addToQueue, job)
						}
					}
					submit(...args) {
						var cb, fn, job, options, ref, ref1, task
						if (typeof args[0] === "function") {
							;(ref = args), ([fn, ...args] = ref), ([cb] = splice.call(args, -1))
							options = parser$5.load({}, this.jobDefaults)
						} else {
							;(ref1 = args), ([options, fn, ...args] = ref1), ([cb] = splice.call(args, -1))
							options = parser$5.load(options, this.jobDefaults)
						}
						task = (...args2) => {
							return new this.Promise(function (resolve, reject) {
								return fn(...args2, function (...args3) {
									return (args3[0] != undefined ? reject : resolve)(args3)
								})
							})
						}
						job = new Job$1(
							task,
							args,
							options,
							this.jobDefaults,
							this.rejectOnDrop,
							this.Events,
							this._states,
							this.Promise
						)
						job.promise
							.then(function (args2) {
								return typeof cb === "function" ? cb(...args2) : void 0
							})
							.catch(function (args2) {
								if (Array.isArray(args2)) {
									return typeof cb === "function" ? cb(...args2) : void 0
								} else {
									return typeof cb === "function" ? cb(args2) : void 0
								}
							})
						return this._receive(job)
					}
					schedule(...args) {
						var job, options, task
						if (typeof args[0] === "function") {
							;[task, ...args] = args
							options = {}
						} else {
							;[options, task, ...args] = args
						}
						job = new Job$1(
							task,
							args,
							options,
							this.jobDefaults,
							this.rejectOnDrop,
							this.Events,
							this._states,
							this.Promise
						)
						this._receive(job)
						return job.promise
					}
					wrap(fn) {
						var schedule, wrapped
						schedule = this.schedule.bind(this)
						wrapped = function (...args) {
							return schedule(fn.bind(this), ...args)
						}
						wrapped.withOptions = function (options, ...args) {
							return schedule(options, fn, ...args)
						}
						return wrapped
					}
					async updateSettings(options = {}) {
						await this._store.__updateSettings__(parser$5.overwrite(options, this.storeDefaults))
						parser$5.overwrite(options, this.instanceDefaults, this)
						return this
					}
					currentReservoir() {
						return this._store.__currentReservoir__()
					}
					incrementReservoir(incr = 0) {
						return this._store.__incrementReservoir__(incr)
					}
				}
				Bottleneck2.default = Bottleneck2
				Bottleneck2.Events = Events$4
				Bottleneck2.version = Bottleneck2.prototype.version = require$$8.version
				Bottleneck2.strategy = Bottleneck2.prototype.strategy = {
					LEAK: 1,
					OVERFLOW: 2,
					OVERFLOW_PRIORITY: 4,
					BLOCK: 3,
				}
				Bottleneck2.BottleneckError = Bottleneck2.prototype.BottleneckError = BottleneckError_1
				Bottleneck2.Group = Bottleneck2.prototype.Group = Group_1
				Bottleneck2.RedisConnection = Bottleneck2.prototype.RedisConnection = require$$2
				Bottleneck2.IORedisConnection = Bottleneck2.prototype.IORedisConnection = require$$3
				Bottleneck2.Batcher = Bottleneck2.prototype.Batcher = Batcher_1
				Bottleneck2.prototype.jobDefaults = {
					priority: DEFAULT_PRIORITY$1,
					weight: 1,
					expiration: null,
					id: "<no-id>",
				}
				Bottleneck2.prototype.storeDefaults = {
					maxConcurrent: null,
					minTime: 0,
					highWater: null,
					strategy: Bottleneck2.prototype.strategy.LEAK,
					penalty: null,
					reservoir: null,
					reservoirRefreshInterval: null,
					reservoirRefreshAmount: null,
					reservoirIncreaseInterval: null,
					reservoirIncreaseAmount: null,
					reservoirIncreaseMaximum: null,
				}
				Bottleneck2.prototype.localStoreDefaults = {
					Promise,
					timeout: null,
					heartbeatInterval: 250,
				}
				Bottleneck2.prototype.redisStoreDefaults = {
					Promise,
					timeout: null,
					heartbeatInterval: 5e3,
					clientTimeout: 1e4,
					Redis: null,
					clientOptions: {},
					clusterNodes: null,
					clearDatastore: false,
					connection: null,
				}
				Bottleneck2.prototype.instanceDefaults = {
					datastore: "local",
					connection: null,
					id: "<no-id>",
					rejectOnDrop: true,
					trackDoneStatus: false,
					Promise,
				}
				Bottleneck2.prototype.stopDefaults = {
					enqueueErrorMessage: "This limiter has been stopped and cannot accept new jobs.",
					dropWaitingJobs: true,
					dropErrorMessage: "This limiter has been stopped.",
				}
				return Bottleneck2
			}.call(commonjsGlobal)
			var Bottleneck_1 = Bottleneck
			var lib = Bottleneck_1
			return lib
		})
	},
})

// ../../../node_modules/.pnpm/@octokit+plugin-retry@6.0.1_@octokit+core@5.2.0/node_modules/@octokit/plugin-retry/dist-node/index.js
var require_dist_node12 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+plugin-retry@6.0.1_@octokit+core@5.2.0/node_modules/@octokit/plugin-retry/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __create2 = Object.create
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __getProtoOf2 = Object.getPrototypeOf
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toESM2 = (mod, isNodeMode, target) => (
			(target = mod != undefined ? __create2(__getProtoOf2(mod)) : {}),
			__copyProps2(
				// If the importer is in node compatibility mode or this is not an ESM
				// file that has been converted to a CommonJS file using a Babel-
				// compatible transform (i.e. "__esModule" has not been set), then set
				// "default" to the CommonJS "module.exports" for node compatibility.
				isNodeMode || !mod || !mod.__esModule
					? __defProp2(target, "default", { value: mod, enumerable: true })
					: target,
				mod
			)
		)
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			VERSION: () => VERSION,
			retry: () => retry,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var import_core = require_dist_node8()
		async function errorRequest(state, octokit, error, options) {
			if (!error.request || !error.request.request) {
				throw error
			}
			if (error.status >= 400 && !state.doNotRetry.includes(error.status)) {
				const retries =
					options.request.retries != undefined ? options.request.retries : state.retries
				const retryAfter = Math.pow((options.request.retryCount || 0) + 1, 2)
				throw octokit.retry.retryRequest(error, retries, retryAfter)
			}
			throw error
		}
		var import_light = __toESM2(require_light())
		var import_request_error = require_dist_node4()
		async function wrapRequest(state, octokit, request, options) {
			const limiter = new import_light.default()
			limiter.on("failed", function (error, info) {
				const maxRetries2 = ~~error.request.request.retries
				const after = ~~error.request.request.retryAfter
				options.request.retryCount = info.retryCount + 1
				if (maxRetries2 > info.retryCount) {
					return after * state.retryAfterBaseValue
				}
			})
			return limiter.schedule(
				requestWithGraphqlErrorHandling.bind(null, state, octokit, request),
				options
			)
		}
		async function requestWithGraphqlErrorHandling(state, octokit, request, options) {
			const response = await request(request, options)
			if (
				response.data &&
				response.data.errors &&
				/Something went wrong while executing your query/.test(response.data.errors[0].message)
			) {
				const error = new import_request_error.RequestError(response.data.errors[0].message, 500, {
					request: options,
					response,
				})
				return errorRequest(state, octokit, error, options)
			}
			return response
		}
		var VERSION = "6.0.1"
		function retry(octokit, octokitOptions) {
			const state = Object.assign(
				{
					enabled: true,
					retryAfterBaseValue: 1e3,
					doNotRetry: [400, 401, 403, 404, 422, 451],
					retries: 3,
				},
				octokitOptions.retry
			)
			if (state.enabled) {
				octokit.hook.error("request", errorRequest.bind(null, state, octokit))
				octokit.hook.wrap("request", wrapRequest.bind(null, state, octokit))
			}
			return {
				retry: {
					retryRequest: (error, retries, retryAfter) => {
						error.request.request = Object.assign({}, error.request.request, {
							retries,
							retryAfter,
						})
						return error
					},
				},
			}
		}
		retry.VERSION = VERSION
	},
})

// ../../../node_modules/.pnpm/@octokit+plugin-throttling@8.2.0_@octokit+core@5.2.0/node_modules/@octokit/plugin-throttling/dist-node/index.js
var require_dist_node13 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+plugin-throttling@8.2.0_@octokit+core@5.2.0/node_modules/@octokit/plugin-throttling/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __create2 = Object.create
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __getProtoOf2 = Object.getPrototypeOf
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toESM2 = (mod, isNodeMode, target) => (
			(target = mod != undefined ? __create2(__getProtoOf2(mod)) : {}),
			__copyProps2(
				// If the importer is in node compatibility mode or this is not an ESM
				// file that has been converted to a CommonJS file using a Babel-
				// compatible transform (i.e. "__esModule" has not been set), then set
				// "default" to the CommonJS "module.exports" for node compatibility.
				isNodeMode || !mod || !mod.__esModule
					? __defProp2(target, "default", { value: mod, enumerable: true })
					: target,
				mod
			)
		)
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			throttling: () => throttling,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var import_light = __toESM2(require_light())
		var import_core = require_dist_node8()
		var VERSION = "8.2.0"
		var noop = () => Promise.resolve()
		function wrapRequest(state, request, options) {
			return state.retryLimiter.schedule(doRequest, state, request, options)
		}
		async function doRequest(state, request, options) {
			const isWrite = options.method !== "GET" && options.method !== "HEAD"
			const { pathname } = new URL(options.url, "http://github.test")
			const isSearch = options.method === "GET" && pathname.startsWith("/search/")
			const isGraphQL = pathname.startsWith("/graphql")
			const retryCount = ~~request.retryCount
			const jobOptions = retryCount > 0 ? { priority: 0, weight: 0 } : {}
			if (state.clustering) {
				jobOptions.expiration = 1e3 * 60
			}
			if (isWrite || isGraphQL) {
				await state.write.key(state.id).schedule(jobOptions, noop)
			}
			if (isWrite && state.triggersNotification(pathname)) {
				await state.notifications.key(state.id).schedule(jobOptions, noop)
			}
			if (isSearch) {
				await state.search.key(state.id).schedule(jobOptions, noop)
			}
			const req = state.global.key(state.id).schedule(jobOptions, request, options)
			if (isGraphQL) {
				const res = await req
				if (
					res.data.errors != undefined &&
					res.data.errors.some((error) => error.type === "RATE_LIMITED")
				) {
					const error = Object.assign(new Error("GraphQL Rate Limit Exceeded"), {
						response: res,
						data: res.data,
					})
					throw error
				}
			}
			return req
		}
		var triggers_notification_paths_default = [
			"/orgs/{org}/invitations",
			"/orgs/{org}/invitations/{invitation_id}",
			"/orgs/{org}/teams/{team_slug}/discussions",
			"/orgs/{org}/teams/{team_slug}/discussions/{discussion_number}/comments",
			"/repos/{owner}/{repo}/collaborators/{username}",
			"/repos/{owner}/{repo}/commits/{commit_sha}/comments",
			"/repos/{owner}/{repo}/issues",
			"/repos/{owner}/{repo}/issues/{issue_number}/comments",
			"/repos/{owner}/{repo}/pulls",
			"/repos/{owner}/{repo}/pulls/{pull_number}/comments",
			"/repos/{owner}/{repo}/pulls/{pull_number}/comments/{comment_id}/replies",
			"/repos/{owner}/{repo}/pulls/{pull_number}/merge",
			"/repos/{owner}/{repo}/pulls/{pull_number}/requested_reviewers",
			"/repos/{owner}/{repo}/pulls/{pull_number}/reviews",
			"/repos/{owner}/{repo}/releases",
			"/teams/{team_id}/discussions",
			"/teams/{team_id}/discussions/{discussion_number}/comments",
		]
		function routeMatcher(paths) {
			const regexes = paths.map((path) =>
				path
					.split("/")
					.map((c) => (c.startsWith("{") ? "(?:.+?)" : c))
					.join("/")
			)
			const regex2 = `^(?:${regexes.map((r) => `(?:${r})`).join("|")})[^/]*$`
			return new RegExp(regex2, "i")
		}
		var regex = routeMatcher(triggers_notification_paths_default)
		var triggersNotification = regex.test.bind(regex)
		var groups = {}
		var createGroups = function (Bottleneck, common) {
			groups.global = new Bottleneck.Group({
				id: "octokit-global",
				maxConcurrent: 10,
				...common,
			})
			groups.search = new Bottleneck.Group({
				id: "octokit-search",
				maxConcurrent: 1,
				minTime: 2e3,
				...common,
			})
			groups.write = new Bottleneck.Group({
				id: "octokit-write",
				maxConcurrent: 1,
				minTime: 1e3,
				...common,
			})
			groups.notifications = new Bottleneck.Group({
				id: "octokit-notifications",
				maxConcurrent: 1,
				minTime: 3e3,
				...common,
			})
		}
		function throttling(octokit, octokitOptions) {
			const {
				enabled = true,
				Bottleneck = import_light.default,
				id = "no-id",
				timeout = 1e3 * 60 * 2,
				// Redis TTL: 2 minutes
				connection,
			} = octokitOptions.throttle || {}
			if (!enabled) {
				return {}
			}
			const common = { connection, timeout }
			if (groups.global == undefined) {
				createGroups(Bottleneck, common)
			}
			const state = Object.assign(
				{
					clustering: connection != undefined,
					triggersNotification,
					fallbackSecondaryRateRetryAfter: 60,
					retryAfterBaseValue: 1e3,
					retryLimiter: new Bottleneck(),
					id,
					...groups,
				},
				octokitOptions.throttle
			)
			if (
				typeof state.onSecondaryRateLimit !== "function" ||
				typeof state.onRateLimit !== "function"
			) {
				throw new Error(`octokit/plugin-throttling error:
        You must pass the onSecondaryRateLimit and onRateLimit error handlers.
        See https://octokit.github.io/rest.js/#throttling

        const octokit = new Octokit({
          throttle: {
            onSecondaryRateLimit: (retryAfter, options) => {/* ... */},
            onRateLimit: (retryAfter, options) => {/* ... */}
          }
        })
    `)
			}
			const events = {}
			const emitter = new Bottleneck.Events(events)
			events.on("secondary-limit", state.onSecondaryRateLimit)
			events.on("rate-limit", state.onRateLimit)
			events.on("error", (e) => octokit.log.warn("Error in throttling-plugin limit handler", e))
			state.retryLimiter.on("failed", async function (error, info) {
				const [state2, request, options] = info.args
				const { pathname } = new URL(options.url, "http://github.test")
				const shouldRetryGraphQL = pathname.startsWith("/graphql") && error.status !== 401
				if (!(shouldRetryGraphQL || error.status === 403)) {
					return
				}
				const retryCount = ~~request.retryCount
				request.retryCount = retryCount
				options.request.retryCount = retryCount
				const { wantRetry, retryAfter = 0 } = await (async function () {
					if (/\bsecondary rate\b/i.test(error.message)) {
						const retryAfter2 =
							Number(error.response.headers["retry-after"]) ||
							state2.fallbackSecondaryRateRetryAfter
						const wantRetry2 = await emitter.trigger(
							"secondary-limit",
							retryAfter2,
							options,
							octokit,
							retryCount
						)
						return { wantRetry: wantRetry2, retryAfter: retryAfter2 }
					}
					if (
						(error.response.headers != undefined &&
							error.response.headers["x-ratelimit-remaining"] === "0") ||
						(error.response.data?.errors ?? []).some((error2) => error2.type === "RATE_LIMITED")
					) {
						const rateLimitReset = new Date(
							~~error.response.headers["x-ratelimit-reset"] * 1e3
						).getTime()
						const retryAfter2 = Math.max(
							// Add one second so we retry _after_ the reset time
							// https://docs.github.com/en/rest/overview/resources-in-the-rest-api?apiVersion=2022-11-28#exceeding-the-rate-limit
							Math.ceil((rateLimitReset - Date.now()) / 1e3) + 1,
							0
						)
						const wantRetry2 = await emitter.trigger(
							"rate-limit",
							retryAfter2,
							options,
							octokit,
							retryCount
						)
						return { wantRetry: wantRetry2, retryAfter: retryAfter2 }
					}
					return {}
				})()
				if (wantRetry) {
					request.retryCount++
					return retryAfter * state2.retryAfterBaseValue
				}
			})
			octokit.hook.wrap("request", wrapRequest.bind(null, state))
			return {}
		}
		throttling.VERSION = VERSION
		throttling.triggersNotification = triggersNotification
	},
})

// ../../../node_modules/.pnpm/btoa-lite@1.0.0/node_modules/btoa-lite/btoa-node.js
var require_btoa_node = __commonJS({
	"../../../node_modules/.pnpm/btoa-lite@1.0.0/node_modules/btoa-lite/btoa-node.js"(
		exports2,
		module2
	) {
		module2.exports = function btoa(str) {
			return new Buffer(str).toString("base64")
		}
	},
})

// ../../../node_modules/.pnpm/@octokit+oauth-authorization-url@6.0.2/node_modules/@octokit/oauth-authorization-url/dist-node/index.js
var require_dist_node14 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+oauth-authorization-url@6.0.2/node_modules/@octokit/oauth-authorization-url/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			oauthAuthorizationUrl: () => oauthAuthorizationUrl,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		function oauthAuthorizationUrl(options) {
			const clientType = options.clientType || "oauth-app"
			const baseUrl = options.baseUrl || "https://github.com"
			const result = {
				clientType,
				allowSignup: options.allowSignup === false ? false : true,
				clientId: options.clientId,
				login: options.login || null,
				redirectUrl: options.redirectUrl || null,
				state: options.state || Math.random().toString(36).slice(2),
				url: "",
			}
			if (clientType === "oauth-app") {
				const scopes = "scopes" in options ? options.scopes : []
				result.scopes = typeof scopes === "string" ? scopes.split(/[,\s]+/).filter(Boolean) : scopes
			}
			result.url = urlBuilderAuthorize(`${baseUrl}/login/oauth/authorize`, result)
			return result
		}
		function urlBuilderAuthorize(base, options) {
			const map = {
				allowSignup: "allow_signup",
				clientId: "client_id",
				login: "login",
				redirectUrl: "redirect_uri",
				scopes: "scope",
				state: "state",
			}
			let url = base
			for (const [index2, [key, value]] of Object.keys(map)
				.filter((k) => options[k] !== null)
				.filter((k) => {
					if (k !== "scopes") return true
					if (options.clientType === "github-app") return false
					return !Array.isArray(options[k]) || options[k].length > 0
				})
				.map((key) => [map[key], `${options[key]}`])
				.entries()) {
				url += index2 === 0 ? `?` : "&"
				url += `${key}=${encodeURIComponent(value)}`
			}
			return url
		}
	},
})

// ../../../node_modules/.pnpm/@octokit+oauth-methods@4.1.0/node_modules/@octokit/oauth-methods/dist-node/index.js
var require_dist_node15 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+oauth-methods@4.1.0/node_modules/@octokit/oauth-methods/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __create2 = Object.create
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __getProtoOf2 = Object.getPrototypeOf
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toESM2 = (mod, isNodeMode, target) => (
			(target = mod != undefined ? __create2(__getProtoOf2(mod)) : {}),
			__copyProps2(
				// If the importer is in node compatibility mode or this is not an ESM
				// file that has been converted to a CommonJS file using a Babel-
				// compatible transform (i.e. "__esModule" has not been set), then set
				// "default" to the CommonJS "module.exports" for node compatibility.
				isNodeMode || !mod || !mod.__esModule
					? __defProp2(target, "default", { value: mod, enumerable: true })
					: target,
				mod
			)
		)
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			VERSION: () => VERSION,
			checkToken: () => checkToken,
			createDeviceCode: () => createDeviceCode,
			deleteAuthorization: () => deleteAuthorization,
			deleteToken: () => deleteToken,
			exchangeDeviceCode: () => exchangeDeviceCode,
			exchangeWebFlowCode: () => exchangeWebFlowCode,
			getWebFlowAuthorizationUrl: () => getWebFlowAuthorizationUrl,
			refreshToken: () => refreshToken,
			resetToken: () => resetToken,
			scopeToken: () => scopeToken,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var VERSION = "4.1.0"
		var import_oauth_authorization_url = require_dist_node14()
		var import_request = require_dist_node5()
		var import_request_error = require_dist_node4()
		function requestToOAuthBaseUrl(request) {
			const endpointDefaults = request.endpoint.DEFAULTS
			return /^https:\/\/(api\.)?github\.com$/.test(endpointDefaults.baseUrl)
				? "https://github.com"
				: endpointDefaults.baseUrl.replace("/api/v3", "")
		}
		async function oauthRequest(request, route, parameters) {
			const withOAuthParameters = {
				baseUrl: requestToOAuthBaseUrl(request),
				headers: {
					accept: "application/json",
				},
				...parameters,
			}
			const response = await request(route, withOAuthParameters)
			if ("error" in response.data) {
				const error = new import_request_error.RequestError(
					`${response.data.error_description} (${response.data.error}, ${response.data.error_uri})`,
					400,
					{
						request: request.endpoint.merge(route, withOAuthParameters),
						headers: response.headers,
					}
				)
				error.response = response
				throw error
			}
			return response
		}
		function getWebFlowAuthorizationUrl({ request = import_request.request, ...options }) {
			const baseUrl = requestToOAuthBaseUrl(request)
			return (0, import_oauth_authorization_url.oauthAuthorizationUrl)({
				...options,
				baseUrl,
			})
		}
		var import_request2 = require_dist_node5()
		async function exchangeWebFlowCode(options) {
			const request =
				options.request /* istanbul ignore next: we always pass a custom request in tests */ ||
				import_request2.request
			const response = await oauthRequest(request, "POST /login/oauth/access_token", {
				client_id: options.clientId,
				client_secret: options.clientSecret,
				code: options.code,
				redirect_uri: options.redirectUrl,
			})
			const authentication = {
				clientType: options.clientType,
				clientId: options.clientId,
				clientSecret: options.clientSecret,
				token: response.data.access_token,
				scopes: response.data.scope.split(/\s+/).filter(Boolean),
			}
			if (options.clientType === "github-app") {
				if ("refresh_token" in response.data) {
					const apiTimeInMs = new Date(response.headers.date).getTime()
					;(authentication.refreshToken = response.data.refresh_token),
						(authentication.expiresAt = toTimestamp(apiTimeInMs, response.data.expires_in)),
						(authentication.refreshTokenExpiresAt = toTimestamp(
							apiTimeInMs,
							response.data.refresh_token_expires_in
						))
				}
				delete authentication.scopes
			}
			return { ...response, authentication }
		}
		function toTimestamp(apiTimeInMs, expirationInSeconds) {
			return new Date(apiTimeInMs + expirationInSeconds * 1e3).toISOString()
		}
		var import_request3 = require_dist_node5()
		async function createDeviceCode(options) {
			const request =
				options.request /* istanbul ignore next: we always pass a custom request in tests */ ||
				import_request3.request
			const parameters = {
				client_id: options.clientId,
			}
			if ("scopes" in options && Array.isArray(options.scopes)) {
				parameters.scope = options.scopes.join(" ")
			}
			return oauthRequest(request, "POST /login/device/code", parameters)
		}
		var import_request4 = require_dist_node5()
		async function exchangeDeviceCode(options) {
			const request =
				options.request /* istanbul ignore next: we always pass a custom request in tests */ ||
				import_request4.request
			const response = await oauthRequest(request, "POST /login/oauth/access_token", {
				client_id: options.clientId,
				device_code: options.code,
				grant_type: "urn:ietf:params:oauth:grant-type:device_code",
			})
			const authentication = {
				clientType: options.clientType,
				clientId: options.clientId,
				token: response.data.access_token,
				scopes: response.data.scope.split(/\s+/).filter(Boolean),
			}
			if ("clientSecret" in options) {
				authentication.clientSecret = options.clientSecret
			}
			if (options.clientType === "github-app") {
				if ("refresh_token" in response.data) {
					const apiTimeInMs = new Date(response.headers.date).getTime()
					;(authentication.refreshToken = response.data.refresh_token),
						(authentication.expiresAt = toTimestamp2(apiTimeInMs, response.data.expires_in)),
						(authentication.refreshTokenExpiresAt = toTimestamp2(
							apiTimeInMs,
							response.data.refresh_token_expires_in
						))
				}
				delete authentication.scopes
			}
			return { ...response, authentication }
		}
		function toTimestamp2(apiTimeInMs, expirationInSeconds) {
			return new Date(apiTimeInMs + expirationInSeconds * 1e3).toISOString()
		}
		var import_request5 = require_dist_node5()
		var import_btoa_lite = __toESM2(require_btoa_node())
		async function checkToken(options) {
			const request =
				options.request /* istanbul ignore next: we always pass a custom request in tests */ ||
				import_request5.request
			const response = await request("POST /applications/{client_id}/token", {
				headers: {
					authorization: `basic ${(0, import_btoa_lite.default)(
						`${options.clientId}:${options.clientSecret}`
					)}`,
				},
				client_id: options.clientId,
				access_token: options.token,
			})
			const authentication = {
				clientType: options.clientType,
				clientId: options.clientId,
				clientSecret: options.clientSecret,
				token: options.token,
				scopes: response.data.scopes,
			}
			if (response.data.expires_at) authentication.expiresAt = response.data.expires_at
			if (options.clientType === "github-app") {
				delete authentication.scopes
			}
			return { ...response, authentication }
		}
		var import_request6 = require_dist_node5()
		async function refreshToken(options) {
			const request =
				options.request /* istanbul ignore next: we always pass a custom request in tests */ ||
				import_request6.request
			const response = await oauthRequest(request, "POST /login/oauth/access_token", {
				client_id: options.clientId,
				client_secret: options.clientSecret,
				grant_type: "refresh_token",
				refresh_token: options.refreshToken,
			})
			const apiTimeInMs = new Date(response.headers.date).getTime()
			const authentication = {
				clientType: "github-app",
				clientId: options.clientId,
				clientSecret: options.clientSecret,
				token: response.data.access_token,
				refreshToken: response.data.refresh_token,
				expiresAt: toTimestamp3(apiTimeInMs, response.data.expires_in),
				refreshTokenExpiresAt: toTimestamp3(apiTimeInMs, response.data.refresh_token_expires_in),
			}
			return { ...response, authentication }
		}
		function toTimestamp3(apiTimeInMs, expirationInSeconds) {
			return new Date(apiTimeInMs + expirationInSeconds * 1e3).toISOString()
		}
		var import_request7 = require_dist_node5()
		var import_btoa_lite2 = __toESM2(require_btoa_node())
		async function scopeToken(options) {
			const {
				request: optionsRequest,
				clientType,
				clientId,
				clientSecret,
				token,
				...requestOptions
			} = options
			const request =
				optionsRequest /* istanbul ignore next: we always pass a custom request in tests */ ||
				import_request7.request
			const response = await request("POST /applications/{client_id}/token/scoped", {
				headers: {
					authorization: `basic ${(0, import_btoa_lite2.default)(`${clientId}:${clientSecret}`)}`,
				},
				client_id: clientId,
				access_token: token,
				...requestOptions,
			})
			const authentication = Object.assign(
				{
					clientType,
					clientId,
					clientSecret,
					token: response.data.token,
				},
				response.data.expires_at ? { expiresAt: response.data.expires_at } : {}
			)
			return { ...response, authentication }
		}
		var import_request8 = require_dist_node5()
		var import_btoa_lite3 = __toESM2(require_btoa_node())
		async function resetToken(options) {
			const request =
				options.request /* istanbul ignore next: we always pass a custom request in tests */ ||
				import_request8.request
			const auth = (0, import_btoa_lite3.default)(`${options.clientId}:${options.clientSecret}`)
			const response = await request("PATCH /applications/{client_id}/token", {
				headers: {
					authorization: `basic ${auth}`,
				},
				client_id: options.clientId,
				access_token: options.token,
			})
			const authentication = {
				clientType: options.clientType,
				clientId: options.clientId,
				clientSecret: options.clientSecret,
				token: response.data.token,
				scopes: response.data.scopes,
			}
			if (response.data.expires_at) authentication.expiresAt = response.data.expires_at
			if (options.clientType === "github-app") {
				delete authentication.scopes
			}
			return { ...response, authentication }
		}
		var import_request9 = require_dist_node5()
		var import_btoa_lite4 = __toESM2(require_btoa_node())
		async function deleteToken(options) {
			const request =
				options.request /* istanbul ignore next: we always pass a custom request in tests */ ||
				import_request9.request
			const auth = (0, import_btoa_lite4.default)(`${options.clientId}:${options.clientSecret}`)
			return request("DELETE /applications/{client_id}/token", {
				headers: {
					authorization: `basic ${auth}`,
				},
				client_id: options.clientId,
				access_token: options.token,
			})
		}
		var import_request10 = require_dist_node5()
		var import_btoa_lite5 = __toESM2(require_btoa_node())
		async function deleteAuthorization(options) {
			const request =
				options.request /* istanbul ignore next: we always pass a custom request in tests */ ||
				import_request10.request
			const auth = (0, import_btoa_lite5.default)(`${options.clientId}:${options.clientSecret}`)
			return request("DELETE /applications/{client_id}/grant", {
				headers: {
					authorization: `basic ${auth}`,
				},
				client_id: options.clientId,
				access_token: options.token,
			})
		}
	},
})

// ../../../node_modules/.pnpm/@octokit+auth-oauth-device@6.1.0/node_modules/@octokit/auth-oauth-device/dist-node/index.js
var require_dist_node16 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+auth-oauth-device@6.1.0/node_modules/@octokit/auth-oauth-device/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			createOAuthDeviceAuth: () => createOAuthDeviceAuth,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var import_universal_user_agent = require_dist_node()
		var import_request = require_dist_node5()
		var import_oauth_methods = require_dist_node15()
		async function getOAuthAccessToken(state, options) {
			const cachedAuthentication = getCachedAuthentication(state, options.auth)
			if (cachedAuthentication) return cachedAuthentication
			const { data: verification } = await (0, import_oauth_methods.createDeviceCode)({
				clientType: state.clientType,
				clientId: state.clientId,
				request: options.request || state.request,
				// @ts-expect-error the extra code to make TS happy is not worth it
				scopes: options.auth.scopes || state.scopes,
			})
			await state.onVerification(verification)
			const authentication = await waitForAccessToken(
				options.request || state.request,
				state.clientId,
				state.clientType,
				verification
			)
			state.authentication = authentication
			return authentication
		}
		function getCachedAuthentication(state, auth2) {
			if (auth2.refresh === true) return false
			if (!state.authentication) return false
			if (state.clientType === "github-app") {
				return state.authentication
			}
			const authentication = state.authentication
			const newScope = (("scopes" in auth2 && auth2.scopes) || state.scopes).join(" ")
			const currentScope = authentication.scopes.join(" ")
			return newScope === currentScope ? authentication : false
		}
		async function wait(seconds) {
			await new Promise((resolve) => setTimeout(resolve, seconds * 1e3))
		}
		async function waitForAccessToken(request, clientId, clientType, verification) {
			try {
				const options = {
					clientId,
					request,
					code: verification.device_code,
				}
				const { authentication } =
					clientType === "oauth-app"
						? await (0, import_oauth_methods.exchangeDeviceCode)({
								...options,
								clientType: "oauth-app",
						  })
						: await (0, import_oauth_methods.exchangeDeviceCode)({
								...options,
								clientType: "github-app",
						  })
				return {
					type: "token",
					tokenType: "oauth",
					...authentication,
				}
			} catch (error) {
				if (!error.response) throw error
				const errorType = error.response.data.error
				if (errorType === "authorization_pending") {
					await wait(verification.interval)
					return waitForAccessToken(request, clientId, clientType, verification)
				}
				if (errorType === "slow_down") {
					await wait(verification.interval + 5)
					return waitForAccessToken(request, clientId, clientType, verification)
				}
				throw error
			}
		}
		async function auth(state, authOptions) {
			return getOAuthAccessToken(state, {
				auth: authOptions,
			})
		}
		async function hook(state, request, route, parameters) {
			let endpoint = request.endpoint.merge(route, parameters)
			if (/\/login\/(oauth\/access_token|device\/code)$/.test(endpoint.url)) {
				return request(endpoint)
			}
			const { token } = await getOAuthAccessToken(state, {
				request,
				auth: { type: "oauth" },
			})
			endpoint.headers.authorization = `token ${token}`
			return request(endpoint)
		}
		var VERSION = "6.1.0"
		function createOAuthDeviceAuth(options) {
			const requestWithDefaults =
				options.request ||
				import_request.request.defaults({
					headers: {
						"user-agent": `octokit-auth-oauth-device.js/${VERSION} ${(0,
						import_universal_user_agent.getUserAgent)()}`,
					},
				})
			const { request = requestWithDefaults, ...otherOptions } = options
			const state =
				options.clientType === "github-app"
					? {
							...otherOptions,
							clientType: "github-app",
							request,
					  }
					: {
							...otherOptions,
							clientType: "oauth-app",
							request,
							scopes: options.scopes || [],
					  }
			if (!options.clientId) {
				throw new Error(
					'[@octokit/auth-oauth-device] "clientId" option must be set (https://github.com/octokit/auth-oauth-device.js#usage)'
				)
			}
			if (!options.onVerification) {
				throw new Error(
					'[@octokit/auth-oauth-device] "onVerification" option must be a function (https://github.com/octokit/auth-oauth-device.js#usage)'
				)
			}
			return Object.assign(auth.bind(null, state), {
				hook: hook.bind(null, state),
			})
		}
	},
})

// ../../../node_modules/.pnpm/@octokit+auth-oauth-user@4.1.0/node_modules/@octokit/auth-oauth-user/dist-node/index.js
var require_dist_node17 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+auth-oauth-user@4.1.0/node_modules/@octokit/auth-oauth-user/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __create2 = Object.create
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __getProtoOf2 = Object.getPrototypeOf
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toESM2 = (mod, isNodeMode, target) => (
			(target = mod != undefined ? __create2(__getProtoOf2(mod)) : {}),
			__copyProps2(
				// If the importer is in node compatibility mode or this is not an ESM
				// file that has been converted to a CommonJS file using a Babel-
				// compatible transform (i.e. "__esModule" has not been set), then set
				// "default" to the CommonJS "module.exports" for node compatibility.
				isNodeMode || !mod || !mod.__esModule
					? __defProp2(target, "default", { value: mod, enumerable: true })
					: target,
				mod
			)
		)
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			createOAuthUserAuth: () => createOAuthUserAuth2,
			requiresBasicAuth: () => requiresBasicAuth,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var import_universal_user_agent = require_dist_node()
		var import_request = require_dist_node5()
		var VERSION = "4.1.0"
		var import_auth_oauth_device = require_dist_node16()
		var import_oauth_methods = require_dist_node15()
		async function getAuthentication(state) {
			if ("code" in state.strategyOptions) {
				const { authentication } = await (0, import_oauth_methods.exchangeWebFlowCode)({
					clientId: state.clientId,
					clientSecret: state.clientSecret,
					clientType: state.clientType,
					onTokenCreated: state.onTokenCreated,
					...state.strategyOptions,
					request: state.request,
				})
				return {
					type: "token",
					tokenType: "oauth",
					...authentication,
				}
			}
			if ("onVerification" in state.strategyOptions) {
				const deviceAuth = (0, import_auth_oauth_device.createOAuthDeviceAuth)({
					clientType: state.clientType,
					clientId: state.clientId,
					onTokenCreated: state.onTokenCreated,
					...state.strategyOptions,
					request: state.request,
				})
				const authentication = await deviceAuth({
					type: "oauth",
				})
				return {
					clientSecret: state.clientSecret,
					...authentication,
				}
			}
			if ("token" in state.strategyOptions) {
				return {
					type: "token",
					tokenType: "oauth",
					clientId: state.clientId,
					clientSecret: state.clientSecret,
					clientType: state.clientType,
					onTokenCreated: state.onTokenCreated,
					...state.strategyOptions,
				}
			}
			throw new Error("[@octokit/auth-oauth-user] Invalid strategy options")
		}
		var import_oauth_methods2 = require_dist_node15()
		async function auth(state, options = {}) {
			var _a, _b
			if (!state.authentication) {
				state.authentication =
					state.clientType === "oauth-app"
						? await getAuthentication(state)
						: await getAuthentication(state)
			}
			if (state.authentication.invalid) {
				throw new Error("[@octokit/auth-oauth-user] Token is invalid")
			}
			const currentAuthentication = state.authentication
			if (
				"expiresAt" in currentAuthentication &&
				(options.type === "refresh" ||
					new Date(currentAuthentication.expiresAt) < /* @__PURE__ */ new Date())
			) {
				const { authentication } = await (0, import_oauth_methods2.refreshToken)({
					clientType: "github-app",
					clientId: state.clientId,
					clientSecret: state.clientSecret,
					refreshToken: currentAuthentication.refreshToken,
					request: state.request,
				})
				state.authentication = {
					tokenType: "oauth",
					type: "token",
					...authentication,
				}
			}
			if (options.type === "refresh") {
				if (state.clientType === "oauth-app") {
					throw new Error("[@octokit/auth-oauth-user] OAuth Apps do not support expiring tokens")
				}
				if (!currentAuthentication.hasOwnProperty("expiresAt")) {
					throw new Error("[@octokit/auth-oauth-user] Refresh token missing")
				}
				await ((_a = state.onTokenCreated) == undefined
					? void 0
					: _a.call(state, state.authentication, {
							type: options.type,
					  }))
			}
			if (options.type === "check" || options.type === "reset") {
				const method =
					options.type === "check"
						? import_oauth_methods2.checkToken
						: import_oauth_methods2.resetToken
				try {
					const { authentication } = await method({
						// @ts-expect-error making TS happy would require unnecessary code so no
						clientType: state.clientType,
						clientId: state.clientId,
						clientSecret: state.clientSecret,
						token: state.authentication.token,
						request: state.request,
					})
					state.authentication = {
						tokenType: "oauth",
						type: "token",
						// @ts-expect-error TBD
						...authentication,
					}
					if (options.type === "reset") {
						await ((_b = state.onTokenCreated) == undefined
							? void 0
							: _b.call(state, state.authentication, {
									type: options.type,
							  }))
					}
					return state.authentication
				} catch (error) {
					if (error.status === 404) {
						error.message = "[@octokit/auth-oauth-user] Token is invalid"
						state.authentication.invalid = true
					}
					throw error
				}
			}
			if (options.type === "delete" || options.type === "deleteAuthorization") {
				const method =
					options.type === "delete"
						? import_oauth_methods2.deleteToken
						: import_oauth_methods2.deleteAuthorization
				try {
					await method({
						// @ts-expect-error making TS happy would require unnecessary code so no
						clientType: state.clientType,
						clientId: state.clientId,
						clientSecret: state.clientSecret,
						token: state.authentication.token,
						request: state.request,
					})
				} catch (error) {
					if (error.status !== 404) throw error
				}
				state.authentication.invalid = true
				return state.authentication
			}
			return state.authentication
		}
		var import_btoa_lite = __toESM2(require_btoa_node())
		var ROUTES_REQUIRING_BASIC_AUTH = /\/applications\/[^/]+\/(token|grant)s?/
		function requiresBasicAuth(url) {
			return url && ROUTES_REQUIRING_BASIC_AUTH.test(url)
		}
		async function hook(state, request, route, parameters = {}) {
			const endpoint = request.endpoint.merge(route, parameters)
			if (/\/login\/(oauth\/access_token|device\/code)$/.test(endpoint.url)) {
				return request(endpoint)
			}
			if (requiresBasicAuth(endpoint.url)) {
				const credentials = (0, import_btoa_lite.default)(`${state.clientId}:${state.clientSecret}`)
				endpoint.headers.authorization = `basic ${credentials}`
				return request(endpoint)
			}
			const { token } =
				state.clientType === "oauth-app"
					? await auth({ ...state, request })
					: await auth({ ...state, request })
			endpoint.headers.authorization = "token " + token
			return request(endpoint)
		}
		function createOAuthUserAuth2({
			clientId,
			clientSecret,
			clientType = "oauth-app",
			request = import_request.request.defaults({
				headers: {
					"user-agent": `octokit-auth-oauth-app.js/${VERSION} ${(0,
					import_universal_user_agent.getUserAgent)()}`,
				},
			}),
			onTokenCreated,
			...strategyOptions
		}) {
			const state = Object.assign({
				clientType,
				clientId,
				clientSecret,
				onTokenCreated,
				strategyOptions,
				request,
			})
			return Object.assign(auth.bind(null, state), {
				// @ts-expect-error not worth the extra code needed to appease TS
				hook: hook.bind(null, state),
			})
		}
		createOAuthUserAuth2.VERSION = VERSION
	},
})

// ../../../node_modules/.pnpm/@octokit+auth-oauth-app@7.1.0/node_modules/@octokit/auth-oauth-app/dist-node/index.js
var require_dist_node18 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+auth-oauth-app@7.1.0/node_modules/@octokit/auth-oauth-app/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __create2 = Object.create
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __getProtoOf2 = Object.getPrototypeOf
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toESM2 = (mod, isNodeMode, target) => (
			(target = mod != undefined ? __create2(__getProtoOf2(mod)) : {}),
			__copyProps2(
				// If the importer is in node compatibility mode or this is not an ESM
				// file that has been converted to a CommonJS file using a Babel-
				// compatible transform (i.e. "__esModule" has not been set), then set
				// "default" to the CommonJS "module.exports" for node compatibility.
				isNodeMode || !mod || !mod.__esModule
					? __defProp2(target, "default", { value: mod, enumerable: true })
					: target,
				mod
			)
		)
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			createOAuthAppAuth: () => createOAuthAppAuth,
			createOAuthUserAuth: () => import_auth_oauth_user3.createOAuthUserAuth,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var import_universal_user_agent = require_dist_node()
		var import_request = require_dist_node5()
		var import_btoa_lite = __toESM2(require_btoa_node())
		var import_auth_oauth_user = require_dist_node17()
		async function auth(state, authOptions) {
			if (authOptions.type === "oauth-app") {
				return {
					type: "oauth-app",
					clientId: state.clientId,
					clientSecret: state.clientSecret,
					clientType: state.clientType,
					headers: {
						authorization: `basic ${(0, import_btoa_lite.default)(
							`${state.clientId}:${state.clientSecret}`
						)}`,
					},
				}
			}
			if ("factory" in authOptions) {
				const { type, ...options } = {
					...authOptions,
					...state,
				}
				return authOptions.factory(options)
			}
			const common = {
				clientId: state.clientId,
				clientSecret: state.clientSecret,
				request: state.request,
				...authOptions,
			}
			const userAuth =
				state.clientType === "oauth-app"
					? await (0, import_auth_oauth_user.createOAuthUserAuth)({
							...common,
							clientType: state.clientType,
					  })
					: await (0, import_auth_oauth_user.createOAuthUserAuth)({
							...common,
							clientType: state.clientType,
					  })
			return userAuth()
		}
		var import_btoa_lite2 = __toESM2(require_btoa_node())
		var import_auth_oauth_user2 = require_dist_node17()
		async function hook(state, request2, route, parameters) {
			let endpoint = request2.endpoint.merge(route, parameters)
			if (/\/login\/(oauth\/access_token|device\/code)$/.test(endpoint.url)) {
				return request2(endpoint)
			}
			if (
				state.clientType === "github-app" &&
				!(0, import_auth_oauth_user2.requiresBasicAuth)(endpoint.url)
			) {
				throw new Error(
					`[@octokit/auth-oauth-app] GitHub Apps cannot use their client ID/secret for basic authentication for endpoints other than "/applications/{client_id}/**". "${endpoint.method} ${endpoint.url}" is not supported.`
				)
			}
			const credentials = (0, import_btoa_lite2.default)(`${state.clientId}:${state.clientSecret}`)
			endpoint.headers.authorization = `basic ${credentials}`
			try {
				return await request2(endpoint)
			} catch (error) {
				if (error.status !== 401) throw error
				error.message = `[@octokit/auth-oauth-app] "${endpoint.method} ${endpoint.url}" does not support clientId/clientSecret basic authentication.`
				throw error
			}
		}
		var VERSION = "7.1.0"
		var import_auth_oauth_user3 = require_dist_node17()
		function createOAuthAppAuth(options) {
			const state = Object.assign(
				{
					request: import_request.request.defaults({
						headers: {
							"user-agent": `octokit-auth-oauth-app.js/${VERSION} ${(0,
							import_universal_user_agent.getUserAgent)()}`,
						},
					}),
					clientType: "oauth-app",
				},
				options
			)
			return Object.assign(auth.bind(null, state), {
				hook: hook.bind(null, state),
			})
		}
	},
})

// ../../../node_modules/.pnpm/jws@3.2.2/node_modules/jws/lib/data-stream.js
var require_data_stream = __commonJS({
	"../../../node_modules/.pnpm/jws@3.2.2/node_modules/jws/lib/data-stream.js"(exports2, module2) {
		var Buffer2 = require_safe_buffer().Buffer
		var Stream = require("node:stream")
		var util = require("node:util")
		function DataStream(data) {
			this.buffer = null
			this.writable = true
			this.readable = true
			if (!data) {
				this.buffer = Buffer2.alloc(0)
				return this
			}
			if (typeof data.pipe === "function") {
				this.buffer = Buffer2.alloc(0)
				data.pipe(this)
				return this
			}
			if (data.length || typeof data === "object") {
				this.buffer = data
				this.writable = false
				process.nextTick(
					function () {
						this.emit("end", data)
						this.readable = false
						this.emit("close")
					}.bind(this)
				)
				return this
			}
			throw new TypeError("Unexpected data type (" + typeof data + ")")
		}
		util.inherits(DataStream, Stream)
		DataStream.prototype.write = function write(data) {
			this.buffer = Buffer2.concat([this.buffer, Buffer2.from(data)])
			this.emit("data", data)
		}
		DataStream.prototype.end = function end(data) {
			if (data) this.write(data)
			this.emit("end", data)
			this.emit("close")
			this.writable = false
			this.readable = false
		}
		module2.exports = DataStream
	},
})

// ../../../node_modules/.pnpm/buffer-equal-constant-time@1.0.1/node_modules/buffer-equal-constant-time/index.js
var require_buffer_equal_constant_time = __commonJS({
	"../../../node_modules/.pnpm/buffer-equal-constant-time@1.0.1/node_modules/buffer-equal-constant-time/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var Buffer2 = require("node:buffer").Buffer
		var SlowBuffer = require("node:buffer").SlowBuffer
		module2.exports = bufferEq
		function bufferEq(a, b) {
			if (!Buffer2.isBuffer(a) || !Buffer2.isBuffer(b)) {
				return false
			}
			if (a.length !== b.length) {
				return false
			}
			var c = 0
			for (const [i, element] of a.entries()) {
				c |= element ^ b[i]
			}
			return c === 0
		}
		bufferEq.install = function () {
			Buffer2.prototype.equal = SlowBuffer.prototype.equal = function equal(that) {
				return bufferEq(this, that)
			}
		}
		var origBufEqual = Buffer2.prototype.equal
		var origSlowBufEqual = SlowBuffer.prototype.equal
		bufferEq.restore = function () {
			Buffer2.prototype.equal = origBufEqual
			SlowBuffer.prototype.equal = origSlowBufEqual
		}
	},
})

// ../../../node_modules/.pnpm/ecdsa-sig-formatter@1.0.11/node_modules/ecdsa-sig-formatter/src/param-bytes-for-alg.js
var require_param_bytes_for_alg = __commonJS({
	"../../../node_modules/.pnpm/ecdsa-sig-formatter@1.0.11/node_modules/ecdsa-sig-formatter/src/param-bytes-for-alg.js"(
		exports2,
		module2
	) {
		"use strict"
		function getParamSize(keySize) {
			var result = ((keySize / 8) | 0) + (keySize % 8 === 0 ? 0 : 1)
			return result
		}
		var paramBytesForAlg = {
			ES256: getParamSize(256),
			ES384: getParamSize(384),
			ES512: getParamSize(521),
		}
		function getParamBytesForAlg(alg) {
			var paramBytes = paramBytesForAlg[alg]
			if (paramBytes) {
				return paramBytes
			}
			throw new Error('Unknown algorithm "' + alg + '"')
		}
		module2.exports = getParamBytesForAlg
	},
})

// ../../../node_modules/.pnpm/ecdsa-sig-formatter@1.0.11/node_modules/ecdsa-sig-formatter/src/ecdsa-sig-formatter.js
var require_ecdsa_sig_formatter = __commonJS({
	"../../../node_modules/.pnpm/ecdsa-sig-formatter@1.0.11/node_modules/ecdsa-sig-formatter/src/ecdsa-sig-formatter.js"(
		exports2,
		module2
	) {
		"use strict"
		var Buffer2 = require_safe_buffer().Buffer
		var getParamBytesForAlg = require_param_bytes_for_alg()
		var MAX_OCTET = 128
		var CLASS_UNIVERSAL = 0
		var PRIMITIVE_BIT = 32
		var TAG_SEQ = 16
		var TAG_INT = 2
		var ENCODED_TAG_SEQ = TAG_SEQ | PRIMITIVE_BIT | (CLASS_UNIVERSAL << 6)
		var ENCODED_TAG_INT = TAG_INT | (CLASS_UNIVERSAL << 6)
		function base64Url(base64) {
			return base64.replace(/=/g, "").replace(/\+/g, "-").replace(/\//g, "_")
		}
		function signatureAsBuffer(signature) {
			if (Buffer2.isBuffer(signature)) {
				return signature
			} else if ("string" === typeof signature) {
				return Buffer2.from(signature, "base64")
			}
			throw new TypeError("ECDSA signature must be a Base64 string or a Buffer")
		}
		function derToJose(signature, alg) {
			signature = signatureAsBuffer(signature)
			var paramBytes = getParamBytesForAlg(alg)
			var maxEncodedParamLength = paramBytes + 1
			var inputLength = signature.length
			var offset = 0
			if (signature[offset++] !== ENCODED_TAG_SEQ) {
				throw new Error('Could not find expected "seq"')
			}
			var seqLength = signature[offset++]
			if (seqLength === (MAX_OCTET | 1)) {
				seqLength = signature[offset++]
			}
			if (inputLength - offset < seqLength) {
				throw new Error(
					'"seq" specified length of "' +
						seqLength +
						'", only "' +
						(inputLength - offset) +
						'" remaining'
				)
			}
			if (signature[offset++] !== ENCODED_TAG_INT) {
				throw new Error('Could not find expected "int" for "r"')
			}
			var rLength = signature[offset++]
			if (inputLength - offset - 2 < rLength) {
				throw new Error(
					'"r" specified length of "' +
						rLength +
						'", only "' +
						(inputLength - offset - 2) +
						'" available'
				)
			}
			if (maxEncodedParamLength < rLength) {
				throw new Error(
					'"r" specified length of "' +
						rLength +
						'", max of "' +
						maxEncodedParamLength +
						'" is acceptable'
				)
			}
			var rOffset = offset
			offset += rLength
			if (signature[offset++] !== ENCODED_TAG_INT) {
				throw new Error('Could not find expected "int" for "s"')
			}
			var sLength = signature[offset++]
			if (inputLength - offset !== sLength) {
				throw new Error(
					'"s" specified length of "' + sLength + '", expected "' + (inputLength - offset) + '"'
				)
			}
			if (maxEncodedParamLength < sLength) {
				throw new Error(
					'"s" specified length of "' +
						sLength +
						'", max of "' +
						maxEncodedParamLength +
						'" is acceptable'
				)
			}
			var sOffset = offset
			offset += sLength
			if (offset !== inputLength) {
				throw new Error(
					'Expected to consume entire buffer, but "' + (inputLength - offset) + '" bytes remain'
				)
			}
			var rPadding = paramBytes - rLength,
				sPadding = paramBytes - sLength
			var dst = Buffer2.allocUnsafe(rPadding + rLength + sPadding + sLength)
			for (offset = 0; offset < rPadding; ++offset) {
				dst[offset] = 0
			}
			signature.copy(dst, offset, rOffset + Math.max(-rPadding, 0), rOffset + rLength)
			offset = paramBytes
			for (var o = offset; offset < o + sPadding; ++offset) {
				dst[offset] = 0
			}
			signature.copy(dst, offset, sOffset + Math.max(-sPadding, 0), sOffset + sLength)
			dst = dst.toString("base64")
			dst = base64Url(dst)
			return dst
		}
		function countPadding(buf, start, stop) {
			var padding = 0
			while (start + padding < stop && buf[start + padding] === 0) {
				++padding
			}
			var needsSign = buf[start + padding] >= MAX_OCTET
			if (needsSign) {
				--padding
			}
			return padding
		}
		function joseToDer(signature, alg) {
			signature = signatureAsBuffer(signature)
			var paramBytes = getParamBytesForAlg(alg)
			var signatureBytes = signature.length
			if (signatureBytes !== paramBytes * 2) {
				throw new TypeError(
					'"' +
						alg +
						'" signatures must be "' +
						paramBytes * 2 +
						'" bytes, saw "' +
						signatureBytes +
						'"'
				)
			}
			var rPadding = countPadding(signature, 0, paramBytes)
			var sPadding = countPadding(signature, paramBytes, signature.length)
			var rLength = paramBytes - rPadding
			var sLength = paramBytes - sPadding
			var rsBytes = 1 + 1 + rLength + 1 + 1 + sLength
			var shortLength = rsBytes < MAX_OCTET
			var dst = Buffer2.allocUnsafe((shortLength ? 2 : 3) + rsBytes)
			var offset = 0
			dst[offset++] = ENCODED_TAG_SEQ
			if (shortLength) {
				dst[offset++] = rsBytes
			} else {
				dst[offset++] = MAX_OCTET | 1
				dst[offset++] = rsBytes & 255
			}
			dst[offset++] = ENCODED_TAG_INT
			dst[offset++] = rLength
			if (rPadding < 0) {
				dst[offset++] = 0
				offset += signature.copy(dst, offset, 0, paramBytes)
			} else {
				offset += signature.copy(dst, offset, rPadding, paramBytes)
			}
			dst[offset++] = ENCODED_TAG_INT
			dst[offset++] = sLength
			if (sPadding < 0) {
				dst[offset++] = 0
				signature.copy(dst, offset, paramBytes)
			} else {
				signature.copy(dst, offset, paramBytes + sPadding)
			}
			return dst
		}
		module2.exports = {
			derToJose,
			joseToDer,
		}
	},
})

// ../../../node_modules/.pnpm/jwa@1.4.1/node_modules/jwa/index.js
var require_jwa = __commonJS({
	"../../../node_modules/.pnpm/jwa@1.4.1/node_modules/jwa/index.js"(exports2, module2) {
		var bufferEqual = require_buffer_equal_constant_time()
		var Buffer2 = require_safe_buffer().Buffer
		var crypto5 = require("node:crypto")
		var formatEcdsa = require_ecdsa_sig_formatter()
		var util = require("node:util")
		var MSG_INVALID_ALGORITHM =
			'"%s" is not a valid algorithm.\n  Supported algorithms are:\n  "HS256", "HS384", "HS512", "RS256", "RS384", "RS512", "PS256", "PS384", "PS512", "ES256", "ES384", "ES512" and "none".'
		var MSG_INVALID_SECRET = "secret must be a string or buffer"
		var MSG_INVALID_VERIFIER_KEY = "key must be a string or a buffer"
		var MSG_INVALID_SIGNER_KEY = "key must be a string, a buffer or an object"
		var supportsKeyObjects = typeof crypto5.createPublicKey === "function"
		if (supportsKeyObjects) {
			MSG_INVALID_VERIFIER_KEY += " or a KeyObject"
			MSG_INVALID_SECRET += "or a KeyObject"
		}
		function checkIsPublicKey(key) {
			if (Buffer2.isBuffer(key)) {
				return
			}
			if (typeof key === "string") {
				return
			}
			if (!supportsKeyObjects) {
				throw typeError(MSG_INVALID_VERIFIER_KEY)
			}
			if (typeof key !== "object") {
				throw typeError(MSG_INVALID_VERIFIER_KEY)
			}
			if (typeof key.type !== "string") {
				throw typeError(MSG_INVALID_VERIFIER_KEY)
			}
			if (typeof key.asymmetricKeyType !== "string") {
				throw typeError(MSG_INVALID_VERIFIER_KEY)
			}
			if (typeof key.export !== "function") {
				throw typeError(MSG_INVALID_VERIFIER_KEY)
			}
		}
		function checkIsPrivateKey(key) {
			if (Buffer2.isBuffer(key)) {
				return
			}
			if (typeof key === "string") {
				return
			}
			if (typeof key === "object") {
				return
			}
			throw typeError(MSG_INVALID_SIGNER_KEY)
		}
		function checkIsSecretKey(key) {
			if (Buffer2.isBuffer(key)) {
				return
			}
			if (typeof key === "string") {
				return key
			}
			if (!supportsKeyObjects) {
				throw typeError(MSG_INVALID_SECRET)
			}
			if (typeof key !== "object") {
				throw typeError(MSG_INVALID_SECRET)
			}
			if (key.type !== "secret") {
				throw typeError(MSG_INVALID_SECRET)
			}
			if (typeof key.export !== "function") {
				throw typeError(MSG_INVALID_SECRET)
			}
		}
		function fromBase64(base64) {
			return base64.replace(/=/g, "").replace(/\+/g, "-").replace(/\//g, "_")
		}
		function toBase64(base64url) {
			base64url = base64url.toString()
			var padding = 4 - (base64url.length % 4)
			if (padding !== 4) {
				for (var i = 0; i < padding; ++i) {
					base64url += "="
				}
			}
			return base64url.replace(/\-/g, "+").replace(/_/g, "/")
		}
		function typeError(template) {
			var args = [].slice.call(arguments, 1)
			var errMsg = util.format.bind(util, template).apply(null, args)
			return new TypeError(errMsg)
		}
		function bufferOrString(obj) {
			return Buffer2.isBuffer(obj) || typeof obj === "string"
		}
		function normalizeInput(thing) {
			if (!bufferOrString(thing)) thing = JSON.stringify(thing)
			return thing
		}
		function createHmacSigner(bits) {
			return function sign(thing, secret) {
				checkIsSecretKey(secret)
				thing = normalizeInput(thing)
				var hmac = crypto5.createHmac("sha" + bits, secret)
				var sig = (hmac.update(thing), hmac.digest("base64"))
				return fromBase64(sig)
			}
		}
		function createHmacVerifier(bits) {
			return function verify(thing, signature, secret) {
				var computedSig = createHmacSigner(bits)(thing, secret)
				return bufferEqual(Buffer2.from(signature), Buffer2.from(computedSig))
			}
		}
		function createKeySigner(bits) {
			return function sign(thing, privateKey) {
				checkIsPrivateKey(privateKey)
				thing = normalizeInput(thing)
				var signer = crypto5.createSign("RSA-SHA" + bits)
				var sig = (signer.update(thing), signer.sign(privateKey, "base64"))
				return fromBase64(sig)
			}
		}
		function createKeyVerifier(bits) {
			return function verify(thing, signature, publicKey) {
				checkIsPublicKey(publicKey)
				thing = normalizeInput(thing)
				signature = toBase64(signature)
				var verifier = crypto5.createVerify("RSA-SHA" + bits)
				verifier.update(thing)
				return verifier.verify(publicKey, signature, "base64")
			}
		}
		function createPSSKeySigner(bits) {
			return function sign(thing, privateKey) {
				checkIsPrivateKey(privateKey)
				thing = normalizeInput(thing)
				var signer = crypto5.createSign("RSA-SHA" + bits)
				var sig =
					(signer.update(thing),
					signer.sign(
						{
							key: privateKey,
							padding: crypto5.constants.RSA_PKCS1_PSS_PADDING,
							saltLength: crypto5.constants.RSA_PSS_SALTLEN_DIGEST,
						},
						"base64"
					))
				return fromBase64(sig)
			}
		}
		function createPSSKeyVerifier(bits) {
			return function verify(thing, signature, publicKey) {
				checkIsPublicKey(publicKey)
				thing = normalizeInput(thing)
				signature = toBase64(signature)
				var verifier = crypto5.createVerify("RSA-SHA" + bits)
				verifier.update(thing)
				return verifier.verify(
					{
						key: publicKey,
						padding: crypto5.constants.RSA_PKCS1_PSS_PADDING,
						saltLength: crypto5.constants.RSA_PSS_SALTLEN_DIGEST,
					},
					signature,
					"base64"
				)
			}
		}
		function createECDSASigner(bits) {
			var inner = createKeySigner(bits)
			return function sign() {
				var signature = inner.apply(null, arguments)
				signature = formatEcdsa.derToJose(signature, "ES" + bits)
				return signature
			}
		}
		function createECDSAVerifer(bits) {
			var inner = createKeyVerifier(bits)
			return function verify(thing, signature, publicKey) {
				signature = formatEcdsa.joseToDer(signature, "ES" + bits).toString("base64")
				var result = inner(thing, signature, publicKey)
				return result
			}
		}
		function createNoneSigner() {
			return function sign() {
				return ""
			}
		}
		function createNoneVerifier() {
			return function verify(thing, signature) {
				return signature === ""
			}
		}
		module2.exports = function jwa(algorithm) {
			var signerFactories = {
				hs: createHmacSigner,
				rs: createKeySigner,
				ps: createPSSKeySigner,
				es: createECDSASigner,
				none: createNoneSigner,
			}
			var verifierFactories = {
				hs: createHmacVerifier,
				rs: createKeyVerifier,
				ps: createPSSKeyVerifier,
				es: createECDSAVerifer,
				none: createNoneVerifier,
			}
			var match = algorithm.match(/^(RS|PS|ES|HS)(256|384|512)$|^(none)$/i)
			if (!match) throw typeError(MSG_INVALID_ALGORITHM, algorithm)
			var algo = (match[1] || match[3]).toLowerCase()
			var bits = match[2]
			return {
				sign: signerFactories[algo](bits),
				verify: verifierFactories[algo](bits),
			}
		}
	},
})

// ../../../node_modules/.pnpm/jws@3.2.2/node_modules/jws/lib/tostring.js
var require_tostring = __commonJS({
	"../../../node_modules/.pnpm/jws@3.2.2/node_modules/jws/lib/tostring.js"(exports2, module2) {
		var Buffer2 = require("node:buffer").Buffer
		module2.exports = function toString(obj) {
			if (typeof obj === "string") return obj
			if (typeof obj === "number" || Buffer2.isBuffer(obj)) return obj.toString()
			return JSON.stringify(obj)
		}
	},
})

// ../../../node_modules/.pnpm/jws@3.2.2/node_modules/jws/lib/sign-stream.js
var require_sign_stream = __commonJS({
	"../../../node_modules/.pnpm/jws@3.2.2/node_modules/jws/lib/sign-stream.js"(exports2, module2) {
		var Buffer2 = require_safe_buffer().Buffer
		var DataStream = require_data_stream()
		var jwa = require_jwa()
		var Stream = require("node:stream")
		var toString = require_tostring()
		var util = require("node:util")
		function base64url(string, encoding) {
			return Buffer2.from(string, encoding)
				.toString("base64")
				.replace(/=/g, "")
				.replace(/\+/g, "-")
				.replace(/\//g, "_")
		}
		function jwsSecuredInput(header, payload, encoding) {
			encoding = encoding || "utf8"
			var encodedHeader = base64url(toString(header), "binary")
			var encodedPayload = base64url(toString(payload), encoding)
			return util.format("%s.%s", encodedHeader, encodedPayload)
		}
		function jwsSign(opts) {
			var header = opts.header
			var payload = opts.payload
			var secretOrKey = opts.secret || opts.privateKey
			var encoding = opts.encoding
			var algo = jwa(header.alg)
			var securedInput = jwsSecuredInput(header, payload, encoding)
			var signature = algo.sign(securedInput, secretOrKey)
			return util.format("%s.%s", securedInput, signature)
		}
		function SignStream(opts) {
			var secret = opts.secret || opts.privateKey || opts.key
			var secretStream = new DataStream(secret)
			this.readable = true
			this.header = opts.header
			this.encoding = opts.encoding
			this.secret = this.privateKey = this.key = secretStream
			this.payload = new DataStream(opts.payload)
			this.secret.once(
				"close",
				function () {
					if (!this.payload.writable && this.readable) this.sign()
				}.bind(this)
			)
			this.payload.once(
				"close",
				function () {
					if (!this.secret.writable && this.readable) this.sign()
				}.bind(this)
			)
		}
		util.inherits(SignStream, Stream)
		SignStream.prototype.sign = function sign() {
			try {
				var signature = jwsSign({
					header: this.header,
					payload: this.payload.buffer,
					secret: this.secret.buffer,
					encoding: this.encoding,
				})
				this.emit("done", signature)
				this.emit("data", signature)
				this.emit("end")
				this.readable = false
				return signature
			} catch (e) {
				this.readable = false
				this.emit("error", e)
				this.emit("close")
			}
		}
		SignStream.sign = jwsSign
		module2.exports = SignStream
	},
})

// ../../../node_modules/.pnpm/jws@3.2.2/node_modules/jws/lib/verify-stream.js
var require_verify_stream = __commonJS({
	"../../../node_modules/.pnpm/jws@3.2.2/node_modules/jws/lib/verify-stream.js"(exports2, module2) {
		var Buffer2 = require_safe_buffer().Buffer
		var DataStream = require_data_stream()
		var jwa = require_jwa()
		var Stream = require("node:stream")
		var toString = require_tostring()
		var util = require("node:util")
		var JWS_REGEX = /^[a-zA-Z0-9\-_]+?\.[a-zA-Z0-9\-_]+?\.([a-zA-Z0-9\-_]+)?$/
		function isObject2(thing) {
			return Object.prototype.toString.call(thing) === "[object Object]"
		}
		function safeJsonParse(thing) {
			if (isObject2(thing)) return thing
			try {
				return JSON.parse(thing)
			} catch (e) {
				return void 0
			}
		}
		function headerFromJWS(jwsSig) {
			var encodedHeader = jwsSig.split(".", 1)[0]
			return safeJsonParse(Buffer2.from(encodedHeader, "base64").toString("binary"))
		}
		function securedInputFromJWS(jwsSig) {
			return jwsSig.split(".", 2).join(".")
		}
		function signatureFromJWS(jwsSig) {
			return jwsSig.split(".")[2]
		}
		function payloadFromJWS(jwsSig, encoding) {
			encoding = encoding || "utf8"
			var payload = jwsSig.split(".")[1]
			return Buffer2.from(payload, "base64").toString(encoding)
		}
		function isValidJws(string) {
			return JWS_REGEX.test(string) && !!headerFromJWS(string)
		}
		function jwsVerify(jwsSig, algorithm, secretOrKey) {
			if (!algorithm) {
				var err = new Error("Missing algorithm parameter for jws.verify")
				err.code = "MISSING_ALGORITHM"
				throw err
			}
			jwsSig = toString(jwsSig)
			var signature = signatureFromJWS(jwsSig)
			var securedInput = securedInputFromJWS(jwsSig)
			var algo = jwa(algorithm)
			return algo.verify(securedInput, signature, secretOrKey)
		}
		function jwsDecode(jwsSig, opts) {
			opts = opts || {}
			jwsSig = toString(jwsSig)
			if (!isValidJws(jwsSig)) return null
			var header = headerFromJWS(jwsSig)
			if (!header) return null
			var payload = payloadFromJWS(jwsSig)
			if (header.typ === "JWT" || opts.json) payload = JSON.parse(payload, opts.encoding)
			return {
				header,
				payload,
				signature: signatureFromJWS(jwsSig),
			}
		}
		function VerifyStream(opts) {
			opts = opts || {}
			var secretOrKey = opts.secret || opts.publicKey || opts.key
			var secretStream = new DataStream(secretOrKey)
			this.readable = true
			this.algorithm = opts.algorithm
			this.encoding = opts.encoding
			this.secret = this.publicKey = this.key = secretStream
			this.signature = new DataStream(opts.signature)
			this.secret.once(
				"close",
				function () {
					if (!this.signature.writable && this.readable) this.verify()
				}.bind(this)
			)
			this.signature.once(
				"close",
				function () {
					if (!this.secret.writable && this.readable) this.verify()
				}.bind(this)
			)
		}
		util.inherits(VerifyStream, Stream)
		VerifyStream.prototype.verify = function verify() {
			try {
				var valid = jwsVerify(this.signature.buffer, this.algorithm, this.key.buffer)
				var obj = jwsDecode(this.signature.buffer, this.encoding)
				this.emit("done", valid, obj)
				this.emit("data", valid)
				this.emit("end")
				this.readable = false
				return valid
			} catch (e) {
				this.readable = false
				this.emit("error", e)
				this.emit("close")
			}
		}
		VerifyStream.decode = jwsDecode
		VerifyStream.isValid = isValidJws
		VerifyStream.verify = jwsVerify
		module2.exports = VerifyStream
	},
})

// ../../../node_modules/.pnpm/jws@3.2.2/node_modules/jws/index.js
var require_jws = __commonJS({
	"../../../node_modules/.pnpm/jws@3.2.2/node_modules/jws/index.js"(exports2) {
		var SignStream = require_sign_stream()
		var VerifyStream = require_verify_stream()
		var ALGORITHMS = [
			"HS256",
			"HS384",
			"HS512",
			"RS256",
			"RS384",
			"RS512",
			"PS256",
			"PS384",
			"PS512",
			"ES256",
			"ES384",
			"ES512",
		]
		exports2.ALGORITHMS = ALGORITHMS
		exports2.sign = SignStream.sign
		exports2.verify = VerifyStream.verify
		exports2.decode = VerifyStream.decode
		exports2.isValid = VerifyStream.isValid
		exports2.createSign = function createSign(opts) {
			return new SignStream(opts)
		}
		exports2.createVerify = function createVerify(opts) {
			return new VerifyStream(opts)
		}
	},
})

// ../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/decode.js
var require_decode = __commonJS({
	"../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/decode.js"(
		exports2,
		module2
	) {
		var jws = require_jws()
		module2.exports = function (jwt, options) {
			options = options || {}
			var decoded = jws.decode(jwt, options)
			if (!decoded) {
				return null
			}
			var payload = decoded.payload
			if (typeof payload === "string") {
				try {
					var obj = JSON.parse(payload)
					if (obj !== null && typeof obj === "object") {
						payload = obj
					}
				} catch (e) {}
			}
			if (options.complete === true) {
				return {
					header: decoded.header,
					payload,
					signature: decoded.signature,
				}
			}
			return payload
		}
	},
})

// ../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/lib/JsonWebTokenError.js
var require_JsonWebTokenError = __commonJS({
	"../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/lib/JsonWebTokenError.js"(
		exports2,
		module2
	) {
		var JsonWebTokenError = function (message, error) {
			Error.call(this, message)
			if (Error.captureStackTrace) {
				Error.captureStackTrace(this, this.constructor)
			}
			this.name = "JsonWebTokenError"
			this.message = message
			if (error) this.inner = error
		}
		JsonWebTokenError.prototype = Object.create(Error.prototype)
		JsonWebTokenError.prototype.constructor = JsonWebTokenError
		module2.exports = JsonWebTokenError
	},
})

// ../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/lib/NotBeforeError.js
var require_NotBeforeError = __commonJS({
	"../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/lib/NotBeforeError.js"(
		exports2,
		module2
	) {
		var JsonWebTokenError = require_JsonWebTokenError()
		var NotBeforeError = function (message, date) {
			JsonWebTokenError.call(this, message)
			this.name = "NotBeforeError"
			this.date = date
		}
		NotBeforeError.prototype = Object.create(JsonWebTokenError.prototype)
		NotBeforeError.prototype.constructor = NotBeforeError
		module2.exports = NotBeforeError
	},
})

// ../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/lib/TokenExpiredError.js
var require_TokenExpiredError = __commonJS({
	"../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/lib/TokenExpiredError.js"(
		exports2,
		module2
	) {
		var JsonWebTokenError = require_JsonWebTokenError()
		var TokenExpiredError = function (message, expiredAt) {
			JsonWebTokenError.call(this, message)
			this.name = "TokenExpiredError"
			this.expiredAt = expiredAt
		}
		TokenExpiredError.prototype = Object.create(JsonWebTokenError.prototype)
		TokenExpiredError.prototype.constructor = TokenExpiredError
		module2.exports = TokenExpiredError
	},
})

// ../../../node_modules/.pnpm/ms@2.1.3/node_modules/ms/index.js
var require_ms = __commonJS({
	"../../../node_modules/.pnpm/ms@2.1.3/node_modules/ms/index.js"(exports2, module2) {
		var s = 1e3
		var m = s * 60
		var h = m * 60
		var d = h * 24
		var w = d * 7
		var y = d * 365.25
		module2.exports = function (val, options) {
			options = options || {}
			var type = typeof val
			if (type === "string" && val.length > 0) {
				return parse2(val)
			} else if (type === "number" && isFinite(val)) {
				return options.long ? fmtLong(val) : fmtShort(val)
			}
			throw new Error("val is not a non-empty string or a valid number. val=" + JSON.stringify(val))
		}
		function parse2(str) {
			str = String(str)
			if (str.length > 100) {
				return
			}
			var match =
				/^(-?(?:\d+)?\.?\d+) *(milliseconds?|msecs?|ms|seconds?|secs?|s|minutes?|mins?|m|hours?|hrs?|h|days?|d|weeks?|w|years?|yrs?|y)?$/i.exec(
					str
				)
			if (!match) {
				return
			}
			var n = parseFloat(match[1])
			var type = (match[2] || "ms").toLowerCase()
			switch (type) {
				case "years":
				case "year":
				case "yrs":
				case "yr":
				case "y":
					return n * y
				case "weeks":
				case "week":
				case "w":
					return n * w
				case "days":
				case "day":
				case "d":
					return n * d
				case "hours":
				case "hour":
				case "hrs":
				case "hr":
				case "h":
					return n * h
				case "minutes":
				case "minute":
				case "mins":
				case "min":
				case "m":
					return n * m
				case "seconds":
				case "second":
				case "secs":
				case "sec":
				case "s":
					return n * s
				case "milliseconds":
				case "millisecond":
				case "msecs":
				case "msec":
				case "ms":
					return n
				default:
					return void 0
			}
		}
		function fmtShort(ms) {
			var msAbs = Math.abs(ms)
			if (msAbs >= d) {
				return Math.round(ms / d) + "d"
			}
			if (msAbs >= h) {
				return Math.round(ms / h) + "h"
			}
			if (msAbs >= m) {
				return Math.round(ms / m) + "m"
			}
			if (msAbs >= s) {
				return Math.round(ms / s) + "s"
			}
			return ms + "ms"
		}
		function fmtLong(ms) {
			var msAbs = Math.abs(ms)
			if (msAbs >= d) {
				return plural(ms, msAbs, d, "day")
			}
			if (msAbs >= h) {
				return plural(ms, msAbs, h, "hour")
			}
			if (msAbs >= m) {
				return plural(ms, msAbs, m, "minute")
			}
			if (msAbs >= s) {
				return plural(ms, msAbs, s, "second")
			}
			return ms + " ms"
		}
		function plural(ms, msAbs, n, name) {
			var isPlural = msAbs >= n * 1.5
			return Math.round(ms / n) + " " + name + (isPlural ? "s" : "")
		}
	},
})

// ../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/lib/timespan.js
var require_timespan = __commonJS({
	"../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/lib/timespan.js"(
		exports2,
		module2
	) {
		var ms = require_ms()
		module2.exports = function (time, iat) {
			var timestamp = iat || Math.floor(Date.now() / 1e3)
			if (typeof time === "string") {
				var milliseconds = ms(time)
				if (typeof milliseconds === "undefined") {
					return
				}
				return Math.floor(timestamp + milliseconds / 1e3)
			} else if (typeof time === "number") {
				return timestamp + time
			} else {
				return
			}
		}
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/internal/constants.js
var require_constants7 = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/internal/constants.js"(
		exports2,
		module2
	) {
		var SEMVER_SPEC_VERSION = "2.0.0"
		var MAX_LENGTH = 256
		var MAX_SAFE_INTEGER = Number.MAX_SAFE_INTEGER /* istanbul ignore next */ || 9007199254740991
		var MAX_SAFE_COMPONENT_LENGTH = 16
		var MAX_SAFE_BUILD_LENGTH = MAX_LENGTH - 6
		var RELEASE_TYPES = [
			"major",
			"premajor",
			"minor",
			"preminor",
			"patch",
			"prepatch",
			"prerelease",
		]
		module2.exports = {
			MAX_LENGTH,
			MAX_SAFE_COMPONENT_LENGTH,
			MAX_SAFE_BUILD_LENGTH,
			MAX_SAFE_INTEGER,
			RELEASE_TYPES,
			SEMVER_SPEC_VERSION,
			FLAG_INCLUDE_PRERELEASE: 1,
			FLAG_LOOSE: 2,
		}
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/internal/debug.js
var require_debug = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/internal/debug.js"(
		exports2,
		module2
	) {
		var debug10 =
			typeof process === "object" &&
			process.env &&
			process.env.NODE_DEBUG &&
			/\bsemver\b/i.test(process.env.NODE_DEBUG)
				? (...args) => console.error("SEMVER", ...args)
				: () => {}
		module2.exports = debug10
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/internal/re.js
var require_re = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/internal/re.js"(exports2, module2) {
		var { MAX_SAFE_COMPONENT_LENGTH, MAX_SAFE_BUILD_LENGTH, MAX_LENGTH } = require_constants7()
		var debug10 = require_debug()
		exports2 = module2.exports = {}
		var re = (exports2.re = [])
		var safeRe = (exports2.safeRe = [])
		var src = (exports2.src = [])
		var t = (exports2.t = {})
		var R = 0
		var LETTERDASHNUMBER = "[a-zA-Z0-9-]"
		var safeRegexReplacements = [
			["\\s", 1],
			["\\d", MAX_LENGTH],
			[LETTERDASHNUMBER, MAX_SAFE_BUILD_LENGTH],
		]
		var makeSafeRegex = (value) => {
			for (const [token, max] of safeRegexReplacements) {
				value = value
					.split(`${token}*`)
					.join(`${token}{0,${max}}`)
					.split(`${token}+`)
					.join(`${token}{1,${max}}`)
			}
			return value
		}
		var createToken = (name, value, isGlobal) => {
			const safe = makeSafeRegex(value)
			const index2 = R++
			debug10(name, index2, value)
			t[name] = index2
			src[index2] = value
			re[index2] = new RegExp(value, isGlobal ? "g" : void 0)
			safeRe[index2] = new RegExp(safe, isGlobal ? "g" : void 0)
		}
		createToken("NUMERICIDENTIFIER", "0|[1-9]\\d*")
		createToken("NUMERICIDENTIFIERLOOSE", "\\d+")
		createToken("NONNUMERICIDENTIFIER", `\\d*[a-zA-Z-]${LETTERDASHNUMBER}*`)
		createToken(
			"MAINVERSION",
			`(${src[t.NUMERICIDENTIFIER]})\\.(${src[t.NUMERICIDENTIFIER]})\\.(${
				src[t.NUMERICIDENTIFIER]
			})`
		)
		createToken(
			"MAINVERSIONLOOSE",
			`(${src[t.NUMERICIDENTIFIERLOOSE]})\\.(${src[t.NUMERICIDENTIFIERLOOSE]})\\.(${
				src[t.NUMERICIDENTIFIERLOOSE]
			})`
		)
		createToken(
			"PRERELEASEIDENTIFIER",
			`(?:${src[t.NUMERICIDENTIFIER]}|${src[t.NONNUMERICIDENTIFIER]})`
		)
		createToken(
			"PRERELEASEIDENTIFIERLOOSE",
			`(?:${src[t.NUMERICIDENTIFIERLOOSE]}|${src[t.NONNUMERICIDENTIFIER]})`
		)
		createToken(
			"PRERELEASE",
			`(?:-(${src[t.PRERELEASEIDENTIFIER]}(?:\\.${src[t.PRERELEASEIDENTIFIER]})*))`
		)
		createToken(
			"PRERELEASELOOSE",
			`(?:-?(${src[t.PRERELEASEIDENTIFIERLOOSE]}(?:\\.${src[t.PRERELEASEIDENTIFIERLOOSE]})*))`
		)
		createToken("BUILDIDENTIFIER", `${LETTERDASHNUMBER}+`)
		createToken("BUILD", `(?:\\+(${src[t.BUILDIDENTIFIER]}(?:\\.${src[t.BUILDIDENTIFIER]})*))`)
		createToken("FULLPLAIN", `v?${src[t.MAINVERSION]}${src[t.PRERELEASE]}?${src[t.BUILD]}?`)
		createToken("FULL", `^${src[t.FULLPLAIN]}$`)
		createToken(
			"LOOSEPLAIN",
			`[v=\\s]*${src[t.MAINVERSIONLOOSE]}${src[t.PRERELEASELOOSE]}?${src[t.BUILD]}?`
		)
		createToken("LOOSE", `^${src[t.LOOSEPLAIN]}$`)
		createToken("GTLT", "((?:<|>)?=?)")
		createToken("XRANGEIDENTIFIERLOOSE", `${src[t.NUMERICIDENTIFIERLOOSE]}|x|X|\\*`)
		createToken("XRANGEIDENTIFIER", `${src[t.NUMERICIDENTIFIER]}|x|X|\\*`)
		createToken(
			"XRANGEPLAIN",
			`[v=\\s]*(${src[t.XRANGEIDENTIFIER]})(?:\\.(${src[t.XRANGEIDENTIFIER]})(?:\\.(${
				src[t.XRANGEIDENTIFIER]
			})(?:${src[t.PRERELEASE]})?${src[t.BUILD]}?)?)?`
		)
		createToken(
			"XRANGEPLAINLOOSE",
			`[v=\\s]*(${src[t.XRANGEIDENTIFIERLOOSE]})(?:\\.(${src[t.XRANGEIDENTIFIERLOOSE]})(?:\\.(${
				src[t.XRANGEIDENTIFIERLOOSE]
			})(?:${src[t.PRERELEASELOOSE]})?${src[t.BUILD]}?)?)?`
		)
		createToken("XRANGE", `^${src[t.GTLT]}\\s*${src[t.XRANGEPLAIN]}$`)
		createToken("XRANGELOOSE", `^${src[t.GTLT]}\\s*${src[t.XRANGEPLAINLOOSE]}$`)
		createToken(
			"COERCEPLAIN",
			`${"(^|[^\\d])(\\d{1,"}${MAX_SAFE_COMPONENT_LENGTH}})(?:\\.(\\d{1,${MAX_SAFE_COMPONENT_LENGTH}}))?(?:\\.(\\d{1,${MAX_SAFE_COMPONENT_LENGTH}}))?`
		)
		createToken("COERCE", `${src[t.COERCEPLAIN]}(?:$|[^\\d])`)
		createToken(
			"COERCEFULL",
			src[t.COERCEPLAIN] + `(?:${src[t.PRERELEASE]})?(?:${src[t.BUILD]})?(?:$|[^\\d])`
		)
		createToken("COERCERTL", src[t.COERCE], true)
		createToken("COERCERTLFULL", src[t.COERCEFULL], true)
		createToken("LONETILDE", "(?:~>?)")
		createToken("TILDETRIM", `(\\s*)${src[t.LONETILDE]}\\s+`, true)
		exports2.tildeTrimReplace = "$1~"
		createToken("TILDE", `^${src[t.LONETILDE]}${src[t.XRANGEPLAIN]}$`)
		createToken("TILDELOOSE", `^${src[t.LONETILDE]}${src[t.XRANGEPLAINLOOSE]}$`)
		createToken("LONECARET", "(?:\\^)")
		createToken("CARETTRIM", `(\\s*)${src[t.LONECARET]}\\s+`, true)
		exports2.caretTrimReplace = "$1^"
		createToken("CARET", `^${src[t.LONECARET]}${src[t.XRANGEPLAIN]}$`)
		createToken("CARETLOOSE", `^${src[t.LONECARET]}${src[t.XRANGEPLAINLOOSE]}$`)
		createToken("COMPARATORLOOSE", `^${src[t.GTLT]}\\s*(${src[t.LOOSEPLAIN]})$|^$`)
		createToken("COMPARATOR", `^${src[t.GTLT]}\\s*(${src[t.FULLPLAIN]})$|^$`)
		createToken(
			"COMPARATORTRIM",
			`(\\s*)${src[t.GTLT]}\\s*(${src[t.LOOSEPLAIN]}|${src[t.XRANGEPLAIN]})`,
			true
		)
		exports2.comparatorTrimReplace = "$1$2$3"
		createToken("HYPHENRANGE", `^\\s*(${src[t.XRANGEPLAIN]})\\s+-\\s+(${src[t.XRANGEPLAIN]})\\s*$`)
		createToken(
			"HYPHENRANGELOOSE",
			`^\\s*(${src[t.XRANGEPLAINLOOSE]})\\s+-\\s+(${src[t.XRANGEPLAINLOOSE]})\\s*$`
		)
		createToken("STAR", "(<|>)?=?\\s*\\*")
		createToken("GTE0", "^\\s*>=\\s*0\\.0\\.0\\s*$")
		createToken("GTE0PRE", "^\\s*>=\\s*0\\.0\\.0-0\\s*$")
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/internal/parse-options.js
var require_parse_options = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/internal/parse-options.js"(
		exports2,
		module2
	) {
		var looseOption = Object.freeze({ loose: true })
		var emptyOpts = Object.freeze({})
		var parseOptions = (options) => {
			if (!options) {
				return emptyOpts
			}
			if (typeof options !== "object") {
				return looseOption
			}
			return options
		}
		module2.exports = parseOptions
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/internal/identifiers.js
var require_identifiers = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/internal/identifiers.js"(
		exports2,
		module2
	) {
		var numeric = /^[0-9]+$/
		var compareIdentifiers = (a, b) => {
			const anum = numeric.test(a)
			const bnum = numeric.test(b)
			if (anum && bnum) {
				a = +a
				b = +b
			}
			return a === b ? 0 : anum && !bnum ? -1 : bnum && !anum ? 1 : a < b ? -1 : 1
		}
		var rcompareIdentifiers = (a, b) => compareIdentifiers(b, a)
		module2.exports = {
			compareIdentifiers,
			rcompareIdentifiers,
		}
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/classes/semver.js
var require_semver = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/classes/semver.js"(
		exports2,
		module2
	) {
		var debug10 = require_debug()
		var { MAX_LENGTH, MAX_SAFE_INTEGER } = require_constants7()
		var { safeRe: re, t } = require_re()
		var parseOptions = require_parse_options()
		var { compareIdentifiers } = require_identifiers()
		var SemVer = class _SemVer {
			constructor(version3, options) {
				options = parseOptions(options)
				if (version3 instanceof _SemVer) {
					if (
						version3.loose === !!options.loose &&
						version3.includePrerelease === !!options.includePrerelease
					) {
						return version3
					} else {
						version3 = version3.version
					}
				} else if (typeof version3 !== "string") {
					throw new TypeError(`Invalid version. Must be a string. Got type "${typeof version3}".`)
				}
				if (version3.length > MAX_LENGTH) {
					throw new TypeError(`version is longer than ${MAX_LENGTH} characters`)
				}
				debug10("SemVer", version3, options)
				this.options = options
				this.loose = !!options.loose
				this.includePrerelease = !!options.includePrerelease
				const m = version3.trim().match(options.loose ? re[t.LOOSE] : re[t.FULL])
				if (!m) {
					throw new TypeError(`Invalid Version: ${version3}`)
				}
				this.raw = version3
				this.major = +m[1]
				this.minor = +m[2]
				this.patch = +m[3]
				if (this.major > MAX_SAFE_INTEGER || this.major < 0) {
					throw new TypeError("Invalid major version")
				}
				if (this.minor > MAX_SAFE_INTEGER || this.minor < 0) {
					throw new TypeError("Invalid minor version")
				}
				if (this.patch > MAX_SAFE_INTEGER || this.patch < 0) {
					throw new TypeError("Invalid patch version")
				}
				if (!m[4]) {
					this.prerelease = []
				} else {
					this.prerelease = m[4].split(".").map((id) => {
						if (/^[0-9]+$/.test(id)) {
							const num2 = +id
							if (num2 >= 0 && num2 < MAX_SAFE_INTEGER) {
								return num2
							}
						}
						return id
					})
				}
				this.build = m[5] ? m[5].split(".") : []
				this.format()
			}
			format() {
				this.version = `${this.major}.${this.minor}.${this.patch}`
				if (this.prerelease.length) {
					this.version += `-${this.prerelease.join(".")}`
				}
				return this.version
			}
			toString() {
				return this.version
			}
			compare(other) {
				debug10("SemVer.compare", this.version, this.options, other)
				if (!(other instanceof _SemVer)) {
					if (typeof other === "string" && other === this.version) {
						return 0
					}
					other = new _SemVer(other, this.options)
				}
				if (other.version === this.version) {
					return 0
				}
				return this.compareMain(other) || this.comparePre(other)
			}
			compareMain(other) {
				if (!(other instanceof _SemVer)) {
					other = new _SemVer(other, this.options)
				}
				return (
					compareIdentifiers(this.major, other.major) ||
					compareIdentifiers(this.minor, other.minor) ||
					compareIdentifiers(this.patch, other.patch)
				)
			}
			comparePre(other) {
				if (!(other instanceof _SemVer)) {
					other = new _SemVer(other, this.options)
				}
				if (this.prerelease.length && !other.prerelease.length) {
					return -1
				} else if (!this.prerelease.length && other.prerelease.length) {
					return 1
				} else if (!this.prerelease.length && !other.prerelease.length) {
					return 0
				}
				let i = 0
				do {
					const a = this.prerelease[i]
					const b = other.prerelease[i]
					debug10("prerelease compare", i, a, b)
					if (a === void 0 && b === void 0) {
						return 0
					} else if (b === void 0) {
						return 1
					} else if (a === void 0) {
						return -1
					} else if (a === b) {
						continue
					} else {
						return compareIdentifiers(a, b)
					}
				} while (++i)
			}
			compareBuild(other) {
				if (!(other instanceof _SemVer)) {
					other = new _SemVer(other, this.options)
				}
				let i = 0
				do {
					const a = this.build[i]
					const b = other.build[i]
					debug10("build compare", i, a, b)
					if (a === void 0 && b === void 0) {
						return 0
					} else if (b === void 0) {
						return 1
					} else if (a === void 0) {
						return -1
					} else if (a === b) {
						continue
					} else {
						return compareIdentifiers(a, b)
					}
				} while (++i)
			}
			// preminor will bump the version up to the next minor release, and immediately
			// down to pre-release. premajor and prepatch work the same way.
			inc(release, identifier, identifierBase) {
				switch (release) {
					case "premajor":
						this.prerelease.length = 0
						this.patch = 0
						this.minor = 0
						this.major++
						this.inc("pre", identifier, identifierBase)
						break
					case "preminor":
						this.prerelease.length = 0
						this.patch = 0
						this.minor++
						this.inc("pre", identifier, identifierBase)
						break
					case "prepatch":
						this.prerelease.length = 0
						this.inc("patch", identifier, identifierBase)
						this.inc("pre", identifier, identifierBase)
						break
					case "prerelease":
						if (this.prerelease.length === 0) {
							this.inc("patch", identifier, identifierBase)
						}
						this.inc("pre", identifier, identifierBase)
						break
					case "major":
						if (this.minor !== 0 || this.patch !== 0 || this.prerelease.length === 0) {
							this.major++
						}
						this.minor = 0
						this.patch = 0
						this.prerelease = []
						break
					case "minor":
						if (this.patch !== 0 || this.prerelease.length === 0) {
							this.minor++
						}
						this.patch = 0
						this.prerelease = []
						break
					case "patch":
						if (this.prerelease.length === 0) {
							this.patch++
						}
						this.prerelease = []
						break
					case "pre": {
						const base = Number(identifierBase) ? 1 : 0
						if (!identifier && identifierBase === false) {
							throw new Error("invalid increment argument: identifier is empty")
						}
						if (this.prerelease.length === 0) {
							this.prerelease = [base]
						} else {
							let i = this.prerelease.length
							while (--i >= 0) {
								if (typeof this.prerelease[i] === "number") {
									this.prerelease[i]++
									i = -2
								}
							}
							if (i === -1) {
								if (identifier === this.prerelease.join(".") && identifierBase === false) {
									throw new Error("invalid increment argument: identifier already exists")
								}
								this.prerelease.push(base)
							}
						}
						if (identifier) {
							let prerelease = [identifier, base]
							if (identifierBase === false) {
								prerelease = [identifier]
							}
							if (compareIdentifiers(this.prerelease[0], identifier) === 0) {
								if (isNaN(this.prerelease[1])) {
									this.prerelease = prerelease
								}
							} else {
								this.prerelease = prerelease
							}
						}
						break
					}
					default:
						throw new Error(`invalid increment argument: ${release}`)
				}
				this.raw = this.format()
				if (this.build.length) {
					this.raw += `+${this.build.join(".")}`
				}
				return this
			}
		}
		module2.exports = SemVer
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/parse.js
var require_parse2 = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/parse.js"(
		exports2,
		module2
	) {
		var SemVer = require_semver()
		var parse2 = (version3, options, throwErrors = false) => {
			if (version3 instanceof SemVer) {
				return version3
			}
			try {
				return new SemVer(version3, options)
			} catch (er) {
				if (!throwErrors) {
					return null
				}
				throw er
			}
		}
		module2.exports = parse2
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/valid.js
var require_valid = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/valid.js"(
		exports2,
		module2
	) {
		var parse2 = require_parse2()
		var valid = (version3, options) => {
			const v = parse2(version3, options)
			return v ? v.version : null
		}
		module2.exports = valid
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/clean.js
var require_clean = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/clean.js"(
		exports2,
		module2
	) {
		var parse2 = require_parse2()
		var clean = (version3, options) => {
			const s = parse2(version3.trim().replace(/^[=v]+/, ""), options)
			return s ? s.version : null
		}
		module2.exports = clean
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/inc.js
var require_inc = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/inc.js"(
		exports2,
		module2
	) {
		var SemVer = require_semver()
		var inc = (version3, release, options, identifier, identifierBase) => {
			if (typeof options === "string") {
				identifierBase = identifier
				identifier = options
				options = void 0
			}
			try {
				return new SemVer(version3 instanceof SemVer ? version3.version : version3, options).inc(
					release,
					identifier,
					identifierBase
				).version
			} catch (er) {
				return null
			}
		}
		module2.exports = inc
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/diff.js
var require_diff = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/diff.js"(
		exports2,
		module2
	) {
		var parse2 = require_parse2()
		var diff = (version1, version22) => {
			const v12 = parse2(version1, null, true)
			const v2 = parse2(version22, null, true)
			const comparison = v12.compare(v2)
			if (comparison === 0) {
				return null
			}
			const v1Higher = comparison > 0
			const highVersion = v1Higher ? v12 : v2
			const lowVersion = v1Higher ? v2 : v12
			const highHasPre = !!highVersion.prerelease.length
			const lowHasPre = !!lowVersion.prerelease.length
			if (lowHasPre && !highHasPre) {
				if (!lowVersion.patch && !lowVersion.minor) {
					return "major"
				}
				if (highVersion.patch) {
					return "patch"
				}
				if (highVersion.minor) {
					return "minor"
				}
				return "major"
			}
			const prefix = highHasPre ? "pre" : ""
			if (v12.major !== v2.major) {
				return prefix + "major"
			}
			if (v12.minor !== v2.minor) {
				return prefix + "minor"
			}
			if (v12.patch !== v2.patch) {
				return prefix + "patch"
			}
			return "prerelease"
		}
		module2.exports = diff
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/major.js
var require_major = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/major.js"(
		exports2,
		module2
	) {
		var SemVer = require_semver()
		var major = (a, loose) => new SemVer(a, loose).major
		module2.exports = major
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/minor.js
var require_minor = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/minor.js"(
		exports2,
		module2
	) {
		var SemVer = require_semver()
		var minor = (a, loose) => new SemVer(a, loose).minor
		module2.exports = minor
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/patch.js
var require_patch = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/patch.js"(
		exports2,
		module2
	) {
		var SemVer = require_semver()
		var patch = (a, loose) => new SemVer(a, loose).patch
		module2.exports = patch
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/prerelease.js
var require_prerelease = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/prerelease.js"(
		exports2,
		module2
	) {
		var parse2 = require_parse2()
		var prerelease = (version3, options) => {
			const parsed = parse2(version3, options)
			return parsed && parsed.prerelease.length ? parsed.prerelease : null
		}
		module2.exports = prerelease
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/compare.js
var require_compare = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/compare.js"(
		exports2,
		module2
	) {
		var SemVer = require_semver()
		var compare = (a, b, loose) => new SemVer(a, loose).compare(new SemVer(b, loose))
		module2.exports = compare
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/rcompare.js
var require_rcompare = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/rcompare.js"(
		exports2,
		module2
	) {
		var compare = require_compare()
		var rcompare = (a, b, loose) => compare(b, a, loose)
		module2.exports = rcompare
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/compare-loose.js
var require_compare_loose = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/compare-loose.js"(
		exports2,
		module2
	) {
		var compare = require_compare()
		var compareLoose = (a, b) => compare(a, b, true)
		module2.exports = compareLoose
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/compare-build.js
var require_compare_build = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/compare-build.js"(
		exports2,
		module2
	) {
		var SemVer = require_semver()
		var compareBuild = (a, b, loose) => {
			const versionA = new SemVer(a, loose)
			const versionB = new SemVer(b, loose)
			return versionA.compare(versionB) || versionA.compareBuild(versionB)
		}
		module2.exports = compareBuild
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/sort.js
var require_sort = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/sort.js"(
		exports2,
		module2
	) {
		var compareBuild = require_compare_build()
		var sort = (list, loose) => list.sort((a, b) => compareBuild(a, b, loose))
		module2.exports = sort
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/rsort.js
var require_rsort = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/rsort.js"(
		exports2,
		module2
	) {
		var compareBuild = require_compare_build()
		var rsort = (list, loose) => list.sort((a, b) => compareBuild(b, a, loose))
		module2.exports = rsort
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/gt.js
var require_gt = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/gt.js"(
		exports2,
		module2
	) {
		var compare = require_compare()
		var gt = (a, b, loose) => compare(a, b, loose) > 0
		module2.exports = gt
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/lt.js
var require_lt = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/lt.js"(
		exports2,
		module2
	) {
		var compare = require_compare()
		var lt = (a, b, loose) => compare(a, b, loose) < 0
		module2.exports = lt
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/eq.js
var require_eq = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/eq.js"(
		exports2,
		module2
	) {
		var compare = require_compare()
		var eq = (a, b, loose) => compare(a, b, loose) === 0
		module2.exports = eq
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/neq.js
var require_neq = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/neq.js"(
		exports2,
		module2
	) {
		var compare = require_compare()
		var neq = (a, b, loose) => compare(a, b, loose) !== 0
		module2.exports = neq
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/gte.js
var require_gte = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/gte.js"(
		exports2,
		module2
	) {
		var compare = require_compare()
		var gte = (a, b, loose) => compare(a, b, loose) >= 0
		module2.exports = gte
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/lte.js
var require_lte = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/lte.js"(
		exports2,
		module2
	) {
		var compare = require_compare()
		var lte = (a, b, loose) => compare(a, b, loose) <= 0
		module2.exports = lte
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/cmp.js
var require_cmp = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/cmp.js"(
		exports2,
		module2
	) {
		var eq = require_eq()
		var neq = require_neq()
		var gt = require_gt()
		var gte = require_gte()
		var lt = require_lt()
		var lte = require_lte()
		var cmp = (a, op, b, loose) => {
			switch (op) {
				case "===":
					if (typeof a === "object") {
						a = a.version
					}
					if (typeof b === "object") {
						b = b.version
					}
					return a === b
				case "!==":
					if (typeof a === "object") {
						a = a.version
					}
					if (typeof b === "object") {
						b = b.version
					}
					return a !== b
				case "":
				case "=":
				case "==":
					return eq(a, b, loose)
				case "!=":
					return neq(a, b, loose)
				case ">":
					return gt(a, b, loose)
				case ">=":
					return gte(a, b, loose)
				case "<":
					return lt(a, b, loose)
				case "<=":
					return lte(a, b, loose)
				default:
					throw new TypeError(`Invalid operator: ${op}`)
			}
		}
		module2.exports = cmp
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/coerce.js
var require_coerce = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/coerce.js"(
		exports2,
		module2
	) {
		var SemVer = require_semver()
		var parse2 = require_parse2()
		var { safeRe: re, t } = require_re()
		var coerce = (version3, options) => {
			if (version3 instanceof SemVer) {
				return version3
			}
			if (typeof version3 === "number") {
				version3 = String(version3)
			}
			if (typeof version3 !== "string") {
				return null
			}
			options = options || {}
			let match = null
			if (!options.rtl) {
				match = version3.match(options.includePrerelease ? re[t.COERCEFULL] : re[t.COERCE])
			} else {
				const coerceRtlRegex = options.includePrerelease ? re[t.COERCERTLFULL] : re[t.COERCERTL]
				let next
				while (
					(next = coerceRtlRegex.exec(version3)) &&
					(!match || match.index + match[0].length !== version3.length)
				) {
					if (!match || next.index + next[0].length !== match.index + match[0].length) {
						match = next
					}
					coerceRtlRegex.lastIndex = next.index + next[1].length + next[2].length
				}
				coerceRtlRegex.lastIndex = -1
			}
			if (match === null) {
				return null
			}
			const major = match[2]
			const minor = match[3] || "0"
			const patch = match[4] || "0"
			const prerelease = options.includePrerelease && match[5] ? `-${match[5]}` : ""
			const build = options.includePrerelease && match[6] ? `+${match[6]}` : ""
			return parse2(`${major}.${minor}.${patch}${prerelease}${build}`, options)
		}
		module2.exports = coerce
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/internal/lrucache.js
var require_lrucache = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/internal/lrucache.js"(
		exports2,
		module2
	) {
		var LRUCache = class {
			constructor() {
				this.max = 1e3
				this.map = /* @__PURE__ */ new Map()
			}
			get(key) {
				const value = this.map.get(key)
				if (value === void 0) {
					return void 0
				} else {
					this.map.delete(key)
					this.map.set(key, value)
					return value
				}
			}
			delete(key) {
				return this.map.delete(key)
			}
			set(key, value) {
				const deleted = this.delete(key)
				if (!deleted && value !== void 0) {
					if (this.map.size >= this.max) {
						const firstKey = this.map.keys().next().value
						this.delete(firstKey)
					}
					this.map.set(key, value)
				}
				return this
			}
		}
		module2.exports = LRUCache
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/classes/range.js
var require_range = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/classes/range.js"(
		exports2,
		module2
	) {
		var Range = class _Range {
			constructor(range, options) {
				options = parseOptions(options)
				if (range instanceof _Range) {
					if (
						range.loose === !!options.loose &&
						range.includePrerelease === !!options.includePrerelease
					) {
						return range
					} else {
						return new _Range(range.raw, options)
					}
				}
				if (range instanceof Comparator) {
					this.raw = range.value
					this.set = [[range]]
					this.format()
					return this
				}
				this.options = options
				this.loose = !!options.loose
				this.includePrerelease = !!options.includePrerelease
				this.raw = range.trim().split(/\s+/).join(" ")
				this.set = this.raw
					.split("||")
					.map((r) => this.parseRange(r.trim()))
					.filter((c) => c.length)
				if (!this.set.length) {
					throw new TypeError(`Invalid SemVer Range: ${this.raw}`)
				}
				if (this.set.length > 1) {
					const first = this.set[0]
					this.set = this.set.filter((c) => !isNullSet(c[0]))
					if (this.set.length === 0) {
						this.set = [first]
					} else if (this.set.length > 1) {
						for (const c of this.set) {
							if (c.length === 1 && isAny(c[0])) {
								this.set = [c]
								break
							}
						}
					}
				}
				this.format()
			}
			format() {
				this.range = this.set
					.map((comps) => comps.join(" ").trim())
					.join("||")
					.trim()
				return this.range
			}
			toString() {
				return this.range
			}
			parseRange(range) {
				const memoOpts =
					(this.options.includePrerelease && FLAG_INCLUDE_PRERELEASE) |
					(this.options.loose && FLAG_LOOSE)
				const memoKey = memoOpts + ":" + range
				const cached = cache2.get(memoKey)
				if (cached) {
					return cached
				}
				const loose = this.options.loose
				const hr = loose ? re[t.HYPHENRANGELOOSE] : re[t.HYPHENRANGE]
				range = range.replace(hr, hyphenReplace(this.options.includePrerelease))
				debug10("hyphen replace", range)
				range = range.replace(re[t.COMPARATORTRIM], comparatorTrimReplace)
				debug10("comparator trim", range)
				range = range.replace(re[t.TILDETRIM], tildeTrimReplace)
				debug10("tilde trim", range)
				range = range.replace(re[t.CARETTRIM], caretTrimReplace)
				debug10("caret trim", range)
				let rangeList = range
					.split(" ")
					.map((comp) => parseComparator(comp, this.options))
					.join(" ")
					.split(/\s+/)
					.map((comp) => replaceGTE0(comp, this.options))
				if (loose) {
					rangeList = rangeList.filter((comp) => {
						debug10("loose invalid filter", comp, this.options)
						return !!comp.match(re[t.COMPARATORLOOSE])
					})
				}
				debug10("range list", rangeList)
				const rangeMap = /* @__PURE__ */ new Map()
				const comparators = rangeList.map((comp) => new Comparator(comp, this.options))
				for (const comp of comparators) {
					if (isNullSet(comp)) {
						return [comp]
					}
					rangeMap.set(comp.value, comp)
				}
				if (rangeMap.size > 1 && rangeMap.has("")) {
					rangeMap.delete("")
				}
				const result = [...rangeMap.values()]
				cache2.set(memoKey, result)
				return result
			}
			intersects(range, options) {
				if (!(range instanceof _Range)) {
					throw new TypeError("a Range is required")
				}
				return this.set.some((thisComparators) => {
					return (
						isSatisfiable(thisComparators, options) &&
						range.set.some((rangeComparators) => {
							return (
								isSatisfiable(rangeComparators, options) &&
								thisComparators.every((thisComparator) => {
									return rangeComparators.every((rangeComparator) => {
										return thisComparator.intersects(rangeComparator, options)
									})
								})
							)
						})
					)
				})
			}
			// if ANY of the sets match ALL of its comparators, then pass
			test(version3) {
				if (!version3) {
					return false
				}
				if (typeof version3 === "string") {
					try {
						version3 = new SemVer(version3, this.options)
					} catch (er) {
						return false
					}
				}
				for (let i = 0; i < this.set.length; i++) {
					if (testSet(this.set[i], version3, this.options)) {
						return true
					}
				}
				return false
			}
		}
		module2.exports = Range
		var LRU = require_lrucache()
		var cache2 = new LRU()
		var parseOptions = require_parse_options()
		var Comparator = require_comparator()
		var debug10 = require_debug()
		var SemVer = require_semver()
		var { safeRe: re, t, comparatorTrimReplace, tildeTrimReplace, caretTrimReplace } = require_re()
		var { FLAG_INCLUDE_PRERELEASE, FLAG_LOOSE } = require_constants7()
		var isNullSet = (c) => c.value === "<0.0.0-0"
		var isAny = (c) => c.value === ""
		var isSatisfiable = (comparators, options) => {
			let result = true
			const remainingComparators = [...comparators]
			let testComparator = remainingComparators.pop()
			while (result && remainingComparators.length) {
				result = remainingComparators.every((otherComparator) => {
					return testComparator.intersects(otherComparator, options)
				})
				testComparator = remainingComparators.pop()
			}
			return result
		}
		var parseComparator = (comp, options) => {
			debug10("comp", comp, options)
			comp = replaceCarets(comp, options)
			debug10("caret", comp)
			comp = replaceTildes(comp, options)
			debug10("tildes", comp)
			comp = replaceXRanges(comp, options)
			debug10("xrange", comp)
			comp = replaceStars(comp, options)
			debug10("stars", comp)
			return comp
		}
		var isX = (id) => !id || id.toLowerCase() === "x" || id === "*"
		var replaceTildes = (comp, options) => {
			return comp
				.trim()
				.split(/\s+/)
				.map((c) => replaceTilde(c, options))
				.join(" ")
		}
		var replaceTilde = (comp, options) => {
			const r = options.loose ? re[t.TILDELOOSE] : re[t.TILDE]
			return comp.replace(r, (_, M, m, p, pr) => {
				debug10("tilde", comp, _, M, m, p, pr)
				let ret
				if (isX(M)) {
					ret = ""
				} else if (isX(m)) {
					ret = `>=${M}.0.0 <${+M + 1}.0.0-0`
				} else if (isX(p)) {
					ret = `>=${M}.${m}.0 <${M}.${+m + 1}.0-0`
				} else if (pr) {
					debug10("replaceTilde pr", pr)
					ret = `>=${M}.${m}.${p}-${pr} <${M}.${+m + 1}.0-0`
				} else {
					ret = `>=${M}.${m}.${p} <${M}.${+m + 1}.0-0`
				}
				debug10("tilde return", ret)
				return ret
			})
		}
		var replaceCarets = (comp, options) => {
			return comp
				.trim()
				.split(/\s+/)
				.map((c) => replaceCaret(c, options))
				.join(" ")
		}
		var replaceCaret = (comp, options) => {
			debug10("caret", comp, options)
			const r = options.loose ? re[t.CARETLOOSE] : re[t.CARET]
			const z = options.includePrerelease ? "-0" : ""
			return comp.replace(r, (_, M, m, p, pr) => {
				debug10("caret", comp, _, M, m, p, pr)
				let ret
				if (isX(M)) {
					ret = ""
				} else if (isX(m)) {
					ret = `>=${M}.0.0${z} <${+M + 1}.0.0-0`
				} else if (isX(p)) {
					if (M === "0") {
						ret = `>=${M}.${m}.0${z} <${M}.${+m + 1}.0-0`
					} else {
						ret = `>=${M}.${m}.0${z} <${+M + 1}.0.0-0`
					}
				} else if (pr) {
					debug10("replaceCaret pr", pr)
					if (M === "0") {
						if (m === "0") {
							ret = `>=${M}.${m}.${p}-${pr} <${M}.${m}.${+p + 1}-0`
						} else {
							ret = `>=${M}.${m}.${p}-${pr} <${M}.${+m + 1}.0-0`
						}
					} else {
						ret = `>=${M}.${m}.${p}-${pr} <${+M + 1}.0.0-0`
					}
				} else {
					debug10("no pr")
					if (M === "0") {
						if (m === "0") {
							ret = `>=${M}.${m}.${p}${z} <${M}.${m}.${+p + 1}-0`
						} else {
							ret = `>=${M}.${m}.${p}${z} <${M}.${+m + 1}.0-0`
						}
					} else {
						ret = `>=${M}.${m}.${p} <${+M + 1}.0.0-0`
					}
				}
				debug10("caret return", ret)
				return ret
			})
		}
		var replaceXRanges = (comp, options) => {
			debug10("replaceXRanges", comp, options)
			return comp
				.split(/\s+/)
				.map((c) => replaceXRange(c, options))
				.join(" ")
		}
		var replaceXRange = (comp, options) => {
			comp = comp.trim()
			const r = options.loose ? re[t.XRANGELOOSE] : re[t.XRANGE]
			return comp.replace(r, (ret, gtlt, M, m, p, pr) => {
				debug10("xRange", comp, ret, gtlt, M, m, p, pr)
				const xM = isX(M)
				const xm = xM || isX(m)
				const xp = xm || isX(p)
				const anyX = xp
				if (gtlt === "=" && anyX) {
					gtlt = ""
				}
				pr = options.includePrerelease ? "-0" : ""
				if (xM) {
					if (gtlt === ">" || gtlt === "<") {
						ret = "<0.0.0-0"
					} else {
						ret = "*"
					}
				} else if (gtlt && anyX) {
					if (xm) {
						m = 0
					}
					p = 0
					if (gtlt === ">") {
						gtlt = ">="
						if (xm) {
							M = +M + 1
							m = 0
							p = 0
						} else {
							m = +m + 1
							p = 0
						}
					} else if (gtlt === "<=") {
						gtlt = "<"
						if (xm) {
							M = +M + 1
						} else {
							m = +m + 1
						}
					}
					if (gtlt === "<") {
						pr = "-0"
					}
					ret = `${gtlt + M}.${m}.${p}${pr}`
				} else if (xm) {
					ret = `>=${M}.0.0${pr} <${+M + 1}.0.0-0`
				} else if (xp) {
					ret = `>=${M}.${m}.0${pr} <${M}.${+m + 1}.0-0`
				}
				debug10("xRange return", ret)
				return ret
			})
		}
		var replaceStars = (comp, options) => {
			debug10("replaceStars", comp, options)
			return comp.trim().replace(re[t.STAR], "")
		}
		var replaceGTE0 = (comp, options) => {
			debug10("replaceGTE0", comp, options)
			return comp.trim().replace(re[options.includePrerelease ? t.GTE0PRE : t.GTE0], "")
		}
		var hyphenReplace = (incPr) => ($0, from2, fM, fm, fp, fpr, fb, to, tM, tm, tp, tpr) => {
			if (isX(fM)) {
				from2 = ""
			} else if (isX(fm)) {
				from2 = `>=${fM}.0.0${incPr ? "-0" : ""}`
			} else if (isX(fp)) {
				from2 = `>=${fM}.${fm}.0${incPr ? "-0" : ""}`
			} else if (fpr) {
				from2 = `>=${from2}`
			} else {
				from2 = `>=${from2}${incPr ? "-0" : ""}`
			}
			if (isX(tM)) {
				to = ""
			} else if (isX(tm)) {
				to = `<${+tM + 1}.0.0-0`
			} else if (isX(tp)) {
				to = `<${tM}.${+tm + 1}.0-0`
			} else if (tpr) {
				to = `<=${tM}.${tm}.${tp}-${tpr}`
			} else if (incPr) {
				to = `<${tM}.${tm}.${+tp + 1}-0`
			} else {
				to = `<=${to}`
			}
			return `${from2} ${to}`.trim()
		}
		var testSet = (set, version3, options) => {
			for (const element of set) {
				if (!element.test(version3)) {
					return false
				}
			}
			if (version3.prerelease.length && !options.includePrerelease) {
				for (const element of set) {
					debug10(element.semver)
					if (element.semver === Comparator.ANY) {
						continue
					}
					if (element.semver.prerelease.length > 0) {
						const allowed = element.semver
						if (
							allowed.major === version3.major &&
							allowed.minor === version3.minor &&
							allowed.patch === version3.patch
						) {
							return true
						}
					}
				}
				return false
			}
			return true
		}
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/classes/comparator.js
var require_comparator = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/classes/comparator.js"(
		exports2,
		module2
	) {
		var ANY = Symbol("SemVer ANY")
		var Comparator = class _Comparator {
			static get ANY() {
				return ANY
			}
			constructor(comp, options) {
				options = parseOptions(options)
				if (comp instanceof _Comparator) {
					if (comp.loose === !!options.loose) {
						return comp
					} else {
						comp = comp.value
					}
				}
				comp = comp.trim().split(/\s+/).join(" ")
				debug10("comparator", comp, options)
				this.options = options
				this.loose = !!options.loose
				this.parse(comp)
				if (this.semver === ANY) {
					this.value = ""
				} else {
					this.value = this.operator + this.semver.version
				}
				debug10("comp", this)
			}
			parse(comp) {
				const r = this.options.loose ? re[t.COMPARATORLOOSE] : re[t.COMPARATOR]
				const m = comp.match(r)
				if (!m) {
					throw new TypeError(`Invalid comparator: ${comp}`)
				}
				this.operator = m[1] !== void 0 ? m[1] : ""
				if (this.operator === "=") {
					this.operator = ""
				}
				if (!m[2]) {
					this.semver = ANY
				} else {
					this.semver = new SemVer(m[2], this.options.loose)
				}
			}
			toString() {
				return this.value
			}
			test(version3) {
				debug10("Comparator.test", version3, this.options.loose)
				if (this.semver === ANY || version3 === ANY) {
					return true
				}
				if (typeof version3 === "string") {
					try {
						version3 = new SemVer(version3, this.options)
					} catch (er) {
						return false
					}
				}
				return cmp(version3, this.operator, this.semver, this.options)
			}
			intersects(comp, options) {
				if (!(comp instanceof _Comparator)) {
					throw new TypeError("a Comparator is required")
				}
				if (this.operator === "") {
					if (this.value === "") {
						return true
					}
					return new Range(comp.value, options).test(this.value)
				} else if (comp.operator === "") {
					if (comp.value === "") {
						return true
					}
					return new Range(this.value, options).test(comp.semver)
				}
				options = parseOptions(options)
				if (options.includePrerelease && (this.value === "<0.0.0-0" || comp.value === "<0.0.0-0")) {
					return false
				}
				if (
					!options.includePrerelease &&
					(this.value.startsWith("<0.0.0") || comp.value.startsWith("<0.0.0"))
				) {
					return false
				}
				if (this.operator.startsWith(">") && comp.operator.startsWith(">")) {
					return true
				}
				if (this.operator.startsWith("<") && comp.operator.startsWith("<")) {
					return true
				}
				if (
					this.semver.version === comp.semver.version &&
					this.operator.includes("=") &&
					comp.operator.includes("=")
				) {
					return true
				}
				if (
					cmp(this.semver, "<", comp.semver, options) &&
					this.operator.startsWith(">") &&
					comp.operator.startsWith("<")
				) {
					return true
				}
				if (
					cmp(this.semver, ">", comp.semver, options) &&
					this.operator.startsWith("<") &&
					comp.operator.startsWith(">")
				) {
					return true
				}
				return false
			}
		}
		module2.exports = Comparator
		var parseOptions = require_parse_options()
		var { safeRe: re, t } = require_re()
		var cmp = require_cmp()
		var debug10 = require_debug()
		var SemVer = require_semver()
		var Range = require_range()
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/satisfies.js
var require_satisfies = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/functions/satisfies.js"(
		exports2,
		module2
	) {
		var Range = require_range()
		var satisfies = (version3, range, options) => {
			try {
				range = new Range(range, options)
			} catch (er) {
				return false
			}
			return range.test(version3)
		}
		module2.exports = satisfies
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/to-comparators.js
var require_to_comparators = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/to-comparators.js"(
		exports2,
		module2
	) {
		var Range = require_range()
		var toComparators = (range, options) =>
			new Range(range, options).set.map((comp) =>
				comp
					.map((c) => c.value)
					.join(" ")
					.trim()
					.split(" ")
			)
		module2.exports = toComparators
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/max-satisfying.js
var require_max_satisfying = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/max-satisfying.js"(
		exports2,
		module2
	) {
		var SemVer = require_semver()
		var Range = require_range()
		var maxSatisfying = (versions, range, options) => {
			let max = null
			let maxSV = null
			let rangeObj = null
			try {
				rangeObj = new Range(range, options)
			} catch (er) {
				return null
			}
			for (const v of versions) {
				if (rangeObj.test(v) && (!max || maxSV.compare(v) === -1)) {
					max = v
					maxSV = new SemVer(max, options)
				}
			}
			return max
		}
		module2.exports = maxSatisfying
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/min-satisfying.js
var require_min_satisfying = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/min-satisfying.js"(
		exports2,
		module2
	) {
		var SemVer = require_semver()
		var Range = require_range()
		var minSatisfying = (versions, range, options) => {
			let min = null
			let minSV = null
			let rangeObj = null
			try {
				rangeObj = new Range(range, options)
			} catch (er) {
				return null
			}
			for (const v of versions) {
				if (rangeObj.test(v) && (!min || minSV.compare(v) === 1)) {
					min = v
					minSV = new SemVer(min, options)
				}
			}
			return min
		}
		module2.exports = minSatisfying
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/min-version.js
var require_min_version = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/min-version.js"(
		exports2,
		module2
	) {
		var SemVer = require_semver()
		var Range = require_range()
		var gt = require_gt()
		var minVersion = (range, loose) => {
			range = new Range(range, loose)
			let minver = new SemVer("0.0.0")
			if (range.test(minver)) {
				return minver
			}
			minver = new SemVer("0.0.0-0")
			if (range.test(minver)) {
				return minver
			}
			minver = null
			for (let i = 0; i < range.set.length; ++i) {
				const comparators = range.set[i]
				let setMin = null
				for (const comparator of comparators) {
					const compver = new SemVer(comparator.semver.version)
					switch (comparator.operator) {
						case ">":
							if (compver.prerelease.length === 0) {
								compver.patch++
							} else {
								compver.prerelease.push(0)
							}
							compver.raw = compver.format()
						case "":
						case ">=":
							if (!setMin || gt(compver, setMin)) {
								setMin = compver
							}
							break
						case "<":
						case "<=":
							break
						default:
							throw new Error(`Unexpected operation: ${comparator.operator}`)
					}
				}
				if (setMin && (!minver || gt(minver, setMin))) {
					minver = setMin
				}
			}
			if (minver && range.test(minver)) {
				return minver
			}
			return null
		}
		module2.exports = minVersion
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/valid.js
var require_valid2 = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/valid.js"(
		exports2,
		module2
	) {
		var Range = require_range()
		var validRange = (range, options) => {
			try {
				return new Range(range, options).range || "*"
			} catch (er) {
				return null
			}
		}
		module2.exports = validRange
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/outside.js
var require_outside = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/outside.js"(
		exports2,
		module2
	) {
		var SemVer = require_semver()
		var Comparator = require_comparator()
		var { ANY } = Comparator
		var Range = require_range()
		var satisfies = require_satisfies()
		var gt = require_gt()
		var lt = require_lt()
		var lte = require_lte()
		var gte = require_gte()
		var outside = (version3, range, hilo, options) => {
			version3 = new SemVer(version3, options)
			range = new Range(range, options)
			let gtfn, ltefn, ltfn, comp, ecomp
			switch (hilo) {
				case ">":
					gtfn = gt
					ltefn = lte
					ltfn = lt
					comp = ">"
					ecomp = ">="
					break
				case "<":
					gtfn = lt
					ltefn = gte
					ltfn = gt
					comp = "<"
					ecomp = "<="
					break
				default:
					throw new TypeError('Must provide a hilo val of "<" or ">"')
			}
			if (satisfies(version3, range, options)) {
				return false
			}
			for (let i = 0; i < range.set.length; ++i) {
				const comparators = range.set[i]
				let high = null
				let low = null
				for (let comparator of comparators) {
					if (comparator.semver === ANY) {
						comparator = new Comparator(">=0.0.0")
					}
					high = high || comparator
					low = low || comparator
					if (gtfn(comparator.semver, high.semver, options)) {
						high = comparator
					} else if (ltfn(comparator.semver, low.semver, options)) {
						low = comparator
					}
				}
				if (high.operator === comp || high.operator === ecomp) {
					return false
				}
				if ((!low.operator || low.operator === comp) && ltefn(version3, low.semver)) {
					return false
				} else if (low.operator === ecomp && ltfn(version3, low.semver)) {
					return false
				}
			}
			return true
		}
		module2.exports = outside
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/gtr.js
var require_gtr = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/gtr.js"(exports2, module2) {
		var outside = require_outside()
		var gtr = (version3, range, options) => outside(version3, range, ">", options)
		module2.exports = gtr
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/ltr.js
var require_ltr = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/ltr.js"(exports2, module2) {
		var outside = require_outside()
		var ltr = (version3, range, options) => outside(version3, range, "<", options)
		module2.exports = ltr
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/intersects.js
var require_intersects = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/intersects.js"(
		exports2,
		module2
	) {
		var Range = require_range()
		var intersects = (r1, r2, options) => {
			r1 = new Range(r1, options)
			r2 = new Range(r2, options)
			return r1.intersects(r2, options)
		}
		module2.exports = intersects
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/simplify.js
var require_simplify = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/simplify.js"(
		exports2,
		module2
	) {
		var satisfies = require_satisfies()
		var compare = require_compare()
		module2.exports = (versions, range, options) => {
			const set = []
			let first = null
			let prev = null
			const v = versions.sort((a, b) => compare(a, b, options))
			for (const version3 of v) {
				const included = satisfies(version3, range, options)
				if (included) {
					prev = version3
					if (!first) {
						first = version3
					}
				} else {
					if (prev) {
						set.push([first, prev])
					}
					prev = null
					first = null
				}
			}
			if (first) {
				set.push([first, null])
			}
			const ranges = []
			for (const [min, max] of set) {
				if (min === max) {
					ranges.push(min)
				} else if (!max && min === v[0]) {
					ranges.push("*")
				} else if (!max) {
					ranges.push(`>=${min}`)
				} else if (min === v[0]) {
					ranges.push(`<=${max}`)
				} else {
					ranges.push(`${min} - ${max}`)
				}
			}
			const simplified = ranges.join(" || ")
			const original = typeof range.raw === "string" ? range.raw : String(range)
			return simplified.length < original.length ? simplified : range
		}
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/subset.js
var require_subset = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/ranges/subset.js"(
		exports2,
		module2
	) {
		var Range = require_range()
		var Comparator = require_comparator()
		var { ANY } = Comparator
		var satisfies = require_satisfies()
		var compare = require_compare()
		var subset = (sub, dom, options = {}) => {
			if (sub === dom) {
				return true
			}
			sub = new Range(sub, options)
			dom = new Range(dom, options)
			let sawNonNull = false
			OUTER: for (const simpleSub of sub.set) {
				for (const simpleDom of dom.set) {
					const isSub = simpleSubset(simpleSub, simpleDom, options)
					sawNonNull = sawNonNull || isSub !== null
					if (isSub) {
						continue OUTER
					}
				}
				if (sawNonNull) {
					return false
				}
			}
			return true
		}
		var minimumVersionWithPreRelease = [new Comparator(">=0.0.0-0")]
		var minimumVersion = [new Comparator(">=0.0.0")]
		var simpleSubset = (sub, dom, options) => {
			if (sub === dom) {
				return true
			}
			if (sub.length === 1 && sub[0].semver === ANY) {
				if (dom.length === 1 && dom[0].semver === ANY) {
					return true
				} else if (options.includePrerelease) {
					sub = minimumVersionWithPreRelease
				} else {
					sub = minimumVersion
				}
			}
			if (dom.length === 1 && dom[0].semver === ANY) {
				if (options.includePrerelease) {
					return true
				} else {
					dom = minimumVersion
				}
			}
			const eqSet = /* @__PURE__ */ new Set()
			let gt, lt
			for (const c of sub) {
				if (c.operator === ">" || c.operator === ">=") {
					gt = higherGT(gt, c, options)
				} else if (c.operator === "<" || c.operator === "<=") {
					lt = lowerLT(lt, c, options)
				} else {
					eqSet.add(c.semver)
				}
			}
			if (eqSet.size > 1) {
				return null
			}
			let gtltComp
			if (gt && lt) {
				gtltComp = compare(gt.semver, lt.semver, options)
				if (gtltComp > 0) {
					return null
				} else if (gtltComp === 0 && (gt.operator !== ">=" || lt.operator !== "<=")) {
					return null
				}
			}
			for (const eq of eqSet) {
				if (gt && !satisfies(eq, String(gt), options)) {
					return null
				}
				if (lt && !satisfies(eq, String(lt), options)) {
					return null
				}
				for (const c of dom) {
					if (!satisfies(eq, String(c), options)) {
						return false
					}
				}
				return true
			}
			let higher, lower2
			let hasDomLT, hasDomGT
			let needDomLTPre =
				lt && !options.includePrerelease && lt.semver.prerelease.length ? lt.semver : false
			let needDomGTPre =
				gt && !options.includePrerelease && gt.semver.prerelease.length ? gt.semver : false
			if (
				needDomLTPre &&
				needDomLTPre.prerelease.length === 1 &&
				lt.operator === "<" &&
				needDomLTPre.prerelease[0] === 0
			) {
				needDomLTPre = false
			}
			for (const c of dom) {
				hasDomGT = hasDomGT || c.operator === ">" || c.operator === ">="
				hasDomLT = hasDomLT || c.operator === "<" || c.operator === "<="
				if (gt) {
					if (
						needDomGTPre &&
						c.semver.prerelease &&
						c.semver.prerelease.length &&
						c.semver.major === needDomGTPre.major &&
						c.semver.minor === needDomGTPre.minor &&
						c.semver.patch === needDomGTPre.patch
					) {
						needDomGTPre = false
					}
					if (c.operator === ">" || c.operator === ">=") {
						higher = higherGT(gt, c, options)
						if (higher === c && higher !== gt) {
							return false
						}
					} else if (gt.operator === ">=" && !satisfies(gt.semver, String(c), options)) {
						return false
					}
				}
				if (lt) {
					if (
						needDomLTPre &&
						c.semver.prerelease &&
						c.semver.prerelease.length &&
						c.semver.major === needDomLTPre.major &&
						c.semver.minor === needDomLTPre.minor &&
						c.semver.patch === needDomLTPre.patch
					) {
						needDomLTPre = false
					}
					if (c.operator === "<" || c.operator === "<=") {
						lower2 = lowerLT(lt, c, options)
						if (lower2 === c && lower2 !== lt) {
							return false
						}
					} else if (lt.operator === "<=" && !satisfies(lt.semver, String(c), options)) {
						return false
					}
				}
				if (!c.operator && (lt || gt) && gtltComp !== 0) {
					return false
				}
			}
			if (gt && hasDomLT && !lt && gtltComp !== 0) {
				return false
			}
			if (lt && hasDomGT && !gt && gtltComp !== 0) {
				return false
			}
			if (needDomGTPre || needDomLTPre) {
				return false
			}
			return true
		}
		var higherGT = (a, b, options) => {
			if (!a) {
				return b
			}
			const comp = compare(a.semver, b.semver, options)
			return comp > 0 ? a : comp < 0 ? b : b.operator === ">" && a.operator === ">=" ? b : a
		}
		var lowerLT = (a, b, options) => {
			if (!a) {
				return b
			}
			const comp = compare(a.semver, b.semver, options)
			return comp < 0 ? a : comp > 0 ? b : b.operator === "<" && a.operator === "<=" ? b : a
		}
		module2.exports = subset
	},
})

// ../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/index.js
var require_semver2 = __commonJS({
	"../../../node_modules/.pnpm/semver@7.6.2/node_modules/semver/index.js"(exports2, module2) {
		var internalRe = require_re()
		var constants = require_constants7()
		var SemVer = require_semver()
		var identifiers = require_identifiers()
		var parse2 = require_parse2()
		var valid = require_valid()
		var clean = require_clean()
		var inc = require_inc()
		var diff = require_diff()
		var major = require_major()
		var minor = require_minor()
		var patch = require_patch()
		var prerelease = require_prerelease()
		var compare = require_compare()
		var rcompare = require_rcompare()
		var compareLoose = require_compare_loose()
		var compareBuild = require_compare_build()
		var sort = require_sort()
		var rsort = require_rsort()
		var gt = require_gt()
		var lt = require_lt()
		var eq = require_eq()
		var neq = require_neq()
		var gte = require_gte()
		var lte = require_lte()
		var cmp = require_cmp()
		var coerce = require_coerce()
		var Comparator = require_comparator()
		var Range = require_range()
		var satisfies = require_satisfies()
		var toComparators = require_to_comparators()
		var maxSatisfying = require_max_satisfying()
		var minSatisfying = require_min_satisfying()
		var minVersion = require_min_version()
		var validRange = require_valid2()
		var outside = require_outside()
		var gtr = require_gtr()
		var ltr = require_ltr()
		var intersects = require_intersects()
		var simplifyRange = require_simplify()
		var subset = require_subset()
		module2.exports = {
			parse: parse2,
			valid,
			clean,
			inc,
			diff,
			major,
			minor,
			patch,
			prerelease,
			compare,
			rcompare,
			compareLoose,
			compareBuild,
			sort,
			rsort,
			gt,
			lt,
			eq,
			neq,
			gte,
			lte,
			cmp,
			coerce,
			Comparator,
			Range,
			satisfies,
			toComparators,
			maxSatisfying,
			minSatisfying,
			minVersion,
			validRange,
			outside,
			gtr,
			ltr,
			intersects,
			simplifyRange,
			subset,
			SemVer,
			re: internalRe.re,
			src: internalRe.src,
			tokens: internalRe.t,
			SEMVER_SPEC_VERSION: constants.SEMVER_SPEC_VERSION,
			RELEASE_TYPES: constants.RELEASE_TYPES,
			compareIdentifiers: identifiers.compareIdentifiers,
			rcompareIdentifiers: identifiers.rcompareIdentifiers,
		}
	},
})

// ../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/lib/asymmetricKeyDetailsSupported.js
var require_asymmetricKeyDetailsSupported = __commonJS({
	"../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/lib/asymmetricKeyDetailsSupported.js"(
		exports2,
		module2
	) {
		var semver = require_semver2()
		module2.exports = semver.satisfies(process.version, ">=15.7.0")
	},
})

// ../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/lib/rsaPssKeyDetailsSupported.js
var require_rsaPssKeyDetailsSupported = __commonJS({
	"../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/lib/rsaPssKeyDetailsSupported.js"(
		exports2,
		module2
	) {
		var semver = require_semver2()
		module2.exports = semver.satisfies(process.version, ">=16.9.0")
	},
})

// ../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/lib/validateAsymmetricKey.js
var require_validateAsymmetricKey = __commonJS({
	"../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/lib/validateAsymmetricKey.js"(
		exports2,
		module2
	) {
		var ASYMMETRIC_KEY_DETAILS_SUPPORTED = require_asymmetricKeyDetailsSupported()
		var RSA_PSS_KEY_DETAILS_SUPPORTED = require_rsaPssKeyDetailsSupported()
		var allowedAlgorithmsForKeys = {
			ec: ["ES256", "ES384", "ES512"],
			rsa: ["RS256", "PS256", "RS384", "PS384", "RS512", "PS512"],
			"rsa-pss": ["PS256", "PS384", "PS512"],
		}
		var allowedCurves = {
			ES256: "prime256v1",
			ES384: "secp384r1",
			ES512: "secp521r1",
		}
		module2.exports = function (algorithm, key) {
			if (!algorithm || !key) return
			const keyType = key.asymmetricKeyType
			if (!keyType) return
			const allowedAlgorithms = allowedAlgorithmsForKeys[keyType]
			if (!allowedAlgorithms) {
				throw new Error(`Unknown key type "${keyType}".`)
			}
			if (!allowedAlgorithms.includes(algorithm)) {
				throw new Error(
					`"alg" parameter for "${keyType}" key type must be one of: ${allowedAlgorithms.join(
						", "
					)}.`
				)
			}
			if (ASYMMETRIC_KEY_DETAILS_SUPPORTED) {
				switch (keyType) {
					case "ec":
						const keyCurve = key.asymmetricKeyDetails.namedCurve
						const allowedCurve = allowedCurves[algorithm]
						if (keyCurve !== allowedCurve) {
							throw new Error(`"alg" parameter "${algorithm}" requires curve "${allowedCurve}".`)
						}
						break
					case "rsa-pss":
						if (RSA_PSS_KEY_DETAILS_SUPPORTED) {
							const length = parseInt(algorithm.slice(-3), 10)
							const { hashAlgorithm, mgf1HashAlgorithm, saltLength } = key.asymmetricKeyDetails
							if (hashAlgorithm !== `sha${length}` || mgf1HashAlgorithm !== hashAlgorithm) {
								throw new Error(
									`Invalid key for this operation, its RSA-PSS parameters do not meet the requirements of "alg" ${algorithm}.`
								)
							}
							if (saltLength !== void 0 && saltLength > length >> 3) {
								throw new Error(
									`Invalid key for this operation, its RSA-PSS parameter saltLength does not meet the requirements of "alg" ${algorithm}.`
								)
							}
						}
						break
				}
			}
		}
	},
})

// ../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/lib/psSupported.js
var require_psSupported = __commonJS({
	"../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/lib/psSupported.js"(
		exports2,
		module2
	) {
		var semver = require_semver2()
		module2.exports = semver.satisfies(process.version, "^6.12.0 || >=8.0.0")
	},
})

// ../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/verify.js
var require_verify = __commonJS({
	"../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/verify.js"(
		exports2,
		module2
	) {
		var JsonWebTokenError = require_JsonWebTokenError()
		var NotBeforeError = require_NotBeforeError()
		var TokenExpiredError = require_TokenExpiredError()
		var decode = require_decode()
		var timespan = require_timespan()
		var validateAsymmetricKey = require_validateAsymmetricKey()
		var PS_SUPPORTED = require_psSupported()
		var jws = require_jws()
		var { KeyObject, createSecretKey, createPublicKey } = require("node:crypto")
		var PUB_KEY_ALGS = ["RS256", "RS384", "RS512"]
		var EC_KEY_ALGS = ["ES256", "ES384", "ES512"]
		var RSA_KEY_ALGS = ["RS256", "RS384", "RS512"]
		var HS_ALGS = ["HS256", "HS384", "HS512"]
		if (PS_SUPPORTED) {
			PUB_KEY_ALGS.splice(PUB_KEY_ALGS.length, 0, "PS256", "PS384", "PS512")
			RSA_KEY_ALGS.splice(RSA_KEY_ALGS.length, 0, "PS256", "PS384", "PS512")
		}
		module2.exports = function (jwtString, secretOrPublicKey, options, callback) {
			if (typeof options === "function" && !callback) {
				callback = options
				options = {}
			}
			if (!options) {
				options = {}
			}
			options = Object.assign({}, options)
			let done
			if (callback) {
				done = callback
			} else {
				done = function (err, data) {
					if (err) throw err
					return data
				}
			}
			if (options.clockTimestamp && typeof options.clockTimestamp !== "number") {
				return done(new JsonWebTokenError("clockTimestamp must be a number"))
			}
			if (
				options.nonce !== void 0 &&
				(typeof options.nonce !== "string" || options.nonce.trim() === "")
			) {
				return done(new JsonWebTokenError("nonce must be a non-empty string"))
			}
			if (
				options.allowInvalidAsymmetricKeyTypes !== void 0 &&
				typeof options.allowInvalidAsymmetricKeyTypes !== "boolean"
			) {
				return done(new JsonWebTokenError("allowInvalidAsymmetricKeyTypes must be a boolean"))
			}
			const clockTimestamp = options.clockTimestamp || Math.floor(Date.now() / 1e3)
			if (!jwtString) {
				return done(new JsonWebTokenError("jwt must be provided"))
			}
			if (typeof jwtString !== "string") {
				return done(new JsonWebTokenError("jwt must be a string"))
			}
			const parts = jwtString.split(".")
			if (parts.length !== 3) {
				return done(new JsonWebTokenError("jwt malformed"))
			}
			let decodedToken
			try {
				decodedToken = decode(jwtString, { complete: true })
			} catch (err) {
				return done(err)
			}
			if (!decodedToken) {
				return done(new JsonWebTokenError("invalid token"))
			}
			const header = decodedToken.header
			let getSecret
			if (typeof secretOrPublicKey === "function") {
				if (!callback) {
					return done(
						new JsonWebTokenError(
							"verify must be called asynchronous if secret or public key is provided as a callback"
						)
					)
				}
				getSecret = secretOrPublicKey
			} else {
				getSecret = function (header2, secretCallback) {
					return secretCallback(null, secretOrPublicKey)
				}
			}
			return getSecret(header, function (err, secretOrPublicKey2) {
				if (err) {
					return done(
						new JsonWebTokenError("error in secret or public key callback: " + err.message)
					)
				}
				const hasSignature = parts[2].trim() !== ""
				if (!hasSignature && secretOrPublicKey2) {
					return done(new JsonWebTokenError("jwt signature is required"))
				}
				if (hasSignature && !secretOrPublicKey2) {
					return done(new JsonWebTokenError("secret or public key must be provided"))
				}
				if (!hasSignature && !options.algorithms) {
					return done(
						new JsonWebTokenError('please specify "none" in "algorithms" to verify unsigned tokens')
					)
				}
				if (secretOrPublicKey2 != undefined && !(secretOrPublicKey2 instanceof KeyObject)) {
					try {
						secretOrPublicKey2 = createPublicKey(secretOrPublicKey2)
					} catch (_) {
						try {
							secretOrPublicKey2 = createSecretKey(
								typeof secretOrPublicKey2 === "string"
									? Buffer.from(secretOrPublicKey2)
									: secretOrPublicKey2
							)
						} catch (_2) {
							return done(new JsonWebTokenError("secretOrPublicKey is not valid key material"))
						}
					}
				}
				if (!options.algorithms) {
					if (secretOrPublicKey2.type === "secret") {
						options.algorithms = HS_ALGS
					} else if (["rsa", "rsa-pss"].includes(secretOrPublicKey2.asymmetricKeyType)) {
						options.algorithms = RSA_KEY_ALGS
					} else if (secretOrPublicKey2.asymmetricKeyType === "ec") {
						options.algorithms = EC_KEY_ALGS
					} else {
						options.algorithms = PUB_KEY_ALGS
					}
				}
				if (!options.algorithms.includes(decodedToken.header.alg)) {
					return done(new JsonWebTokenError("invalid algorithm"))
				}
				if (header.alg.startsWith("HS") && secretOrPublicKey2.type !== "secret") {
					return done(
						new JsonWebTokenError(
							`secretOrPublicKey must be a symmetric key when using ${header.alg}`
						)
					)
				} else if (/^(?:RS|PS|ES)/.test(header.alg) && secretOrPublicKey2.type !== "public") {
					return done(
						new JsonWebTokenError(
							`secretOrPublicKey must be an asymmetric key when using ${header.alg}`
						)
					)
				}
				if (!options.allowInvalidAsymmetricKeyTypes) {
					try {
						validateAsymmetricKey(header.alg, secretOrPublicKey2)
					} catch (e) {
						return done(e)
					}
				}
				let valid
				try {
					valid = jws.verify(jwtString, decodedToken.header.alg, secretOrPublicKey2)
				} catch (e) {
					return done(e)
				}
				if (!valid) {
					return done(new JsonWebTokenError("invalid signature"))
				}
				const payload = decodedToken.payload
				if (typeof payload.nbf !== "undefined" && !options.ignoreNotBefore) {
					if (typeof payload.nbf !== "number") {
						return done(new JsonWebTokenError("invalid nbf value"))
					}
					if (payload.nbf > clockTimestamp + (options.clockTolerance || 0)) {
						return done(new NotBeforeError("jwt not active", new Date(payload.nbf * 1e3)))
					}
				}
				if (typeof payload.exp !== "undefined" && !options.ignoreExpiration) {
					if (typeof payload.exp !== "number") {
						return done(new JsonWebTokenError("invalid exp value"))
					}
					if (clockTimestamp >= payload.exp + (options.clockTolerance || 0)) {
						return done(new TokenExpiredError("jwt expired", new Date(payload.exp * 1e3)))
					}
				}
				if (options.audience) {
					const audiences = Array.isArray(options.audience) ? options.audience : [options.audience]
					const target = Array.isArray(payload.aud) ? payload.aud : [payload.aud]
					const match = target.some(function (targetAudience) {
						return audiences.some(function (audience) {
							return audience instanceof RegExp
								? audience.test(targetAudience)
								: audience === targetAudience
						})
					})
					if (!match) {
						return done(
							new JsonWebTokenError("jwt audience invalid. expected: " + audiences.join(" or "))
						)
					}
				}
				if (options.issuer) {
					const invalid_issuer =
						(typeof options.issuer === "string" && payload.iss !== options.issuer) ||
						(Array.isArray(options.issuer) && !options.issuer.includes(payload.iss))
					if (invalid_issuer) {
						return done(new JsonWebTokenError("jwt issuer invalid. expected: " + options.issuer))
					}
				}
				if (options.subject && payload.sub !== options.subject) {
					return done(new JsonWebTokenError("jwt subject invalid. expected: " + options.subject))
				}
				if (options.jwtid && payload.jti !== options.jwtid) {
					return done(new JsonWebTokenError("jwt jwtid invalid. expected: " + options.jwtid))
				}
				if (options.nonce && payload.nonce !== options.nonce) {
					return done(new JsonWebTokenError("jwt nonce invalid. expected: " + options.nonce))
				}
				if (options.maxAge) {
					if (typeof payload.iat !== "number") {
						return done(new JsonWebTokenError("iat required when maxAge is specified"))
					}
					const maxAgeTimestamp = timespan(options.maxAge, payload.iat)
					if (typeof maxAgeTimestamp === "undefined") {
						return done(
							new JsonWebTokenError(
								'"maxAge" should be a number of seconds or string representing a timespan eg: "1d", "20h", 60'
							)
						)
					}
					if (clockTimestamp >= maxAgeTimestamp + (options.clockTolerance || 0)) {
						return done(new TokenExpiredError("maxAge exceeded", new Date(maxAgeTimestamp * 1e3)))
					}
				}
				if (options.complete === true) {
					const signature = decodedToken.signature
					return done(null, {
						header,
						payload,
						signature,
					})
				}
				return done(null, payload)
			})
		}
	},
})

// ../../../node_modules/.pnpm/lodash.includes@4.3.0/node_modules/lodash.includes/index.js
var require_lodash = __commonJS({
	"../../../node_modules/.pnpm/lodash.includes@4.3.0/node_modules/lodash.includes/index.js"(
		exports2,
		module2
	) {
		var INFINITY = 1 / 0
		var MAX_SAFE_INTEGER = 9007199254740991
		var MAX_INTEGER = 17976931348623157e292
		var NAN = 0 / 0
		var argsTag = "[object Arguments]"
		var funcTag = "[object Function]"
		var genTag = "[object GeneratorFunction]"
		var stringTag = "[object String]"
		var symbolTag = "[object Symbol]"
		var reTrim = /^\s+|\s+$/g
		var reIsBadHex = /^[-+]0x[0-9a-f]+$/i
		var reIsBinary = /^0b[01]+$/i
		var reIsOctal = /^0o[0-7]+$/i
		var reIsUint = /^(?:0|[1-9]\d*)$/
		var freeParseInt = parseInt
		function arrayMap(array, iteratee) {
			var index2 = -1,
				length = array ? array.length : 0,
				result = Array(length)
			while (++index2 < length) {
				result[index2] = iteratee(array[index2], index2, array)
			}
			return result
		}
		function baseFindIndex(array, predicate, fromIndex, fromRight) {
			var length = array.length,
				index2 = fromIndex + (fromRight ? 1 : -1)
			while (fromRight ? index2-- : ++index2 < length) {
				if (predicate(array[index2], index2, array)) {
					return index2
				}
			}
			return -1
		}
		function baseIndexOf(array, value, fromIndex) {
			if (value !== value) {
				return baseFindIndex(array, baseIsNaN, fromIndex)
			}
			var index2 = fromIndex - 1,
				length = array.length
			while (++index2 < length) {
				if (array[index2] === value) {
					return index2
				}
			}
			return -1
		}
		function baseIsNaN(value) {
			return value !== value
		}
		function baseTimes(n, iteratee) {
			var index2 = -1,
				result = Array(n)
			while (++index2 < n) {
				result[index2] = iteratee(index2)
			}
			return result
		}
		function baseValues(object, props) {
			return arrayMap(props, function (key) {
				return object[key]
			})
		}
		function overArg(func, transform) {
			return function (arg) {
				return func(transform(arg))
			}
		}
		var objectProto = Object.prototype
		var hasOwnProperty = objectProto.hasOwnProperty
		var objectToString = objectProto.toString
		var propertyIsEnumerable = objectProto.propertyIsEnumerable
		var nativeKeys = overArg(Object.keys, Object)
		var nativeMax = Math.max
		function arrayLikeKeys(value, inherited) {
			var result = isArray(value) || isArguments(value) ? baseTimes(value.length, String) : []
			var length = result.length,
				skipIndexes = !!length
			for (var key in value) {
				if (
					(inherited || hasOwnProperty.call(value, key)) &&
					!(skipIndexes && (key == "length" || isIndex(key, length)))
				) {
					result.push(key)
				}
			}
			return result
		}
		function baseKeys(object) {
			if (!isPrototype(object)) {
				return nativeKeys(object)
			}
			var result = []
			for (var key in Object(object)) {
				if (hasOwnProperty.call(object, key) && key != "constructor") {
					result.push(key)
				}
			}
			return result
		}
		function isIndex(value, length) {
			length = length == undefined ? MAX_SAFE_INTEGER : length
			return (
				!!length &&
				(typeof value == "number" || reIsUint.test(value)) &&
				value > -1 &&
				value % 1 == 0 &&
				value < length
			)
		}
		function isPrototype(value) {
			var Ctor = value && value.constructor,
				proto = (typeof Ctor == "function" && Ctor.prototype) || objectProto
			return value === proto
		}
		function includes(collection, value, fromIndex, guard) {
			collection = isArrayLike(collection) ? collection : values(collection)
			fromIndex = fromIndex && !guard ? toInteger(fromIndex) : 0
			var length = collection.length
			if (fromIndex < 0) {
				fromIndex = nativeMax(length + fromIndex, 0)
			}
			return isString(collection)
				? fromIndex <= length && collection.includes(value, fromIndex)
				: !!length && baseIndexOf(collection, value, fromIndex) > -1
		}
		function isArguments(value) {
			return (
				isArrayLikeObject(value) &&
				hasOwnProperty.call(value, "callee") &&
				(!propertyIsEnumerable.call(value, "callee") || objectToString.call(value) == argsTag)
			)
		}
		var isArray = Array.isArray
		function isArrayLike(value) {
			return value != undefined && isLength(value.length) && !isFunction2(value)
		}
		function isArrayLikeObject(value) {
			return isObjectLike(value) && isArrayLike(value)
		}
		function isFunction2(value) {
			var tag2 = isObject2(value) ? objectToString.call(value) : ""
			return tag2 == funcTag || tag2 == genTag
		}
		function isLength(value) {
			return typeof value == "number" && value > -1 && value % 1 == 0 && value <= MAX_SAFE_INTEGER
		}
		function isObject2(value) {
			var type = typeof value
			return !!value && (type == "object" || type == "function")
		}
		function isObjectLike(value) {
			return !!value && typeof value == "object"
		}
		function isString(value) {
			return (
				typeof value == "string" ||
				(!isArray(value) && isObjectLike(value) && objectToString.call(value) == stringTag)
			)
		}
		function isSymbol(value) {
			return (
				typeof value == "symbol" || (isObjectLike(value) && objectToString.call(value) == symbolTag)
			)
		}
		function toFinite(value) {
			if (!value) {
				return value === 0 ? value : 0
			}
			value = toNumber(value)
			if (value === INFINITY || value === -INFINITY) {
				var sign = value < 0 ? -1 : 1
				return sign * MAX_INTEGER
			}
			return value === value ? value : 0
		}
		function toInteger(value) {
			var result = toFinite(value),
				remainder = result % 1
			return result === result ? (remainder ? result - remainder : result) : 0
		}
		function toNumber(value) {
			if (typeof value == "number") {
				return value
			}
			if (isSymbol(value)) {
				return NAN
			}
			if (isObject2(value)) {
				var other = typeof value.valueOf == "function" ? value.valueOf() : value
				value = isObject2(other) ? other + "" : other
			}
			if (typeof value != "string") {
				return value === 0 ? value : +value
			}
			value = value.replace(reTrim, "")
			var isBinary2 = reIsBinary.test(value)
			return isBinary2 || reIsOctal.test(value)
				? freeParseInt(value.slice(2), isBinary2 ? 2 : 8)
				: reIsBadHex.test(value)
				? NAN
				: +value
		}
		function keys(object) {
			return isArrayLike(object) ? arrayLikeKeys(object) : baseKeys(object)
		}
		function values(object) {
			return object ? baseValues(object, keys(object)) : []
		}
		module2.exports = includes
	},
})

// ../../../node_modules/.pnpm/lodash.isboolean@3.0.3/node_modules/lodash.isboolean/index.js
var require_lodash2 = __commonJS({
	"../../../node_modules/.pnpm/lodash.isboolean@3.0.3/node_modules/lodash.isboolean/index.js"(
		exports2,
		module2
	) {
		var boolTag = "[object Boolean]"
		var objectProto = Object.prototype
		var objectToString = objectProto.toString
		function isBoolean(value) {
			return (
				value === true ||
				value === false ||
				(isObjectLike(value) && objectToString.call(value) == boolTag)
			)
		}
		function isObjectLike(value) {
			return !!value && typeof value == "object"
		}
		module2.exports = isBoolean
	},
})

// ../../../node_modules/.pnpm/lodash.isinteger@4.0.4/node_modules/lodash.isinteger/index.js
var require_lodash3 = __commonJS({
	"../../../node_modules/.pnpm/lodash.isinteger@4.0.4/node_modules/lodash.isinteger/index.js"(
		exports2,
		module2
	) {
		var INFINITY = 1 / 0
		var MAX_INTEGER = 17976931348623157e292
		var NAN = 0 / 0
		var symbolTag = "[object Symbol]"
		var reTrim = /^\s+|\s+$/g
		var reIsBadHex = /^[-+]0x[0-9a-f]+$/i
		var reIsBinary = /^0b[01]+$/i
		var reIsOctal = /^0o[0-7]+$/i
		var freeParseInt = parseInt
		var objectProto = Object.prototype
		var objectToString = objectProto.toString
		function isInteger(value) {
			return typeof value == "number" && value == toInteger(value)
		}
		function isObject2(value) {
			var type = typeof value
			return !!value && (type == "object" || type == "function")
		}
		function isObjectLike(value) {
			return !!value && typeof value == "object"
		}
		function isSymbol(value) {
			return (
				typeof value == "symbol" || (isObjectLike(value) && objectToString.call(value) == symbolTag)
			)
		}
		function toFinite(value) {
			if (!value) {
				return value === 0 ? value : 0
			}
			value = toNumber(value)
			if (value === INFINITY || value === -INFINITY) {
				var sign = value < 0 ? -1 : 1
				return sign * MAX_INTEGER
			}
			return value === value ? value : 0
		}
		function toInteger(value) {
			var result = toFinite(value),
				remainder = result % 1
			return result === result ? (remainder ? result - remainder : result) : 0
		}
		function toNumber(value) {
			if (typeof value == "number") {
				return value
			}
			if (isSymbol(value)) {
				return NAN
			}
			if (isObject2(value)) {
				var other = typeof value.valueOf == "function" ? value.valueOf() : value
				value = isObject2(other) ? other + "" : other
			}
			if (typeof value != "string") {
				return value === 0 ? value : +value
			}
			value = value.replace(reTrim, "")
			var isBinary2 = reIsBinary.test(value)
			return isBinary2 || reIsOctal.test(value)
				? freeParseInt(value.slice(2), isBinary2 ? 2 : 8)
				: reIsBadHex.test(value)
				? NAN
				: +value
		}
		module2.exports = isInteger
	},
})

// ../../../node_modules/.pnpm/lodash.isnumber@3.0.3/node_modules/lodash.isnumber/index.js
var require_lodash4 = __commonJS({
	"../../../node_modules/.pnpm/lodash.isnumber@3.0.3/node_modules/lodash.isnumber/index.js"(
		exports2,
		module2
	) {
		var numberTag = "[object Number]"
		var objectProto = Object.prototype
		var objectToString = objectProto.toString
		function isObjectLike(value) {
			return !!value && typeof value == "object"
		}
		function isNumber(value) {
			return (
				typeof value == "number" || (isObjectLike(value) && objectToString.call(value) == numberTag)
			)
		}
		module2.exports = isNumber
	},
})

// ../../../node_modules/.pnpm/lodash.isplainobject@4.0.6/node_modules/lodash.isplainobject/index.js
var require_lodash5 = __commonJS({
	"../../../node_modules/.pnpm/lodash.isplainobject@4.0.6/node_modules/lodash.isplainobject/index.js"(
		exports2,
		module2
	) {
		var objectTag = "[object Object]"
		function isHostObject(value) {
			var result = false
			if (value != undefined && typeof value.toString != "function") {
				try {
					result = !!(value + "")
				} catch (e) {}
			}
			return result
		}
		function overArg(func, transform) {
			return function (arg) {
				return func(transform(arg))
			}
		}
		var funcProto = Function.prototype
		var objectProto = Object.prototype
		var funcToString = funcProto.toString
		var hasOwnProperty = objectProto.hasOwnProperty
		var objectCtorString = funcToString.call(Object)
		var objectToString = objectProto.toString
		var getPrototype = overArg(Object.getPrototypeOf, Object)
		function isObjectLike(value) {
			return !!value && typeof value == "object"
		}
		function isPlainObject(value) {
			if (!isObjectLike(value) || objectToString.call(value) != objectTag || isHostObject(value)) {
				return false
			}
			var proto = getPrototype(value)
			if (proto === null) {
				return true
			}
			var Ctor = hasOwnProperty.call(proto, "constructor") && proto.constructor
			return (
				typeof Ctor == "function" &&
				Ctor instanceof Ctor &&
				funcToString.call(Ctor) == objectCtorString
			)
		}
		module2.exports = isPlainObject
	},
})

// ../../../node_modules/.pnpm/lodash.isstring@4.0.1/node_modules/lodash.isstring/index.js
var require_lodash6 = __commonJS({
	"../../../node_modules/.pnpm/lodash.isstring@4.0.1/node_modules/lodash.isstring/index.js"(
		exports2,
		module2
	) {
		var stringTag = "[object String]"
		var objectProto = Object.prototype
		var objectToString = objectProto.toString
		var isArray = Array.isArray
		function isObjectLike(value) {
			return !!value && typeof value == "object"
		}
		function isString(value) {
			return (
				typeof value == "string" ||
				(!isArray(value) && isObjectLike(value) && objectToString.call(value) == stringTag)
			)
		}
		module2.exports = isString
	},
})

// ../../../node_modules/.pnpm/lodash.once@4.1.1/node_modules/lodash.once/index.js
var require_lodash7 = __commonJS({
	"../../../node_modules/.pnpm/lodash.once@4.1.1/node_modules/lodash.once/index.js"(
		exports2,
		module2
	) {
		var FUNC_ERROR_TEXT = "Expected a function"
		var INFINITY = 1 / 0
		var MAX_INTEGER = 17976931348623157e292
		var NAN = 0 / 0
		var symbolTag = "[object Symbol]"
		var reTrim = /^\s+|\s+$/g
		var reIsBadHex = /^[-+]0x[0-9a-f]+$/i
		var reIsBinary = /^0b[01]+$/i
		var reIsOctal = /^0o[0-7]+$/i
		var freeParseInt = parseInt
		var objectProto = Object.prototype
		var objectToString = objectProto.toString
		function before(n, func) {
			var result
			if (typeof func != "function") {
				throw new TypeError(FUNC_ERROR_TEXT)
			}
			n = toInteger(n)
			return function () {
				if (--n > 0) {
					result = func.apply(this, arguments)
				}
				if (n <= 1) {
					func = void 0
				}
				return result
			}
		}
		function once(func) {
			return before(2, func)
		}
		function isObject2(value) {
			var type = typeof value
			return !!value && (type == "object" || type == "function")
		}
		function isObjectLike(value) {
			return !!value && typeof value == "object"
		}
		function isSymbol(value) {
			return (
				typeof value == "symbol" || (isObjectLike(value) && objectToString.call(value) == symbolTag)
			)
		}
		function toFinite(value) {
			if (!value) {
				return value === 0 ? value : 0
			}
			value = toNumber(value)
			if (value === INFINITY || value === -INFINITY) {
				var sign = value < 0 ? -1 : 1
				return sign * MAX_INTEGER
			}
			return value === value ? value : 0
		}
		function toInteger(value) {
			var result = toFinite(value),
				remainder = result % 1
			return result === result ? (remainder ? result - remainder : result) : 0
		}
		function toNumber(value) {
			if (typeof value == "number") {
				return value
			}
			if (isSymbol(value)) {
				return NAN
			}
			if (isObject2(value)) {
				var other = typeof value.valueOf == "function" ? value.valueOf() : value
				value = isObject2(other) ? other + "" : other
			}
			if (typeof value != "string") {
				return value === 0 ? value : +value
			}
			value = value.replace(reTrim, "")
			var isBinary2 = reIsBinary.test(value)
			return isBinary2 || reIsOctal.test(value)
				? freeParseInt(value.slice(2), isBinary2 ? 2 : 8)
				: reIsBadHex.test(value)
				? NAN
				: +value
		}
		module2.exports = once
	},
})

// ../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/sign.js
var require_sign = __commonJS({
	"../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/sign.js"(
		exports2,
		module2
	) {
		var timespan = require_timespan()
		var PS_SUPPORTED = require_psSupported()
		var validateAsymmetricKey = require_validateAsymmetricKey()
		var jws = require_jws()
		var includes = require_lodash()
		var isBoolean = require_lodash2()
		var isInteger = require_lodash3()
		var isNumber = require_lodash4()
		var isPlainObject = require_lodash5()
		var isString = require_lodash6()
		var once = require_lodash7()
		var { KeyObject, createSecretKey, createPrivateKey } = require("node:crypto")
		var SUPPORTED_ALGS = [
			"RS256",
			"RS384",
			"RS512",
			"ES256",
			"ES384",
			"ES512",
			"HS256",
			"HS384",
			"HS512",
			"none",
		]
		if (PS_SUPPORTED) {
			SUPPORTED_ALGS.splice(3, 0, "PS256", "PS384", "PS512")
		}
		var sign_options_schema = {
			expiresIn: {
				isValid: function (value) {
					return isInteger(value) || (isString(value) && value)
				},
				message: '"expiresIn" should be a number of seconds or string representing a timespan',
			},
			notBefore: {
				isValid: function (value) {
					return isInteger(value) || (isString(value) && value)
				},
				message: '"notBefore" should be a number of seconds or string representing a timespan',
			},
			audience: {
				isValid: function (value) {
					return isString(value) || Array.isArray(value)
				},
				message: '"audience" must be a string or array',
			},
			algorithm: {
				isValid: includes.bind(null, SUPPORTED_ALGS),
				message: '"algorithm" must be a valid string enum value',
			},
			header: { isValid: isPlainObject, message: '"header" must be an object' },
			encoding: { isValid: isString, message: '"encoding" must be a string' },
			issuer: { isValid: isString, message: '"issuer" must be a string' },
			subject: { isValid: isString, message: '"subject" must be a string' },
			jwtid: { isValid: isString, message: '"jwtid" must be a string' },
			noTimestamp: { isValid: isBoolean, message: '"noTimestamp" must be a boolean' },
			keyid: { isValid: isString, message: '"keyid" must be a string' },
			mutatePayload: { isValid: isBoolean, message: '"mutatePayload" must be a boolean' },
			allowInsecureKeySizes: {
				isValid: isBoolean,
				message: '"allowInsecureKeySizes" must be a boolean',
			},
			allowInvalidAsymmetricKeyTypes: {
				isValid: isBoolean,
				message: '"allowInvalidAsymmetricKeyTypes" must be a boolean',
			},
		}
		var registered_claims_schema = {
			iat: { isValid: isNumber, message: '"iat" should be a number of seconds' },
			exp: { isValid: isNumber, message: '"exp" should be a number of seconds' },
			nbf: { isValid: isNumber, message: '"nbf" should be a number of seconds' },
		}
		function validate2(schema2, allowUnknown, object, parameterName) {
			if (!isPlainObject(object)) {
				throw new Error('Expected "' + parameterName + '" to be a plain object.')
			}
			for (const key of Object.keys(object)) {
				const validator = schema2[key]
				if (!validator) {
					if (!allowUnknown) {
						throw new Error('"' + key + '" is not allowed in "' + parameterName + '"')
					}
					continue
				}
				if (!validator.isValid(object[key])) {
					throw new Error(validator.message)
				}
			}
		}
		function validateOptions(options) {
			return validate2(sign_options_schema, false, options, "options")
		}
		function validatePayload(payload) {
			return validate2(registered_claims_schema, true, payload, "payload")
		}
		var options_to_payload = {
			audience: "aud",
			issuer: "iss",
			subject: "sub",
			jwtid: "jti",
		}
		var options_for_objects = [
			"expiresIn",
			"notBefore",
			"noTimestamp",
			"audience",
			"issuer",
			"subject",
			"jwtid",
		]
		module2.exports = function (payload, secretOrPrivateKey, options, callback) {
			if (typeof options === "function") {
				callback = options
				options = {}
			} else {
				options = options || {}
			}
			const isObjectPayload = typeof payload === "object" && !Buffer.isBuffer(payload)
			const header = Object.assign(
				{
					alg: options.algorithm || "HS256",
					typ: isObjectPayload ? "JWT" : void 0,
					kid: options.keyid,
				},
				options.header
			)
			function failure(err) {
				if (callback) {
					return callback(err)
				}
				throw err
			}
			if (!secretOrPrivateKey && options.algorithm !== "none") {
				return failure(new Error("secretOrPrivateKey must have a value"))
			}
			if (secretOrPrivateKey != undefined && !(secretOrPrivateKey instanceof KeyObject)) {
				try {
					secretOrPrivateKey = createPrivateKey(secretOrPrivateKey)
				} catch (_) {
					try {
						secretOrPrivateKey = createSecretKey(
							typeof secretOrPrivateKey === "string"
								? Buffer.from(secretOrPrivateKey)
								: secretOrPrivateKey
						)
					} catch (_2) {
						return failure(new Error("secretOrPrivateKey is not valid key material"))
					}
				}
			}
			if (header.alg.startsWith("HS") && secretOrPrivateKey.type !== "secret") {
				return failure(
					new Error(`secretOrPrivateKey must be a symmetric key when using ${header.alg}`)
				)
			} else if (/^(?:RS|PS|ES)/.test(header.alg)) {
				if (secretOrPrivateKey.type !== "private") {
					return failure(
						new Error(`secretOrPrivateKey must be an asymmetric key when using ${header.alg}`)
					)
				}
				if (
					!options.allowInsecureKeySizes &&
					!header.alg.startsWith("ES") &&
					secretOrPrivateKey.asymmetricKeyDetails !== void 0 && //KeyObject.asymmetricKeyDetails is supported in Node 15+
					secretOrPrivateKey.asymmetricKeyDetails.modulusLength < 2048
				) {
					return failure(
						new Error(`secretOrPrivateKey has a minimum key size of 2048 bits for ${header.alg}`)
					)
				}
			}
			if (typeof payload === "undefined") {
				return failure(new Error("payload is required"))
			} else if (isObjectPayload) {
				try {
					validatePayload(payload)
				} catch (error) {
					return failure(error)
				}
				if (!options.mutatePayload) {
					payload = Object.assign({}, payload)
				}
			} else {
				const invalid_options = options_for_objects.filter(function (opt) {
					return typeof options[opt] !== "undefined"
				})
				if (invalid_options.length > 0) {
					return failure(
						new Error(
							"invalid " + invalid_options.join(",") + " option for " + typeof payload + " payload"
						)
					)
				}
			}
			if (typeof payload.exp !== "undefined" && typeof options.expiresIn !== "undefined") {
				return failure(
					new Error('Bad "options.expiresIn" option the payload already has an "exp" property.')
				)
			}
			if (typeof payload.nbf !== "undefined" && typeof options.notBefore !== "undefined") {
				return failure(
					new Error('Bad "options.notBefore" option the payload already has an "nbf" property.')
				)
			}
			try {
				validateOptions(options)
			} catch (error) {
				return failure(error)
			}
			if (!options.allowInvalidAsymmetricKeyTypes) {
				try {
					validateAsymmetricKey(header.alg, secretOrPrivateKey)
				} catch (error) {
					return failure(error)
				}
			}
			const timestamp = payload.iat || Math.floor(Date.now() / 1e3)
			if (options.noTimestamp) {
				delete payload.iat
			} else if (isObjectPayload) {
				payload.iat = timestamp
			}
			if (typeof options.notBefore !== "undefined") {
				try {
					payload.nbf = timespan(options.notBefore, timestamp)
				} catch (err) {
					return failure(err)
				}
				if (typeof payload.nbf === "undefined") {
					return failure(
						new Error(
							'"notBefore" should be a number of seconds or string representing a timespan eg: "1d", "20h", 60'
						)
					)
				}
			}
			if (typeof options.expiresIn !== "undefined" && typeof payload === "object") {
				try {
					payload.exp = timespan(options.expiresIn, timestamp)
				} catch (err) {
					return failure(err)
				}
				if (typeof payload.exp === "undefined") {
					return failure(
						new Error(
							'"expiresIn" should be a number of seconds or string representing a timespan eg: "1d", "20h", 60'
						)
					)
				}
			}
			for (const key of Object.keys(options_to_payload)) {
				const claim = options_to_payload[key]
				if (typeof options[key] !== "undefined") {
					if (typeof payload[claim] !== "undefined") {
						failure(
							new Error(
								'Bad "options.' +
									key +
									'" option. The payload already has an "' +
									claim +
									'" property.'
							)
						)
						continue
					}
					payload[claim] = options[key]
				}
			}
			const encoding = options.encoding || "utf8"
			if (typeof callback === "function") {
				callback = callback && once(callback)
				jws
					.createSign({
						header,
						privateKey: secretOrPrivateKey,
						payload,
						encoding,
					})
					.once("error", callback)
					.once("done", function (signature) {
						if (
							!options.allowInsecureKeySizes &&
							/^(?:RS|PS)/.test(header.alg) &&
							signature.length < 256
						) {
							return callback(
								new Error(
									`secretOrPrivateKey has a minimum key size of 2048 bits for ${header.alg}`
								)
							)
						}
						callback(null, signature)
					})
			} else {
				let signature = jws.sign({ header, payload, secret: secretOrPrivateKey, encoding })
				if (
					!options.allowInsecureKeySizes &&
					/^(?:RS|PS)/.test(header.alg) &&
					signature.length < 256
				) {
					throw new Error(
						`secretOrPrivateKey has a minimum key size of 2048 bits for ${header.alg}`
					)
				}
				return signature
			}
		}
	},
})

// ../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/index.js
var require_jsonwebtoken = __commonJS({
	"../../../node_modules/.pnpm/jsonwebtoken@9.0.2/node_modules/jsonwebtoken/index.js"(
		exports2,
		module2
	) {
		module2.exports = {
			decode: require_decode(),
			verify: require_verify(),
			sign: require_sign(),
			JsonWebTokenError: require_JsonWebTokenError(),
			NotBeforeError: require_NotBeforeError(),
			TokenExpiredError: require_TokenExpiredError(),
		}
	},
})

// ../../../node_modules/.pnpm/universal-github-app-jwt@1.1.2/node_modules/universal-github-app-jwt/dist-node/index.js
var require_dist_node19 = __commonJS({
	"../../../node_modules/.pnpm/universal-github-app-jwt@1.1.2/node_modules/universal-github-app-jwt/dist-node/index.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		function _interopDefault(ex) {
			return ex && typeof ex === "object" && "default" in ex ? ex["default"] : ex
		}
		var jsonwebtoken = _interopDefault(require_jsonwebtoken())
		async function getToken({ privateKey, payload }) {
			return jsonwebtoken.sign(payload, privateKey, {
				algorithm: "RS256",
			})
		}
		async function githubAppJwt({ id, privateKey, now = Math.floor(Date.now() / 1e3) }) {
			const nowWithSafetyMargin = now - 30
			const expiration = nowWithSafetyMargin + 60 * 10
			const payload = {
				iat: nowWithSafetyMargin,
				exp: expiration,
				iss: id,
			}
			const token = await getToken({
				privateKey,
				payload,
			})
			return {
				appId: id,
				expiration,
				token,
			}
		}
		exports2.githubAppJwt = githubAppJwt
	},
})

// ../../../node_modules/.pnpm/lru-cache@10.2.2/node_modules/lru-cache/dist/commonjs/index.js
var require_commonjs = __commonJS({
	"../../../node_modules/.pnpm/lru-cache@10.2.2/node_modules/lru-cache/dist/commonjs/index.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.LRUCache = void 0
		var perf =
			typeof performance === "object" && performance && typeof performance.now === "function"
				? performance
				: Date
		var warned = /* @__PURE__ */ new Set()
		var PROCESS = typeof process === "object" && !!process ? process : {}
		var emitWarning = (msg, type, code, fn) => {
			typeof PROCESS.emitWarning === "function"
				? PROCESS.emitWarning(msg, type, code, fn)
				: console.error(`[${code}] ${type}: ${msg}`)
		}
		var AC = globalThis.AbortController
		var AS = globalThis.AbortSignal
		if (typeof AC === "undefined") {
			AS = class AbortSignal {
				onabort
				_onabort = []
				reason
				aborted = false
				addEventListener(_, fn) {
					this._onabort.push(fn)
				}
			}
			AC = class AbortController {
				constructor() {
					warnACPolyfill()
				}
				signal = new AS()
				abort(reason) {
					if (this.signal.aborted) return
					this.signal.reason = reason
					this.signal.aborted = true
					for (const fn of this.signal._onabort) {
						fn(reason)
					}
					this.signal.onabort?.(reason)
				}
			}
			let printACPolyfillWarning = PROCESS.env?.LRU_CACHE_IGNORE_AC_WARNING !== "1"
			const warnACPolyfill = () => {
				if (!printACPolyfillWarning) return
				printACPolyfillWarning = false
				emitWarning(
					"AbortController is not defined. If using lru-cache in node 14, load an AbortController polyfill from the `node-abort-controller` package. A minimal polyfill is provided for use by LRUCache.fetch(), but it should not be relied upon in other contexts (eg, passing it to other APIs that use AbortController/AbortSignal might have undesirable effects). You may disable this with LRU_CACHE_IGNORE_AC_WARNING=1 in the env.",
					"NO_ABORT_CONTROLLER",
					"ENOTSUP",
					warnACPolyfill
				)
			}
		}
		var shouldWarn = (code) => !warned.has(code)
		var TYPE = Symbol("type")
		var isPosInt = (n) => n && n === Math.floor(n) && n > 0 && isFinite(n)
		var getUintArray = (max) =>
			!isPosInt(max)
				? null
				: max <= Math.pow(2, 8)
				? Uint8Array
				: max <= Math.pow(2, 16)
				? Uint16Array
				: max <= Math.pow(2, 32)
				? Uint32Array
				: max <= Number.MAX_SAFE_INTEGER
				? ZeroArray
				: null
		var ZeroArray = class extends Array {
			constructor(size) {
				super(size)
				this.fill(0)
			}
		}
		var Stack = class _Stack {
			heap
			length
			// private constructor
			static #constructing = false
			static create(max) {
				const HeapCls = getUintArray(max)
				if (!HeapCls) return []
				_Stack.#constructing = true
				const s = new _Stack(max, HeapCls)
				_Stack.#constructing = false
				return s
			}
			constructor(max, HeapCls) {
				if (!_Stack.#constructing) {
					throw new TypeError("instantiate Stack using Stack.create(n)")
				}
				this.heap = new HeapCls(max)
				this.length = 0
			}
			push(n) {
				this.heap[this.length++] = n
			}
			pop() {
				return this.heap[--this.length]
			}
		}
		var LRUCache = class _LRUCache {
			// properties coming in from the options of these, only max and maxSize
			// really *need* to be protected. The rest can be modified, as they just
			// set defaults for various methods.
			#max
			#maxSize
			#dispose
			#disposeAfter
			#fetchMethod
			/**
			 * {@link LRUCache.OptionsBase.ttl}
			 */
			ttl
			/**
			 * {@link LRUCache.OptionsBase.ttlResolution}
			 */
			ttlResolution
			/**
			 * {@link LRUCache.OptionsBase.ttlAutopurge}
			 */
			ttlAutopurge
			/**
			 * {@link LRUCache.OptionsBase.updateAgeOnGet}
			 */
			updateAgeOnGet
			/**
			 * {@link LRUCache.OptionsBase.updateAgeOnHas}
			 */
			updateAgeOnHas
			/**
			 * {@link LRUCache.OptionsBase.allowStale}
			 */
			allowStale
			/**
			 * {@link LRUCache.OptionsBase.noDisposeOnSet}
			 */
			noDisposeOnSet
			/**
			 * {@link LRUCache.OptionsBase.noUpdateTTL}
			 */
			noUpdateTTL
			/**
			 * {@link LRUCache.OptionsBase.maxEntrySize}
			 */
			maxEntrySize
			/**
			 * {@link LRUCache.OptionsBase.sizeCalculation}
			 */
			sizeCalculation
			/**
			 * {@link LRUCache.OptionsBase.noDeleteOnFetchRejection}
			 */
			noDeleteOnFetchRejection
			/**
			 * {@link LRUCache.OptionsBase.noDeleteOnStaleGet}
			 */
			noDeleteOnStaleGet
			/**
			 * {@link LRUCache.OptionsBase.allowStaleOnFetchAbort}
			 */
			allowStaleOnFetchAbort
			/**
			 * {@link LRUCache.OptionsBase.allowStaleOnFetchRejection}
			 */
			allowStaleOnFetchRejection
			/**
			 * {@link LRUCache.OptionsBase.ignoreFetchAbort}
			 */
			ignoreFetchAbort
			// computed properties
			#size
			#calculatedSize
			#keyMap
			#keyList
			#valList
			#next
			#prev
			#head
			#tail
			#free
			#disposed
			#sizes
			#starts
			#ttls
			#hasDispose
			#hasFetchMethod
			#hasDisposeAfter
			/**
			 * Do not call this method unless you need to inspect the
			 * inner workings of the cache.  If anything returned by this
			 * object is modified in any way, strange breakage may occur.
			 *
			 * These fields are private for a reason!
			 *
			 * @internal
			 */
			static unsafeExposeInternals(c) {
				return {
					// properties
					starts: c.#starts,
					ttls: c.#ttls,
					sizes: c.#sizes,
					keyMap: c.#keyMap,
					keyList: c.#keyList,
					valList: c.#valList,
					next: c.#next,
					prev: c.#prev,
					get head() {
						return c.#head
					},
					get tail() {
						return c.#tail
					},
					free: c.#free,
					// methods
					isBackgroundFetch: (p) => c.#isBackgroundFetch(p),
					backgroundFetch: (k, index2, options, context2) =>
						c.#backgroundFetch(k, index2, options, context2),
					moveToTail: (index2) => c.#moveToTail(index2),
					indexes: (options) => c.#indexes(options),
					rindexes: (options) => c.#rindexes(options),
					isStale: (index2) => c.#isStale(index2),
				}
			}
			// Protected read-only members
			/**
			 * {@link LRUCache.OptionsBase.max} (read-only)
			 */
			get max() {
				return this.#max
			}
			/**
			 * {@link LRUCache.OptionsBase.maxSize} (read-only)
			 */
			get maxSize() {
				return this.#maxSize
			}
			/**
			 * The total computed size of items in the cache (read-only)
			 */
			get calculatedSize() {
				return this.#calculatedSize
			}
			/**
			 * The number of items stored in the cache (read-only)
			 */
			get size() {
				return this.#size
			}
			/**
			 * {@link LRUCache.OptionsBase.fetchMethod} (read-only)
			 */
			get fetchMethod() {
				return this.#fetchMethod
			}
			/**
			 * {@link LRUCache.OptionsBase.dispose} (read-only)
			 */
			get dispose() {
				return this.#dispose
			}
			/**
			 * {@link LRUCache.OptionsBase.disposeAfter} (read-only)
			 */
			get disposeAfter() {
				return this.#disposeAfter
			}
			constructor(options) {
				const {
					max = 0,
					ttl,
					ttlResolution = 1,
					ttlAutopurge,
					updateAgeOnGet,
					updateAgeOnHas,
					allowStale,
					dispose,
					disposeAfter,
					noDisposeOnSet,
					noUpdateTTL,
					maxSize = 0,
					maxEntrySize = 0,
					sizeCalculation,
					fetchMethod,
					noDeleteOnFetchRejection,
					noDeleteOnStaleGet,
					allowStaleOnFetchRejection,
					allowStaleOnFetchAbort,
					ignoreFetchAbort,
				} = options
				if (max !== 0 && !isPosInt(max)) {
					throw new TypeError("max option must be a nonnegative integer")
				}
				const UintArray = max ? getUintArray(max) : Array
				if (!UintArray) {
					throw new Error("invalid max value: " + max)
				}
				this.#max = max
				this.#maxSize = maxSize
				this.maxEntrySize = maxEntrySize || this.#maxSize
				this.sizeCalculation = sizeCalculation
				if (this.sizeCalculation) {
					if (!this.#maxSize && !this.maxEntrySize) {
						throw new TypeError(
							"cannot set sizeCalculation without setting maxSize or maxEntrySize"
						)
					}
					if (typeof this.sizeCalculation !== "function") {
						throw new TypeError("sizeCalculation set to non-function")
					}
				}
				if (fetchMethod !== void 0 && typeof fetchMethod !== "function") {
					throw new TypeError("fetchMethod must be a function if specified")
				}
				this.#fetchMethod = fetchMethod
				this.#hasFetchMethod = !!fetchMethod
				this.#keyMap = /* @__PURE__ */ new Map()
				this.#keyList = new Array(max).fill(void 0)
				this.#valList = new Array(max).fill(void 0)
				this.#next = new UintArray(max)
				this.#prev = new UintArray(max)
				this.#head = 0
				this.#tail = 0
				this.#free = Stack.create(max)
				this.#size = 0
				this.#calculatedSize = 0
				if (typeof dispose === "function") {
					this.#dispose = dispose
				}
				if (typeof disposeAfter === "function") {
					this.#disposeAfter = disposeAfter
					this.#disposed = []
				} else {
					this.#disposeAfter = void 0
					this.#disposed = void 0
				}
				this.#hasDispose = !!this.#dispose
				this.#hasDisposeAfter = !!this.#disposeAfter
				this.noDisposeOnSet = !!noDisposeOnSet
				this.noUpdateTTL = !!noUpdateTTL
				this.noDeleteOnFetchRejection = !!noDeleteOnFetchRejection
				this.allowStaleOnFetchRejection = !!allowStaleOnFetchRejection
				this.allowStaleOnFetchAbort = !!allowStaleOnFetchAbort
				this.ignoreFetchAbort = !!ignoreFetchAbort
				if (this.maxEntrySize !== 0) {
					if (this.#maxSize !== 0 && !isPosInt(this.#maxSize)) {
						throw new TypeError("maxSize must be a positive integer if specified")
					}
					if (!isPosInt(this.maxEntrySize)) {
						throw new TypeError("maxEntrySize must be a positive integer if specified")
					}
					this.#initializeSizeTracking()
				}
				this.allowStale = !!allowStale
				this.noDeleteOnStaleGet = !!noDeleteOnStaleGet
				this.updateAgeOnGet = !!updateAgeOnGet
				this.updateAgeOnHas = !!updateAgeOnHas
				this.ttlResolution = isPosInt(ttlResolution) || ttlResolution === 0 ? ttlResolution : 1
				this.ttlAutopurge = !!ttlAutopurge
				this.ttl = ttl || 0
				if (this.ttl) {
					if (!isPosInt(this.ttl)) {
						throw new TypeError("ttl must be a positive integer if specified")
					}
					this.#initializeTTLTracking()
				}
				if (this.#max === 0 && this.ttl === 0 && this.#maxSize === 0) {
					throw new TypeError("At least one of max, maxSize, or ttl is required")
				}
				if (!this.ttlAutopurge && !this.#max && !this.#maxSize) {
					const code = "LRU_CACHE_UNBOUNDED"
					if (shouldWarn(code)) {
						warned.add(code)
						const msg =
							"TTL caching without ttlAutopurge, max, or maxSize can result in unbounded memory consumption."
						emitWarning(msg, "UnboundedCacheWarning", code, _LRUCache)
					}
				}
			}
			/**
			 * Return the remaining TTL time for a given entry key
			 */
			getRemainingTTL(key) {
				return this.#keyMap.has(key) ? Infinity : 0
			}
			#initializeTTLTracking() {
				const ttls = new ZeroArray(this.#max)
				const starts = new ZeroArray(this.#max)
				this.#ttls = ttls
				this.#starts = starts
				this.#setItemTTL = (index2, ttl, start = perf.now()) => {
					starts[index2] = ttl !== 0 ? start : 0
					ttls[index2] = ttl
					if (ttl !== 0 && this.ttlAutopurge) {
						const t = setTimeout(() => {
							if (this.#isStale(index2)) {
								this.delete(this.#keyList[index2])
							}
						}, ttl + 1)
						if (t.unref) {
							t.unref()
						}
					}
				}
				this.#updateItemAge = (index2) => {
					starts[index2] = ttls[index2] !== 0 ? perf.now() : 0
				}
				this.#statusTTL = (status3, index2) => {
					if (ttls[index2]) {
						const ttl = ttls[index2]
						const start = starts[index2]
						if (!ttl || !start) return
						status3.ttl = ttl
						status3.start = start
						status3.now = cachedNow || getNow()
						const age = status3.now - start
						status3.remainingTTL = ttl - age
					}
				}
				let cachedNow = 0
				const getNow = () => {
					const n = perf.now()
					if (this.ttlResolution > 0) {
						cachedNow = n
						const t = setTimeout(() => (cachedNow = 0), this.ttlResolution)
						if (t.unref) {
							t.unref()
						}
					}
					return n
				}
				this.getRemainingTTL = (key) => {
					const index2 = this.#keyMap.get(key)
					if (index2 === void 0) {
						return 0
					}
					const ttl = ttls[index2]
					const start = starts[index2]
					if (!ttl || !start) {
						return Infinity
					}
					const age = (cachedNow || getNow()) - start
					return ttl - age
				}
				this.#isStale = (index2) => {
					const s = starts[index2]
					const t = ttls[index2]
					return !!t && !!s && (cachedNow || getNow()) - s > t
				}
			}
			// conditionally set private methods related to TTL
			#updateItemAge = () => {}
			#statusTTL = () => {}
			#setItemTTL = () => {}
			/* c8 ignore stop */
			#isStale = () => false
			#initializeSizeTracking() {
				const sizes = new ZeroArray(this.#max)
				this.#calculatedSize = 0
				this.#sizes = sizes
				this.#removeItemSize = (index2) => {
					this.#calculatedSize -= sizes[index2]
					sizes[index2] = 0
				}
				this.#requireSize = (k, v, size, sizeCalculation) => {
					if (this.#isBackgroundFetch(v)) {
						return 0
					}
					if (!isPosInt(size)) {
						if (sizeCalculation) {
							if (typeof sizeCalculation !== "function") {
								throw new TypeError("sizeCalculation must be a function")
							}
							size = sizeCalculation(v, k)
							if (!isPosInt(size)) {
								throw new TypeError("sizeCalculation return invalid (expect positive integer)")
							}
						} else {
							throw new TypeError(
								"invalid size value (must be positive integer). When maxSize or maxEntrySize is used, sizeCalculation or size must be set."
							)
						}
					}
					return size
				}
				this.#addItemSize = (index2, size, status3) => {
					sizes[index2] = size
					if (this.#maxSize) {
						const maxSize = this.#maxSize - sizes[index2]
						while (this.#calculatedSize > maxSize) {
							this.#evict(true)
						}
					}
					this.#calculatedSize += sizes[index2]
					if (status3) {
						status3.entrySize = size
						status3.totalCalculatedSize = this.#calculatedSize
					}
				}
			}
			#removeItemSize = (_i) => {}
			#addItemSize = (_i, _s, _st) => {}
			#requireSize = (_k, _v, size, sizeCalculation) => {
				if (size || sizeCalculation) {
					throw new TypeError("cannot set size without setting maxSize or maxEntrySize on cache")
				}
				return 0
			};
			*#indexes({ allowStale = this.allowStale } = {}) {
				if (this.#size) {
					for (let i = this.#tail; true; ) {
						if (!this.#isValidIndex(i)) {
							break
						}
						if (allowStale || !this.#isStale(i)) {
							yield i
						}
						if (i === this.#head) {
							break
						} else {
							i = this.#prev[i]
						}
					}
				}
			}
			*#rindexes({ allowStale = this.allowStale } = {}) {
				if (this.#size) {
					for (let i = this.#head; true; ) {
						if (!this.#isValidIndex(i)) {
							break
						}
						if (allowStale || !this.#isStale(i)) {
							yield i
						}
						if (i === this.#tail) {
							break
						} else {
							i = this.#next[i]
						}
					}
				}
			}
			#isValidIndex(index2) {
				return index2 !== void 0 && this.#keyMap.get(this.#keyList[index2]) === index2
			}
			/**
			 * Return a generator yielding `[key, value]` pairs,
			 * in order from most recently used to least recently used.
			 */
			*entries() {
				for (const i of this.#indexes()) {
					if (
						this.#valList[i] !== void 0 &&
						this.#keyList[i] !== void 0 &&
						!this.#isBackgroundFetch(this.#valList[i])
					) {
						yield [this.#keyList[i], this.#valList[i]]
					}
				}
			}
			/**
			 * Inverse order version of {@link LRUCache.entries}
			 *
			 * Return a generator yielding `[key, value]` pairs,
			 * in order from least recently used to most recently used.
			 */
			*rentries() {
				for (const i of this.#rindexes()) {
					if (
						this.#valList[i] !== void 0 &&
						this.#keyList[i] !== void 0 &&
						!this.#isBackgroundFetch(this.#valList[i])
					) {
						yield [this.#keyList[i], this.#valList[i]]
					}
				}
			}
			/**
			 * Return a generator yielding the keys in the cache,
			 * in order from most recently used to least recently used.
			 */
			*keys() {
				for (const i of this.#indexes()) {
					const k = this.#keyList[i]
					if (k !== void 0 && !this.#isBackgroundFetch(this.#valList[i])) {
						yield k
					}
				}
			}
			/**
			 * Inverse order version of {@link LRUCache.keys}
			 *
			 * Return a generator yielding the keys in the cache,
			 * in order from least recently used to most recently used.
			 */
			*rkeys() {
				for (const i of this.#rindexes()) {
					const k = this.#keyList[i]
					if (k !== void 0 && !this.#isBackgroundFetch(this.#valList[i])) {
						yield k
					}
				}
			}
			/**
			 * Return a generator yielding the values in the cache,
			 * in order from most recently used to least recently used.
			 */
			*values() {
				for (const i of this.#indexes()) {
					const v = this.#valList[i]
					if (v !== void 0 && !this.#isBackgroundFetch(this.#valList[i])) {
						yield this.#valList[i]
					}
				}
			}
			/**
			 * Inverse order version of {@link LRUCache.values}
			 *
			 * Return a generator yielding the values in the cache,
			 * in order from least recently used to most recently used.
			 */
			*rvalues() {
				for (const i of this.#rindexes()) {
					const v = this.#valList[i]
					if (v !== void 0 && !this.#isBackgroundFetch(this.#valList[i])) {
						yield this.#valList[i]
					}
				}
			}
			/**
			 * Iterating over the cache itself yields the same results as
			 * {@link LRUCache.entries}
			 */
			[Symbol.iterator]() {
				return this.entries()
			}
			/**
			 * A String value that is used in the creation of the default string description of an object.
			 * Called by the built-in method Object.prototype.toString.
			 */
			[Symbol.toStringTag] = "LRUCache"
			/**
			 * Find a value for which the supplied fn method returns a truthy value,
			 * similar to Array.find().  fn is called as fn(value, key, cache).
			 */
			find(fn, getOptions = {}) {
				for (const i of this.#indexes()) {
					const v = this.#valList[i]
					const value = this.#isBackgroundFetch(v) ? v.__staleWhileFetching : v
					if (value === void 0) continue
					if (fn(value, this.#keyList[i], this)) {
						return this.get(this.#keyList[i], getOptions)
					}
				}
			}
			/**
			 * Call the supplied function on each item in the cache, in order from
			 * most recently used to least recently used.  fn is called as
			 * fn(value, key, cache).  Does not update age or recenty of use.
			 * Does not iterate over stale values.
			 */
			forEach(fn, thisp = this) {
				for (const i of this.#indexes()) {
					const v = this.#valList[i]
					const value = this.#isBackgroundFetch(v) ? v.__staleWhileFetching : v
					if (value === void 0) continue
					fn.call(thisp, value, this.#keyList[i], this)
				}
			}
			/**
			 * The same as {@link LRUCache.forEach} but items are iterated over in
			 * reverse order.  (ie, less recently used items are iterated over first.)
			 */
			rforEach(fn, thisp = this) {
				for (const i of this.#rindexes()) {
					const v = this.#valList[i]
					const value = this.#isBackgroundFetch(v) ? v.__staleWhileFetching : v
					if (value === void 0) continue
					fn.call(thisp, value, this.#keyList[i], this)
				}
			}
			/**
			 * Delete any stale entries. Returns true if anything was removed,
			 * false otherwise.
			 */
			purgeStale() {
				let deleted = false
				for (const i of this.#rindexes({ allowStale: true })) {
					if (this.#isStale(i)) {
						this.delete(this.#keyList[i])
						deleted = true
					}
				}
				return deleted
			}
			/**
			 * Get the extended info about a given entry, to get its value, size, and
			 * TTL info simultaneously. Like {@link LRUCache#dump}, but just for a
			 * single key. Always returns stale values, if their info is found in the
			 * cache, so be sure to check for expired TTLs if relevant.
			 */
			info(key) {
				const i = this.#keyMap.get(key)
				if (i === void 0) return void 0
				const v = this.#valList[i]
				const value = this.#isBackgroundFetch(v) ? v.__staleWhileFetching : v
				if (value === void 0) return void 0
				const entry = { value }
				if (this.#ttls && this.#starts) {
					const ttl = this.#ttls[i]
					const start = this.#starts[i]
					if (ttl && start) {
						const remain = ttl - (perf.now() - start)
						entry.ttl = remain
						entry.start = Date.now()
					}
				}
				if (this.#sizes) {
					entry.size = this.#sizes[i]
				}
				return entry
			}
			/**
			 * Return an array of [key, {@link LRUCache.Entry}] tuples which can be
			 * passed to cache.load()
			 */
			dump() {
				const arr = []
				for (const i of this.#indexes({ allowStale: true })) {
					const key = this.#keyList[i]
					const v = this.#valList[i]
					const value = this.#isBackgroundFetch(v) ? v.__staleWhileFetching : v
					if (value === void 0 || key === void 0) continue
					const entry = { value }
					if (this.#ttls && this.#starts) {
						entry.ttl = this.#ttls[i]
						const age = perf.now() - this.#starts[i]
						entry.start = Math.floor(Date.now() - age)
					}
					if (this.#sizes) {
						entry.size = this.#sizes[i]
					}
					arr.unshift([key, entry])
				}
				return arr
			}
			/**
			 * Reset the cache and load in the items in entries in the order listed.
			 * Note that the shape of the resulting cache may be different if the
			 * same options are not used in both caches.
			 */
			load(arr) {
				this.clear()
				for (const [key, entry] of arr) {
					if (entry.start) {
						const age = Date.now() - entry.start
						entry.start = perf.now() - age
					}
					this.set(key, entry.value, entry)
				}
			}
			/**
			 * Add a value to the cache.
			 *
			 * Note: if `undefined` is specified as a value, this is an alias for
			 * {@link LRUCache#delete}
			 */
			set(k, v, setOptions = {}) {
				if (v === void 0) {
					this.delete(k)
					return this
				}
				const {
					ttl = this.ttl,
					start,
					noDisposeOnSet = this.noDisposeOnSet,
					sizeCalculation = this.sizeCalculation,
					status: status3,
				} = setOptions
				let { noUpdateTTL = this.noUpdateTTL } = setOptions
				const size = this.#requireSize(k, v, setOptions.size || 0, sizeCalculation)
				if (this.maxEntrySize && size > this.maxEntrySize) {
					if (status3) {
						status3.set = "miss"
						status3.maxEntrySizeExceeded = true
					}
					this.delete(k)
					return this
				}
				let index2 = this.#size === 0 ? void 0 : this.#keyMap.get(k)
				if (index2 === void 0) {
					index2 =
						this.#size === 0
							? this.#tail
							: this.#free.length !== 0
							? this.#free.pop()
							: this.#size === this.#max
							? this.#evict(false)
							: this.#size
					this.#keyList[index2] = k
					this.#valList[index2] = v
					this.#keyMap.set(k, index2)
					this.#next[this.#tail] = index2
					this.#prev[index2] = this.#tail
					this.#tail = index2
					this.#size++
					this.#addItemSize(index2, size, status3)
					if (status3) status3.set = "add"
					noUpdateTTL = false
				} else {
					this.#moveToTail(index2)
					const oldVal = this.#valList[index2]
					if (v !== oldVal) {
						if (this.#hasFetchMethod && this.#isBackgroundFetch(oldVal)) {
							oldVal.__abortController.abort(new Error("replaced"))
							const { __staleWhileFetching: s } = oldVal
							if (s !== void 0 && !noDisposeOnSet) {
								if (this.#hasDispose) {
									this.#dispose?.(s, k, "set")
								}
								if (this.#hasDisposeAfter) {
									this.#disposed?.push([s, k, "set"])
								}
							}
						} else if (!noDisposeOnSet) {
							if (this.#hasDispose) {
								this.#dispose?.(oldVal, k, "set")
							}
							if (this.#hasDisposeAfter) {
								this.#disposed?.push([oldVal, k, "set"])
							}
						}
						this.#removeItemSize(index2)
						this.#addItemSize(index2, size, status3)
						this.#valList[index2] = v
						if (status3) {
							status3.set = "replace"
							const oldValue =
								oldVal && this.#isBackgroundFetch(oldVal) ? oldVal.__staleWhileFetching : oldVal
							if (oldValue !== void 0) status3.oldValue = oldValue
						}
					} else if (status3) {
						status3.set = "update"
					}
				}
				if (ttl !== 0 && !this.#ttls) {
					this.#initializeTTLTracking()
				}
				if (this.#ttls) {
					if (!noUpdateTTL) {
						this.#setItemTTL(index2, ttl, start)
					}
					if (status3) this.#statusTTL(status3, index2)
				}
				if (!noDisposeOnSet && this.#hasDisposeAfter && this.#disposed) {
					const dt = this.#disposed
					let task
					while ((task = dt?.shift())) {
						this.#disposeAfter?.(...task)
					}
				}
				return this
			}
			/**
			 * Evict the least recently used item, returning its value or
			 * `undefined` if cache is empty.
			 */
			pop() {
				try {
					while (this.#size) {
						const val = this.#valList[this.#head]
						this.#evict(true)
						if (this.#isBackgroundFetch(val)) {
							if (val.__staleWhileFetching) {
								return val.__staleWhileFetching
							}
						} else if (val !== void 0) {
							return val
						}
					}
				} finally {
					if (this.#hasDisposeAfter && this.#disposed) {
						const dt = this.#disposed
						let task
						while ((task = dt?.shift())) {
							this.#disposeAfter?.(...task)
						}
					}
				}
			}
			#evict(free) {
				const head = this.#head
				const k = this.#keyList[head]
				const v = this.#valList[head]
				if (this.#hasFetchMethod && this.#isBackgroundFetch(v)) {
					v.__abortController.abort(new Error("evicted"))
				} else if (this.#hasDispose || this.#hasDisposeAfter) {
					if (this.#hasDispose) {
						this.#dispose?.(v, k, "evict")
					}
					if (this.#hasDisposeAfter) {
						this.#disposed?.push([v, k, "evict"])
					}
				}
				this.#removeItemSize(head)
				if (free) {
					this.#keyList[head] = void 0
					this.#valList[head] = void 0
					this.#free.push(head)
				}
				if (this.#size === 1) {
					this.#head = this.#tail = 0
					this.#free.length = 0
				} else {
					this.#head = this.#next[head]
				}
				this.#keyMap.delete(k)
				this.#size--
				return head
			}
			/**
			 * Check if a key is in the cache, without updating the recency of use.
			 * Will return false if the item is stale, even though it is technically
			 * in the cache.
			 *
			 * Will not update item age unless
			 * {@link LRUCache.OptionsBase.updateAgeOnHas} is set.
			 */
			has(k, hasOptions = {}) {
				const { updateAgeOnHas = this.updateAgeOnHas, status: status3 } = hasOptions
				const index2 = this.#keyMap.get(k)
				if (index2 !== void 0) {
					const v = this.#valList[index2]
					if (this.#isBackgroundFetch(v) && v.__staleWhileFetching === void 0) {
						return false
					}
					if (!this.#isStale(index2)) {
						if (updateAgeOnHas) {
							this.#updateItemAge(index2)
						}
						if (status3) {
							status3.has = "hit"
							this.#statusTTL(status3, index2)
						}
						return true
					} else if (status3) {
						status3.has = "stale"
						this.#statusTTL(status3, index2)
					}
				} else if (status3) {
					status3.has = "miss"
				}
				return false
			}
			/**
			 * Like {@link LRUCache#get} but doesn't update recency or delete stale
			 * items.
			 *
			 * Returns `undefined` if the item is stale, unless
			 * {@link LRUCache.OptionsBase.allowStale} is set.
			 */
			peek(k, peekOptions = {}) {
				const { allowStale = this.allowStale } = peekOptions
				const index2 = this.#keyMap.get(k)
				if (index2 === void 0 || (!allowStale && this.#isStale(index2))) {
					return
				}
				const v = this.#valList[index2]
				return this.#isBackgroundFetch(v) ? v.__staleWhileFetching : v
			}
			#backgroundFetch(k, index2, options, context2) {
				const v = index2 === void 0 ? void 0 : this.#valList[index2]
				if (this.#isBackgroundFetch(v)) {
					return v
				}
				const ac = new AC()
				const { signal } = options
				signal?.addEventListener("abort", () => ac.abort(signal.reason), {
					signal: ac.signal,
				})
				const fetchOpts = {
					signal: ac.signal,
					options,
					context: context2,
				}
				const cb = (v2, updateCache = false) => {
					const { aborted } = ac.signal
					const ignoreAbort = options.ignoreFetchAbort && v2 !== void 0
					if (options.status) {
						if (aborted && !updateCache) {
							options.status.fetchAborted = true
							options.status.fetchError = ac.signal.reason
							if (ignoreAbort) options.status.fetchAbortIgnored = true
						} else {
							options.status.fetchResolved = true
						}
					}
					if (aborted && !ignoreAbort && !updateCache) {
						return fetchFail(ac.signal.reason)
					}
					const bf2 = p
					if (this.#valList[index2] === p) {
						if (v2 === void 0) {
							if (bf2.__staleWhileFetching) {
								this.#valList[index2] = bf2.__staleWhileFetching
							} else {
								this.delete(k)
							}
						} else {
							if (options.status) options.status.fetchUpdated = true
							this.set(k, v2, fetchOpts.options)
						}
					}
					return v2
				}
				const eb = (er) => {
					if (options.status) {
						options.status.fetchRejected = true
						options.status.fetchError = er
					}
					return fetchFail(er)
				}
				const fetchFail = (er) => {
					const { aborted } = ac.signal
					const allowStaleAborted = aborted && options.allowStaleOnFetchAbort
					const allowStale = allowStaleAborted || options.allowStaleOnFetchRejection
					const noDelete = allowStale || options.noDeleteOnFetchRejection
					const bf2 = p
					if (this.#valList[index2] === p) {
						const del = !noDelete || bf2.__staleWhileFetching === void 0
						if (del) {
							this.delete(k)
						} else if (!allowStaleAborted) {
							this.#valList[index2] = bf2.__staleWhileFetching
						}
					}
					if (allowStale) {
						if (options.status && bf2.__staleWhileFetching !== void 0) {
							options.status.returnedStale = true
						}
						return bf2.__staleWhileFetching
					} else if (bf2.__returned === bf2) {
						throw er
					}
				}
				const pcall = (res, rej) => {
					const fmp = this.#fetchMethod?.(k, v, fetchOpts)
					if (fmp && fmp instanceof Promise) {
						fmp.then((v2) => res(v2 === void 0 ? void 0 : v2), rej)
					}
					ac.signal.addEventListener("abort", () => {
						if (!options.ignoreFetchAbort || options.allowStaleOnFetchAbort) {
							res(void 0)
							if (options.allowStaleOnFetchAbort) {
								res = (v2) => cb(v2, true)
							}
						}
					})
				}
				if (options.status) options.status.fetchDispatched = true
				const p = new Promise(pcall).then(cb, eb)
				const bf = Object.assign(p, {
					__abortController: ac,
					__staleWhileFetching: v,
					__returned: void 0,
				})
				if (index2 === void 0) {
					this.set(k, bf, { ...fetchOpts.options, status: void 0 })
					index2 = this.#keyMap.get(k)
				} else {
					this.#valList[index2] = bf
				}
				return bf
			}
			#isBackgroundFetch(p) {
				if (!this.#hasFetchMethod) return false
				const b = p
				return (
					!!b &&
					b instanceof Promise &&
					b.hasOwnProperty("__staleWhileFetching") &&
					b.__abortController instanceof AC
				)
			}
			async fetch(k, fetchOptions = {}) {
				const {
					// get options
					allowStale = this.allowStale,
					updateAgeOnGet = this.updateAgeOnGet,
					noDeleteOnStaleGet = this.noDeleteOnStaleGet,
					// set options
					ttl = this.ttl,
					noDisposeOnSet = this.noDisposeOnSet,
					size = 0,
					sizeCalculation = this.sizeCalculation,
					noUpdateTTL = this.noUpdateTTL,
					// fetch exclusive options
					noDeleteOnFetchRejection = this.noDeleteOnFetchRejection,
					allowStaleOnFetchRejection = this.allowStaleOnFetchRejection,
					ignoreFetchAbort = this.ignoreFetchAbort,
					allowStaleOnFetchAbort = this.allowStaleOnFetchAbort,
					context: context2,
					forceRefresh = false,
					status: status3,
					signal,
				} = fetchOptions
				if (!this.#hasFetchMethod) {
					if (status3) status3.fetch = "get"
					return this.get(k, {
						allowStale,
						updateAgeOnGet,
						noDeleteOnStaleGet,
						status: status3,
					})
				}
				const options = {
					allowStale,
					updateAgeOnGet,
					noDeleteOnStaleGet,
					ttl,
					noDisposeOnSet,
					size,
					sizeCalculation,
					noUpdateTTL,
					noDeleteOnFetchRejection,
					allowStaleOnFetchRejection,
					allowStaleOnFetchAbort,
					ignoreFetchAbort,
					status: status3,
					signal,
				}
				let index2 = this.#keyMap.get(k)
				if (index2 === void 0) {
					if (status3) status3.fetch = "miss"
					const p = this.#backgroundFetch(k, index2, options, context2)
					return (p.__returned = p)
				} else {
					const v = this.#valList[index2]
					if (this.#isBackgroundFetch(v)) {
						const stale = allowStale && v.__staleWhileFetching !== void 0
						if (status3) {
							status3.fetch = "inflight"
							if (stale) status3.returnedStale = true
						}
						return stale ? v.__staleWhileFetching : (v.__returned = v)
					}
					const isStale = this.#isStale(index2)
					if (!forceRefresh && !isStale) {
						if (status3) status3.fetch = "hit"
						this.#moveToTail(index2)
						if (updateAgeOnGet) {
							this.#updateItemAge(index2)
						}
						if (status3) this.#statusTTL(status3, index2)
						return v
					}
					const p = this.#backgroundFetch(k, index2, options, context2)
					const hasStale = p.__staleWhileFetching !== void 0
					const staleVal = hasStale && allowStale
					if (status3) {
						status3.fetch = isStale ? "stale" : "refresh"
						if (staleVal && isStale) status3.returnedStale = true
					}
					return staleVal ? p.__staleWhileFetching : (p.__returned = p)
				}
			}
			/**
			 * Return a value from the cache. Will update the recency of the cache
			 * entry found.
			 *
			 * If the key is not found, get() will return `undefined`.
			 */
			get(k, getOptions = {}) {
				const {
					allowStale = this.allowStale,
					updateAgeOnGet = this.updateAgeOnGet,
					noDeleteOnStaleGet = this.noDeleteOnStaleGet,
					status: status3,
				} = getOptions
				const index2 = this.#keyMap.get(k)
				if (index2 !== void 0) {
					const value = this.#valList[index2]
					const fetching = this.#isBackgroundFetch(value)
					if (status3) this.#statusTTL(status3, index2)
					if (this.#isStale(index2)) {
						if (status3) status3.get = "stale"
						if (!fetching) {
							if (!noDeleteOnStaleGet) {
								this.delete(k)
							}
							if (status3 && allowStale) status3.returnedStale = true
							return allowStale ? value : void 0
						} else {
							if (status3 && allowStale && value.__staleWhileFetching !== void 0) {
								status3.returnedStale = true
							}
							return allowStale ? value.__staleWhileFetching : void 0
						}
					} else {
						if (status3) status3.get = "hit"
						if (fetching) {
							return value.__staleWhileFetching
						}
						this.#moveToTail(index2)
						if (updateAgeOnGet) {
							this.#updateItemAge(index2)
						}
						return value
					}
				} else if (status3) {
					status3.get = "miss"
				}
			}
			#connect(p, n) {
				this.#prev[n] = p
				this.#next[p] = n
			}
			#moveToTail(index2) {
				if (index2 !== this.#tail) {
					if (index2 === this.#head) {
						this.#head = this.#next[index2]
					} else {
						this.#connect(this.#prev[index2], this.#next[index2])
					}
					this.#connect(this.#tail, index2)
					this.#tail = index2
				}
			}
			/**
			 * Deletes a key out of the cache.
			 * Returns true if the key was deleted, false otherwise.
			 */
			delete(k) {
				let deleted = false
				if (this.#size !== 0) {
					const index2 = this.#keyMap.get(k)
					if (index2 !== void 0) {
						deleted = true
						if (this.#size === 1) {
							this.clear()
						} else {
							this.#removeItemSize(index2)
							const v = this.#valList[index2]
							if (this.#isBackgroundFetch(v)) {
								v.__abortController.abort(new Error("deleted"))
							} else if (this.#hasDispose || this.#hasDisposeAfter) {
								if (this.#hasDispose) {
									this.#dispose?.(v, k, "delete")
								}
								if (this.#hasDisposeAfter) {
									this.#disposed?.push([v, k, "delete"])
								}
							}
							this.#keyMap.delete(k)
							this.#keyList[index2] = void 0
							this.#valList[index2] = void 0
							if (index2 === this.#tail) {
								this.#tail = this.#prev[index2]
							} else if (index2 === this.#head) {
								this.#head = this.#next[index2]
							} else {
								const pi = this.#prev[index2]
								this.#next[pi] = this.#next[index2]
								const ni = this.#next[index2]
								this.#prev[ni] = this.#prev[index2]
							}
							this.#size--
							this.#free.push(index2)
						}
					}
				}
				if (this.#hasDisposeAfter && this.#disposed?.length) {
					const dt = this.#disposed
					let task
					while ((task = dt?.shift())) {
						this.#disposeAfter?.(...task)
					}
				}
				return deleted
			}
			/**
			 * Clear the cache entirely, throwing away all values.
			 */
			clear() {
				for (const index2 of this.#rindexes({ allowStale: true })) {
					const v = this.#valList[index2]
					if (this.#isBackgroundFetch(v)) {
						v.__abortController.abort(new Error("deleted"))
					} else {
						const k = this.#keyList[index2]
						if (this.#hasDispose) {
							this.#dispose?.(v, k, "delete")
						}
						if (this.#hasDisposeAfter) {
							this.#disposed?.push([v, k, "delete"])
						}
					}
				}
				this.#keyMap.clear()
				this.#valList.fill(void 0)
				this.#keyList.fill(void 0)
				if (this.#ttls && this.#starts) {
					this.#ttls.fill(0)
					this.#starts.fill(0)
				}
				if (this.#sizes) {
					this.#sizes.fill(0)
				}
				this.#head = 0
				this.#tail = 0
				this.#free.length = 0
				this.#calculatedSize = 0
				this.#size = 0
				if (this.#hasDisposeAfter && this.#disposed) {
					const dt = this.#disposed
					let task
					while ((task = dt?.shift())) {
						this.#disposeAfter?.(...task)
					}
				}
			}
		}
		exports2.LRUCache = LRUCache
	},
})

// ../../../node_modules/.pnpm/@octokit+auth-app@6.1.1/node_modules/@octokit/auth-app/dist-node/index.js
var require_dist_node20 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+auth-app@6.1.1/node_modules/@octokit/auth-app/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __create2 = Object.create
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __getProtoOf2 = Object.getPrototypeOf
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toESM2 = (mod, isNodeMode, target) => (
			(target = mod != undefined ? __create2(__getProtoOf2(mod)) : {}),
			__copyProps2(
				// If the importer is in node compatibility mode or this is not an ESM
				// file that has been converted to a CommonJS file using a Babel-
				// compatible transform (i.e. "__esModule" has not been set), then set
				// "default" to the CommonJS "module.exports" for node compatibility.
				isNodeMode || !mod || !mod.__esModule
					? __defProp2(target, "default", { value: mod, enumerable: true })
					: target,
				mod
			)
		)
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			createAppAuth: () => createAppAuth,
			createOAuthUserAuth: () => import_auth_oauth_user2.createOAuthUserAuth,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var import_universal_user_agent = require_dist_node()
		var import_request = require_dist_node5()
		var import_auth_oauth_app = require_dist_node18()
		var import_deprecation = require_dist_node3()
		var OAuthAppAuth = __toESM2(require_dist_node18())
		var import_universal_github_app_jwt = require_dist_node19()
		async function getAppAuthentication({ appId, privateKey, timeDifference }) {
			try {
				const appAuthentication = await (0, import_universal_github_app_jwt.githubAppJwt)({
					id: +appId,
					privateKey,
					now: timeDifference && Math.floor(Date.now() / 1e3) + timeDifference,
				})
				return {
					type: "app",
					token: appAuthentication.token,
					appId: appAuthentication.appId,
					expiresAt: new Date(appAuthentication.expiration * 1e3).toISOString(),
				}
			} catch (error) {
				if (privateKey === "-----BEGIN RSA PRIVATE KEY-----") {
					throw new Error(
						"The 'privateKey` option contains only the first line '-----BEGIN RSA PRIVATE KEY-----'. If you are setting it using a `.env` file, make sure it is set on a single line with newlines replaced by '\n'"
					)
				} else {
					throw error
				}
			}
		}
		var import_lru_cache = require_commonjs()
		function getCache() {
			return new import_lru_cache.LRUCache({
				// cache max. 15000 tokens, that will use less than 10mb memory
				max: 15e3,
				// Cache for 1 minute less than GitHub expiry
				ttl: 1e3 * 60 * 59,
			})
		}
		async function get(cache2, options) {
			const cacheKey = optionsToCacheKey(options)
			const result = await cache2.get(cacheKey)
			if (!result) {
				return
			}
			const [token, createdAt, expiresAt, repositorySelection, permissionsString, singleFileName] =
				result.split("|")
			const permissions =
				options.permissions ||
				permissionsString.split(/,/).reduce((permissions2, string) => {
					if (/!$/.test(string)) {
						permissions2[string.slice(0, -1)] = "write"
					} else {
						permissions2[string] = "read"
					}
					return permissions2
				}, {})
			return {
				token,
				createdAt,
				expiresAt,
				permissions,
				repositoryIds: options.repositoryIds,
				repositoryNames: options.repositoryNames,
				singleFileName,
				repositorySelection,
			}
		}
		async function set(cache2, options, data) {
			const key = optionsToCacheKey(options)
			const permissionsString = options.permissions
				? ""
				: Object.keys(data.permissions)
						.map((name) => `${name}${data.permissions[name] === "write" ? "!" : ""}`)
						.join(",")
			const value = [
				data.token,
				data.createdAt,
				data.expiresAt,
				data.repositorySelection,
				permissionsString,
				data.singleFileName,
			].join("|")
			await cache2.set(key, value)
		}
		function optionsToCacheKey({
			installationId,
			permissions = {},
			repositoryIds = [],
			repositoryNames = [],
		}) {
			const permissionsString = Object.keys(permissions)
				.sort()
				.map((name) => (permissions[name] === "read" ? name : `${name}!`))
				.join(",")
			const repositoryIdsString = repositoryIds.sort().join(",")
			const repositoryNamesString = repositoryNames.join(",")
			return [installationId, repositoryIdsString, repositoryNamesString, permissionsString]
				.filter(Boolean)
				.join("|")
		}
		function toTokenAuthentication({
			installationId,
			token,
			createdAt,
			expiresAt,
			repositorySelection,
			permissions,
			repositoryIds,
			repositoryNames,
			singleFileName,
		}) {
			return Object.assign(
				{
					type: "token",
					tokenType: "installation",
					token,
					installationId,
					permissions,
					createdAt,
					expiresAt,
					repositorySelection,
				},
				repositoryIds ? { repositoryIds } : null,
				repositoryNames ? { repositoryNames } : null,
				singleFileName ? { singleFileName } : null
			)
		}
		async function getInstallationAuthentication(state, options, customRequest) {
			const installationId = Number(options.installationId || state.installationId)
			if (!installationId) {
				throw new Error(
					"[@octokit/auth-app] installationId option is required for installation authentication."
				)
			}
			if (options.factory) {
				const { type, factory, oauthApp, ...factoryAuthOptions } = {
					...state,
					...options,
				}
				return factory(factoryAuthOptions)
			}
			const optionsWithInstallationTokenFromState = Object.assign({ installationId }, options)
			if (!options.refresh) {
				const result = await get(state.cache, optionsWithInstallationTokenFromState)
				if (result) {
					const {
						token: token2,
						createdAt: createdAt2,
						expiresAt: expiresAt2,
						permissions: permissions2,
						repositoryIds: repositoryIds2,
						repositoryNames: repositoryNames2,
						singleFileName: singleFileName2,
						repositorySelection: repositorySelection2,
					} = result
					return toTokenAuthentication({
						installationId,
						token: token2,
						createdAt: createdAt2,
						expiresAt: expiresAt2,
						permissions: permissions2,
						repositorySelection: repositorySelection2,
						repositoryIds: repositoryIds2,
						repositoryNames: repositoryNames2,
						singleFileName: singleFileName2,
					})
				}
			}
			const appAuthentication = await getAppAuthentication(state)
			const request = customRequest || state.request
			const {
				data: {
					token,
					expires_at: expiresAt,
					repositories,
					permissions: permissionsOptional,
					repository_selection: repositorySelectionOptional,
					single_file: singleFileName,
				},
			} = await request("POST /app/installations/{installation_id}/access_tokens", {
				installation_id: installationId,
				repository_ids: options.repositoryIds,
				repositories: options.repositoryNames,
				permissions: options.permissions,
				mediaType: {
					previews: ["machine-man"],
				},
				headers: {
					authorization: `bearer ${appAuthentication.token}`,
				},
			})
			const permissions = permissionsOptional || {}
			const repositorySelection = repositorySelectionOptional || "all"
			const repositoryIds = repositories ? repositories.map((r) => r.id) : void 0
			const repositoryNames = repositories ? repositories.map((repo) => repo.name) : void 0
			const createdAt = /* @__PURE__ */ new Date().toISOString()
			await set(state.cache, optionsWithInstallationTokenFromState, {
				token,
				createdAt,
				expiresAt,
				repositorySelection,
				permissions,
				repositoryIds,
				repositoryNames,
				singleFileName,
			})
			return toTokenAuthentication({
				installationId,
				token,
				createdAt,
				expiresAt,
				repositorySelection,
				permissions,
				repositoryIds,
				repositoryNames,
				singleFileName,
			})
		}
		async function auth(state, authOptions) {
			switch (authOptions.type) {
				case "app":
					return getAppAuthentication(state)
				case "oauth":
					state.log.warn(
						// @ts-expect-error `log.warn()` expects string
						new import_deprecation.Deprecation(
							`[@octokit/auth-app] {type: "oauth"} is deprecated. Use {type: "oauth-app"} instead`
						)
					)
				case "oauth-app":
					return state.oauthApp({ type: "oauth-app" })
				case "installation":
					authOptions
					return getInstallationAuthentication(state, {
						...authOptions,
						type: "installation",
					})
				case "oauth-user":
					return state.oauthApp(authOptions)
				default:
					throw new Error(`Invalid auth type: ${authOptions.type}`)
			}
		}
		var import_auth_oauth_user = require_dist_node17()
		var import_request_error = require_dist_node4()
		var PATHS = [
			"/app",
			"/app/hook/config",
			"/app/hook/deliveries",
			"/app/hook/deliveries/{delivery_id}",
			"/app/hook/deliveries/{delivery_id}/attempts",
			"/app/installations",
			"/app/installations/{installation_id}",
			"/app/installations/{installation_id}/access_tokens",
			"/app/installations/{installation_id}/suspended",
			"/app/installation-requests",
			"/marketplace_listing/accounts/{account_id}",
			"/marketplace_listing/plan",
			"/marketplace_listing/plans",
			"/marketplace_listing/plans/{plan_id}/accounts",
			"/marketplace_listing/stubbed/accounts/{account_id}",
			"/marketplace_listing/stubbed/plan",
			"/marketplace_listing/stubbed/plans",
			"/marketplace_listing/stubbed/plans/{plan_id}/accounts",
			"/orgs/{org}/installation",
			"/repos/{owner}/{repo}/installation",
			"/users/{username}/installation",
		]
		function routeMatcher(paths) {
			const regexes = paths.map((p) =>
				p
					.split("/")
					.map((c) => (c.startsWith("{") ? "(?:.+?)" : c))
					.join("/")
			)
			const regex = `^(?:${regexes.map((r) => `(?:${r})`).join("|")})$`
			return new RegExp(regex, "i")
		}
		var REGEX = routeMatcher(PATHS)
		function requiresAppAuth(url) {
			return !!url && REGEX.test(url.split("?")[0])
		}
		var FIVE_SECONDS_IN_MS = 5 * 1e3
		function isNotTimeSkewError(error) {
			return !(
				error.message.match(
					/'Expiration time' claim \('exp'\) must be a numeric value representing the future time at which the assertion expires/
				) ||
				error.message.match(
					/'Issued at' claim \('iat'\) must be an Integer representing the time that the assertion was issued/
				)
			)
		}
		async function hook(state, request, route, parameters) {
			const endpoint = request.endpoint.merge(route, parameters)
			const url = endpoint.url
			if (/\/login\/oauth\/access_token$/.test(url)) {
				return request(endpoint)
			}
			if (requiresAppAuth(url.replace(request.endpoint.DEFAULTS.baseUrl, ""))) {
				const { token: token2 } = await getAppAuthentication(state)
				endpoint.headers.authorization = `bearer ${token2}`
				let response
				try {
					response = await request(endpoint)
				} catch (error) {
					if (isNotTimeSkewError(error)) {
						throw error
					}
					if (typeof error.response.headers.date === "undefined") {
						throw error
					}
					const diff = Math.floor(
						(Date.parse(error.response.headers.date) -
							Date.parse(/* @__PURE__ */ new Date().toString())) /
							1e3
					)
					state.log.warn(error.message)
					state.log.warn(
						`[@octokit/auth-app] GitHub API time and system time are different by ${diff} seconds. Retrying request with the difference accounted for.`
					)
					const { token: token3 } = await getAppAuthentication({
						...state,
						timeDifference: diff,
					})
					endpoint.headers.authorization = `bearer ${token3}`
					return request(endpoint)
				}
				return response
			}
			if ((0, import_auth_oauth_user.requiresBasicAuth)(url)) {
				const authentication = await state.oauthApp({ type: "oauth-app" })
				endpoint.headers.authorization = authentication.headers.authorization
				return request(endpoint)
			}
			const { token, createdAt } = await getInstallationAuthentication(
				state,
				// @ts-expect-error TBD
				{},
				request
			)
			endpoint.headers.authorization = `token ${token}`
			return sendRequestWithRetries(state, request, endpoint, createdAt)
		}
		async function sendRequestWithRetries(state, request, options, createdAt, retries = 0) {
			const timeSinceTokenCreationInMs = +(/* @__PURE__ */ new Date()) - +new Date(createdAt)
			try {
				return await request(options)
			} catch (error) {
				if (error.status !== 401) {
					throw error
				}
				if (timeSinceTokenCreationInMs >= FIVE_SECONDS_IN_MS) {
					if (retries > 0) {
						error.message = `After ${retries} retries within ${
							timeSinceTokenCreationInMs / 1e3
						}s of creating the installation access token, the response remains 401. At this point, the cause may be an authentication problem or a system outage. Please check https://www.githubstatus.com for status information`
					}
					throw error
				}
				++retries
				const awaitTime = retries * 1e3
				state.log.warn(
					`[@octokit/auth-app] Retrying after 401 response to account for token replication delay (retry: ${retries}, wait: ${
						awaitTime / 1e3
					}s)`
				)
				await new Promise((resolve) => setTimeout(resolve, awaitTime))
				return sendRequestWithRetries(state, request, options, createdAt, retries)
			}
		}
		var VERSION = "6.1.1"
		var import_auth_oauth_user2 = require_dist_node17()
		function createAppAuth(options) {
			if (!options.appId) {
				throw new Error("[@octokit/auth-app] appId option is required")
			}
			if (!Number.isFinite(+options.appId)) {
				throw new Error("[@octokit/auth-app] appId option must be a number or numeric string")
			}
			if (!options.privateKey) {
				throw new Error("[@octokit/auth-app] privateKey option is required")
			}
			if ("installationId" in options && !options.installationId) {
				throw new Error("[@octokit/auth-app] installationId is set to a falsy value")
			}
			const log3 = Object.assign(
				{
					warn: console.warn.bind(console),
				},
				options.log
			)
			const request =
				options.request ||
				import_request.request.defaults({
					headers: {
						"user-agent": `octokit-auth-app.js/${VERSION} ${(0,
						import_universal_user_agent.getUserAgent)()}`,
					},
				})
			const state = Object.assign(
				{
					request,
					cache: getCache(),
				},
				options,
				options.installationId ? { installationId: Number(options.installationId) } : {},
				{
					log: log3,
					oauthApp: (0, import_auth_oauth_app.createOAuthAppAuth)({
						clientType: "github-app",
						clientId: options.clientId || "",
						clientSecret: options.clientSecret || "",
						request,
					}),
				}
			)
			return Object.assign(auth.bind(null, state), {
				hook: hook.bind(null, state),
			})
		}
	},
})

// ../../../node_modules/.pnpm/@octokit+auth-unauthenticated@5.0.1/node_modules/@octokit/auth-unauthenticated/dist-node/index.js
var require_dist_node21 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+auth-unauthenticated@5.0.1/node_modules/@octokit/auth-unauthenticated/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			createUnauthenticatedAuth: () => createUnauthenticatedAuth,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		async function auth(reason) {
			return {
				type: "unauthenticated",
				reason,
			}
		}
		var import_request_error = require_dist_node4()
		function isRateLimitError(error) {
			if (error.status !== 403) {
				return false
			}
			if (!error.response) {
				return false
			}
			return error.response.headers["x-ratelimit-remaining"] === "0"
		}
		var import_request_error2 = require_dist_node4()
		var REGEX_ABUSE_LIMIT_MESSAGE = /\babuse\b/i
		function isAbuseLimitError(error) {
			if (error.status !== 403) {
				return false
			}
			return REGEX_ABUSE_LIMIT_MESSAGE.test(error.message)
		}
		async function hook(reason, request, route, parameters) {
			const endpoint = request.endpoint.merge(route, parameters)
			return request(endpoint).catch((error) => {
				if (error.status === 404) {
					error.message = `Not found. May be due to lack of authentication. Reason: ${reason}`
					throw error
				}
				if (isRateLimitError(error)) {
					error.message = `API rate limit exceeded. This maybe caused by the lack of authentication. Reason: ${reason}`
					throw error
				}
				if (isAbuseLimitError(error)) {
					error.message = `You have triggered an abuse detection mechanism. This maybe caused by the lack of authentication. Reason: ${reason}`
					throw error
				}
				if (error.status === 401) {
					error.message = `Unauthorized. "${endpoint.method} ${endpoint.url}" failed most likely due to lack of authentication. Reason: ${reason}`
					throw error
				}
				if (error.status >= 400 && error.status < 500) {
					error.message = error.message.replace(
						/\.?$/,
						`. May be caused by lack of authentication (${reason}).`
					)
				}
				throw error
			})
		}
		var createUnauthenticatedAuth = function createUnauthenticatedAuth2(options) {
			if (!options || !options.reason) {
				throw new Error(
					"[@octokit/auth-unauthenticated] No reason passed to createUnauthenticatedAuth"
				)
			}
			return Object.assign(auth.bind(null, options.reason), {
				hook: hook.bind(null, options.reason),
			})
		}
	},
})

// ../../../node_modules/.pnpm/@octokit+oauth-app@6.1.0/node_modules/@octokit/oauth-app/dist-node/index.js
var require_dist_node22 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+oauth-app@6.1.0/node_modules/@octokit/oauth-app/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __create2 = Object.create
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __getProtoOf2 = Object.getPrototypeOf
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toESM2 = (mod, isNodeMode, target) => (
			(target = mod != undefined ? __create2(__getProtoOf2(mod)) : {}),
			__copyProps2(
				// If the importer is in node compatibility mode or this is not an ESM
				// file that has been converted to a CommonJS file using a Babel-
				// compatible transform (i.e. "__esModule" has not been set), then set
				// "default" to the CommonJS "module.exports" for node compatibility.
				isNodeMode || !mod || !mod.__esModule
					? __defProp2(target, "default", { value: mod, enumerable: true })
					: target,
				mod
			)
		)
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			OAuthApp: () => OAuthApp,
			createAWSLambdaAPIGatewayV2Handler: () => createAWSLambdaAPIGatewayV2Handler,
			createNodeMiddleware: () => createNodeMiddleware2,
			createWebWorkerHandler: () => createWebWorkerHandler,
			handleRequest: () => handleRequest,
			sendNodeResponse: () => sendResponse,
			unknownRouteResponse: () => unknownRouteResponse,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var import_auth_oauth_app = require_dist_node18()
		var VERSION = "6.1.0"
		function addEventHandler(state, eventName, eventHandler) {
			if (Array.isArray(eventName)) {
				for (const singleEventName of eventName) {
					addEventHandler(state, singleEventName, eventHandler)
				}
				return
			}
			if (!state.eventHandlers[eventName]) {
				state.eventHandlers[eventName] = []
			}
			state.eventHandlers[eventName].push(eventHandler)
		}
		var import_core = require_dist_node8()
		var import_universal_user_agent = require_dist_node()
		var OAuthAppOctokit = import_core.Octokit.defaults({
			userAgent: `octokit-oauth-app.js/${VERSION} ${(0,
			import_universal_user_agent.getUserAgent)()}`,
		})
		var import_auth_oauth_user = require_dist_node17()
		async function emitEvent(state, context2) {
			const { name, action } = context2
			if (state.eventHandlers[`${name}.${action}`]) {
				for (const eventHandler of state.eventHandlers[`${name}.${action}`]) {
					await eventHandler(context2)
				}
			}
			if (state.eventHandlers[name]) {
				for (const eventHandler of state.eventHandlers[name]) {
					await eventHandler(context2)
				}
			}
		}
		async function getUserOctokitWithState(state, options) {
			return state.octokit.auth({
				type: "oauth-user",
				...options,
				async factory(options2) {
					const octokit = new state.Octokit({
						authStrategy: import_auth_oauth_user.createOAuthUserAuth,
						auth: options2,
					})
					const authentication = await octokit.auth({
						type: "get",
					})
					await emitEvent(state, {
						name: "token",
						action: "created",
						token: authentication.token,
						scopes: authentication.scopes,
						authentication,
						octokit,
					})
					return octokit
				},
			})
		}
		var OAuthMethods = __toESM2(require_dist_node15())
		function getWebFlowAuthorizationUrlWithState(state, options) {
			const optionsWithDefaults = {
				clientId: state.clientId,
				request: state.octokit.request,
				...options,
				allowSignup: state.allowSignup ?? options.allowSignup,
				redirectUrl: options.redirectUrl ?? state.redirectUrl,
				scopes: options.scopes ?? state.defaultScopes,
			}
			return OAuthMethods.getWebFlowAuthorizationUrl({
				clientType: state.clientType,
				...optionsWithDefaults,
			})
		}
		var OAuthAppAuth = __toESM2(require_dist_node18())
		async function createTokenWithState(state, options) {
			const authentication = await state.octokit.auth({
				type: "oauth-user",
				...options,
			})
			await emitEvent(state, {
				name: "token",
				action: "created",
				token: authentication.token,
				scopes: authentication.scopes,
				authentication,
				octokit: new state.Octokit({
					authStrategy: OAuthAppAuth.createOAuthUserAuth,
					auth: {
						clientType: state.clientType,
						clientId: state.clientId,
						clientSecret: state.clientSecret,
						token: authentication.token,
						scopes: authentication.scopes,
						refreshToken: authentication.refreshToken,
						expiresAt: authentication.expiresAt,
						refreshTokenExpiresAt: authentication.refreshTokenExpiresAt,
					},
				}),
			})
			return { authentication }
		}
		var OAuthMethods2 = __toESM2(require_dist_node15())
		async function checkTokenWithState(state, options) {
			const result = await OAuthMethods2.checkToken({
				// @ts-expect-error not worth the extra code to appease TS
				clientType: state.clientType,
				clientId: state.clientId,
				clientSecret: state.clientSecret,
				request: state.octokit.request,
				...options,
			})
			Object.assign(result.authentication, { type: "token", tokenType: "oauth" })
			return result
		}
		var OAuthMethods3 = __toESM2(require_dist_node15())
		var import_auth_oauth_user2 = require_dist_node17()
		async function resetTokenWithState(state, options) {
			const optionsWithDefaults = {
				clientId: state.clientId,
				clientSecret: state.clientSecret,
				request: state.octokit.request,
				...options,
			}
			if (state.clientType === "oauth-app") {
				const response2 = await OAuthMethods3.resetToken({
					clientType: "oauth-app",
					...optionsWithDefaults,
				})
				const authentication2 = Object.assign(response2.authentication, {
					type: "token",
					tokenType: "oauth",
				})
				await emitEvent(state, {
					name: "token",
					action: "reset",
					token: response2.authentication.token,
					scopes: response2.authentication.scopes || void 0,
					authentication: authentication2,
					octokit: new state.Octokit({
						authStrategy: import_auth_oauth_user2.createOAuthUserAuth,
						auth: {
							clientType: state.clientType,
							clientId: state.clientId,
							clientSecret: state.clientSecret,
							token: response2.authentication.token,
							scopes: response2.authentication.scopes,
						},
					}),
				})
				return { ...response2, authentication: authentication2 }
			}
			const response = await OAuthMethods3.resetToken({
				clientType: "github-app",
				...optionsWithDefaults,
			})
			const authentication = Object.assign(response.authentication, {
				type: "token",
				tokenType: "oauth",
			})
			await emitEvent(state, {
				name: "token",
				action: "reset",
				token: response.authentication.token,
				authentication,
				octokit: new state.Octokit({
					authStrategy: import_auth_oauth_user2.createOAuthUserAuth,
					auth: {
						clientType: state.clientType,
						clientId: state.clientId,
						clientSecret: state.clientSecret,
						token: response.authentication.token,
					},
				}),
			})
			return { ...response, authentication }
		}
		var OAuthMethods4 = __toESM2(require_dist_node15())
		var import_auth_oauth_user3 = require_dist_node17()
		async function refreshTokenWithState(state, options) {
			if (state.clientType === "oauth-app") {
				throw new Error("[@octokit/oauth-app] app.refreshToken() is not supported for OAuth Apps")
			}
			const response = await OAuthMethods4.refreshToken({
				clientType: "github-app",
				clientId: state.clientId,
				clientSecret: state.clientSecret,
				request: state.octokit.request,
				refreshToken: options.refreshToken,
			})
			const authentication = Object.assign(response.authentication, {
				type: "token",
				tokenType: "oauth",
			})
			await emitEvent(state, {
				name: "token",
				action: "refreshed",
				token: response.authentication.token,
				authentication,
				octokit: new state.Octokit({
					authStrategy: import_auth_oauth_user3.createOAuthUserAuth,
					auth: {
						clientType: state.clientType,
						clientId: state.clientId,
						clientSecret: state.clientSecret,
						token: response.authentication.token,
					},
				}),
			})
			return { ...response, authentication }
		}
		var OAuthMethods5 = __toESM2(require_dist_node15())
		var import_auth_oauth_user4 = require_dist_node17()
		async function scopeTokenWithState(state, options) {
			if (state.clientType === "oauth-app") {
				throw new Error("[@octokit/oauth-app] app.scopeToken() is not supported for OAuth Apps")
			}
			const response = await OAuthMethods5.scopeToken({
				clientType: "github-app",
				clientId: state.clientId,
				clientSecret: state.clientSecret,
				request: state.octokit.request,
				...options,
			})
			const authentication = Object.assign(response.authentication, {
				type: "token",
				tokenType: "oauth",
			})
			await emitEvent(state, {
				name: "token",
				action: "scoped",
				token: response.authentication.token,
				authentication,
				octokit: new state.Octokit({
					authStrategy: import_auth_oauth_user4.createOAuthUserAuth,
					auth: {
						clientType: state.clientType,
						clientId: state.clientId,
						clientSecret: state.clientSecret,
						token: response.authentication.token,
					},
				}),
			})
			return { ...response, authentication }
		}
		var OAuthMethods6 = __toESM2(require_dist_node15())
		var import_auth_unauthenticated = require_dist_node21()
		async function deleteTokenWithState(state, options) {
			const optionsWithDefaults = {
				clientId: state.clientId,
				clientSecret: state.clientSecret,
				request: state.octokit.request,
				...options,
			}
			const response =
				state.clientType === "oauth-app"
					? await OAuthMethods6.deleteToken({
							clientType: "oauth-app",
							...optionsWithDefaults,
					  })
					: // istanbul ignore next
					  await OAuthMethods6.deleteToken({
							clientType: "github-app",
							...optionsWithDefaults,
					  })
			await emitEvent(state, {
				name: "token",
				action: "deleted",
				token: options.token,
				octokit: new state.Octokit({
					authStrategy: import_auth_unauthenticated.createUnauthenticatedAuth,
					auth: {
						reason: `Handling "token.deleted" event. The access for the token has been revoked.`,
					},
				}),
			})
			return response
		}
		var OAuthMethods7 = __toESM2(require_dist_node15())
		var import_auth_unauthenticated2 = require_dist_node21()
		async function deleteAuthorizationWithState(state, options) {
			const optionsWithDefaults = {
				clientId: state.clientId,
				clientSecret: state.clientSecret,
				request: state.octokit.request,
				...options,
			}
			const response =
				state.clientType === "oauth-app"
					? await OAuthMethods7.deleteAuthorization({
							clientType: "oauth-app",
							...optionsWithDefaults,
					  })
					: // istanbul ignore next
					  await OAuthMethods7.deleteAuthorization({
							clientType: "github-app",
							...optionsWithDefaults,
					  })
			await emitEvent(state, {
				name: "token",
				action: "deleted",
				token: options.token,
				octokit: new state.Octokit({
					authStrategy: import_auth_unauthenticated2.createUnauthenticatedAuth,
					auth: {
						reason: `Handling "token.deleted" event. The access for the token has been revoked.`,
					},
				}),
			})
			await emitEvent(state, {
				name: "authorization",
				action: "deleted",
				token: options.token,
				octokit: new state.Octokit({
					authStrategy: import_auth_unauthenticated2.createUnauthenticatedAuth,
					auth: {
						reason: `Handling "authorization.deleted" event. The access for the app has been revoked.`,
					},
				}),
			})
			return response
		}
		function unknownRouteResponse(request) {
			return {
				status: 404,
				headers: { "content-type": "application/json" },
				text: JSON.stringify({
					error: `Unknown route: ${request.method} ${request.url}`,
				}),
			}
		}
		async function handleRequest(app, { pathPrefix = "/api/github/oauth" }, request) {
			if (request.method === "OPTIONS") {
				return {
					status: 200,
					headers: {
						"access-control-allow-origin": "*",
						"access-control-allow-methods": "*",
						"access-control-allow-headers": "Content-Type, User-Agent, Authorization",
					},
				}
			}
			let { pathname } = new URL(request.url, "http://localhost")
			if (!pathname.startsWith(`${pathPrefix}/`)) {
				return void 0
			}
			pathname = pathname.slice(pathPrefix.length + 1)
			const route = [request.method, pathname].join(" ")
			const routes = {
				getLogin: `GET login`,
				getCallback: `GET callback`,
				createToken: `POST token`,
				getToken: `GET token`,
				patchToken: `PATCH token`,
				patchRefreshToken: `PATCH refresh-token`,
				scopeToken: `POST token/scoped`,
				deleteToken: `DELETE token`,
				deleteGrant: `DELETE grant`,
			}
			if (!Object.values(routes).includes(route)) {
				return unknownRouteResponse(request)
			}
			let json
			try {
				const text = await request.text()
				json = text ? JSON.parse(text) : {}
			} catch (error) {
				return {
					status: 400,
					headers: {
						"content-type": "application/json",
						"access-control-allow-origin": "*",
					},
					text: JSON.stringify({
						error: "[@octokit/oauth-app] request error",
					}),
				}
			}
			const { searchParams } = new URL(request.url, "http://localhost")
			const query = Object.fromEntries(searchParams)
			const headers = request.headers
			try {
				if (route === routes.getLogin) {
					const { url } = app.getWebFlowAuthorizationUrl({
						state: query.state,
						scopes: query.scopes ? query.scopes.split(",") : void 0,
						allowSignup: query.allowSignup ? query.allowSignup === "true" : void 0,
						redirectUrl: query.redirectUrl,
					})
					return { status: 302, headers: { location: url } }
				}
				if (route === routes.getCallback) {
					if (query.error) {
						throw new Error(`[@octokit/oauth-app] ${query.error} ${query.error_description}`)
					}
					if (!query.code) {
						throw new Error('[@octokit/oauth-app] "code" parameter is required')
					}
					const {
						authentication: { token: token2 },
					} = await app.createToken({
						code: query.code,
					})
					return {
						status: 200,
						headers: {
							"content-type": "text/html",
						},
						text: `<h1>Token created successfully</h1>

<p>Your token is: <strong>${token2}</strong>. Copy it now as it cannot be shown again.</p>`,
					}
				}
				if (route === routes.createToken) {
					const { code, redirectUrl } = json
					if (!code) {
						throw new Error('[@octokit/oauth-app] "code" parameter is required')
					}
					const result = await app.createToken({
						code,
						redirectUrl,
					})
					delete result.authentication.clientSecret
					return {
						status: 201,
						headers: {
							"content-type": "application/json",
							"access-control-allow-origin": "*",
						},
						text: JSON.stringify(result),
					}
				}
				if (route === routes.getToken) {
					const token2 = headers.authorization?.slice("token ".length)
					if (!token2) {
						throw new Error('[@octokit/oauth-app] "Authorization" header is required')
					}
					const result = await app.checkToken({
						token: token2,
					})
					delete result.authentication.clientSecret
					return {
						status: 200,
						headers: {
							"content-type": "application/json",
							"access-control-allow-origin": "*",
						},
						text: JSON.stringify(result),
					}
				}
				if (route === routes.patchToken) {
					const token2 = headers.authorization?.slice("token ".length)
					if (!token2) {
						throw new Error('[@octokit/oauth-app] "Authorization" header is required')
					}
					const result = await app.resetToken({ token: token2 })
					delete result.authentication.clientSecret
					return {
						status: 200,
						headers: {
							"content-type": "application/json",
							"access-control-allow-origin": "*",
						},
						text: JSON.stringify(result),
					}
				}
				if (route === routes.patchRefreshToken) {
					const token2 = headers.authorization?.slice("token ".length)
					if (!token2) {
						throw new Error('[@octokit/oauth-app] "Authorization" header is required')
					}
					const { refreshToken: refreshToken2 } = json
					if (!refreshToken2) {
						throw new Error("[@octokit/oauth-app] refreshToken must be sent in request body")
					}
					const result = await app.refreshToken({ refreshToken: refreshToken2 })
					delete result.authentication.clientSecret
					return {
						status: 200,
						headers: {
							"content-type": "application/json",
							"access-control-allow-origin": "*",
						},
						text: JSON.stringify(result),
					}
				}
				if (route === routes.scopeToken) {
					const token2 = headers.authorization?.slice("token ".length)
					if (!token2) {
						throw new Error('[@octokit/oauth-app] "Authorization" header is required')
					}
					const result = await app.scopeToken({
						token: token2,
						...json,
					})
					delete result.authentication.clientSecret
					return {
						status: 200,
						headers: {
							"content-type": "application/json",
							"access-control-allow-origin": "*",
						},
						text: JSON.stringify(result),
					}
				}
				if (route === routes.deleteToken) {
					const token2 = headers.authorization?.slice("token ".length)
					if (!token2) {
						throw new Error('[@octokit/oauth-app] "Authorization" header is required')
					}
					await app.deleteToken({
						token: token2,
					})
					return {
						status: 204,
						headers: { "access-control-allow-origin": "*" },
					}
				}
				const token = headers.authorization?.slice("token ".length)
				if (!token) {
					throw new Error('[@octokit/oauth-app] "Authorization" header is required')
				}
				await app.deleteAuthorization({
					token,
				})
				return {
					status: 204,
					headers: { "access-control-allow-origin": "*" },
				}
			} catch (error) {
				return {
					status: 400,
					headers: {
						"content-type": "application/json",
						"access-control-allow-origin": "*",
					},
					text: JSON.stringify({ error: error.message }),
				}
			}
		}
		function parseRequest(request) {
			const { method, url, headers } = request
			async function text() {
				const text2 = await new Promise((resolve, reject) => {
					let bodyChunks = []
					request
						.on("error", reject)
						.on("data", (chunk) => bodyChunks.push(chunk))
						.on("end", () => resolve(Buffer.concat(bodyChunks).toString()))
				})
				return text2
			}
			return { method, url, headers, text }
		}
		function sendResponse(octokitResponse, response) {
			response.writeHead(octokitResponse.status, octokitResponse.headers)
			response.end(octokitResponse.text)
		}
		function createNodeMiddleware2(app, options = {}) {
			return async function (request, response, next) {
				const octokitRequest = await parseRequest(request)
				const octokitResponse = await handleRequest(app, options, octokitRequest)
				if (octokitResponse) {
					sendResponse(octokitResponse, response)
					return true
				} else {
					next?.()
					return false
				}
			}
		}
		function parseRequest2(request) {
			const headers = Object.fromEntries(request.headers.entries())
			return {
				method: request.method,
				url: request.url,
				headers,
				text: () => request.text(),
			}
		}
		function sendResponse2(octokitResponse) {
			return new Response(octokitResponse.text, {
				status: octokitResponse.status,
				headers: octokitResponse.headers,
			})
		}
		function createWebWorkerHandler(app, options = {}) {
			return async function (request) {
				const octokitRequest = await parseRequest2(request)
				const octokitResponse = await handleRequest(app, options, octokitRequest)
				return octokitResponse ? sendResponse2(octokitResponse) : void 0
			}
		}
		function parseRequest3(request) {
			const { method } = request.requestContext.http
			let url = request.rawPath
			const { stage } = request.requestContext
			if (url.startsWith("/" + stage)) url = url.slice(Math.max(0, stage.length + 1))
			if (request.rawQueryString) url += "?" + request.rawQueryString
			const headers = request.headers
			const text = async () => request.body || ""
			return { method, url, headers, text }
		}
		function sendResponse3(octokitResponse) {
			return {
				statusCode: octokitResponse.status,
				headers: octokitResponse.headers,
				body: octokitResponse.text,
			}
		}
		function createAWSLambdaAPIGatewayV2Handler(app, options = {}) {
			return async function (event) {
				const request = parseRequest3(event)
				const response = await handleRequest(app, options, request)
				return response ? sendResponse3(response) : void 0
			}
		}
		var OAuthApp = class {
			static {
				this.VERSION = VERSION
			}
			static defaults(defaults) {
				const OAuthAppWithDefaults = class extends this {
					constructor(...args) {
						super({
							...defaults,
							...args[0],
						})
					}
				}
				return OAuthAppWithDefaults
			}
			constructor(options) {
				const Octokit2 = options.Octokit || OAuthAppOctokit
				this.type = options.clientType || "oauth-app"
				const octokit = new Octokit2({
					authStrategy: import_auth_oauth_app.createOAuthAppAuth,
					auth: {
						clientType: this.type,
						clientId: options.clientId,
						clientSecret: options.clientSecret,
					},
				})
				const state = {
					clientType: this.type,
					clientId: options.clientId,
					clientSecret: options.clientSecret,
					// @ts-expect-error defaultScopes not permitted for GitHub Apps
					defaultScopes: options.defaultScopes || [],
					allowSignup: options.allowSignup,
					baseUrl: options.baseUrl,
					redirectUrl: options.redirectUrl,
					log: options.log,
					Octokit: Octokit2,
					octokit,
					eventHandlers: {},
				}
				this.on = addEventHandler.bind(null, state)
				this.octokit = octokit
				this.getUserOctokit = getUserOctokitWithState.bind(null, state)
				this.getWebFlowAuthorizationUrl = getWebFlowAuthorizationUrlWithState.bind(null, state)
				this.createToken = createTokenWithState.bind(null, state)
				this.checkToken = checkTokenWithState.bind(null, state)
				this.resetToken = resetTokenWithState.bind(null, state)
				this.refreshToken = refreshTokenWithState.bind(null, state)
				this.scopeToken = scopeTokenWithState.bind(null, state)
				this.deleteToken = deleteTokenWithState.bind(null, state)
				this.deleteAuthorization = deleteAuthorizationWithState.bind(null, state)
			}
		}
	},
})

// ../../../node_modules/.pnpm/indent-string@4.0.0/node_modules/indent-string/index.js
var require_indent_string = __commonJS({
	"../../../node_modules/.pnpm/indent-string@4.0.0/node_modules/indent-string/index.js"(
		exports2,
		module2
	) {
		"use strict"
		module2.exports = (string, count = 1, options) => {
			options = {
				indent: " ",
				includeEmptyLines: false,
				...options,
			}
			if (typeof string !== "string") {
				throw new TypeError(`Expected \`input\` to be a \`string\`, got \`${typeof string}\``)
			}
			if (typeof count !== "number") {
				throw new TypeError(`Expected \`count\` to be a \`number\`, got \`${typeof count}\``)
			}
			if (typeof options.indent !== "string") {
				throw new TypeError(
					`Expected \`options.indent\` to be a \`string\`, got \`${typeof options.indent}\``
				)
			}
			if (count === 0) {
				return string
			}
			const regex = options.includeEmptyLines ? /^/gm : /^(?!\s*$)/gm
			return string.replace(regex, options.indent.repeat(count))
		}
	},
})

// ../../../node_modules/.pnpm/clean-stack@2.2.0/node_modules/clean-stack/index.js
var require_clean_stack = __commonJS({
	"../../../node_modules/.pnpm/clean-stack@2.2.0/node_modules/clean-stack/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var os = require("node:os")
		var extractPathRegex = /\s+at.*(?:\(|\s)(.*)\)?/
		var pathRegex =
			/^(?:(?:(?:node|(?:internal\/[\w/]*|.*node_modules\/(?:babel-polyfill|pirates)\/.*)?\w+)\.js:\d+:\d+)|native)/
		var homeDir = typeof os.homedir === "undefined" ? "" : os.homedir()
		module2.exports = (stack, options) => {
			options = Object.assign({ pretty: false }, options)
			return stack
				.replace(/\\/g, "/")
				.split("\n")
				.filter((line) => {
					const pathMatches = line.match(extractPathRegex)
					if (pathMatches === null || !pathMatches[1]) {
						return true
					}
					const match = pathMatches[1]
					if (
						match.includes(".app/Contents/Resources/electron.asar") ||
						match.includes(".app/Contents/Resources/default_app.asar")
					) {
						return false
					}
					return !pathRegex.test(match)
				})
				.filter((line) => line.trim() !== "")
				.map((line) => {
					if (options.pretty) {
						return line.replace(extractPathRegex, (m, p1) =>
							m.replace(p1, p1.replace(homeDir, "~"))
						)
					}
					return line
				})
				.join("\n")
		}
	},
})

// ../../../node_modules/.pnpm/aggregate-error@3.1.0/node_modules/aggregate-error/index.js
var require_aggregate_error = __commonJS({
	"../../../node_modules/.pnpm/aggregate-error@3.1.0/node_modules/aggregate-error/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var indentString = require_indent_string()
		var cleanStack = require_clean_stack()
		var cleanInternalStack = (stack) =>
			stack.replace(/\s+at .*aggregate-error\/index.js:\d+:\d+\)?/g, "")
		var AggregateError = class extends Error {
			constructor(errors) {
				if (!Array.isArray(errors)) {
					throw new TypeError(`Expected input to be an Array, got ${typeof errors}`)
				}
				errors = [...errors].map((error) => {
					if (error instanceof Error) {
						return error
					}
					if (error !== null && typeof error === "object") {
						return Object.assign(new Error(error.message), error)
					}
					return new Error(error)
				})
				let message = errors
					.map((error) => {
						return typeof error.stack === "string"
							? cleanInternalStack(cleanStack(error.stack))
							: String(error)
					})
					.join("\n")
				message = "\n" + indentString(message, 4)
				super(message)
				this.name = "AggregateError"
				Object.defineProperty(this, "_errors", { value: errors })
			}
			*[Symbol.iterator]() {
				for (const error of this._errors) {
					yield error
				}
			}
		}
		module2.exports = AggregateError
	},
})

// ../../../node_modules/.pnpm/@octokit+webhooks-methods@4.1.0/node_modules/@octokit/webhooks-methods/dist-node/index.js
var require_dist_node23 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+webhooks-methods@4.1.0/node_modules/@octokit/webhooks-methods/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			sign: () => sign,
			verify: () => verify,
			verifyWithFallback: () => verifyWithFallback,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var import_node_crypto = require("node:crypto")
		var Algorithm = /* @__PURE__ */ ((Algorithm2) => {
			Algorithm2["SHA1"] = "sha1"
			Algorithm2["SHA256"] = "sha256"
			return Algorithm2
		})(Algorithm || {})
		var VERSION = "4.1.0"
		async function sign(options, payload) {
			const { secret, algorithm } =
				typeof options === "object"
					? {
							secret: options.secret,
							algorithm: options.algorithm || Algorithm.SHA256,
					  }
					: { secret: options, algorithm: Algorithm.SHA256 }
			if (!secret || !payload) {
				throw new TypeError("[@octokit/webhooks-methods] secret & payload required for sign()")
			}
			if (typeof payload !== "string") {
				throw new TypeError("[@octokit/webhooks-methods] payload must be a string")
			}
			if (!Object.values(Algorithm).includes(algorithm)) {
				throw new TypeError(
					`[@octokit/webhooks] Algorithm ${algorithm} is not supported. Must be  'sha1' or 'sha256'`
				)
			}
			return `${algorithm}=${(0, import_node_crypto.createHmac)(algorithm, secret)
				.update(payload)
				.digest("hex")}`
		}
		sign.VERSION = VERSION
		var import_node_crypto2 = require("node:crypto")
		var import_node_buffer = require("node:buffer")
		var getAlgorithm = (signature) => {
			return signature.startsWith("sha256=") ? "sha256" : "sha1"
		}
		async function verify(secret, eventPayload, signature) {
			if (!secret || !eventPayload || !signature) {
				throw new TypeError("[@octokit/webhooks-methods] secret, eventPayload & signature required")
			}
			if (typeof eventPayload !== "string") {
				throw new TypeError("[@octokit/webhooks-methods] eventPayload must be a string")
			}
			const signatureBuffer = import_node_buffer.Buffer.from(signature)
			const algorithm = getAlgorithm(signature)
			const verificationBuffer = import_node_buffer.Buffer.from(
				await sign({ secret, algorithm }, eventPayload)
			)
			if (signatureBuffer.length !== verificationBuffer.length) {
				return false
			}
			return (0, import_node_crypto2.timingSafeEqual)(signatureBuffer, verificationBuffer)
		}
		verify.VERSION = VERSION
		async function verifyWithFallback(secret, payload, signature, additionalSecrets) {
			const firstPass = await verify(secret, payload, signature)
			if (firstPass) {
				return true
			}
			if (additionalSecrets !== void 0) {
				for (const s of additionalSecrets) {
					const v = await verify(s, payload, signature)
					if (v) {
						return v
					}
				}
			}
			return false
		}
	},
})

// ../../../node_modules/.pnpm/@octokit+webhooks@12.2.0/node_modules/@octokit/webhooks/dist-node/index.js
var require_dist_node24 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+webhooks@12.2.0/node_modules/@octokit/webhooks/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __create2 = Object.create
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __getProtoOf2 = Object.getPrototypeOf
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toESM2 = (mod, isNodeMode, target) => (
			(target = mod != undefined ? __create2(__getProtoOf2(mod)) : {}),
			__copyProps2(
				// If the importer is in node compatibility mode or this is not an ESM
				// file that has been converted to a CommonJS file using a Babel-
				// compatible transform (i.e. "__esModule" has not been set), then set
				// "default" to the CommonJS "module.exports" for node compatibility.
				isNodeMode || !mod || !mod.__esModule
					? __defProp2(target, "default", { value: mod, enumerable: true })
					: target,
				mod
			)
		)
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			Webhooks: () => Webhooks,
			createEventHandler: () => createEventHandler,
			createNodeMiddleware: () => createNodeMiddleware2,
			emitterEventNames: () => emitterEventNames,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var createLogger = (logger) => ({
			debug: () => {},
			info: () => {},
			warn: console.warn.bind(console),
			error: console.error.bind(console),
			...logger,
		})
		var emitterEventNames = [
			"branch_protection_configuration",
			"branch_protection_rule.disabled",
			"branch_protection_rule.enabled",
			"branch_protection_rule",
			"branch_protection_rule.created",
			"branch_protection_rule.deleted",
			"branch_protection_rule.edited",
			"check_run",
			"check_run.completed",
			"check_run.created",
			"check_run.requested_action",
			"check_run.rerequested",
			"check_suite",
			"check_suite.completed",
			"check_suite.requested",
			"check_suite.rerequested",
			"code_scanning_alert",
			"code_scanning_alert.appeared_in_branch",
			"code_scanning_alert.closed_by_user",
			"code_scanning_alert.created",
			"code_scanning_alert.fixed",
			"code_scanning_alert.reopened",
			"code_scanning_alert.reopened_by_user",
			"commit_comment",
			"commit_comment.created",
			"create",
			"custom_property",
			"custom_property.created",
			"custom_property.deleted",
			"custom_property_values",
			"custom_property_values.updated",
			"delete",
			"dependabot_alert",
			"dependabot_alert.created",
			"dependabot_alert.dismissed",
			"dependabot_alert.fixed",
			"dependabot_alert.reintroduced",
			"dependabot_alert.reopened",
			"deploy_key",
			"deploy_key.created",
			"deploy_key.deleted",
			"deployment",
			"deployment.created",
			"deployment_protection_rule",
			"deployment_protection_rule.requested",
			"deployment_review",
			"deployment_review.approved",
			"deployment_review.rejected",
			"deployment_review.requested",
			"deployment_status",
			"deployment_status.created",
			"discussion",
			"discussion.answered",
			"discussion.category_changed",
			"discussion.created",
			"discussion.deleted",
			"discussion.edited",
			"discussion.labeled",
			"discussion.locked",
			"discussion.pinned",
			"discussion.transferred",
			"discussion.unanswered",
			"discussion.unlabeled",
			"discussion.unlocked",
			"discussion.unpinned",
			"discussion_comment",
			"discussion_comment.created",
			"discussion_comment.deleted",
			"discussion_comment.edited",
			"fork",
			"github_app_authorization",
			"github_app_authorization.revoked",
			"gollum",
			"installation",
			"installation.created",
			"installation.deleted",
			"installation.new_permissions_accepted",
			"installation.suspend",
			"installation.unsuspend",
			"installation_repositories",
			"installation_repositories.added",
			"installation_repositories.removed",
			"installation_target",
			"installation_target.renamed",
			"issue_comment",
			"issue_comment.created",
			"issue_comment.deleted",
			"issue_comment.edited",
			"issues",
			"issues.assigned",
			"issues.closed",
			"issues.deleted",
			"issues.demilestoned",
			"issues.edited",
			"issues.labeled",
			"issues.locked",
			"issues.milestoned",
			"issues.opened",
			"issues.pinned",
			"issues.reopened",
			"issues.transferred",
			"issues.unassigned",
			"issues.unlabeled",
			"issues.unlocked",
			"issues.unpinned",
			"label",
			"label.created",
			"label.deleted",
			"label.edited",
			"marketplace_purchase",
			"marketplace_purchase.cancelled",
			"marketplace_purchase.changed",
			"marketplace_purchase.pending_change",
			"marketplace_purchase.pending_change_cancelled",
			"marketplace_purchase.purchased",
			"member",
			"member.added",
			"member.edited",
			"member.removed",
			"membership",
			"membership.added",
			"membership.removed",
			"merge_group",
			"merge_group.checks_requested",
			"meta",
			"meta.deleted",
			"milestone",
			"milestone.closed",
			"milestone.created",
			"milestone.deleted",
			"milestone.edited",
			"milestone.opened",
			"org_block",
			"org_block.blocked",
			"org_block.unblocked",
			"organization",
			"organization.deleted",
			"organization.member_added",
			"organization.member_invited",
			"organization.member_removed",
			"organization.renamed",
			"package",
			"package.published",
			"package.updated",
			"page_build",
			"ping",
			"project",
			"project.closed",
			"project.created",
			"project.deleted",
			"project.edited",
			"project.reopened",
			"project_card",
			"project_card.converted",
			"project_card.created",
			"project_card.deleted",
			"project_card.edited",
			"project_card.moved",
			"project_column",
			"project_column.created",
			"project_column.deleted",
			"project_column.edited",
			"project_column.moved",
			"projects_v2_item",
			"projects_v2_item.archived",
			"projects_v2_item.converted",
			"projects_v2_item.created",
			"projects_v2_item.deleted",
			"projects_v2_item.edited",
			"projects_v2_item.reordered",
			"projects_v2_item.restored",
			"public",
			"pull_request",
			"pull_request.assigned",
			"pull_request.auto_merge_disabled",
			"pull_request.auto_merge_enabled",
			"pull_request.closed",
			"pull_request.converted_to_draft",
			"pull_request.demilestoned",
			"pull_request.dequeued",
			"pull_request.edited",
			"pull_request.enqueued",
			"pull_request.labeled",
			"pull_request.locked",
			"pull_request.milestoned",
			"pull_request.opened",
			"pull_request.ready_for_review",
			"pull_request.reopened",
			"pull_request.review_request_removed",
			"pull_request.review_requested",
			"pull_request.synchronize",
			"pull_request.unassigned",
			"pull_request.unlabeled",
			"pull_request.unlocked",
			"pull_request_review",
			"pull_request_review.dismissed",
			"pull_request_review.edited",
			"pull_request_review.submitted",
			"pull_request_review_comment",
			"pull_request_review_comment.created",
			"pull_request_review_comment.deleted",
			"pull_request_review_comment.edited",
			"pull_request_review_thread",
			"pull_request_review_thread.resolved",
			"pull_request_review_thread.unresolved",
			"push",
			"registry_package",
			"registry_package.published",
			"registry_package.updated",
			"release",
			"release.created",
			"release.deleted",
			"release.edited",
			"release.prereleased",
			"release.published",
			"release.released",
			"release.unpublished",
			"repository",
			"repository.archived",
			"repository.created",
			"repository.deleted",
			"repository.edited",
			"repository.privatized",
			"repository.publicized",
			"repository.renamed",
			"repository.transferred",
			"repository.unarchived",
			"repository_dispatch",
			"repository_import",
			"repository_vulnerability_alert",
			"repository_vulnerability_alert.create",
			"repository_vulnerability_alert.dismiss",
			"repository_vulnerability_alert.reopen",
			"repository_vulnerability_alert.resolve",
			"secret_scanning_alert",
			"secret_scanning_alert.created",
			"secret_scanning_alert.reopened",
			"secret_scanning_alert.resolved",
			"secret_scanning_alert.revoked",
			"secret_scanning_alert_location",
			"secret_scanning_alert_location.created",
			"security_advisory",
			"security_advisory.performed",
			"security_advisory.published",
			"security_advisory.updated",
			"security_advisory.withdrawn",
			"sponsorship",
			"sponsorship.cancelled",
			"sponsorship.created",
			"sponsorship.edited",
			"sponsorship.pending_cancellation",
			"sponsorship.pending_tier_change",
			"sponsorship.tier_changed",
			"star",
			"star.created",
			"star.deleted",
			"status",
			"team",
			"team.added_to_repository",
			"team.created",
			"team.deleted",
			"team.edited",
			"team.removed_from_repository",
			"team_add",
			"watch",
			"watch.started",
			"workflow_dispatch",
			"workflow_job",
			"workflow_job.completed",
			"workflow_job.in_progress",
			"workflow_job.queued",
			"workflow_job.waiting",
			"workflow_run",
			"workflow_run.completed",
			"workflow_run.in_progress",
			"workflow_run.requested",
		]
		function handleEventHandlers(state, webhookName, handler) {
			if (!state.hooks[webhookName]) {
				state.hooks[webhookName] = []
			}
			state.hooks[webhookName].push(handler)
		}
		function receiverOn(state, webhookNameOrNames, handler) {
			if (Array.isArray(webhookNameOrNames)) {
				for (const webhookName of webhookNameOrNames) receiverOn(state, webhookName, handler)
				return
			}
			if (["*", "error"].includes(webhookNameOrNames)) {
				const webhookName = webhookNameOrNames === "*" ? "any" : webhookNameOrNames
				const message = `Using the "${webhookNameOrNames}" event with the regular Webhooks.on() function is not supported. Please use the Webhooks.on${
					webhookName.charAt(0).toUpperCase() + webhookName.slice(1)
				}() method instead`
				throw new Error(message)
			}
			if (!emitterEventNames.includes(webhookNameOrNames)) {
				state.log.warn(
					`"${webhookNameOrNames}" is not a known webhook name (https://developer.github.com/v3/activity/events/types/)`
				)
			}
			handleEventHandlers(state, webhookNameOrNames, handler)
		}
		function receiverOnAny(state, handler) {
			handleEventHandlers(state, "*", handler)
		}
		function receiverOnError(state, handler) {
			handleEventHandlers(state, "error", handler)
		}
		var import_aggregate_error = __toESM2(require_aggregate_error())
		function wrapErrorHandler(handler, error) {
			let returnValue
			try {
				returnValue = handler(error)
			} catch (error2) {
				console.log('FATAL: Error occurred in "error" event handler')
				console.log(error2)
			}
			if (returnValue && returnValue.catch) {
				returnValue.catch((error2) => {
					console.log('FATAL: Error occurred in "error" event handler')
					console.log(error2)
				})
			}
		}
		function getHooks(state, eventPayloadAction, eventName) {
			const hooks = [state.hooks[eventName], state.hooks["*"]]
			if (eventPayloadAction) {
				hooks.unshift(state.hooks[`${eventName}.${eventPayloadAction}`])
			}
			return [].concat(...hooks.filter(Boolean))
		}
		function receiverHandle(state, event) {
			const errorHandlers = state.hooks.error || []
			if (event instanceof Error) {
				const error = Object.assign(new import_aggregate_error.default([event]), {
					event,
					errors: [event],
				})
				for (const handler of errorHandlers) wrapErrorHandler(handler, error)
				return Promise.reject(error)
			}
			if (!event || !event.name) {
				throw new import_aggregate_error.default(["Event name not passed"])
			}
			if (!event.payload) {
				throw new import_aggregate_error.default(["Event payload not passed"])
			}
			const hooks = getHooks(
				state,
				"action" in event.payload ? event.payload.action : null,
				event.name
			)
			if (hooks.length === 0) {
				return Promise.resolve()
			}
			const errors = []
			const promises = hooks.map((handler) => {
				let promise = Promise.resolve(event)
				if (state.transform) {
					promise = promise.then(state.transform)
				}
				return promise
					.then((event2) => {
						return handler(event2)
					})
					.catch((error) => errors.push(Object.assign(error, { event })))
			})
			return Promise.all(promises).then(() => {
				if (errors.length === 0) {
					return
				}
				const error = new import_aggregate_error.default(errors)
				Object.assign(error, {
					event,
					errors,
				})
				for (const handler of errorHandlers) wrapErrorHandler(handler, error)
				throw error
			})
		}
		function removeListener(state, webhookNameOrNames, handler) {
			if (Array.isArray(webhookNameOrNames)) {
				for (const webhookName of webhookNameOrNames) removeListener(state, webhookName, handler)
				return
			}
			if (!state.hooks[webhookNameOrNames]) {
				return
			}
			for (let i = state.hooks[webhookNameOrNames].length - 1; i >= 0; i--) {
				if (state.hooks[webhookNameOrNames][i] === handler) {
					state.hooks[webhookNameOrNames].splice(i, 1)
					return
				}
			}
		}
		function createEventHandler(options) {
			const state = {
				hooks: {},
				log: createLogger(options && options.log),
			}
			if (options && options.transform) {
				state.transform = options.transform
			}
			return {
				on: receiverOn.bind(null, state),
				onAny: receiverOnAny.bind(null, state),
				onError: receiverOnError.bind(null, state),
				removeListener: removeListener.bind(null, state),
				receive: receiverHandle.bind(null, state),
			}
		}
		var import_webhooks_methods2 = require_dist_node23()
		var import_aggregate_error2 = __toESM2(require_aggregate_error())
		var import_webhooks_methods = require_dist_node23()
		async function verifyAndReceive(state, event) {
			const matchesSignature = await (0, import_webhooks_methods.verify)(
				state.secret,
				event.payload,
				event.signature
			).catch(() => false)
			if (!matchesSignature) {
				const error = new Error(
					"[@octokit/webhooks] signature does not match event payload and secret"
				)
				return state.eventHandler.receive(Object.assign(error, { event, status: 400 }))
			}
			let payload
			try {
				payload = JSON.parse(event.payload)
			} catch (error) {
				error.message = "Invalid JSON"
				error.status = 400
				throw new import_aggregate_error2.default([error])
			}
			return state.eventHandler.receive({
				id: event.id,
				name: event.name,
				payload,
			})
		}
		var WEBHOOK_HEADERS = ["x-github-event", "x-hub-signature-256", "x-github-delivery"]
		function getMissingHeaders(request) {
			return WEBHOOK_HEADERS.filter((header) => !(header in request.headers))
		}
		var import_aggregate_error3 = __toESM2(require_aggregate_error())
		function getPayload(request) {
			if ("body" in request) {
				if (
					typeof request.body === "object" &&
					"rawBody" in request &&
					request.rawBody instanceof Buffer
				) {
					return Promise.resolve(request.rawBody.toString("utf8"))
				} else {
					return Promise.resolve(request.body)
				}
			}
			return new Promise((resolve, reject) => {
				let data = []
				request.on("error", (error) => reject(new import_aggregate_error3.default([error])))
				request.on("data", (chunk) => data.push(chunk))
				request.on("end", () =>
					// setImmediate improves the throughput by reducing the pressure from
					// the event loop
					setImmediate(
						resolve,
						data.length === 1 ? data[0].toString("utf8") : Buffer.concat(data).toString("utf8")
					)
				)
			})
		}
		function onUnhandledRequestDefault(request, response) {
			response.writeHead(404, {
				"content-type": "application/json",
			})
			response.end(
				JSON.stringify({
					error: `Unknown route: ${request.method} ${request.url}`,
				})
			)
		}
		async function middleware(webhooks, options, request, response, next) {
			let pathname
			try {
				pathname = new URL(request.url, "http://localhost").pathname
			} catch (error) {
				response.writeHead(422, {
					"content-type": "application/json",
				})
				response.end(
					JSON.stringify({
						error: `Request URL could not be parsed: ${request.url}`,
					})
				)
				return true
			}
			if (pathname !== options.path) {
				next?.()
				return false
			} else if (request.method !== "POST") {
				onUnhandledRequestDefault(request, response)
				return true
			}
			if (
				!request.headers["content-type"] ||
				!request.headers["content-type"].startsWith("application/json")
			) {
				response.writeHead(415, {
					"content-type": "application/json",
					accept: "application/json",
				})
				response.end(
					JSON.stringify({
						error: `Unsupported "Content-Type" header value. Must be "application/json"`,
					})
				)
				return true
			}
			const missingHeaders = getMissingHeaders(request).join(", ")
			if (missingHeaders) {
				response.writeHead(400, {
					"content-type": "application/json",
				})
				response.end(
					JSON.stringify({
						error: `Required headers missing: ${missingHeaders}`,
					})
				)
				return true
			}
			const eventName = request.headers["x-github-event"]
			const signatureSHA256 = request.headers["x-hub-signature-256"]
			const id = request.headers["x-github-delivery"]
			options.log.debug(`${eventName} event received (id: ${id})`)
			let didTimeout = false
			const timeout = setTimeout(() => {
				didTimeout = true
				response.statusCode = 202
				response.end("still processing\n")
			}, 9e3).unref()
			try {
				const payload = await getPayload(request)
				await webhooks.verifyAndReceive({
					id,
					name: eventName,
					payload,
					signature: signatureSHA256,
				})
				clearTimeout(timeout)
				if (didTimeout) return true
				response.end("ok\n")
				return true
			} catch (error) {
				clearTimeout(timeout)
				if (didTimeout) return true
				const err = [...error][0]
				const errorMessage = err.message
					? `${err.name}: ${err.message}`
					: "Error: An Unspecified error occurred"
				response.statusCode = typeof err.status !== "undefined" ? err.status : 500
				options.log.error(error)
				response.end(
					JSON.stringify({
						error: errorMessage,
					})
				)
				return true
			}
		}
		function createNodeMiddleware2(
			webhooks,
			{ path = "/api/github/webhooks", log: log3 = createLogger() } = {}
		) {
			return middleware.bind(null, webhooks, {
				path,
				log: log3,
			})
		}
		var Webhooks = class {
			constructor(options) {
				if (!options || !options.secret) {
					throw new Error("[@octokit/webhooks] options.secret required")
				}
				const state = {
					eventHandler: createEventHandler(options),
					secret: options.secret,
					hooks: {},
					log: createLogger(options.log),
				}
				this.sign = import_webhooks_methods2.sign.bind(null, options.secret)
				this.verify = import_webhooks_methods2.verify.bind(null, options.secret)
				this.on = state.eventHandler.on
				this.onAny = state.eventHandler.onAny
				this.onError = state.eventHandler.onError
				this.removeListener = state.eventHandler.removeListener
				this.receive = state.eventHandler.receive
				this.verifyAndReceive = verifyAndReceive.bind(null, state)
			}
		}
	},
})

// ../../../node_modules/.pnpm/@octokit+app@14.1.0/node_modules/@octokit/app/dist-node/index.js
var require_dist_node25 = __commonJS({
	"../../../node_modules/.pnpm/@octokit+app@14.1.0/node_modules/@octokit/app/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			App: () => App,
			createNodeMiddleware: () => createNodeMiddleware2,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var import_core = require_dist_node8()
		var import_auth_app3 = require_dist_node20()
		var import_oauth_app2 = require_dist_node22()
		var VERSION = "14.1.0"
		var import_auth_app = require_dist_node20()
		var import_auth_unauthenticated = require_dist_node21()
		var import_webhooks = require_dist_node24()
		function webhooks(appOctokit, options) {
			return new import_webhooks.Webhooks({
				secret: options.secret,
				transform: async (event) => {
					if (
						!("installation" in event.payload) ||
						typeof event.payload.installation !== "object"
					) {
						const octokit2 = new appOctokit.constructor({
							authStrategy: import_auth_unauthenticated.createUnauthenticatedAuth,
							auth: {
								reason: `"installation" key missing in webhook event payload`,
							},
						})
						return {
							...event,
							octokit: octokit2,
						}
					}
					const installationId = event.payload.installation.id
					const octokit = await appOctokit.auth({
						type: "installation",
						installationId,
						factory(auth) {
							return new auth.octokit.constructor({
								...auth.octokitOptions,
								authStrategy: import_auth_app.createAppAuth,
								...{
									auth: {
										...auth,
										installationId,
									},
								},
							})
						},
					})
					octokit.hook.before("request", (options2) => {
						options2.headers["x-github-delivery"] = event.id
					})
					return {
						...event,
						octokit,
					}
				},
			})
		}
		var import_plugin_paginate_rest = require_dist_node10()
		var import_auth_app2 = require_dist_node20()
		async function getInstallationOctokit(app, installationId) {
			return app.octokit.auth({
				type: "installation",
				installationId,
				factory(auth) {
					const options = {
						...auth.octokitOptions,
						authStrategy: import_auth_app2.createAppAuth,
						...{ auth: { ...auth, installationId } },
					}
					return new auth.octokit.constructor(options)
				},
			})
		}
		function eachInstallationFactory(app) {
			return Object.assign(eachInstallation.bind(null, app), {
				iterator: eachInstallationIterator.bind(null, app),
			})
		}
		async function eachInstallation(app, callback) {
			const i = eachInstallationIterator(app)[Symbol.asyncIterator]()
			let result = await i.next()
			while (!result.done) {
				await callback(result.value)
				result = await i.next()
			}
		}
		function eachInstallationIterator(app) {
			return {
				async *[Symbol.asyncIterator]() {
					const iterator = import_plugin_paginate_rest.composePaginateRest.iterator(
						app.octokit,
						"GET /app/installations"
					)
					for await (const { data: installations } of iterator) {
						for (const installation of installations) {
							const installationOctokit = await getInstallationOctokit(app, installation.id)
							yield { octokit: installationOctokit, installation }
						}
					}
				},
			}
		}
		var import_plugin_paginate_rest2 = require_dist_node10()
		function eachRepositoryFactory(app) {
			return Object.assign(eachRepository.bind(null, app), {
				iterator: eachRepositoryIterator.bind(null, app),
			})
		}
		async function eachRepository(app, queryOrCallback, callback) {
			const i = eachRepositoryIterator(app, callback ? queryOrCallback : void 0)[
				Symbol.asyncIterator
			]()
			let result = await i.next()
			while (!result.done) {
				if (callback) {
					await callback(result.value)
				} else {
					await queryOrCallback(result.value)
				}
				result = await i.next()
			}
		}
		function singleInstallationIterator(app, installationId) {
			return {
				async *[Symbol.asyncIterator]() {
					yield {
						octokit: await app.getInstallationOctokit(installationId),
					}
				},
			}
		}
		function eachRepositoryIterator(app, query) {
			return {
				async *[Symbol.asyncIterator]() {
					const iterator = query
						? singleInstallationIterator(app, query.installationId)
						: app.eachInstallation.iterator()
					for await (const { octokit } of iterator) {
						const repositoriesIterator = import_plugin_paginate_rest2.composePaginateRest.iterator(
							octokit,
							"GET /installation/repositories"
						)
						for await (const { data: repositories } of repositoriesIterator) {
							for (const repository of repositories) {
								yield { octokit, repository }
							}
						}
					}
				},
			}
		}
		var import_oauth_app = require_dist_node22()
		var import_webhooks2 = require_dist_node24()
		function noop() {}
		function createNodeMiddleware2(app, options = {}) {
			const log3 = Object.assign(
				{
					debug: noop,
					info: noop,
					warn: console.warn.bind(console),
					error: console.error.bind(console),
				},
				options.log
			)
			const optionsWithDefaults = {
				pathPrefix: "/api/github",
				...options,
				log: log3,
			}
			const webhooksMiddleware = (0, import_webhooks2.createNodeMiddleware)(app.webhooks, {
				path: optionsWithDefaults.pathPrefix + "/webhooks",
				log: log3,
			})
			const oauthMiddleware = (0, import_oauth_app.createNodeMiddleware)(app.oauth, {
				pathPrefix: optionsWithDefaults.pathPrefix + "/oauth",
			})
			return middleware.bind(
				null,
				optionsWithDefaults.pathPrefix,
				webhooksMiddleware,
				oauthMiddleware
			)
		}
		async function middleware(
			pathPrefix,
			webhooksMiddleware,
			oauthMiddleware,
			request,
			response,
			next
		) {
			const { pathname } = new URL(request.url, "http://localhost")
			if (pathname.startsWith(`${pathPrefix}/`)) {
				if (pathname === `${pathPrefix}/webhooks`) {
					webhooksMiddleware(request, response)
				} else if (pathname.startsWith(`${pathPrefix}/oauth/`)) {
					oauthMiddleware(request, response)
				} else {
					;(0, import_oauth_app.sendNodeResponse)(
						(0, import_oauth_app.unknownRouteResponse)(request),
						response
					)
				}
				return true
			} else {
				next == undefined ? void 0 : next()
				return false
			}
		}
		var _App = class _App {
			static defaults(defaults) {
				const AppWithDefaults = class extends this {
					constructor(...args) {
						super({
							...defaults,
							...args[0],
						})
					}
				}
				return AppWithDefaults
			}
			constructor(options) {
				const Octokit2 = options.Octokit || import_core.Octokit
				const authOptions = Object.assign(
					{
						appId: options.appId,
						privateKey: options.privateKey,
					},
					options.oauth
						? {
								clientId: options.oauth.clientId,
								clientSecret: options.oauth.clientSecret,
						  }
						: {}
				)
				this.octokit = new Octokit2({
					authStrategy: import_auth_app3.createAppAuth,
					auth: authOptions,
					log: options.log,
				})
				this.log = Object.assign(
					{
						debug: () => {},
						info: () => {},
						warn: console.warn.bind(console),
						error: console.error.bind(console),
					},
					options.log
				)
				if (options.webhooks) {
					this.webhooks = webhooks(this.octokit, options.webhooks)
				} else {
					Object.defineProperty(this, "webhooks", {
						get() {
							throw new Error("[@octokit/app] webhooks option not set")
						},
					})
				}
				if (options.oauth) {
					this.oauth = new import_oauth_app2.OAuthApp({
						...options.oauth,
						clientType: "github-app",
						Octokit: Octokit2,
					})
				} else {
					Object.defineProperty(this, "oauth", {
						get() {
							throw new Error(
								"[@octokit/app] oauth.clientId / oauth.clientSecret options are not set"
							)
						},
					})
				}
				this.getInstallationOctokit = getInstallationOctokit.bind(null, this)
				this.eachInstallation = eachInstallationFactory(this)
				this.eachRepository = eachRepositoryFactory(this)
			}
		}
		_App.VERSION = VERSION
		var App = _App
	},
})

// ../../../node_modules/.pnpm/octokit@3.1.2/node_modules/octokit/dist-node/index.js
var require_dist_node26 = __commonJS({
	"../../../node_modules/.pnpm/octokit@3.1.2/node_modules/octokit/dist-node/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var __defProp2 = Object.defineProperty
		var __getOwnPropDesc2 = Object.getOwnPropertyDescriptor
		var __getOwnPropNames2 = Object.getOwnPropertyNames
		var __hasOwnProp2 = Object.prototype.hasOwnProperty
		var __export2 = (target, all) => {
			for (var name in all) __defProp2(target, name, { get: all[name], enumerable: true })
		}
		var __copyProps2 = (to, from2, except, desc) => {
			if ((from2 && typeof from2 === "object") || typeof from2 === "function") {
				for (let key of __getOwnPropNames2(from2))
					if (!__hasOwnProp2.call(to, key) && key !== except)
						__defProp2(to, key, {
							get: () => from2[key],
							enumerable: !(desc = __getOwnPropDesc2(from2, key)) || desc.enumerable,
						})
			}
			return to
		}
		var __toCommonJS2 = (mod) => __copyProps2(__defProp2({}, "__esModule", { value: true }), mod)
		var dist_src_exports = {}
		__export2(dist_src_exports, {
			App: () => App,
			OAuthApp: () => OAuthApp,
			Octokit: () => Octokit2,
			RequestError: () => import_request_error.RequestError,
			createNodeMiddleware: () => import_app2.createNodeMiddleware,
		})
		module2.exports = __toCommonJS2(dist_src_exports)
		var import_core = require_dist_node8()
		var import_plugin_paginate_rest = require_dist_node10()
		var import_plugin_paginate_graphql = require_dist_node11()
		var import_plugin_rest_endpoint_methods = require_dist_node9()
		var import_plugin_retry = require_dist_node12()
		var import_plugin_throttling = require_dist_node13()
		var VERSION = "3.1.2"
		var import_request_error = require_dist_node4()
		var Octokit2 = import_core.Octokit.plugin(
			import_plugin_rest_endpoint_methods.restEndpointMethods,
			import_plugin_paginate_rest.paginateRest,
			import_plugin_paginate_graphql.paginateGraphql,
			import_plugin_retry.retry,
			import_plugin_throttling.throttling
		).defaults({
			userAgent: `octokit.js/${VERSION}`,
			throttle: {
				onRateLimit,
				onSecondaryRateLimit,
			},
		})
		function onRateLimit(retryAfter, options, octokit) {
			octokit.log.warn(`Request quota exhausted for request ${options.method} ${options.url}`)
			if (options.request.retryCount === 0) {
				octokit.log.info(`Retrying after ${retryAfter} seconds!`)
				return true
			}
		}
		function onSecondaryRateLimit(retryAfter, options, octokit) {
			octokit.log.warn(`SecondaryRateLimit detected for request ${options.method} ${options.url}`)
			if (options.request.retryCount === 0) {
				octokit.log.info(`Retrying after ${retryAfter} seconds!`)
				return true
			}
		}
		var import_app = require_dist_node25()
		var import_oauth_app = require_dist_node22()
		var import_app2 = require_dist_node25()
		var App = import_app.App.defaults({ Octokit: Octokit2 })
		var OAuthApp = import_oauth_app.OAuthApp.defaults({ Octokit: Octokit2 })
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/typebox.js
var require_typebox = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/typebox.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.Type =
			exports2.JsonType =
			exports2.JavaScriptTypeBuilder =
			exports2.JsonTypeBuilder =
			exports2.TypeBuilder =
			exports2.TypeBuilderError =
			exports2.TransformEncodeBuilder =
			exports2.TransformDecodeBuilder =
			exports2.TemplateLiteralDslParser =
			exports2.TemplateLiteralGenerator =
			exports2.TemplateLiteralGeneratorError =
			exports2.TemplateLiteralFinite =
			exports2.TemplateLiteralFiniteError =
			exports2.TemplateLiteralParser =
			exports2.TemplateLiteralParserError =
			exports2.TemplateLiteralResolver =
			exports2.TemplateLiteralPattern =
			exports2.TemplateLiteralPatternError =
			exports2.UnionResolver =
			exports2.KeyArrayResolver =
			exports2.KeyArrayResolverError =
			exports2.KeyResolver =
			exports2.ObjectMap =
			exports2.Intrinsic =
			exports2.IndexedAccessor =
			exports2.TypeClone =
			exports2.TypeExtends =
			exports2.TypeExtendsResult =
			exports2.TypeExtendsError =
			exports2.ExtendsUndefined =
			exports2.TypeGuard =
			exports2.TypeGuardUnknownTypeError =
			exports2.ValueGuard =
			exports2.FormatRegistry =
			exports2.TypeBoxError =
			exports2.TypeRegistry =
			exports2.PatternStringExact =
			exports2.PatternNumberExact =
			exports2.PatternBooleanExact =
			exports2.PatternString =
			exports2.PatternNumber =
			exports2.PatternBoolean =
			exports2.Kind =
			exports2.Hint =
			exports2.Optional =
			exports2.Readonly =
			exports2.Transform =
				void 0
		exports2.Transform = Symbol.for("TypeBox.Transform")
		exports2.Readonly = Symbol.for("TypeBox.Readonly")
		exports2.Optional = Symbol.for("TypeBox.Optional")
		exports2.Hint = Symbol.for("TypeBox.Hint")
		exports2.Kind = Symbol.for("TypeBox.Kind")
		exports2.PatternBoolean = "(true|false)"
		exports2.PatternNumber = "(0|[1-9][0-9]*)"
		exports2.PatternString = "(.*)"
		exports2.PatternBooleanExact = `^${exports2.PatternBoolean}$`
		exports2.PatternNumberExact = `^${exports2.PatternNumber}$`
		exports2.PatternStringExact = `^${exports2.PatternString}$`
		var TypeRegistry
		;(function (TypeRegistry2) {
			const map = /* @__PURE__ */ new Map()
			function Entries() {
				return new Map(map)
			}
			TypeRegistry2.Entries = Entries
			function Clear() {
				return map.clear()
			}
			TypeRegistry2.Clear = Clear
			function Delete(kind) {
				return map.delete(kind)
			}
			TypeRegistry2.Delete = Delete
			function Has(kind) {
				return map.has(kind)
			}
			TypeRegistry2.Has = Has
			function Set2(kind, func) {
				map.set(kind, func)
			}
			TypeRegistry2.Set = Set2
			function Get(kind) {
				return map.get(kind)
			}
			TypeRegistry2.Get = Get
		})(TypeRegistry || (exports2.TypeRegistry = TypeRegistry = {}))
		var TypeBoxError = class extends Error {
			constructor(message) {
				super(message)
			}
		}
		exports2.TypeBoxError = TypeBoxError
		var FormatRegistry
		;(function (FormatRegistry2) {
			const map = /* @__PURE__ */ new Map()
			function Entries() {
				return new Map(map)
			}
			FormatRegistry2.Entries = Entries
			function Clear() {
				return map.clear()
			}
			FormatRegistry2.Clear = Clear
			function Delete(format) {
				return map.delete(format)
			}
			FormatRegistry2.Delete = Delete
			function Has(format) {
				return map.has(format)
			}
			FormatRegistry2.Has = Has
			function Set2(format, func) {
				map.set(format, func)
			}
			FormatRegistry2.Set = Set2
			function Get(format) {
				return map.get(format)
			}
			FormatRegistry2.Get = Get
		})(FormatRegistry || (exports2.FormatRegistry = FormatRegistry = {}))
		var ValueGuard
		;(function (ValueGuard2) {
			function IsArray(value) {
				return Array.isArray(value)
			}
			ValueGuard2.IsArray = IsArray
			function IsBigInt(value) {
				return typeof value === "bigint"
			}
			ValueGuard2.IsBigInt = IsBigInt
			function IsBoolean(value) {
				return typeof value === "boolean"
			}
			ValueGuard2.IsBoolean = IsBoolean
			function IsDate(value) {
				return value instanceof globalThis.Date
			}
			ValueGuard2.IsDate = IsDate
			function IsNull(value) {
				return value === null
			}
			ValueGuard2.IsNull = IsNull
			function IsNumber(value) {
				return typeof value === "number"
			}
			ValueGuard2.IsNumber = IsNumber
			function IsObject(value) {
				return typeof value === "object" && value !== null
			}
			ValueGuard2.IsObject = IsObject
			function IsString(value) {
				return typeof value === "string"
			}
			ValueGuard2.IsString = IsString
			function IsUint8Array(value) {
				return value instanceof globalThis.Uint8Array
			}
			ValueGuard2.IsUint8Array = IsUint8Array
			function IsUndefined(value) {
				return value === void 0
			}
			ValueGuard2.IsUndefined = IsUndefined
		})(ValueGuard || (exports2.ValueGuard = ValueGuard = {}))
		var TypeGuardUnknownTypeError = class extends TypeBoxError {}
		exports2.TypeGuardUnknownTypeError = TypeGuardUnknownTypeError
		var TypeGuard
		;(function (TypeGuard2) {
			function IsPattern(value) {
				try {
					new RegExp(value)
					return true
				} catch {
					return false
				}
			}
			function IsControlCharacterFree(value) {
				if (!ValueGuard.IsString(value)) return false
				for (let i = 0; i < value.length; i++) {
					const code = value.charCodeAt(i)
					if ((code >= 7 && code <= 13) || code === 27 || code === 127) {
						return false
					}
				}
				return true
			}
			function IsAdditionalProperties(value) {
				return IsOptionalBoolean(value) || TSchema(value)
			}
			function IsOptionalBigInt(value) {
				return ValueGuard.IsUndefined(value) || ValueGuard.IsBigInt(value)
			}
			function IsOptionalNumber(value) {
				return ValueGuard.IsUndefined(value) || ValueGuard.IsNumber(value)
			}
			function IsOptionalBoolean(value) {
				return ValueGuard.IsUndefined(value) || ValueGuard.IsBoolean(value)
			}
			function IsOptionalString(value) {
				return ValueGuard.IsUndefined(value) || ValueGuard.IsString(value)
			}
			function IsOptionalPattern(value) {
				return (
					ValueGuard.IsUndefined(value) ||
					(ValueGuard.IsString(value) && IsControlCharacterFree(value) && IsPattern(value))
				)
			}
			function IsOptionalFormat(value) {
				return (
					ValueGuard.IsUndefined(value) ||
					(ValueGuard.IsString(value) && IsControlCharacterFree(value))
				)
			}
			function IsOptionalSchema(value) {
				return ValueGuard.IsUndefined(value) || TSchema(value)
			}
			function TAny(schema2) {
				return TKindOf(schema2, "Any") && IsOptionalString(schema2.$id)
			}
			TypeGuard2.TAny = TAny
			function TArray(schema2) {
				return (
					TKindOf(schema2, "Array") &&
					schema2.type === "array" &&
					IsOptionalString(schema2.$id) &&
					TSchema(schema2.items) &&
					IsOptionalNumber(schema2.minItems) &&
					IsOptionalNumber(schema2.maxItems) &&
					IsOptionalBoolean(schema2.uniqueItems) &&
					IsOptionalSchema(schema2.contains) &&
					IsOptionalNumber(schema2.minContains) &&
					IsOptionalNumber(schema2.maxContains)
				)
			}
			TypeGuard2.TArray = TArray
			function TAsyncIterator(schema2) {
				return (
					TKindOf(schema2, "AsyncIterator") &&
					schema2.type === "AsyncIterator" &&
					IsOptionalString(schema2.$id) &&
					TSchema(schema2.items)
				)
			}
			TypeGuard2.TAsyncIterator = TAsyncIterator
			function TBigInt(schema2) {
				return (
					TKindOf(schema2, "BigInt") &&
					schema2.type === "bigint" &&
					IsOptionalString(schema2.$id) &&
					IsOptionalBigInt(schema2.exclusiveMaximum) &&
					IsOptionalBigInt(schema2.exclusiveMinimum) &&
					IsOptionalBigInt(schema2.maximum) &&
					IsOptionalBigInt(schema2.minimum) &&
					IsOptionalBigInt(schema2.multipleOf)
				)
			}
			TypeGuard2.TBigInt = TBigInt
			function TBoolean(schema2) {
				return (
					TKindOf(schema2, "Boolean") && schema2.type === "boolean" && IsOptionalString(schema2.$id)
				)
			}
			TypeGuard2.TBoolean = TBoolean
			function TConstructor(schema2) {
				return (
					TKindOf(schema2, "Constructor") &&
					schema2.type === "Constructor" &&
					IsOptionalString(schema2.$id) &&
					ValueGuard.IsArray(schema2.parameters) &&
					schema2.parameters.every((schema3) => TSchema(schema3)) &&
					TSchema(schema2.returns)
				)
			}
			TypeGuard2.TConstructor = TConstructor
			function TDate(schema2) {
				return (
					TKindOf(schema2, "Date") &&
					schema2.type === "Date" &&
					IsOptionalString(schema2.$id) &&
					IsOptionalNumber(schema2.exclusiveMaximumTimestamp) &&
					IsOptionalNumber(schema2.exclusiveMinimumTimestamp) &&
					IsOptionalNumber(schema2.maximumTimestamp) &&
					IsOptionalNumber(schema2.minimumTimestamp) &&
					IsOptionalNumber(schema2.multipleOfTimestamp)
				)
			}
			TypeGuard2.TDate = TDate
			function TFunction(schema2) {
				return (
					TKindOf(schema2, "Function") &&
					schema2.type === "Function" &&
					IsOptionalString(schema2.$id) &&
					ValueGuard.IsArray(schema2.parameters) &&
					schema2.parameters.every((schema3) => TSchema(schema3)) &&
					TSchema(schema2.returns)
				)
			}
			TypeGuard2.TFunction = TFunction
			function TInteger(schema2) {
				return (
					TKindOf(schema2, "Integer") &&
					schema2.type === "integer" &&
					IsOptionalString(schema2.$id) &&
					IsOptionalNumber(schema2.exclusiveMaximum) &&
					IsOptionalNumber(schema2.exclusiveMinimum) &&
					IsOptionalNumber(schema2.maximum) &&
					IsOptionalNumber(schema2.minimum) &&
					IsOptionalNumber(schema2.multipleOf)
				)
			}
			TypeGuard2.TInteger = TInteger
			function TIntersect(schema2) {
				return (
					TKindOf(schema2, "Intersect") &&
					(ValueGuard.IsString(schema2.type) && schema2.type !== "object" ? false : true) &&
					ValueGuard.IsArray(schema2.allOf) &&
					schema2.allOf.every((schema3) => TSchema(schema3) && !TTransform(schema3)) &&
					IsOptionalString(schema2.type) &&
					(IsOptionalBoolean(schema2.unevaluatedProperties) ||
						IsOptionalSchema(schema2.unevaluatedProperties)) &&
					IsOptionalString(schema2.$id)
				)
			}
			TypeGuard2.TIntersect = TIntersect
			function TIterator(schema2) {
				return (
					TKindOf(schema2, "Iterator") &&
					schema2.type === "Iterator" &&
					IsOptionalString(schema2.$id) &&
					TSchema(schema2.items)
				)
			}
			TypeGuard2.TIterator = TIterator
			function TKindOf(schema2, kind) {
				return TKind(schema2) && schema2[exports2.Kind] === kind
			}
			TypeGuard2.TKindOf = TKindOf
			function TKind(schema2) {
				return (
					ValueGuard.IsObject(schema2) &&
					exports2.Kind in schema2 &&
					ValueGuard.IsString(schema2[exports2.Kind])
				)
			}
			TypeGuard2.TKind = TKind
			function TLiteralString(schema2) {
				return TLiteral(schema2) && ValueGuard.IsString(schema2.const)
			}
			TypeGuard2.TLiteralString = TLiteralString
			function TLiteralNumber(schema2) {
				return TLiteral(schema2) && ValueGuard.IsNumber(schema2.const)
			}
			TypeGuard2.TLiteralNumber = TLiteralNumber
			function TLiteralBoolean(schema2) {
				return TLiteral(schema2) && ValueGuard.IsBoolean(schema2.const)
			}
			TypeGuard2.TLiteralBoolean = TLiteralBoolean
			function TLiteral(schema2) {
				return (
					TKindOf(schema2, "Literal") &&
					IsOptionalString(schema2.$id) &&
					(ValueGuard.IsBoolean(schema2.const) ||
						ValueGuard.IsNumber(schema2.const) ||
						ValueGuard.IsString(schema2.const))
				)
			}
			TypeGuard2.TLiteral = TLiteral
			function TNever(schema2) {
				return (
					TKindOf(schema2, "Never") &&
					ValueGuard.IsObject(schema2.not) &&
					Object.getOwnPropertyNames(schema2.not).length === 0
				)
			}
			TypeGuard2.TNever = TNever
			function TNot(schema2) {
				return TKindOf(schema2, "Not") && TSchema(schema2.not)
			}
			TypeGuard2.TNot = TNot
			function TNull(schema2) {
				return TKindOf(schema2, "Null") && schema2.type === "null" && IsOptionalString(schema2.$id)
			}
			TypeGuard2.TNull = TNull
			function TNumber(schema2) {
				return (
					TKindOf(schema2, "Number") &&
					schema2.type === "number" &&
					IsOptionalString(schema2.$id) &&
					IsOptionalNumber(schema2.exclusiveMaximum) &&
					IsOptionalNumber(schema2.exclusiveMinimum) &&
					IsOptionalNumber(schema2.maximum) &&
					IsOptionalNumber(schema2.minimum) &&
					IsOptionalNumber(schema2.multipleOf)
				)
			}
			TypeGuard2.TNumber = TNumber
			function TObject(schema2) {
				return (
					TKindOf(schema2, "Object") &&
					schema2.type === "object" &&
					IsOptionalString(schema2.$id) &&
					ValueGuard.IsObject(schema2.properties) &&
					IsAdditionalProperties(schema2.additionalProperties) &&
					IsOptionalNumber(schema2.minProperties) &&
					IsOptionalNumber(schema2.maxProperties) &&
					Object.entries(schema2.properties).every(
						([key, schema3]) => IsControlCharacterFree(key) && TSchema(schema3)
					)
				)
			}
			TypeGuard2.TObject = TObject
			function TPromise(schema2) {
				return (
					TKindOf(schema2, "Promise") &&
					schema2.type === "Promise" &&
					IsOptionalString(schema2.$id) &&
					TSchema(schema2.item)
				)
			}
			TypeGuard2.TPromise = TPromise
			function TRecord(schema2) {
				return (
					TKindOf(schema2, "Record") &&
					schema2.type === "object" &&
					IsOptionalString(schema2.$id) &&
					IsAdditionalProperties(schema2.additionalProperties) &&
					ValueGuard.IsObject(schema2.patternProperties) &&
					((schema3) => {
						const keys = Object.getOwnPropertyNames(schema3.patternProperties)
						return (
							keys.length === 1 &&
							IsPattern(keys[0]) &&
							ValueGuard.IsObject(schema3.patternProperties) &&
							TSchema(schema3.patternProperties[keys[0]])
						)
					})(schema2)
				)
			}
			TypeGuard2.TRecord = TRecord
			function TRecursive(schema2) {
				return (
					ValueGuard.IsObject(schema2) &&
					exports2.Hint in schema2 &&
					schema2[exports2.Hint] === "Recursive"
				)
			}
			TypeGuard2.TRecursive = TRecursive
			function TRef(schema2) {
				return (
					TKindOf(schema2, "Ref") &&
					IsOptionalString(schema2.$id) &&
					ValueGuard.IsString(schema2.$ref)
				)
			}
			TypeGuard2.TRef = TRef
			function TString(schema2) {
				return (
					TKindOf(schema2, "String") &&
					schema2.type === "string" &&
					IsOptionalString(schema2.$id) &&
					IsOptionalNumber(schema2.minLength) &&
					IsOptionalNumber(schema2.maxLength) &&
					IsOptionalPattern(schema2.pattern) &&
					IsOptionalFormat(schema2.format)
				)
			}
			TypeGuard2.TString = TString
			function TSymbol(schema2) {
				return (
					TKindOf(schema2, "Symbol") && schema2.type === "symbol" && IsOptionalString(schema2.$id)
				)
			}
			TypeGuard2.TSymbol = TSymbol
			function TTemplateLiteral(schema2) {
				return (
					TKindOf(schema2, "TemplateLiteral") &&
					schema2.type === "string" &&
					ValueGuard.IsString(schema2.pattern) &&
					schema2.pattern[0] === "^" &&
					schema2.pattern.at(-1) === "$"
				)
			}
			TypeGuard2.TTemplateLiteral = TTemplateLiteral
			function TThis(schema2) {
				return (
					TKindOf(schema2, "This") &&
					IsOptionalString(schema2.$id) &&
					ValueGuard.IsString(schema2.$ref)
				)
			}
			TypeGuard2.TThis = TThis
			function TTransform(schema2) {
				return ValueGuard.IsObject(schema2) && exports2.Transform in schema2
			}
			TypeGuard2.TTransform = TTransform
			function TTuple(schema2) {
				return (
					TKindOf(schema2, "Tuple") &&
					schema2.type === "array" &&
					IsOptionalString(schema2.$id) &&
					ValueGuard.IsNumber(schema2.minItems) &&
					ValueGuard.IsNumber(schema2.maxItems) &&
					schema2.minItems === schema2.maxItems && // empty
					((ValueGuard.IsUndefined(schema2.items) &&
						ValueGuard.IsUndefined(schema2.additionalItems) &&
						schema2.minItems === 0) ||
						(ValueGuard.IsArray(schema2.items) &&
							schema2.items.every((schema3) => TSchema(schema3))))
				)
			}
			TypeGuard2.TTuple = TTuple
			function TUndefined(schema2) {
				return (
					TKindOf(schema2, "Undefined") &&
					schema2.type === "undefined" &&
					IsOptionalString(schema2.$id)
				)
			}
			TypeGuard2.TUndefined = TUndefined
			function TUnionLiteral(schema2) {
				return (
					TUnion(schema2) &&
					schema2.anyOf.every((schema3) => TLiteralString(schema3) || TLiteralNumber(schema3))
				)
			}
			TypeGuard2.TUnionLiteral = TUnionLiteral
			function TUnion(schema2) {
				return (
					TKindOf(schema2, "Union") &&
					IsOptionalString(schema2.$id) &&
					ValueGuard.IsObject(schema2) &&
					ValueGuard.IsArray(schema2.anyOf) &&
					schema2.anyOf.every((schema3) => TSchema(schema3))
				)
			}
			TypeGuard2.TUnion = TUnion
			function TUint8Array(schema2) {
				return (
					TKindOf(schema2, "Uint8Array") &&
					schema2.type === "Uint8Array" &&
					IsOptionalString(schema2.$id) &&
					IsOptionalNumber(schema2.minByteLength) &&
					IsOptionalNumber(schema2.maxByteLength)
				)
			}
			TypeGuard2.TUint8Array = TUint8Array
			function TUnknown(schema2) {
				return TKindOf(schema2, "Unknown") && IsOptionalString(schema2.$id)
			}
			TypeGuard2.TUnknown = TUnknown
			function TUnsafe(schema2) {
				return TKindOf(schema2, "Unsafe")
			}
			TypeGuard2.TUnsafe = TUnsafe
			function TVoid(schema2) {
				return TKindOf(schema2, "Void") && schema2.type === "void" && IsOptionalString(schema2.$id)
			}
			TypeGuard2.TVoid = TVoid
			function TReadonly(schema2) {
				return ValueGuard.IsObject(schema2) && schema2[exports2.Readonly] === "Readonly"
			}
			TypeGuard2.TReadonly = TReadonly
			function TOptional(schema2) {
				return ValueGuard.IsObject(schema2) && schema2[exports2.Optional] === "Optional"
			}
			TypeGuard2.TOptional = TOptional
			function TSchema(schema2) {
				return (
					ValueGuard.IsObject(schema2) &&
					(TAny(schema2) ||
						TArray(schema2) ||
						TBoolean(schema2) ||
						TBigInt(schema2) ||
						TAsyncIterator(schema2) ||
						TConstructor(schema2) ||
						TDate(schema2) ||
						TFunction(schema2) ||
						TInteger(schema2) ||
						TIntersect(schema2) ||
						TIterator(schema2) ||
						TLiteral(schema2) ||
						TNever(schema2) ||
						TNot(schema2) ||
						TNull(schema2) ||
						TNumber(schema2) ||
						TObject(schema2) ||
						TPromise(schema2) ||
						TRecord(schema2) ||
						TRef(schema2) ||
						TString(schema2) ||
						TSymbol(schema2) ||
						TTemplateLiteral(schema2) ||
						TThis(schema2) ||
						TTuple(schema2) ||
						TUndefined(schema2) ||
						TUnion(schema2) ||
						TUint8Array(schema2) ||
						TUnknown(schema2) ||
						TUnsafe(schema2) ||
						TVoid(schema2) ||
						(TKind(schema2) && TypeRegistry.Has(schema2[exports2.Kind])))
				)
			}
			TypeGuard2.TSchema = TSchema
		})(TypeGuard || (exports2.TypeGuard = TypeGuard = {}))
		var ExtendsUndefined
		;(function (ExtendsUndefined2) {
			function Check(schema2) {
				return schema2[exports2.Kind] === "Intersect"
					? schema2.allOf.every((schema3) => Check(schema3))
					: schema2[exports2.Kind] === "Union"
					? schema2.anyOf.some((schema3) => Check(schema3))
					: schema2[exports2.Kind] === "Undefined"
					? true
					: schema2[exports2.Kind] === "Not"
					? !Check(schema2.not)
					: false
			}
			ExtendsUndefined2.Check = Check
		})(ExtendsUndefined || (exports2.ExtendsUndefined = ExtendsUndefined = {}))
		var TypeExtendsError = class extends TypeBoxError {}
		exports2.TypeExtendsError = TypeExtendsError
		var TypeExtendsResult
		;(function (TypeExtendsResult2) {
			TypeExtendsResult2[(TypeExtendsResult2["Union"] = 0)] = "Union"
			TypeExtendsResult2[(TypeExtendsResult2["True"] = 1)] = "True"
			TypeExtendsResult2[(TypeExtendsResult2["False"] = 2)] = "False"
		})(TypeExtendsResult || (exports2.TypeExtendsResult = TypeExtendsResult = {}))
		var TypeExtends
		;(function (TypeExtends2) {
			function IntoBooleanResult(result) {
				return result === TypeExtendsResult.False ? result : TypeExtendsResult.True
			}
			function Throw(message) {
				throw new TypeExtendsError(message)
			}
			function IsStructuralRight(right) {
				return (
					TypeGuard.TNever(right) ||
					TypeGuard.TIntersect(right) ||
					TypeGuard.TUnion(right) ||
					TypeGuard.TUnknown(right) ||
					TypeGuard.TAny(right)
				)
			}
			function StructuralRight(left, right) {
				return TypeGuard.TNever(right)
					? TNeverRight(left, right)
					: TypeGuard.TIntersect(right)
					? TIntersectRight(left, right)
					: TypeGuard.TUnion(right)
					? TUnionRight(left, right)
					: TypeGuard.TUnknown(right)
					? TUnknownRight(left, right)
					: TypeGuard.TAny(right)
					? TAnyRight(left, right)
					: Throw("StructuralRight")
			}
			function TAnyRight(left, right) {
				return TypeExtendsResult.True
			}
			function TAny(left, right) {
				return TypeGuard.TIntersect(right)
					? TIntersectRight(left, right)
					: TypeGuard.TUnion(right) &&
					  right.anyOf.some((schema2) => TypeGuard.TAny(schema2) || TypeGuard.TUnknown(schema2))
					? TypeExtendsResult.True
					: TypeGuard.TUnion(right)
					? TypeExtendsResult.Union
					: TypeGuard.TUnknown(right)
					? TypeExtendsResult.True
					: TypeGuard.TAny(right)
					? TypeExtendsResult.True
					: TypeExtendsResult.Union
			}
			function TArrayRight(left, right) {
				return TypeGuard.TUnknown(left)
					? TypeExtendsResult.False
					: TypeGuard.TAny(left)
					? TypeExtendsResult.Union
					: TypeGuard.TNever(left)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TArray(left, right) {
				return TypeGuard.TObject(right) && IsObjectArrayLike(right)
					? TypeExtendsResult.True
					: IsStructuralRight(right)
					? StructuralRight(left, right)
					: !TypeGuard.TArray(right)
					? TypeExtendsResult.False
					: IntoBooleanResult(Visit(left.items, right.items))
			}
			function TAsyncIterator(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: !TypeGuard.TAsyncIterator(right)
					? TypeExtendsResult.False
					: IntoBooleanResult(Visit(left.items, right.items))
			}
			function TBigInt(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TObject(right)
					? TObjectRight(left, right)
					: TypeGuard.TRecord(right)
					? TRecordRight(left, right)
					: TypeGuard.TBigInt(right)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TBooleanRight(left, right) {
				return TypeGuard.TLiteral(left) && ValueGuard.IsBoolean(left.const)
					? TypeExtendsResult.True
					: TypeGuard.TBoolean(left)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TBoolean(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TObject(right)
					? TObjectRight(left, right)
					: TypeGuard.TRecord(right)
					? TRecordRight(left, right)
					: TypeGuard.TBoolean(right)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TConstructor(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TObject(right)
					? TObjectRight(left, right)
					: !TypeGuard.TConstructor(right)
					? TypeExtendsResult.False
					: left.parameters.length > right.parameters.length
					? TypeExtendsResult.False
					: !left.parameters.every(
							(schema2, index2) =>
								IntoBooleanResult(Visit(right.parameters[index2], schema2)) ===
								TypeExtendsResult.True
					  )
					? TypeExtendsResult.False
					: IntoBooleanResult(Visit(left.returns, right.returns))
			}
			function TDate(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TObject(right)
					? TObjectRight(left, right)
					: TypeGuard.TRecord(right)
					? TRecordRight(left, right)
					: TypeGuard.TDate(right)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TFunction(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TObject(right)
					? TObjectRight(left, right)
					: !TypeGuard.TFunction(right)
					? TypeExtendsResult.False
					: left.parameters.length > right.parameters.length
					? TypeExtendsResult.False
					: !left.parameters.every(
							(schema2, index2) =>
								IntoBooleanResult(Visit(right.parameters[index2], schema2)) ===
								TypeExtendsResult.True
					  )
					? TypeExtendsResult.False
					: IntoBooleanResult(Visit(left.returns, right.returns))
			}
			function TIntegerRight(left, right) {
				return TypeGuard.TLiteral(left) && ValueGuard.IsNumber(left.const)
					? TypeExtendsResult.True
					: TypeGuard.TNumber(left) || TypeGuard.TInteger(left)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TInteger(left, right) {
				return TypeGuard.TInteger(right) || TypeGuard.TNumber(right)
					? TypeExtendsResult.True
					: IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TObject(right)
					? TObjectRight(left, right)
					: TypeGuard.TRecord(right)
					? TRecordRight(left, right)
					: TypeExtendsResult.False
			}
			function TIntersectRight(left, right) {
				return right.allOf.every((schema2) => Visit(left, schema2) === TypeExtendsResult.True)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TIntersect(left, right) {
				return left.allOf.some((schema2) => Visit(schema2, right) === TypeExtendsResult.True)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TIterator(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: !TypeGuard.TIterator(right)
					? TypeExtendsResult.False
					: IntoBooleanResult(Visit(left.items, right.items))
			}
			function TLiteral(left, right) {
				return TypeGuard.TLiteral(right) && right.const === left.const
					? TypeExtendsResult.True
					: IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TObject(right)
					? TObjectRight(left, right)
					: TypeGuard.TRecord(right)
					? TRecordRight(left, right)
					: TypeGuard.TString(right)
					? TStringRight(left, right)
					: TypeGuard.TNumber(right)
					? TNumberRight(left, right)
					: TypeGuard.TInteger(right)
					? TIntegerRight(left, right)
					: TypeGuard.TBoolean(right)
					? TBooleanRight(left, right)
					: TypeExtendsResult.False
			}
			function TNeverRight(left, right) {
				return TypeExtendsResult.False
			}
			function TNever(left, right) {
				return TypeExtendsResult.True
			}
			function UnwrapTNot(schema2) {
				let [current, depth] = [schema2, 0]
				while (true) {
					if (!TypeGuard.TNot(current)) break
					current = current.not
					depth += 1
				}
				return depth % 2 === 0 ? current : exports2.Type.Unknown()
			}
			function TNot(left, right) {
				return TypeGuard.TNot(left)
					? Visit(UnwrapTNot(left), right)
					: TypeGuard.TNot(right)
					? Visit(left, UnwrapTNot(right))
					: Throw("Invalid fallthrough for Not")
			}
			function TNull(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TObject(right)
					? TObjectRight(left, right)
					: TypeGuard.TRecord(right)
					? TRecordRight(left, right)
					: TypeGuard.TNull(right)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TNumberRight(left, right) {
				return TypeGuard.TLiteralNumber(left)
					? TypeExtendsResult.True
					: TypeGuard.TNumber(left) || TypeGuard.TInteger(left)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TNumber(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TObject(right)
					? TObjectRight(left, right)
					: TypeGuard.TRecord(right)
					? TRecordRight(left, right)
					: TypeGuard.TInteger(right) || TypeGuard.TNumber(right)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function IsObjectPropertyCount(schema2, count) {
				return Object.getOwnPropertyNames(schema2.properties).length === count
			}
			function IsObjectStringLike(schema2) {
				return IsObjectArrayLike(schema2)
			}
			function IsObjectSymbolLike(schema2) {
				return (
					IsObjectPropertyCount(schema2, 0) ||
					(IsObjectPropertyCount(schema2, 1) &&
						"description" in schema2.properties &&
						TypeGuard.TUnion(schema2.properties.description) &&
						schema2.properties.description.anyOf.length === 2 &&
						((TypeGuard.TString(schema2.properties.description.anyOf[0]) &&
							TypeGuard.TUndefined(schema2.properties.description.anyOf[1])) ||
							(TypeGuard.TString(schema2.properties.description.anyOf[1]) &&
								TypeGuard.TUndefined(schema2.properties.description.anyOf[0]))))
				)
			}
			function IsObjectNumberLike(schema2) {
				return IsObjectPropertyCount(schema2, 0)
			}
			function IsObjectBooleanLike(schema2) {
				return IsObjectPropertyCount(schema2, 0)
			}
			function IsObjectBigIntLike(schema2) {
				return IsObjectPropertyCount(schema2, 0)
			}
			function IsObjectDateLike(schema2) {
				return IsObjectPropertyCount(schema2, 0)
			}
			function IsObjectUint8ArrayLike(schema2) {
				return IsObjectArrayLike(schema2)
			}
			function IsObjectFunctionLike(schema2) {
				const length = exports2.Type.Number()
				return (
					IsObjectPropertyCount(schema2, 0) ||
					(IsObjectPropertyCount(schema2, 1) &&
						"length" in schema2.properties &&
						IntoBooleanResult(Visit(schema2.properties["length"], length)) ===
							TypeExtendsResult.True)
				)
			}
			function IsObjectConstructorLike(schema2) {
				return IsObjectPropertyCount(schema2, 0)
			}
			function IsObjectArrayLike(schema2) {
				const length = exports2.Type.Number()
				return (
					IsObjectPropertyCount(schema2, 0) ||
					(IsObjectPropertyCount(schema2, 1) &&
						"length" in schema2.properties &&
						IntoBooleanResult(Visit(schema2.properties["length"], length)) ===
							TypeExtendsResult.True)
				)
			}
			function IsObjectPromiseLike(schema2) {
				const then = exports2.Type.Function([exports2.Type.Any()], exports2.Type.Any())
				return (
					IsObjectPropertyCount(schema2, 0) ||
					(IsObjectPropertyCount(schema2, 1) &&
						"then" in schema2.properties &&
						IntoBooleanResult(Visit(schema2.properties["then"], then)) === TypeExtendsResult.True)
				)
			}
			function Property(left, right) {
				return Visit(left, right) === TypeExtendsResult.False
					? TypeExtendsResult.False
					: TypeGuard.TOptional(left) && !TypeGuard.TOptional(right)
					? TypeExtendsResult.False
					: TypeExtendsResult.True
			}
			function TObjectRight(left, right) {
				return TypeGuard.TUnknown(left)
					? TypeExtendsResult.False
					: TypeGuard.TAny(left)
					? TypeExtendsResult.Union
					: TypeGuard.TNever(left) ||
					  (TypeGuard.TLiteralString(left) && IsObjectStringLike(right)) ||
					  (TypeGuard.TLiteralNumber(left) && IsObjectNumberLike(right)) ||
					  (TypeGuard.TLiteralBoolean(left) && IsObjectBooleanLike(right)) ||
					  (TypeGuard.TSymbol(left) && IsObjectSymbolLike(right)) ||
					  (TypeGuard.TBigInt(left) && IsObjectBigIntLike(right)) ||
					  (TypeGuard.TString(left) && IsObjectStringLike(right)) ||
					  (TypeGuard.TSymbol(left) && IsObjectSymbolLike(right)) ||
					  (TypeGuard.TNumber(left) && IsObjectNumberLike(right)) ||
					  (TypeGuard.TInteger(left) && IsObjectNumberLike(right)) ||
					  (TypeGuard.TBoolean(left) && IsObjectBooleanLike(right)) ||
					  (TypeGuard.TUint8Array(left) && IsObjectUint8ArrayLike(right)) ||
					  (TypeGuard.TDate(left) && IsObjectDateLike(right)) ||
					  (TypeGuard.TConstructor(left) && IsObjectConstructorLike(right)) ||
					  (TypeGuard.TFunction(left) && IsObjectFunctionLike(right))
					? TypeExtendsResult.True
					: TypeGuard.TRecord(left) && TypeGuard.TString(RecordKey(left))
					? (() => {
							return right[exports2.Hint] === "Record"
								? TypeExtendsResult.True
								: TypeExtendsResult.False
					  })()
					: TypeGuard.TRecord(left) && TypeGuard.TNumber(RecordKey(left))
					? (() => {
							return IsObjectPropertyCount(right, 0)
								? TypeExtendsResult.True
								: TypeExtendsResult.False
					  })()
					: TypeExtendsResult.False
			}
			function TObject(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TRecord(right)
					? TRecordRight(left, right)
					: !TypeGuard.TObject(right)
					? TypeExtendsResult.False
					: (() => {
							for (const key of Object.getOwnPropertyNames(right.properties)) {
								if (!(key in left.properties) && !TypeGuard.TOptional(right.properties[key])) {
									return TypeExtendsResult.False
								}
								if (TypeGuard.TOptional(right.properties[key])) {
									return TypeExtendsResult.True
								}
								if (
									Property(left.properties[key], right.properties[key]) === TypeExtendsResult.False
								) {
									return TypeExtendsResult.False
								}
							}
							return TypeExtendsResult.True
					  })()
			}
			function TPromise(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TObject(right) && IsObjectPromiseLike(right)
					? TypeExtendsResult.True
					: !TypeGuard.TPromise(right)
					? TypeExtendsResult.False
					: IntoBooleanResult(Visit(left.item, right.item))
			}
			function RecordKey(schema2) {
				return exports2.PatternNumberExact in schema2.patternProperties
					? exports2.Type.Number()
					: exports2.PatternStringExact in schema2.patternProperties
					? exports2.Type.String()
					: Throw("Unknown record key pattern")
			}
			function RecordValue(schema2) {
				return exports2.PatternNumberExact in schema2.patternProperties
					? schema2.patternProperties[exports2.PatternNumberExact]
					: exports2.PatternStringExact in schema2.patternProperties
					? schema2.patternProperties[exports2.PatternStringExact]
					: Throw("Unable to get record value schema")
			}
			function TRecordRight(left, right) {
				const [Key, Value3] = [RecordKey(right), RecordValue(right)]
				return TypeGuard.TLiteralString(left) &&
					TypeGuard.TNumber(Key) &&
					IntoBooleanResult(Visit(left, Value3)) === TypeExtendsResult.True
					? TypeExtendsResult.True
					: TypeGuard.TUint8Array(left) && TypeGuard.TNumber(Key)
					? Visit(left, Value3)
					: TypeGuard.TString(left) && TypeGuard.TNumber(Key)
					? Visit(left, Value3)
					: TypeGuard.TArray(left) && TypeGuard.TNumber(Key)
					? Visit(left, Value3)
					: TypeGuard.TObject(left)
					? (() => {
							for (const key of Object.getOwnPropertyNames(left.properties)) {
								if (Property(Value3, left.properties[key]) === TypeExtendsResult.False) {
									return TypeExtendsResult.False
								}
							}
							return TypeExtendsResult.True
					  })()
					: TypeExtendsResult.False
			}
			function TRecord(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TObject(right)
					? TObjectRight(left, right)
					: !TypeGuard.TRecord(right)
					? TypeExtendsResult.False
					: Visit(RecordValue(left), RecordValue(right))
			}
			function TStringRight(left, right) {
				return TypeGuard.TLiteral(left) && ValueGuard.IsString(left.const)
					? TypeExtendsResult.True
					: TypeGuard.TString(left)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TString(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TObject(right)
					? TObjectRight(left, right)
					: TypeGuard.TRecord(right)
					? TRecordRight(left, right)
					: TypeGuard.TString(right)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TSymbol(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TObject(right)
					? TObjectRight(left, right)
					: TypeGuard.TRecord(right)
					? TRecordRight(left, right)
					: TypeGuard.TSymbol(right)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TTemplateLiteral(left, right) {
				return TypeGuard.TTemplateLiteral(left)
					? Visit(TemplateLiteralResolver.Resolve(left), right)
					: TypeGuard.TTemplateLiteral(right)
					? Visit(left, TemplateLiteralResolver.Resolve(right))
					: Throw("Invalid fallthrough for TemplateLiteral")
			}
			function IsArrayOfTuple(left, right) {
				return (
					TypeGuard.TArray(right) &&
					left.items !== void 0 &&
					left.items.every((schema2) => Visit(schema2, right.items) === TypeExtendsResult.True)
				)
			}
			function TTupleRight(left, right) {
				return TypeGuard.TNever(left)
					? TypeExtendsResult.True
					: TypeGuard.TUnknown(left)
					? TypeExtendsResult.False
					: TypeGuard.TAny(left)
					? TypeExtendsResult.Union
					: TypeExtendsResult.False
			}
			function TTuple(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TObject(right) && IsObjectArrayLike(right)
					? TypeExtendsResult.True
					: TypeGuard.TArray(right) && IsArrayOfTuple(left, right)
					? TypeExtendsResult.True
					: !TypeGuard.TTuple(right)
					? TypeExtendsResult.False
					: (ValueGuard.IsUndefined(left.items) && !ValueGuard.IsUndefined(right.items)) ||
					  (!ValueGuard.IsUndefined(left.items) && ValueGuard.IsUndefined(right.items))
					? TypeExtendsResult.False
					: ValueGuard.IsUndefined(left.items) && !ValueGuard.IsUndefined(right.items)
					? TypeExtendsResult.True
					: left.items.every(
							(schema2, index2) => Visit(schema2, right.items[index2]) === TypeExtendsResult.True
					  )
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TUint8Array(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TObject(right)
					? TObjectRight(left, right)
					: TypeGuard.TRecord(right)
					? TRecordRight(left, right)
					: TypeGuard.TUint8Array(right)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TUndefined(left, right) {
				return IsStructuralRight(right)
					? StructuralRight(left, right)
					: TypeGuard.TObject(right)
					? TObjectRight(left, right)
					: TypeGuard.TRecord(right)
					? TRecordRight(left, right)
					: TypeGuard.TVoid(right)
					? VoidRight(left, right)
					: TypeGuard.TUndefined(right)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TUnionRight(left, right) {
				return right.anyOf.some((schema2) => Visit(left, schema2) === TypeExtendsResult.True)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TUnion(left, right) {
				return left.anyOf.every((schema2) => Visit(schema2, right) === TypeExtendsResult.True)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TUnknownRight(left, right) {
				return TypeExtendsResult.True
			}
			function TUnknown(left, right) {
				return TypeGuard.TNever(right)
					? TNeverRight(left, right)
					: TypeGuard.TIntersect(right)
					? TIntersectRight(left, right)
					: TypeGuard.TUnion(right)
					? TUnionRight(left, right)
					: TypeGuard.TAny(right)
					? TAnyRight(left, right)
					: TypeGuard.TString(right)
					? TStringRight(left, right)
					: TypeGuard.TNumber(right)
					? TNumberRight(left, right)
					: TypeGuard.TInteger(right)
					? TIntegerRight(left, right)
					: TypeGuard.TBoolean(right)
					? TBooleanRight(left, right)
					: TypeGuard.TArray(right)
					? TArrayRight(left, right)
					: TypeGuard.TTuple(right)
					? TTupleRight(left, right)
					: TypeGuard.TObject(right)
					? TObjectRight(left, right)
					: TypeGuard.TUnknown(right)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function VoidRight(left, right) {
				return TypeGuard.TUndefined(left)
					? TypeExtendsResult.True
					: TypeGuard.TUndefined(left)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function TVoid(left, right) {
				return TypeGuard.TIntersect(right)
					? TIntersectRight(left, right)
					: TypeGuard.TUnion(right)
					? TUnionRight(left, right)
					: TypeGuard.TUnknown(right)
					? TUnknownRight(left, right)
					: TypeGuard.TAny(right)
					? TAnyRight(left, right)
					: TypeGuard.TObject(right)
					? TObjectRight(left, right)
					: TypeGuard.TVoid(right)
					? TypeExtendsResult.True
					: TypeExtendsResult.False
			}
			function Visit(left, right) {
				return (
					// resolvable
					TypeGuard.TTemplateLiteral(left) || TypeGuard.TTemplateLiteral(right)
						? TTemplateLiteral(left, right)
						: TypeGuard.TNot(left) || TypeGuard.TNot(right)
						? TNot(left, right)
						: // standard
						TypeGuard.TAny(left)
						? TAny(left, right)
						: TypeGuard.TArray(left)
						? TArray(left, right)
						: TypeGuard.TBigInt(left)
						? TBigInt(left, right)
						: TypeGuard.TBoolean(left)
						? TBoolean(left, right)
						: TypeGuard.TAsyncIterator(left)
						? TAsyncIterator(left, right)
						: TypeGuard.TConstructor(left)
						? TConstructor(left, right)
						: TypeGuard.TDate(left)
						? TDate(left, right)
						: TypeGuard.TFunction(left)
						? TFunction(left, right)
						: TypeGuard.TInteger(left)
						? TInteger(left, right)
						: TypeGuard.TIntersect(left)
						? TIntersect(left, right)
						: TypeGuard.TIterator(left)
						? TIterator(left, right)
						: TypeGuard.TLiteral(left)
						? TLiteral(left, right)
						: TypeGuard.TNever(left)
						? TNever(left, right)
						: TypeGuard.TNull(left)
						? TNull(left, right)
						: TypeGuard.TNumber(left)
						? TNumber(left, right)
						: TypeGuard.TObject(left)
						? TObject(left, right)
						: TypeGuard.TRecord(left)
						? TRecord(left, right)
						: TypeGuard.TString(left)
						? TString(left, right)
						: TypeGuard.TSymbol(left)
						? TSymbol(left, right)
						: TypeGuard.TTuple(left)
						? TTuple(left, right)
						: TypeGuard.TPromise(left)
						? TPromise(left, right)
						: TypeGuard.TUint8Array(left)
						? TUint8Array(left, right)
						: TypeGuard.TUndefined(left)
						? TUndefined(left, right)
						: TypeGuard.TUnion(left)
						? TUnion(left, right)
						: TypeGuard.TUnknown(left)
						? TUnknown(left, right)
						: TypeGuard.TVoid(left)
						? TVoid(left, right)
						: Throw(`Unknown left type operand '${left[exports2.Kind]}'`)
				)
			}
			function Extends(left, right) {
				return Visit(left, right)
			}
			TypeExtends2.Extends = Extends
		})(TypeExtends || (exports2.TypeExtends = TypeExtends = {}))
		var TypeClone
		;(function (TypeClone2) {
			function ArrayType(value) {
				return value.map((value2) => Visit(value2))
			}
			function DateType(value) {
				return new Date(value.getTime())
			}
			function Uint8ArrayType(value) {
				return new Uint8Array(value)
			}
			function ObjectType(value) {
				const clonedProperties = Object.fromEntries(
					Object.getOwnPropertyNames(value).map((key) => [key, Visit(value[key])])
				)
				const clonedSymbols = Object.fromEntries(
					Object.getOwnPropertySymbols(value).map((key) => [key, Visit(value[key])])
				)
				return { ...clonedProperties, ...clonedSymbols }
			}
			function Visit(value) {
				return ValueGuard.IsArray(value)
					? ArrayType(value)
					: ValueGuard.IsDate(value)
					? DateType(value)
					: ValueGuard.IsUint8Array(value)
					? Uint8ArrayType(value)
					: ValueGuard.IsObject(value)
					? ObjectType(value)
					: value
			}
			function Rest(schemas) {
				return schemas.map((schema2) => Type11(schema2))
			}
			TypeClone2.Rest = Rest
			function Type11(schema2, options = {}) {
				return { ...Visit(schema2), ...options }
			}
			TypeClone2.Type = Type11
		})(TypeClone || (exports2.TypeClone = TypeClone = {}))
		var IndexedAccessor
		;(function (IndexedAccessor2) {
			function OptionalUnwrap(schema2) {
				return schema2.map((schema3) => {
					const { [exports2.Optional]: _, ...clone2 } = TypeClone.Type(schema3)
					return clone2
				})
			}
			function IsIntersectOptional(schema2) {
				return schema2.every((schema3) => TypeGuard.TOptional(schema3))
			}
			function IsUnionOptional(schema2) {
				return schema2.some((schema3) => TypeGuard.TOptional(schema3))
			}
			function ResolveIntersect(schema2) {
				return IsIntersectOptional(schema2.allOf)
					? exports2.Type.Optional(exports2.Type.Intersect(OptionalUnwrap(schema2.allOf)))
					: schema2
			}
			function ResolveUnion(schema2) {
				return IsUnionOptional(schema2.anyOf)
					? exports2.Type.Optional(exports2.Type.Union(OptionalUnwrap(schema2.anyOf)))
					: schema2
			}
			function ResolveOptional(schema2) {
				return schema2[exports2.Kind] === "Intersect"
					? ResolveIntersect(schema2)
					: schema2[exports2.Kind] === "Union"
					? ResolveUnion(schema2)
					: schema2
			}
			function TIntersect(schema2, key) {
				const resolved = schema2.allOf.reduce((acc, schema3) => {
					const indexed = Visit(schema3, key)
					return indexed[exports2.Kind] === "Never" ? acc : [...acc, indexed]
				}, [])
				return ResolveOptional(exports2.Type.Intersect(resolved))
			}
			function TUnion(schema2, key) {
				const resolved = schema2.anyOf.map((schema3) => Visit(schema3, key))
				return ResolveOptional(exports2.Type.Union(resolved))
			}
			function TObject(schema2, key) {
				const property = schema2.properties[key]
				return ValueGuard.IsUndefined(property)
					? exports2.Type.Never()
					: exports2.Type.Union([property])
			}
			function TTuple(schema2, key) {
				const items = schema2.items
				if (ValueGuard.IsUndefined(items)) return exports2.Type.Never()
				const element = items[key]
				if (ValueGuard.IsUndefined(element)) return exports2.Type.Never()
				return element
			}
			function Visit(schema2, key) {
				return schema2[exports2.Kind] === "Intersect"
					? TIntersect(schema2, key)
					: schema2[exports2.Kind] === "Union"
					? TUnion(schema2, key)
					: schema2[exports2.Kind] === "Object"
					? TObject(schema2, key)
					: schema2[exports2.Kind] === "Tuple"
					? TTuple(schema2, key)
					: exports2.Type.Never()
			}
			function Resolve(schema2, keys, options = {}) {
				const resolved = keys.map((key) => Visit(schema2, key.toString()))
				return ResolveOptional(exports2.Type.Union(resolved, options))
			}
			IndexedAccessor2.Resolve = Resolve
		})(IndexedAccessor || (exports2.IndexedAccessor = IndexedAccessor = {}))
		var Intrinsic
		;(function (Intrinsic2) {
			function Uncapitalize(value) {
				const [first, rest] = [value.slice(0, 1), value.slice(1)]
				return `${first.toLowerCase()}${rest}`
			}
			function Capitalize(value) {
				const [first, rest] = [value.slice(0, 1), value.slice(1)]
				return `${first.toUpperCase()}${rest}`
			}
			function Uppercase(value) {
				return value.toUpperCase()
			}
			function Lowercase(value) {
				return value.toLowerCase()
			}
			function IntrinsicTemplateLiteral(schema2, mode) {
				const expression = TemplateLiteralParser.ParseExact(schema2.pattern)
				const finite = TemplateLiteralFinite.Check(expression)
				if (!finite) return { ...schema2, pattern: IntrinsicLiteral(schema2.pattern, mode) }
				const strings = [...TemplateLiteralGenerator.Generate(expression)]
				const literals = strings.map((value) => exports2.Type.Literal(value))
				const mapped = IntrinsicRest(literals, mode)
				const union = exports2.Type.Union(mapped)
				return exports2.Type.TemplateLiteral([union])
			}
			function IntrinsicLiteral(value, mode) {
				return typeof value === "string"
					? mode === "Uncapitalize"
						? Uncapitalize(value)
						: mode === "Capitalize"
						? Capitalize(value)
						: mode === "Uppercase"
						? Uppercase(value)
						: mode === "Lowercase"
						? Lowercase(value)
						: value
					: value.toString()
			}
			function IntrinsicRest(schema2, mode) {
				if (schema2.length === 0) return []
				const [L, ...R] = schema2
				return [Map2(L, mode), ...IntrinsicRest(R, mode)]
			}
			function Visit(schema2, mode) {
				return TypeGuard.TTemplateLiteral(schema2)
					? IntrinsicTemplateLiteral(schema2, mode)
					: TypeGuard.TUnion(schema2)
					? exports2.Type.Union(IntrinsicRest(schema2.anyOf, mode))
					: TypeGuard.TLiteral(schema2)
					? exports2.Type.Literal(IntrinsicLiteral(schema2.const, mode))
					: schema2
			}
			function Map2(schema2, mode) {
				return Visit(schema2, mode)
			}
			Intrinsic2.Map = Map2
		})(Intrinsic || (exports2.Intrinsic = Intrinsic = {}))
		var ObjectMap
		;(function (ObjectMap2) {
			function TIntersect(schema2, callback) {
				return exports2.Type.Intersect(
					schema2.allOf.map((inner) => Visit(inner, callback)),
					{ ...schema2 }
				)
			}
			function TUnion(schema2, callback) {
				return exports2.Type.Union(
					schema2.anyOf.map((inner) => Visit(inner, callback)),
					{ ...schema2 }
				)
			}
			function TObject(schema2, callback) {
				return callback(schema2)
			}
			function Visit(schema2, callback) {
				return schema2[exports2.Kind] === "Intersect"
					? TIntersect(schema2, callback)
					: schema2[exports2.Kind] === "Union"
					? TUnion(schema2, callback)
					: schema2[exports2.Kind] === "Object"
					? TObject(schema2, callback)
					: schema2
			}
			function Map2(schema2, callback, options) {
				return { ...Visit(TypeClone.Type(schema2), callback), ...options }
			}
			ObjectMap2.Map = Map2
		})(ObjectMap || (exports2.ObjectMap = ObjectMap = {}))
		var KeyResolver
		;(function (KeyResolver2) {
			function UnwrapPattern(key) {
				return key[0] === "^" && key.at(-1) === "$" ? key.slice(1, key.length - 1) : key
			}
			function TIntersect(schema2, options) {
				return schema2.allOf.reduce((acc, schema3) => [...acc, ...Visit(schema3, options)], [])
			}
			function TUnion(schema2, options) {
				const sets = schema2.anyOf.map((inner) => Visit(inner, options))
				return [
					...sets.reduce(
						(set, outer) =>
							outer.map((key) =>
								sets.every((inner) => inner.includes(key)) ? set.add(key) : set
							)[0],
						/* @__PURE__ */ new Set()
					),
				]
			}
			function TObject(schema2, options) {
				return Object.getOwnPropertyNames(schema2.properties)
			}
			function TRecord(schema2, options) {
				return options.includePatterns ? Object.getOwnPropertyNames(schema2.patternProperties) : []
			}
			function Visit(schema2, options) {
				return TypeGuard.TIntersect(schema2)
					? TIntersect(schema2, options)
					: TypeGuard.TUnion(schema2)
					? TUnion(schema2, options)
					: TypeGuard.TObject(schema2)
					? TObject(schema2, options)
					: TypeGuard.TRecord(schema2)
					? TRecord(schema2, options)
					: []
			}
			function ResolveKeys(schema2, options) {
				return [...new Set(Visit(schema2, options))]
			}
			KeyResolver2.ResolveKeys = ResolveKeys
			function ResolvePattern(schema2) {
				const keys = ResolveKeys(schema2, { includePatterns: true })
				const pattern3 = keys.map((key) => `(${UnwrapPattern(key)})`)
				return `^(${pattern3.join("|")})$`
			}
			KeyResolver2.ResolvePattern = ResolvePattern
		})(KeyResolver || (exports2.KeyResolver = KeyResolver = {}))
		var KeyArrayResolverError = class extends TypeBoxError {}
		exports2.KeyArrayResolverError = KeyArrayResolverError
		var KeyArrayResolver
		;(function (KeyArrayResolver2) {
			function Resolve(schema2) {
				return Array.isArray(schema2)
					? schema2
					: TypeGuard.TUnionLiteral(schema2)
					? schema2.anyOf.map((schema3) => schema3.const.toString())
					: TypeGuard.TLiteral(schema2)
					? [schema2.const]
					: TypeGuard.TTemplateLiteral(schema2)
					? (() => {
							const expression = TemplateLiteralParser.ParseExact(schema2.pattern)
							if (!TemplateLiteralFinite.Check(expression))
								throw new KeyArrayResolverError(
									"Cannot resolve keys from infinite template expression"
								)
							return [...TemplateLiteralGenerator.Generate(expression)]
					  })()
					: []
			}
			KeyArrayResolver2.Resolve = Resolve
		})(KeyArrayResolver || (exports2.KeyArrayResolver = KeyArrayResolver = {}))
		var UnionResolver
		;(function (UnionResolver2) {
			function* TUnion(union) {
				for (const schema2 of union.anyOf) {
					if (schema2[exports2.Kind] === "Union") {
						yield* TUnion(schema2)
					} else {
						yield schema2
					}
				}
			}
			function Resolve(union) {
				return exports2.Type.Union([...TUnion(union)], { ...union })
			}
			UnionResolver2.Resolve = Resolve
		})(UnionResolver || (exports2.UnionResolver = UnionResolver = {}))
		var TemplateLiteralPatternError = class extends TypeBoxError {}
		exports2.TemplateLiteralPatternError = TemplateLiteralPatternError
		var TemplateLiteralPattern
		;(function (TemplateLiteralPattern2) {
			function Throw(message) {
				throw new TemplateLiteralPatternError(message)
			}
			function Escape(value) {
				return value.replace(/[.*+?^${}()|[\]\\]/g, "\\$&")
			}
			function Visit(schema2, acc) {
				return TypeGuard.TTemplateLiteral(schema2)
					? schema2.pattern.slice(1, schema2.pattern.length - 1)
					: TypeGuard.TUnion(schema2)
					? `(${schema2.anyOf.map((schema3) => Visit(schema3, acc)).join("|")})`
					: TypeGuard.TNumber(schema2)
					? `${acc}${exports2.PatternNumber}`
					: TypeGuard.TInteger(schema2)
					? `${acc}${exports2.PatternNumber}`
					: TypeGuard.TBigInt(schema2)
					? `${acc}${exports2.PatternNumber}`
					: TypeGuard.TString(schema2)
					? `${acc}${exports2.PatternString}`
					: TypeGuard.TLiteral(schema2)
					? `${acc}${Escape(schema2.const.toString())}`
					: TypeGuard.TBoolean(schema2)
					? `${acc}${exports2.PatternBoolean}`
					: Throw(`Unexpected Kind '${schema2[exports2.Kind]}'`)
			}
			function Create(kinds) {
				return `^${kinds.map((schema2) => Visit(schema2, "")).join("")}$`
			}
			TemplateLiteralPattern2.Create = Create
		})(TemplateLiteralPattern || (exports2.TemplateLiteralPattern = TemplateLiteralPattern = {}))
		var TemplateLiteralResolver
		;(function (TemplateLiteralResolver2) {
			function Resolve(template) {
				const expression = TemplateLiteralParser.ParseExact(template.pattern)
				if (!TemplateLiteralFinite.Check(expression)) return exports2.Type.String()
				const literals = [...TemplateLiteralGenerator.Generate(expression)].map((value) =>
					exports2.Type.Literal(value)
				)
				return exports2.Type.Union(literals)
			}
			TemplateLiteralResolver2.Resolve = Resolve
		})(TemplateLiteralResolver || (exports2.TemplateLiteralResolver = TemplateLiteralResolver = {}))
		var TemplateLiteralParserError = class extends TypeBoxError {}
		exports2.TemplateLiteralParserError = TemplateLiteralParserError
		var TemplateLiteralParser
		;(function (TemplateLiteralParser2) {
			function IsNonEscaped(pattern3, index2, char) {
				return pattern3[index2] === char && pattern3.charCodeAt(index2 - 1) !== 92
			}
			function IsOpenParen(pattern3, index2) {
				return IsNonEscaped(pattern3, index2, "(")
			}
			function IsCloseParen(pattern3, index2) {
				return IsNonEscaped(pattern3, index2, ")")
			}
			function IsSeparator(pattern3, index2) {
				return IsNonEscaped(pattern3, index2, "|")
			}
			function IsGroup(pattern3) {
				if (!(IsOpenParen(pattern3, 0) && IsCloseParen(pattern3, pattern3.length - 1))) return false
				let count = 0
				for (let index2 = 0; index2 < pattern3.length; index2++) {
					if (IsOpenParen(pattern3, index2)) count += 1
					if (IsCloseParen(pattern3, index2)) count -= 1
					if (count === 0 && index2 !== pattern3.length - 1) return false
				}
				return true
			}
			function InGroup(pattern3) {
				return pattern3.slice(1, pattern3.length - 1)
			}
			function IsPrecedenceOr(pattern3) {
				let count = 0
				for (let index2 = 0; index2 < pattern3.length; index2++) {
					if (IsOpenParen(pattern3, index2)) count += 1
					if (IsCloseParen(pattern3, index2)) count -= 1
					if (IsSeparator(pattern3, index2) && count === 0) return true
				}
				return false
			}
			function IsPrecedenceAnd(pattern3) {
				for (let index2 = 0; index2 < pattern3.length; index2++) {
					if (IsOpenParen(pattern3, index2)) return true
				}
				return false
			}
			function Or(pattern3) {
				let [count, start] = [0, 0]
				const expressions = []
				for (let index2 = 0; index2 < pattern3.length; index2++) {
					if (IsOpenParen(pattern3, index2)) count += 1
					if (IsCloseParen(pattern3, index2)) count -= 1
					if (IsSeparator(pattern3, index2) && count === 0) {
						const range2 = pattern3.slice(start, index2)
						if (range2.length > 0) expressions.push(Parse(range2))
						start = index2 + 1
					}
				}
				const range = pattern3.slice(start)
				if (range.length > 0) expressions.push(Parse(range))
				if (expressions.length === 0) return { type: "const", const: "" }
				if (expressions.length === 1) return expressions[0]
				return { type: "or", expr: expressions }
			}
			function And(pattern3) {
				function Group(value, index2) {
					if (!IsOpenParen(value, index2))
						throw new TemplateLiteralParserError(
							`TemplateLiteralParser: Index must point to open parens`
						)
					let count = 0
					for (let scan = index2; scan < value.length; scan++) {
						if (IsOpenParen(value, scan)) count += 1
						if (IsCloseParen(value, scan)) count -= 1
						if (count === 0) return [index2, scan]
					}
					throw new TemplateLiteralParserError(
						`TemplateLiteralParser: Unclosed group parens in expression`
					)
				}
				function Range(pattern4, index2) {
					for (let scan = index2; scan < pattern4.length; scan++) {
						if (IsOpenParen(pattern4, scan)) return [index2, scan]
					}
					return [index2, pattern4.length]
				}
				const expressions = []
				for (let index2 = 0; index2 < pattern3.length; index2++) {
					if (IsOpenParen(pattern3, index2)) {
						const [start, end] = Group(pattern3, index2)
						const range = pattern3.slice(start, end + 1)
						expressions.push(Parse(range))
						index2 = end
					} else {
						const [start, end] = Range(pattern3, index2)
						const range = pattern3.slice(start, end)
						if (range.length > 0) expressions.push(Parse(range))
						index2 = end - 1
					}
				}
				return expressions.length === 0
					? { type: "const", const: "" }
					: expressions.length === 1
					? expressions[0]
					: { type: "and", expr: expressions }
			}
			function Parse(pattern3) {
				return IsGroup(pattern3)
					? Parse(InGroup(pattern3))
					: IsPrecedenceOr(pattern3)
					? Or(pattern3)
					: IsPrecedenceAnd(pattern3)
					? And(pattern3)
					: { type: "const", const: pattern3 }
			}
			TemplateLiteralParser2.Parse = Parse
			function ParseExact(pattern3) {
				return Parse(pattern3.slice(1, pattern3.length - 1))
			}
			TemplateLiteralParser2.ParseExact = ParseExact
		})(TemplateLiteralParser || (exports2.TemplateLiteralParser = TemplateLiteralParser = {}))
		var TemplateLiteralFiniteError = class extends TypeBoxError {}
		exports2.TemplateLiteralFiniteError = TemplateLiteralFiniteError
		var TemplateLiteralFinite
		;(function (TemplateLiteralFinite2) {
			function Throw(message) {
				throw new TemplateLiteralFiniteError(message)
			}
			function IsNumber(expression) {
				return (
					expression.type === "or" &&
					expression.expr.length === 2 &&
					expression.expr[0].type === "const" &&
					expression.expr[0].const === "0" &&
					expression.expr[1].type === "const" &&
					expression.expr[1].const === "[1-9][0-9]*"
				)
			}
			function IsBoolean(expression) {
				return (
					expression.type === "or" &&
					expression.expr.length === 2 &&
					expression.expr[0].type === "const" &&
					expression.expr[0].const === "true" &&
					expression.expr[1].type === "const" &&
					expression.expr[1].const === "false"
				)
			}
			function IsString(expression) {
				return expression.type === "const" && expression.const === ".*"
			}
			function Check(expression) {
				return IsBoolean(expression)
					? true
					: IsNumber(expression) || IsString(expression)
					? false
					: expression.type === "and"
					? expression.expr.every((expr) => Check(expr))
					: expression.type === "or"
					? expression.expr.every((expr) => Check(expr))
					: expression.type === "const"
					? true
					: Throw(`Unknown expression type`)
			}
			TemplateLiteralFinite2.Check = Check
		})(TemplateLiteralFinite || (exports2.TemplateLiteralFinite = TemplateLiteralFinite = {}))
		var TemplateLiteralGeneratorError = class extends TypeBoxError {}
		exports2.TemplateLiteralGeneratorError = TemplateLiteralGeneratorError
		var TemplateLiteralGenerator
		;(function (TemplateLiteralGenerator2) {
			function* Reduce(buffer) {
				if (buffer.length === 1) return yield* buffer[0]
				for (const left of buffer[0]) {
					for (const right of Reduce(buffer.slice(1))) {
						yield `${left}${right}`
					}
				}
			}
			function* And(expression) {
				return yield* Reduce(expression.expr.map((expr) => [...Generate(expr)]))
			}
			function* Or(expression) {
				for (const expr of expression.expr) yield* Generate(expr)
			}
			function* Const(expression) {
				return yield expression.const
			}
			function* Generate(expression) {
				return expression.type === "and"
					? yield* And(expression)
					: expression.type === "or"
					? yield* Or(expression)
					: expression.type === "const"
					? yield* Const(expression)
					: (() => {
							throw new TemplateLiteralGeneratorError("Unknown expression")
					  })()
			}
			TemplateLiteralGenerator2.Generate = Generate
		})(
			TemplateLiteralGenerator ||
				(exports2.TemplateLiteralGenerator = TemplateLiteralGenerator = {})
		)
		var TemplateLiteralDslParser
		;(function (TemplateLiteralDslParser2) {
			function* ParseUnion(template) {
				const trim = template.trim().replace(/"|'/g, "")
				return trim === "boolean"
					? yield exports2.Type.Boolean()
					: trim === "number"
					? yield exports2.Type.Number()
					: trim === "bigint"
					? yield exports2.Type.BigInt()
					: trim === "string"
					? yield exports2.Type.String()
					: yield (() => {
							const literals = trim
								.split("|")
								.map((literal) => exports2.Type.Literal(literal.trim()))
							return literals.length === 0
								? exports2.Type.Never()
								: literals.length === 1
								? literals[0]
								: exports2.Type.Union(literals)
					  })()
			}
			function* ParseTerminal(template) {
				if (template[1] !== "{") {
					const L = exports2.Type.Literal("$")
					const R = ParseLiteral(template.slice(1))
					return yield* [L, ...R]
				}
				for (let i = 2; i < template.length; i++) {
					if (template[i] === "}") {
						const L = ParseUnion(template.slice(2, i))
						const R = ParseLiteral(template.slice(i + 1))
						return yield* [...L, ...R]
					}
				}
				yield exports2.Type.Literal(template)
			}
			function* ParseLiteral(template) {
				for (let i = 0; i < template.length; i++) {
					if (template[i] === "$") {
						const L = exports2.Type.Literal(template.slice(0, i))
						const R = ParseTerminal(template.slice(i))
						return yield* [L, ...R]
					}
				}
				yield exports2.Type.Literal(template)
			}
			function Parse(template_dsl) {
				return [...ParseLiteral(template_dsl)]
			}
			TemplateLiteralDslParser2.Parse = Parse
		})(
			TemplateLiteralDslParser ||
				(exports2.TemplateLiteralDslParser = TemplateLiteralDslParser = {})
		)
		var TransformDecodeBuilder = class {
			constructor(schema2) {
				this.schema = schema2
			}
			Decode(decode) {
				return new TransformEncodeBuilder(this.schema, decode)
			}
		}
		exports2.TransformDecodeBuilder = TransformDecodeBuilder
		var TransformEncodeBuilder = class {
			constructor(schema2, decode) {
				this.schema = schema2
				this.decode = decode
			}
			Encode(encode) {
				const schema2 = TypeClone.Type(this.schema)
				return TypeGuard.TTransform(schema2)
					? (() => {
							const Encode = (value) => schema2[exports2.Transform].Encode(encode(value))
							const Decode = (value) => this.decode(schema2[exports2.Transform].Decode(value))
							const Codec = { Encode, Decode }
							return { ...schema2, [exports2.Transform]: Codec }
					  })()
					: (() => {
							const Codec = { Decode: this.decode, Encode: encode }
							return { ...schema2, [exports2.Transform]: Codec }
					  })()
			}
		}
		exports2.TransformEncodeBuilder = TransformEncodeBuilder
		var TypeOrdinal = 0
		var TypeBuilderError = class extends TypeBoxError {}
		exports2.TypeBuilderError = TypeBuilderError
		var TypeBuilder = class {
			/** `[Internal]` Creates a schema without `static` and `params` types */
			Create(schema2) {
				return schema2
			}
			/** `[Internal]` Throws a TypeBuilder error with the given message */
			Throw(message) {
				throw new TypeBuilderError(message)
			}
			/** `[Internal]` Discards property keys from the given record type */
			Discard(record, keys) {
				return keys.reduce((acc, key) => {
					const { [key]: _, ...rest } = acc
					return rest
				}, record)
			}
			/** `[Json]` Omits compositing symbols from this schema */
			Strict(schema2) {
				return JSON.parse(JSON.stringify(schema2))
			}
		}
		exports2.TypeBuilder = TypeBuilder
		var JsonTypeBuilder = class extends TypeBuilder {
			// ------------------------------------------------------------------------
			// Modifiers
			// ------------------------------------------------------------------------
			/** `[Json]` Creates a Readonly and Optional property */
			ReadonlyOptional(schema2) {
				return this.Readonly(this.Optional(schema2))
			}
			/** `[Json]` Creates a Readonly property */
			Readonly(schema2) {
				return { ...TypeClone.Type(schema2), [exports2.Readonly]: "Readonly" }
			}
			/** `[Json]` Creates an Optional property */
			Optional(schema2) {
				return { ...TypeClone.Type(schema2), [exports2.Optional]: "Optional" }
			}
			// ------------------------------------------------------------------------
			// Types
			// ------------------------------------------------------------------------
			/** `[Json]` Creates an Any type */
			Any(options = {}) {
				return this.Create({ ...options, [exports2.Kind]: "Any" })
			}
			/** `[Json]` Creates an Array type */
			Array(schema2, options = {}) {
				return this.Create({
					...options,
					[exports2.Kind]: "Array",
					type: "array",
					items: TypeClone.Type(schema2),
				})
			}
			/** `[Json]` Creates a Boolean type */
			Boolean(options = {}) {
				return this.Create({ ...options, [exports2.Kind]: "Boolean", type: "boolean" })
			}
			/** `[Json]` Intrinsic function to Capitalize LiteralString types */
			Capitalize(schema2, options = {}) {
				return { ...Intrinsic.Map(TypeClone.Type(schema2), "Capitalize"), ...options }
			}
			/** `[Json]` Creates a Composite object type */
			Composite(objects, options) {
				const intersect = exports2.Type.Intersect(objects, {})
				const keys = KeyResolver.ResolveKeys(intersect, { includePatterns: false })
				const properties = Object.fromEntries(
					keys.map((key) => [key, exports2.Type.Index(intersect, [key])])
				)
				return exports2.Type.Object(properties, options)
			}
			/** `[Json]` Creates a Enum type */
			Enum(item, options = {}) {
				if (ValueGuard.IsUndefined(item)) return this.Throw("Enum undefined or empty")
				const values1 = Object.getOwnPropertyNames(item)
					.filter((key) => isNaN(key))
					.map((key) => item[key])
				const values2 = [...new Set(values1)]
				const anyOf = values2.map((value) => exports2.Type.Literal(value))
				return this.Union(anyOf, { ...options, [exports2.Hint]: "Enum" })
			}
			/** `[Json]` Creates a Conditional type */
			Extends(left, right, trueType, falseType, options = {}) {
				switch (TypeExtends.Extends(left, right)) {
					case TypeExtendsResult.Union:
						return this.Union([
							TypeClone.Type(trueType, options),
							TypeClone.Type(falseType, options),
						])
					case TypeExtendsResult.True:
						return TypeClone.Type(trueType, options)
					case TypeExtendsResult.False:
						return TypeClone.Type(falseType, options)
				}
			}
			/** `[Json]` Constructs a type by excluding from unionType all union members that are assignable to excludedMembers */
			Exclude(unionType, excludedMembers, options = {}) {
				return TypeGuard.TTemplateLiteral(unionType)
					? this.Exclude(TemplateLiteralResolver.Resolve(unionType), excludedMembers, options)
					: TypeGuard.TTemplateLiteral(excludedMembers)
					? this.Exclude(unionType, TemplateLiteralResolver.Resolve(excludedMembers), options)
					: TypeGuard.TUnion(unionType)
					? (() => {
							const narrowed = unionType.anyOf.filter(
								(inner) => TypeExtends.Extends(inner, excludedMembers) === TypeExtendsResult.False
							)
							return narrowed.length === 1
								? TypeClone.Type(narrowed[0], options)
								: this.Union(narrowed, options)
					  })()
					: TypeExtends.Extends(unionType, excludedMembers) !== TypeExtendsResult.False
					? this.Never(options)
					: TypeClone.Type(unionType, options)
			}
			/** `[Json]` Constructs a type by extracting from type all union members that are assignable to union */
			Extract(type, union, options = {}) {
				return TypeGuard.TTemplateLiteral(type)
					? this.Extract(TemplateLiteralResolver.Resolve(type), union, options)
					: TypeGuard.TTemplateLiteral(union)
					? this.Extract(type, TemplateLiteralResolver.Resolve(union), options)
					: TypeGuard.TUnion(type)
					? (() => {
							const narrowed = type.anyOf.filter(
								(inner) => TypeExtends.Extends(inner, union) !== TypeExtendsResult.False
							)
							return narrowed.length === 1
								? TypeClone.Type(narrowed[0], options)
								: this.Union(narrowed, options)
					  })()
					: TypeExtends.Extends(type, union) !== TypeExtendsResult.False
					? TypeClone.Type(type, options)
					: this.Never(options)
			}
			/** `[Json]` Returns an Indexed property type for the given keys */
			Index(schema2, unresolved, options = {}) {
				return TypeGuard.TArray(schema2) && TypeGuard.TNumber(unresolved)
					? (() => {
							return TypeClone.Type(schema2.items, options)
					  })()
					: TypeGuard.TTuple(schema2) && TypeGuard.TNumber(unresolved)
					? (() => {
							const items = ValueGuard.IsUndefined(schema2.items) ? [] : schema2.items
							const cloned = items.map((schema3) => TypeClone.Type(schema3))
							return this.Union(cloned, options)
					  })()
					: (() => {
							const keys = KeyArrayResolver.Resolve(unresolved)
							const clone2 = TypeClone.Type(schema2)
							return IndexedAccessor.Resolve(clone2, keys, options)
					  })()
			}
			/** `[Json]` Creates an Integer type */
			Integer(options = {}) {
				return this.Create({ ...options, [exports2.Kind]: "Integer", type: "integer" })
			}
			/** `[Json]` Creates an Intersect type */
			Intersect(allOf, options = {}) {
				if (allOf.length === 0) return exports2.Type.Never()
				if (allOf.length === 1) return TypeClone.Type(allOf[0], options)
				if (allOf.some((schema2) => TypeGuard.TTransform(schema2)))
					this.Throw("Cannot intersect transform types")
				const objects = allOf.every((schema2) => TypeGuard.TObject(schema2))
				const cloned = TypeClone.Rest(allOf)
				const clonedUnevaluatedProperties = TypeGuard.TSchema(options.unevaluatedProperties)
					? { unevaluatedProperties: TypeClone.Type(options.unevaluatedProperties) }
					: {}
				return options.unevaluatedProperties === false ||
					TypeGuard.TSchema(options.unevaluatedProperties) ||
					objects
					? this.Create({
							...options,
							...clonedUnevaluatedProperties,
							[exports2.Kind]: "Intersect",
							type: "object",
							allOf: cloned,
					  })
					: this.Create({
							...options,
							...clonedUnevaluatedProperties,
							[exports2.Kind]: "Intersect",
							allOf: cloned,
					  })
			}
			/** `[Json]` Creates a KeyOf type */
			KeyOf(schema2, options = {}) {
				return TypeGuard.TRecord(schema2)
					? (() => {
							const pattern3 = Object.getOwnPropertyNames(schema2.patternProperties)[0]
							return pattern3 === exports2.PatternNumberExact
								? this.Number(options)
								: pattern3 === exports2.PatternStringExact
								? this.String(options)
								: this.Throw("Unable to resolve key type from Record key pattern")
					  })()
					: TypeGuard.TTuple(schema2)
					? (() => {
							const items = ValueGuard.IsUndefined(schema2.items) ? [] : schema2.items
							const literals = items.map((_, index2) => exports2.Type.Literal(index2.toString()))
							return this.Union(literals, options)
					  })()
					: TypeGuard.TArray(schema2)
					? (() => {
							return this.Number(options)
					  })()
					: (() => {
							const keys = KeyResolver.ResolveKeys(schema2, { includePatterns: false })
							if (keys.length === 0) return this.Never(options)
							const literals = keys.map((key) => this.Literal(key))
							return this.Union(literals, options)
					  })()
			}
			/** `[Json]` Creates a Literal type */
			Literal(value, options = {}) {
				return this.Create({
					...options,
					[exports2.Kind]: "Literal",
					const: value,
					type: typeof value,
				})
			}
			/** `[Json]` Intrinsic function to Lowercase LiteralString types */
			Lowercase(schema2, options = {}) {
				return { ...Intrinsic.Map(TypeClone.Type(schema2), "Lowercase"), ...options }
			}
			/** `[Json]` Creates a Never type */
			Never(options = {}) {
				return this.Create({ ...options, [exports2.Kind]: "Never", not: {} })
			}
			/** `[Json]` Creates a Not type */
			Not(schema2, options) {
				return this.Create({ ...options, [exports2.Kind]: "Not", not: TypeClone.Type(schema2) })
			}
			/** `[Json]` Creates a Null type */
			Null(options = {}) {
				return this.Create({ ...options, [exports2.Kind]: "Null", type: "null" })
			}
			/** `[Json]` Creates a Number type */
			Number(options = {}) {
				return this.Create({ ...options, [exports2.Kind]: "Number", type: "number" })
			}
			/** `[Json]` Creates an Object type */
			Object(properties, options = {}) {
				const propertyKeys = Object.getOwnPropertyNames(properties)
				const optionalKeys = propertyKeys.filter((key) => TypeGuard.TOptional(properties[key]))
				const requiredKeys = propertyKeys.filter((name) => !optionalKeys.includes(name))
				const clonedAdditionalProperties = TypeGuard.TSchema(options.additionalProperties)
					? { additionalProperties: TypeClone.Type(options.additionalProperties) }
					: {}
				const clonedProperties = Object.fromEntries(
					propertyKeys.map((key) => [key, TypeClone.Type(properties[key])])
				)
				return requiredKeys.length > 0
					? this.Create({
							...options,
							...clonedAdditionalProperties,
							[exports2.Kind]: "Object",
							type: "object",
							properties: clonedProperties,
							required: requiredKeys,
					  })
					: this.Create({
							...options,
							...clonedAdditionalProperties,
							[exports2.Kind]: "Object",
							type: "object",
							properties: clonedProperties,
					  })
			}
			/** `[Json]` Constructs a type whose keys are omitted from the given type */
			Omit(schema2, unresolved, options = {}) {
				const keys = KeyArrayResolver.Resolve(unresolved)
				return ObjectMap.Map(
					this.Discard(TypeClone.Type(schema2), ["$id", exports2.Transform]),
					(object) => {
						if (ValueGuard.IsArray(object.required)) {
							object.required = object.required.filter((key) => !keys.includes(key))
							if (object.required.length === 0) delete object.required
						}
						for (const key of Object.getOwnPropertyNames(object.properties)) {
							if (keys.includes(key)) delete object.properties[key]
						}
						return this.Create(object)
					},
					options
				)
			}
			/** `[Json]` Constructs a type where all properties are optional */
			Partial(schema2, options = {}) {
				return ObjectMap.Map(
					this.Discard(TypeClone.Type(schema2), ["$id", exports2.Transform]),
					(object) => {
						const properties = Object.getOwnPropertyNames(object.properties).reduce((acc, key) => {
							return { ...acc, [key]: this.Optional(object.properties[key]) }
						}, {})
						return this.Object(
							properties,
							this.Discard(object, ["required"])
							/* object used as options to retain other constraints */
						)
					},
					options
				)
			}
			/** `[Json]` Constructs a type whose keys are picked from the given type */
			Pick(schema2, unresolved, options = {}) {
				const keys = KeyArrayResolver.Resolve(unresolved)
				return ObjectMap.Map(
					this.Discard(TypeClone.Type(schema2), ["$id", exports2.Transform]),
					(object) => {
						if (ValueGuard.IsArray(object.required)) {
							object.required = object.required.filter((key) => keys.includes(key))
							if (object.required.length === 0) delete object.required
						}
						for (const key of Object.getOwnPropertyNames(object.properties)) {
							if (!keys.includes(key)) delete object.properties[key]
						}
						return this.Create(object)
					},
					options
				)
			}
			/** `[Json]` Creates a Record type */
			Record(key, schema2, options = {}) {
				return TypeGuard.TTemplateLiteral(key)
					? (() => {
							const expression = TemplateLiteralParser.ParseExact(key.pattern)
							return TemplateLiteralFinite.Check(expression)
								? this.Object(
										Object.fromEntries(
											[...TemplateLiteralGenerator.Generate(expression)].map((key2) => [
												key2,
												TypeClone.Type(schema2),
											])
										),
										options
								  )
								: this.Create({
										...options,
										[exports2.Kind]: "Record",
										type: "object",
										patternProperties: { [key.pattern]: TypeClone.Type(schema2) },
								  })
					  })()
					: TypeGuard.TUnion(key)
					? (() => {
							const union = UnionResolver.Resolve(key)
							if (TypeGuard.TUnionLiteral(union)) {
								const properties = Object.fromEntries(
									union.anyOf.map((literal) => [literal.const, TypeClone.Type(schema2)])
								)
								return this.Object(properties, { ...options, [exports2.Hint]: "Record" })
							} else this.Throw("Record key of type union contains non-literal types")
					  })()
					: TypeGuard.TLiteral(key)
					? (() => {
							return ValueGuard.IsString(key.const) || ValueGuard.IsNumber(key.const)
								? this.Object({ [key.const]: TypeClone.Type(schema2) }, options)
								: this.Throw("Record key of type literal is not of type string or number")
					  })()
					: TypeGuard.TInteger(key) || TypeGuard.TNumber(key)
					? (() => {
							return this.Create({
								...options,
								[exports2.Kind]: "Record",
								type: "object",
								patternProperties: { [exports2.PatternNumberExact]: TypeClone.Type(schema2) },
							})
					  })()
					: TypeGuard.TString(key)
					? (() => {
							const pattern3 = ValueGuard.IsUndefined(key.pattern)
								? exports2.PatternStringExact
								: key.pattern
							return this.Create({
								...options,
								[exports2.Kind]: "Record",
								type: "object",
								patternProperties: { [pattern3]: TypeClone.Type(schema2) },
							})
					  })()
					: this.Never()
			}
			/** `[Json]` Creates a Recursive type */
			Recursive(callback, options = {}) {
				if (ValueGuard.IsUndefined(options.$id)) options.$id = `T${TypeOrdinal++}`
				const thisType = callback({ [exports2.Kind]: "This", $ref: `${options.$id}` })
				thisType.$id = options.$id
				return this.Create({ ...options, [exports2.Hint]: "Recursive", ...thisType })
			}
			/** `[Json]` Creates a Ref type. */
			Ref(unresolved, options = {}) {
				if (ValueGuard.IsString(unresolved))
					return this.Create({ ...options, [exports2.Kind]: "Ref", $ref: unresolved })
				if (ValueGuard.IsUndefined(unresolved.$id))
					this.Throw("Reference target type must specify an $id")
				return this.Create({ ...options, [exports2.Kind]: "Ref", $ref: unresolved.$id })
			}
			/** `[Json]` Constructs a type where all properties are required */
			Required(schema2, options = {}) {
				return ObjectMap.Map(
					this.Discard(TypeClone.Type(schema2), ["$id", exports2.Transform]),
					(object) => {
						const properties = Object.getOwnPropertyNames(object.properties).reduce((acc, key) => {
							return { ...acc, [key]: this.Discard(object.properties[key], [exports2.Optional]) }
						}, {})
						return this.Object(
							properties,
							object
							/* object used as options to retain other constraints  */
						)
					},
					options
				)
			}
			/** `[Json]` Extracts interior Rest elements from Tuple, Intersect and Union types */
			Rest(schema2) {
				return TypeGuard.TTuple(schema2) && !ValueGuard.IsUndefined(schema2.items)
					? TypeClone.Rest(schema2.items)
					: TypeGuard.TIntersect(schema2)
					? TypeClone.Rest(schema2.allOf)
					: TypeGuard.TUnion(schema2)
					? TypeClone.Rest(schema2.anyOf)
					: []
			}
			/** `[Json]` Creates a String type */
			String(options = {}) {
				return this.Create({ ...options, [exports2.Kind]: "String", type: "string" })
			}
			/** `[Json]` Creates a TemplateLiteral type */
			TemplateLiteral(unresolved, options = {}) {
				const pattern3 = ValueGuard.IsString(unresolved)
					? TemplateLiteralPattern.Create(TemplateLiteralDslParser.Parse(unresolved))
					: TemplateLiteralPattern.Create(unresolved)
				return this.Create({
					...options,
					[exports2.Kind]: "TemplateLiteral",
					type: "string",
					pattern: pattern3,
				})
			}
			/** `[Json]` Creates a Transform type */
			Transform(schema2) {
				return new TransformDecodeBuilder(schema2)
			}
			/** `[Json]` Creates a Tuple type */
			Tuple(items, options = {}) {
				const [additionalItems, minItems, maxItems] = [false, items.length, items.length]
				const clonedItems = TypeClone.Rest(items)
				const schema2 =
					items.length > 0
						? {
								...options,
								[exports2.Kind]: "Tuple",
								type: "array",
								items: clonedItems,
								additionalItems,
								minItems,
								maxItems,
						  }
						: { ...options, [exports2.Kind]: "Tuple", type: "array", minItems, maxItems }
				return this.Create(schema2)
			}
			/** `[Json]` Intrinsic function to Uncapitalize LiteralString types */
			Uncapitalize(schema2, options = {}) {
				return { ...Intrinsic.Map(TypeClone.Type(schema2), "Uncapitalize"), ...options }
			}
			/** `[Json]` Creates a Union type */
			Union(union, options = {}) {
				return TypeGuard.TTemplateLiteral(union)
					? TemplateLiteralResolver.Resolve(union)
					: (() => {
							const anyOf = union
							if (anyOf.length === 0) return this.Never(options)
							if (anyOf.length === 1) return this.Create(TypeClone.Type(anyOf[0], options))
							const clonedAnyOf = TypeClone.Rest(anyOf)
							return this.Create({ ...options, [exports2.Kind]: "Union", anyOf: clonedAnyOf })
					  })()
			}
			/** `[Json]` Creates an Unknown type */
			Unknown(options = {}) {
				return this.Create({ ...options, [exports2.Kind]: "Unknown" })
			}
			/** `[Json]` Creates a Unsafe type that will infers as the generic argument T */
			Unsafe(options = {}) {
				return this.Create({ ...options, [exports2.Kind]: options[exports2.Kind] || "Unsafe" })
			}
			/** `[Json]` Intrinsic function to Uppercase LiteralString types */
			Uppercase(schema2, options = {}) {
				return { ...Intrinsic.Map(TypeClone.Type(schema2), "Uppercase"), ...options }
			}
		}
		exports2.JsonTypeBuilder = JsonTypeBuilder
		var JavaScriptTypeBuilder = class extends JsonTypeBuilder {
			/** `[JavaScript]` Creates a AsyncIterator type */
			AsyncIterator(items, options = {}) {
				return this.Create({
					...options,
					[exports2.Kind]: "AsyncIterator",
					type: "AsyncIterator",
					items: TypeClone.Type(items),
				})
			}
			/** `[JavaScript]` Constructs a type by recursively unwrapping Promise types */
			Awaited(schema2, options = {}) {
				const Unwrap = (rest) =>
					rest.length > 0
						? (() => {
								const [L, ...R] = rest
								return [this.Awaited(L), ...Unwrap(R)]
						  })()
						: rest
				return TypeGuard.TIntersect(schema2)
					? exports2.Type.Intersect(Unwrap(schema2.allOf))
					: TypeGuard.TUnion(schema2)
					? exports2.Type.Union(Unwrap(schema2.anyOf))
					: TypeGuard.TPromise(schema2)
					? this.Awaited(schema2.item)
					: TypeClone.Type(schema2, options)
			}
			/** `[JavaScript]` Creates a BigInt type */
			BigInt(options = {}) {
				return this.Create({ ...options, [exports2.Kind]: "BigInt", type: "bigint" })
			}
			/** `[JavaScript]` Extracts the ConstructorParameters from the given Constructor type */
			ConstructorParameters(schema2, options = {}) {
				return this.Tuple([...schema2.parameters], { ...options })
			}
			/** `[JavaScript]` Creates a Constructor type */
			Constructor(parameters, returns, options) {
				const [clonedParameters, clonedReturns] = [
					TypeClone.Rest(parameters),
					TypeClone.Type(returns),
				]
				return this.Create({
					...options,
					[exports2.Kind]: "Constructor",
					type: "Constructor",
					parameters: clonedParameters,
					returns: clonedReturns,
				})
			}
			/** `[JavaScript]` Creates a Date type */
			Date(options = {}) {
				return this.Create({ ...options, [exports2.Kind]: "Date", type: "Date" })
			}
			/** `[JavaScript]` Creates a Function type */
			Function(parameters, returns, options) {
				const [clonedParameters, clonedReturns] = [
					TypeClone.Rest(parameters),
					TypeClone.Type(returns),
				]
				return this.Create({
					...options,
					[exports2.Kind]: "Function",
					type: "Function",
					parameters: clonedParameters,
					returns: clonedReturns,
				})
			}
			/** `[JavaScript]` Extracts the InstanceType from the given Constructor type */
			InstanceType(schema2, options = {}) {
				return TypeClone.Type(schema2.returns, options)
			}
			/** `[JavaScript]` Creates an Iterator type */
			Iterator(items, options = {}) {
				return this.Create({
					...options,
					[exports2.Kind]: "Iterator",
					type: "Iterator",
					items: TypeClone.Type(items),
				})
			}
			/** `[JavaScript]` Extracts the Parameters from the given Function type */
			Parameters(schema2, options = {}) {
				return this.Tuple(schema2.parameters, { ...options })
			}
			/** `[JavaScript]` Creates a Promise type */
			Promise(item, options = {}) {
				return this.Create({
					...options,
					[exports2.Kind]: "Promise",
					type: "Promise",
					item: TypeClone.Type(item),
				})
			}
			/** `[Extended]` Creates a String type */
			RegExp(unresolved, options = {}) {
				const pattern3 = ValueGuard.IsString(unresolved) ? unresolved : unresolved.source
				return this.Create({
					...options,
					[exports2.Kind]: "String",
					type: "string",
					pattern: pattern3,
				})
			}
			/**
			 * @deprecated Use `Type.RegExp`
			 */
			RegEx(regex, options = {}) {
				return this.RegExp(regex, options)
			}
			/** `[JavaScript]` Extracts the ReturnType from the given Function type */
			ReturnType(schema2, options = {}) {
				return TypeClone.Type(schema2.returns, options)
			}
			/** `[JavaScript]` Creates a Symbol type */
			Symbol(options) {
				return this.Create({ ...options, [exports2.Kind]: "Symbol", type: "symbol" })
			}
			/** `[JavaScript]` Creates a Undefined type */
			Undefined(options = {}) {
				return this.Create({ ...options, [exports2.Kind]: "Undefined", type: "undefined" })
			}
			/** `[JavaScript]` Creates a Uint8Array type */
			Uint8Array(options = {}) {
				return this.Create({ ...options, [exports2.Kind]: "Uint8Array", type: "Uint8Array" })
			}
			/** `[JavaScript]` Creates a Void type */
			Void(options = {}) {
				return this.Create({ ...options, [exports2.Kind]: "Void", type: "void" })
			}
		}
		exports2.JavaScriptTypeBuilder = JavaScriptTypeBuilder
		exports2.JsonType = new JsonTypeBuilder()
		exports2.Type = new JavaScriptTypeBuilder()
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/guard.js
var require_guard = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/guard.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.IsValueType =
			exports2.IsSymbol =
			exports2.IsFunction =
			exports2.IsString =
			exports2.IsBigInt =
			exports2.IsInteger =
			exports2.IsNumber =
			exports2.IsBoolean =
			exports2.IsNull =
			exports2.IsUndefined =
			exports2.IsArray =
			exports2.IsObject =
			exports2.IsPlainObject =
			exports2.HasPropertyKey =
			exports2.IsDate =
			exports2.IsUint8Array =
			exports2.IsPromise =
			exports2.IsTypedArray =
			exports2.IsIterator =
			exports2.IsAsyncIterator =
				void 0
		function IsAsyncIterator(value) {
			return IsObject(value) && Symbol.asyncIterator in value
		}
		exports2.IsAsyncIterator = IsAsyncIterator
		function IsIterator(value) {
			return IsObject(value) && Symbol.iterator in value
		}
		exports2.IsIterator = IsIterator
		function IsTypedArray(value) {
			return ArrayBuffer.isView(value)
		}
		exports2.IsTypedArray = IsTypedArray
		function IsPromise(value) {
			return value instanceof Promise
		}
		exports2.IsPromise = IsPromise
		function IsUint8Array(value) {
			return value instanceof Uint8Array
		}
		exports2.IsUint8Array = IsUint8Array
		function IsDate(value) {
			return value instanceof Date && Number.isFinite(value.getTime())
		}
		exports2.IsDate = IsDate
		function HasPropertyKey(value, key) {
			return key in value
		}
		exports2.HasPropertyKey = HasPropertyKey
		function IsPlainObject(value) {
			return IsObject(value) && IsFunction(value.constructor) && value.constructor.name === "Object"
		}
		exports2.IsPlainObject = IsPlainObject
		function IsObject(value) {
			return value !== null && typeof value === "object"
		}
		exports2.IsObject = IsObject
		function IsArray(value) {
			return Array.isArray(value) && !ArrayBuffer.isView(value)
		}
		exports2.IsArray = IsArray
		function IsUndefined(value) {
			return value === void 0
		}
		exports2.IsUndefined = IsUndefined
		function IsNull(value) {
			return value === null
		}
		exports2.IsNull = IsNull
		function IsBoolean(value) {
			return typeof value === "boolean"
		}
		exports2.IsBoolean = IsBoolean
		function IsNumber(value) {
			return typeof value === "number"
		}
		exports2.IsNumber = IsNumber
		function IsInteger(value) {
			return IsNumber(value) && Number.isInteger(value)
		}
		exports2.IsInteger = IsInteger
		function IsBigInt(value) {
			return typeof value === "bigint"
		}
		exports2.IsBigInt = IsBigInt
		function IsString(value) {
			return typeof value === "string"
		}
		exports2.IsString = IsString
		function IsFunction(value) {
			return typeof value === "function"
		}
		exports2.IsFunction = IsFunction
		function IsSymbol(value) {
			return typeof value === "symbol"
		}
		exports2.IsSymbol = IsSymbol
		function IsValueType(value) {
			return (
				IsBigInt(value) ||
				IsBoolean(value) ||
				IsNull(value) ||
				IsNumber(value) ||
				IsString(value) ||
				IsSymbol(value) ||
				IsUndefined(value)
			)
		}
		exports2.IsValueType = IsValueType
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/system/system.js
var require_system = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/system/system.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.DefaultErrorFunction =
			exports2.TypeSystemPolicy =
			exports2.TypeSystemErrorFunction =
			exports2.TypeSystem =
			exports2.TypeSystemDuplicateFormat =
			exports2.TypeSystemDuplicateTypeKind =
				void 0
		var guard_1 = require_guard()
		var errors_1 = require_errors2()
		var Types = require_typebox()
		var TypeSystemDuplicateTypeKind = class extends Types.TypeBoxError {
			constructor(kind) {
				super(`Duplicate type kind '${kind}' detected`)
			}
		}
		exports2.TypeSystemDuplicateTypeKind = TypeSystemDuplicateTypeKind
		var TypeSystemDuplicateFormat = class extends Types.TypeBoxError {
			constructor(kind) {
				super(`Duplicate string format '${kind}' detected`)
			}
		}
		exports2.TypeSystemDuplicateFormat = TypeSystemDuplicateFormat
		var TypeSystem
		;(function (TypeSystem2) {
			function Type11(kind, check) {
				if (Types.TypeRegistry.Has(kind)) throw new TypeSystemDuplicateTypeKind(kind)
				Types.TypeRegistry.Set(kind, check)
				return (options = {}) => Types.Type.Unsafe({ ...options, [Types.Kind]: kind })
			}
			TypeSystem2.Type = Type11
			function Format(format, check) {
				if (Types.FormatRegistry.Has(format)) throw new TypeSystemDuplicateFormat(format)
				Types.FormatRegistry.Set(format, check)
				return format
			}
			TypeSystem2.Format = Format
		})(TypeSystem || (exports2.TypeSystem = TypeSystem = {}))
		var TypeSystemErrorFunction
		;(function (TypeSystemErrorFunction2) {
			let errorMessageFunction = DefaultErrorFunction
			function Reset() {
				errorMessageFunction = DefaultErrorFunction
			}
			TypeSystemErrorFunction2.Reset = Reset
			function Set2(callback) {
				errorMessageFunction = callback
			}
			TypeSystemErrorFunction2.Set = Set2
			function Get() {
				return errorMessageFunction
			}
			TypeSystemErrorFunction2.Get = Get
		})(TypeSystemErrorFunction || (exports2.TypeSystemErrorFunction = TypeSystemErrorFunction = {}))
		var TypeSystemPolicy
		;(function (TypeSystemPolicy2) {
			TypeSystemPolicy2.ExactOptionalPropertyTypes = false
			TypeSystemPolicy2.AllowArrayObject = false
			TypeSystemPolicy2.AllowNaN = false
			TypeSystemPolicy2.AllowNullVoid = false
			function IsExactOptionalProperty(value, key) {
				return TypeSystemPolicy2.ExactOptionalPropertyTypes ? key in value : value[key] !== void 0
			}
			TypeSystemPolicy2.IsExactOptionalProperty = IsExactOptionalProperty
			function IsObjectLike(value) {
				const isObject2 = (0, guard_1.IsObject)(value)
				return TypeSystemPolicy2.AllowArrayObject
					? isObject2
					: isObject2 && !(0, guard_1.IsArray)(value)
			}
			TypeSystemPolicy2.IsObjectLike = IsObjectLike
			function IsRecordLike(value) {
				return IsObjectLike(value) && !(value instanceof Date) && !(value instanceof Uint8Array)
			}
			TypeSystemPolicy2.IsRecordLike = IsRecordLike
			function IsNumberLike(value) {
				const isNumber = (0, guard_1.IsNumber)(value)
				return TypeSystemPolicy2.AllowNaN ? isNumber : isNumber && Number.isFinite(value)
			}
			TypeSystemPolicy2.IsNumberLike = IsNumberLike
			function IsVoidLike(value) {
				const isUndefined = (0, guard_1.IsUndefined)(value)
				return TypeSystemPolicy2.AllowNullVoid ? isUndefined || value === null : isUndefined
			}
			TypeSystemPolicy2.IsVoidLike = IsVoidLike
		})(TypeSystemPolicy || (exports2.TypeSystemPolicy = TypeSystemPolicy = {}))
		function DefaultErrorFunction(schema2, errorType) {
			switch (errorType) {
				case errors_1.ValueErrorType.ArrayContains:
					return "Expected array to contain at least one matching value"
				case errors_1.ValueErrorType.ArrayMaxContains:
					return `Expected array to contain no more than ${schema2.maxContains} matching values`
				case errors_1.ValueErrorType.ArrayMinContains:
					return `Expected array to contain at least ${schema2.minContains} matching values`
				case errors_1.ValueErrorType.ArrayMaxItems:
					return `Expected array length to be less or equal to ${schema2.maxItems}`
				case errors_1.ValueErrorType.ArrayMinItems:
					return `Expected array length to be greater or equal to ${schema2.minItems}`
				case errors_1.ValueErrorType.ArrayUniqueItems:
					return "Expected array elements to be unique"
				case errors_1.ValueErrorType.Array:
					return "Expected array"
				case errors_1.ValueErrorType.AsyncIterator:
					return "Expected AsyncIterator"
				case errors_1.ValueErrorType.BigIntExclusiveMaximum:
					return `Expected bigint to be less than ${schema2.exclusiveMaximum}`
				case errors_1.ValueErrorType.BigIntExclusiveMinimum:
					return `Expected bigint to be greater than ${schema2.exclusiveMinimum}`
				case errors_1.ValueErrorType.BigIntMaximum:
					return `Expected bigint to be less or equal to ${schema2.maximum}`
				case errors_1.ValueErrorType.BigIntMinimum:
					return `Expected bigint to be greater or equal to ${schema2.minimum}`
				case errors_1.ValueErrorType.BigIntMultipleOf:
					return `Expected bigint to be a multiple of ${schema2.multipleOf}`
				case errors_1.ValueErrorType.BigInt:
					return "Expected bigint"
				case errors_1.ValueErrorType.Boolean:
					return "Expected boolean"
				case errors_1.ValueErrorType.DateExclusiveMinimumTimestamp:
					return `Expected Date timestamp to be greater than ${schema2.exclusiveMinimumTimestamp}`
				case errors_1.ValueErrorType.DateExclusiveMaximumTimestamp:
					return `Expected Date timestamp to be less than ${schema2.exclusiveMaximumTimestamp}`
				case errors_1.ValueErrorType.DateMinimumTimestamp:
					return `Expected Date timestamp to be greater or equal to ${schema2.minimumTimestamp}`
				case errors_1.ValueErrorType.DateMaximumTimestamp:
					return `Expected Date timestamp to be less or equal to ${schema2.maximumTimestamp}`
				case errors_1.ValueErrorType.DateMultipleOfTimestamp:
					return `Expected Date timestamp to be a multiple of ${schema2.multipleOfTimestamp}`
				case errors_1.ValueErrorType.Date:
					return "Expected Date"
				case errors_1.ValueErrorType.Function:
					return "Expected function"
				case errors_1.ValueErrorType.IntegerExclusiveMaximum:
					return `Expected integer to be less than ${schema2.exclusiveMaximum}`
				case errors_1.ValueErrorType.IntegerExclusiveMinimum:
					return `Expected integer to be greater than ${schema2.exclusiveMinimum}`
				case errors_1.ValueErrorType.IntegerMaximum:
					return `Expected integer to be less or equal to ${schema2.maximum}`
				case errors_1.ValueErrorType.IntegerMinimum:
					return `Expected integer to be greater or equal to ${schema2.minimum}`
				case errors_1.ValueErrorType.IntegerMultipleOf:
					return `Expected integer to be a multiple of ${schema2.multipleOf}`
				case errors_1.ValueErrorType.Integer:
					return "Expected integer"
				case errors_1.ValueErrorType.IntersectUnevaluatedProperties:
					return "Unexpected property"
				case errors_1.ValueErrorType.Intersect:
					return "Expected all values to match"
				case errors_1.ValueErrorType.Iterator:
					return "Expected Iterator"
				case errors_1.ValueErrorType.Literal:
					return `Expected ${
						typeof schema2.const === "string" ? `'${schema2.const}'` : schema2.const
					}`
				case errors_1.ValueErrorType.Never:
					return "Never"
				case errors_1.ValueErrorType.Not:
					return "Value should not match"
				case errors_1.ValueErrorType.Null:
					return "Expected null"
				case errors_1.ValueErrorType.NumberExclusiveMaximum:
					return `Expected number to be less than ${schema2.exclusiveMaximum}`
				case errors_1.ValueErrorType.NumberExclusiveMinimum:
					return `Expected number to be greater than ${schema2.exclusiveMinimum}`
				case errors_1.ValueErrorType.NumberMaximum:
					return `Expected number to be less or equal to ${schema2.maximum}`
				case errors_1.ValueErrorType.NumberMinimum:
					return `Expected number to be greater or equal to ${schema2.minimum}`
				case errors_1.ValueErrorType.NumberMultipleOf:
					return `Expected number to be a multiple of ${schema2.multipleOf}`
				case errors_1.ValueErrorType.Number:
					return "Expected number"
				case errors_1.ValueErrorType.Object:
					return "Expected object"
				case errors_1.ValueErrorType.ObjectAdditionalProperties:
					return "Unexpected property"
				case errors_1.ValueErrorType.ObjectMaxProperties:
					return `Expected object to have no more than ${schema2.maxProperties} properties`
				case errors_1.ValueErrorType.ObjectMinProperties:
					return `Expected object to have at least ${schema2.minProperties} properties`
				case errors_1.ValueErrorType.ObjectRequiredProperty:
					return "Required property"
				case errors_1.ValueErrorType.Promise:
					return "Expected Promise"
				case errors_1.ValueErrorType.StringFormatUnknown:
					return `Unknown format '${schema2.format}'`
				case errors_1.ValueErrorType.StringFormat:
					return `Expected string to match '${schema2.format}' format`
				case errors_1.ValueErrorType.StringMaxLength:
					return `Expected string length less or equal to ${schema2.maxLength}`
				case errors_1.ValueErrorType.StringMinLength:
					return `Expected string length greater or equal to ${schema2.minLength}`
				case errors_1.ValueErrorType.StringPattern:
					return `Expected string to match '${schema2.pattern}'`
				case errors_1.ValueErrorType.String:
					return "Expected string"
				case errors_1.ValueErrorType.Symbol:
					return "Expected symbol"
				case errors_1.ValueErrorType.TupleLength:
					return `Expected tuple to have ${schema2.maxItems || 0} elements`
				case errors_1.ValueErrorType.Tuple:
					return "Expected tuple"
				case errors_1.ValueErrorType.Uint8ArrayMaxByteLength:
					return `Expected byte length less or equal to ${schema2.maxByteLength}`
				case errors_1.ValueErrorType.Uint8ArrayMinByteLength:
					return `Expected byte length greater or equal to ${schema2.minByteLength}`
				case errors_1.ValueErrorType.Uint8Array:
					return "Expected Uint8Array"
				case errors_1.ValueErrorType.Undefined:
					return "Expected undefined"
				case errors_1.ValueErrorType.Union:
					return "Expected union value"
				case errors_1.ValueErrorType.Void:
					return "Expected void"
				case errors_1.ValueErrorType.Kind:
					return `Expected kind '${schema2[Types.Kind]}'`
				default:
					return "Unknown error type"
			}
		}
		exports2.DefaultErrorFunction = DefaultErrorFunction
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/deref.js
var require_deref = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/deref.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.Deref = exports2.TypeDereferenceError = void 0
		var typebox_1 = require_typebox()
		var TypeDereferenceError = class extends typebox_1.TypeBoxError {
			constructor(schema2) {
				super(`Unable to dereference schema with $id '${schema2.$id}'`)
				this.schema = schema2
			}
		}
		exports2.TypeDereferenceError = TypeDereferenceError
		function Deref(schema2, references) {
			const index2 = references.findIndex((target) => target.$id === schema2.$ref)
			if (index2 === -1) throw new TypeDereferenceError(schema2)
			return references[index2]
		}
		exports2.Deref = Deref
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/hash.js
var require_hash2 = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/hash.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.Hash = exports2.ByteMarker = exports2.ValueHashError = void 0
		var guard_1 = require_guard()
		var ValueHashError = class extends Error {
			constructor(value) {
				super(`Unable to hash value`)
				this.value = value
			}
		}
		exports2.ValueHashError = ValueHashError
		var ByteMarker
		;(function (ByteMarker2) {
			ByteMarker2[(ByteMarker2["Undefined"] = 0)] = "Undefined"
			ByteMarker2[(ByteMarker2["Null"] = 1)] = "Null"
			ByteMarker2[(ByteMarker2["Boolean"] = 2)] = "Boolean"
			ByteMarker2[(ByteMarker2["Number"] = 3)] = "Number"
			ByteMarker2[(ByteMarker2["String"] = 4)] = "String"
			ByteMarker2[(ByteMarker2["Object"] = 5)] = "Object"
			ByteMarker2[(ByteMarker2["Array"] = 6)] = "Array"
			ByteMarker2[(ByteMarker2["Date"] = 7)] = "Date"
			ByteMarker2[(ByteMarker2["Uint8Array"] = 8)] = "Uint8Array"
			ByteMarker2[(ByteMarker2["Symbol"] = 9)] = "Symbol"
			ByteMarker2[(ByteMarker2["BigInt"] = 10)] = "BigInt"
		})(ByteMarker || (exports2.ByteMarker = ByteMarker = {}))
		var Accumulator = BigInt("14695981039346656037")
		var [Prime, Size] = [BigInt("1099511628211"), BigInt("2") ** BigInt("64")]
		var Bytes = Array.from({ length: 256 }).map((_, i) => BigInt(i))
		var F64 = new Float64Array(1)
		var F64In = new DataView(F64.buffer)
		var F64Out = new Uint8Array(F64.buffer)
		function* NumberToBytes(value) {
			const byteCount = value === 0 ? 1 : Math.ceil(Math.floor(Math.log2(value) + 1) / 8)
			for (let i = 0; i < byteCount; i++) {
				yield (value >> (8 * (byteCount - 1 - i))) & 255
			}
		}
		function ArrayType(value) {
			FNV1A64(ByteMarker.Array)
			for (const item of value) {
				Visit(item)
			}
		}
		function BooleanType(value) {
			FNV1A64(ByteMarker.Boolean)
			FNV1A64(value ? 1 : 0)
		}
		function BigIntType(value) {
			FNV1A64(ByteMarker.BigInt)
			F64In.setBigInt64(0, value)
			for (const byte of F64Out) {
				FNV1A64(byte)
			}
		}
		function DateType(value) {
			FNV1A64(ByteMarker.Date)
			Visit(value.getTime())
		}
		function NullType(value) {
			FNV1A64(ByteMarker.Null)
		}
		function NumberType(value) {
			FNV1A64(ByteMarker.Number)
			F64In.setFloat64(0, value)
			for (const byte of F64Out) {
				FNV1A64(byte)
			}
		}
		function ObjectType(value) {
			FNV1A64(ByteMarker.Object)
			for (const key of globalThis.Object.keys(value).sort()) {
				Visit(key)
				Visit(value[key])
			}
		}
		function StringType(value) {
			FNV1A64(ByteMarker.String)
			for (let i = 0; i < value.length; i++) {
				for (const byte of NumberToBytes(value.charCodeAt(i))) {
					FNV1A64(byte)
				}
			}
		}
		function SymbolType(value) {
			FNV1A64(ByteMarker.Symbol)
			Visit(value.description)
		}
		function Uint8ArrayType(value) {
			FNV1A64(ByteMarker.Uint8Array)
			for (const element of value) {
				FNV1A64(element)
			}
		}
		function UndefinedType(value) {
			return FNV1A64(ByteMarker.Undefined)
		}
		function Visit(value) {
			if ((0, guard_1.IsArray)(value)) return ArrayType(value)
			if ((0, guard_1.IsBoolean)(value)) return BooleanType(value)
			if ((0, guard_1.IsBigInt)(value)) return BigIntType(value)
			if ((0, guard_1.IsDate)(value)) return DateType(value)
			if ((0, guard_1.IsNull)(value)) return NullType(value)
			if ((0, guard_1.IsNumber)(value)) return NumberType(value)
			if ((0, guard_1.IsPlainObject)(value)) return ObjectType(value)
			if ((0, guard_1.IsString)(value)) return StringType(value)
			if ((0, guard_1.IsSymbol)(value)) return SymbolType(value)
			if ((0, guard_1.IsUint8Array)(value)) return Uint8ArrayType(value)
			if ((0, guard_1.IsUndefined)(value)) return UndefinedType(value)
			throw new ValueHashError(value)
		}
		function FNV1A64(byte) {
			Accumulator = Accumulator ^ Bytes[byte]
			Accumulator = (Accumulator * Prime) % Size
		}
		function Hash2(value) {
			Accumulator = BigInt("14695981039346656037")
			Visit(value)
			return Accumulator
		}
		exports2.Hash = Hash2
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/errors/errors.js
var require_errors2 = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/errors/errors.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.Errors =
			exports2.ValueErrorIterator =
			exports2.EscapeKey =
			exports2.ValueErrorsUnknownTypeError =
			exports2.ValueErrorType =
				void 0
		var guard_1 = require_guard()
		var system_1 = require_system()
		var deref_1 = require_deref()
		var hash_1 = require_hash2()
		var Types = require_typebox()
		var ValueErrorType2
		;(function (ValueErrorType3) {
			ValueErrorType3[(ValueErrorType3["ArrayContains"] = 0)] = "ArrayContains"
			ValueErrorType3[(ValueErrorType3["ArrayMaxContains"] = 1)] = "ArrayMaxContains"
			ValueErrorType3[(ValueErrorType3["ArrayMaxItems"] = 2)] = "ArrayMaxItems"
			ValueErrorType3[(ValueErrorType3["ArrayMinContains"] = 3)] = "ArrayMinContains"
			ValueErrorType3[(ValueErrorType3["ArrayMinItems"] = 4)] = "ArrayMinItems"
			ValueErrorType3[(ValueErrorType3["ArrayUniqueItems"] = 5)] = "ArrayUniqueItems"
			ValueErrorType3[(ValueErrorType3["Array"] = 6)] = "Array"
			ValueErrorType3[(ValueErrorType3["AsyncIterator"] = 7)] = "AsyncIterator"
			ValueErrorType3[(ValueErrorType3["BigIntExclusiveMaximum"] = 8)] = "BigIntExclusiveMaximum"
			ValueErrorType3[(ValueErrorType3["BigIntExclusiveMinimum"] = 9)] = "BigIntExclusiveMinimum"
			ValueErrorType3[(ValueErrorType3["BigIntMaximum"] = 10)] = "BigIntMaximum"
			ValueErrorType3[(ValueErrorType3["BigIntMinimum"] = 11)] = "BigIntMinimum"
			ValueErrorType3[(ValueErrorType3["BigIntMultipleOf"] = 12)] = "BigIntMultipleOf"
			ValueErrorType3[(ValueErrorType3["BigInt"] = 13)] = "BigInt"
			ValueErrorType3[(ValueErrorType3["Boolean"] = 14)] = "Boolean"
			ValueErrorType3[(ValueErrorType3["DateExclusiveMaximumTimestamp"] = 15)] =
				"DateExclusiveMaximumTimestamp"
			ValueErrorType3[(ValueErrorType3["DateExclusiveMinimumTimestamp"] = 16)] =
				"DateExclusiveMinimumTimestamp"
			ValueErrorType3[(ValueErrorType3["DateMaximumTimestamp"] = 17)] = "DateMaximumTimestamp"
			ValueErrorType3[(ValueErrorType3["DateMinimumTimestamp"] = 18)] = "DateMinimumTimestamp"
			ValueErrorType3[(ValueErrorType3["DateMultipleOfTimestamp"] = 19)] = "DateMultipleOfTimestamp"
			ValueErrorType3[(ValueErrorType3["Date"] = 20)] = "Date"
			ValueErrorType3[(ValueErrorType3["Function"] = 21)] = "Function"
			ValueErrorType3[(ValueErrorType3["IntegerExclusiveMaximum"] = 22)] = "IntegerExclusiveMaximum"
			ValueErrorType3[(ValueErrorType3["IntegerExclusiveMinimum"] = 23)] = "IntegerExclusiveMinimum"
			ValueErrorType3[(ValueErrorType3["IntegerMaximum"] = 24)] = "IntegerMaximum"
			ValueErrorType3[(ValueErrorType3["IntegerMinimum"] = 25)] = "IntegerMinimum"
			ValueErrorType3[(ValueErrorType3["IntegerMultipleOf"] = 26)] = "IntegerMultipleOf"
			ValueErrorType3[(ValueErrorType3["Integer"] = 27)] = "Integer"
			ValueErrorType3[(ValueErrorType3["IntersectUnevaluatedProperties"] = 28)] =
				"IntersectUnevaluatedProperties"
			ValueErrorType3[(ValueErrorType3["Intersect"] = 29)] = "Intersect"
			ValueErrorType3[(ValueErrorType3["Iterator"] = 30)] = "Iterator"
			ValueErrorType3[(ValueErrorType3["Kind"] = 31)] = "Kind"
			ValueErrorType3[(ValueErrorType3["Literal"] = 32)] = "Literal"
			ValueErrorType3[(ValueErrorType3["Never"] = 33)] = "Never"
			ValueErrorType3[(ValueErrorType3["Not"] = 34)] = "Not"
			ValueErrorType3[(ValueErrorType3["Null"] = 35)] = "Null"
			ValueErrorType3[(ValueErrorType3["NumberExclusiveMaximum"] = 36)] = "NumberExclusiveMaximum"
			ValueErrorType3[(ValueErrorType3["NumberExclusiveMinimum"] = 37)] = "NumberExclusiveMinimum"
			ValueErrorType3[(ValueErrorType3["NumberMaximum"] = 38)] = "NumberMaximum"
			ValueErrorType3[(ValueErrorType3["NumberMinimum"] = 39)] = "NumberMinimum"
			ValueErrorType3[(ValueErrorType3["NumberMultipleOf"] = 40)] = "NumberMultipleOf"
			ValueErrorType3[(ValueErrorType3["Number"] = 41)] = "Number"
			ValueErrorType3[(ValueErrorType3["ObjectAdditionalProperties"] = 42)] =
				"ObjectAdditionalProperties"
			ValueErrorType3[(ValueErrorType3["ObjectMaxProperties"] = 43)] = "ObjectMaxProperties"
			ValueErrorType3[(ValueErrorType3["ObjectMinProperties"] = 44)] = "ObjectMinProperties"
			ValueErrorType3[(ValueErrorType3["ObjectRequiredProperty"] = 45)] = "ObjectRequiredProperty"
			ValueErrorType3[(ValueErrorType3["Object"] = 46)] = "Object"
			ValueErrorType3[(ValueErrorType3["Promise"] = 47)] = "Promise"
			ValueErrorType3[(ValueErrorType3["StringFormatUnknown"] = 48)] = "StringFormatUnknown"
			ValueErrorType3[(ValueErrorType3["StringFormat"] = 49)] = "StringFormat"
			ValueErrorType3[(ValueErrorType3["StringMaxLength"] = 50)] = "StringMaxLength"
			ValueErrorType3[(ValueErrorType3["StringMinLength"] = 51)] = "StringMinLength"
			ValueErrorType3[(ValueErrorType3["StringPattern"] = 52)] = "StringPattern"
			ValueErrorType3[(ValueErrorType3["String"] = 53)] = "String"
			ValueErrorType3[(ValueErrorType3["Symbol"] = 54)] = "Symbol"
			ValueErrorType3[(ValueErrorType3["TupleLength"] = 55)] = "TupleLength"
			ValueErrorType3[(ValueErrorType3["Tuple"] = 56)] = "Tuple"
			ValueErrorType3[(ValueErrorType3["Uint8ArrayMaxByteLength"] = 57)] = "Uint8ArrayMaxByteLength"
			ValueErrorType3[(ValueErrorType3["Uint8ArrayMinByteLength"] = 58)] = "Uint8ArrayMinByteLength"
			ValueErrorType3[(ValueErrorType3["Uint8Array"] = 59)] = "Uint8Array"
			ValueErrorType3[(ValueErrorType3["Undefined"] = 60)] = "Undefined"
			ValueErrorType3[(ValueErrorType3["Union"] = 61)] = "Union"
			ValueErrorType3[(ValueErrorType3["Void"] = 62)] = "Void"
		})(ValueErrorType2 || (exports2.ValueErrorType = ValueErrorType2 = {}))
		var ValueErrorsUnknownTypeError = class extends Types.TypeBoxError {
			constructor(schema2) {
				super("Unknown type")
				this.schema = schema2
			}
		}
		exports2.ValueErrorsUnknownTypeError = ValueErrorsUnknownTypeError
		function EscapeKey(key) {
			return key.replace(/~/g, "~0").replace(/\//g, "~1")
		}
		exports2.EscapeKey = EscapeKey
		function IsDefined(value) {
			return value !== void 0
		}
		var ValueErrorIterator = class {
			constructor(iterator) {
				this.iterator = iterator
			}
			[Symbol.iterator]() {
				return this.iterator
			}
			/** Returns the first value error or undefined if no errors */
			First() {
				const next = this.iterator.next()
				return next.done ? void 0 : next.value
			}
		}
		exports2.ValueErrorIterator = ValueErrorIterator
		function Create(type, schema2, path, value) {
			return {
				type,
				schema: schema2,
				path,
				value,
				message: system_1.TypeSystemErrorFunction.Get()(schema2, type),
			}
		}
		function* TAny(schema2, references, path, value) {}
		function* TArray(schema2, references, path, value) {
			if (!(0, guard_1.IsArray)(value)) {
				return yield Create(ValueErrorType2.Array, schema2, path, value)
			}
			if (IsDefined(schema2.minItems) && !(value.length >= schema2.minItems)) {
				yield Create(ValueErrorType2.ArrayMinItems, schema2, path, value)
			}
			if (IsDefined(schema2.maxItems) && !(value.length <= schema2.maxItems)) {
				yield Create(ValueErrorType2.ArrayMaxItems, schema2, path, value)
			}
			for (const [i, element] of value.entries()) {
				yield* Visit(schema2.items, references, `${path}/${i}`, element)
			}
			if (
				schema2.uniqueItems === true &&
				!(function () {
					const set = /* @__PURE__ */ new Set()
					for (const element of value) {
						const hashed = (0, hash_1.Hash)(element)
						if (set.has(hashed)) {
							return false
						} else {
							set.add(hashed)
						}
					}
					return true
				})()
			) {
				yield Create(ValueErrorType2.ArrayUniqueItems, schema2, path, value)
			}
			if (
				!(
					IsDefined(schema2.contains) ||
					IsDefined(schema2.minContains) ||
					IsDefined(schema2.maxContains)
				)
			) {
				return
			}
			const containsSchema = IsDefined(schema2.contains) ? schema2.contains : Types.Type.Never()
			const containsCount = value.reduce(
				(acc, value2, index2) =>
					Visit(containsSchema, references, `${path}${index2}`, value2).next().done === true
						? acc + 1
						: acc,
				0
			)
			if (containsCount === 0) {
				yield Create(ValueErrorType2.ArrayContains, schema2, path, value)
			}
			if ((0, guard_1.IsNumber)(schema2.minContains) && containsCount < schema2.minContains) {
				yield Create(ValueErrorType2.ArrayMinContains, schema2, path, value)
			}
			if ((0, guard_1.IsNumber)(schema2.maxContains) && containsCount > schema2.maxContains) {
				yield Create(ValueErrorType2.ArrayMaxContains, schema2, path, value)
			}
		}
		function* TAsyncIterator(schema2, references, path, value) {
			if (!(0, guard_1.IsAsyncIterator)(value))
				yield Create(ValueErrorType2.AsyncIterator, schema2, path, value)
		}
		function* TBigInt(schema2, references, path, value) {
			if (!(0, guard_1.IsBigInt)(value))
				return yield Create(ValueErrorType2.BigInt, schema2, path, value)
			if (IsDefined(schema2.exclusiveMaximum) && !(value < schema2.exclusiveMaximum)) {
				yield Create(ValueErrorType2.BigIntExclusiveMaximum, schema2, path, value)
			}
			if (IsDefined(schema2.exclusiveMinimum) && !(value > schema2.exclusiveMinimum)) {
				yield Create(ValueErrorType2.BigIntExclusiveMinimum, schema2, path, value)
			}
			if (IsDefined(schema2.maximum) && !(value <= schema2.maximum)) {
				yield Create(ValueErrorType2.BigIntMaximum, schema2, path, value)
			}
			if (IsDefined(schema2.minimum) && !(value >= schema2.minimum)) {
				yield Create(ValueErrorType2.BigIntMinimum, schema2, path, value)
			}
			if (IsDefined(schema2.multipleOf) && !(value % schema2.multipleOf === BigInt(0))) {
				yield Create(ValueErrorType2.BigIntMultipleOf, schema2, path, value)
			}
		}
		function* TBoolean(schema2, references, path, value) {
			if (!(0, guard_1.IsBoolean)(value))
				yield Create(ValueErrorType2.Boolean, schema2, path, value)
		}
		function* TConstructor(schema2, references, path, value) {
			yield* Visit(schema2.returns, references, path, value.prototype)
		}
		function* TDate(schema2, references, path, value) {
			if (!(0, guard_1.IsDate)(value))
				return yield Create(ValueErrorType2.Date, schema2, path, value)
			if (
				IsDefined(schema2.exclusiveMaximumTimestamp) &&
				!(value.getTime() < schema2.exclusiveMaximumTimestamp)
			) {
				yield Create(ValueErrorType2.DateExclusiveMaximumTimestamp, schema2, path, value)
			}
			if (
				IsDefined(schema2.exclusiveMinimumTimestamp) &&
				!(value.getTime() > schema2.exclusiveMinimumTimestamp)
			) {
				yield Create(ValueErrorType2.DateExclusiveMinimumTimestamp, schema2, path, value)
			}
			if (IsDefined(schema2.maximumTimestamp) && !(value.getTime() <= schema2.maximumTimestamp)) {
				yield Create(ValueErrorType2.DateMaximumTimestamp, schema2, path, value)
			}
			if (IsDefined(schema2.minimumTimestamp) && !(value.getTime() >= schema2.minimumTimestamp)) {
				yield Create(ValueErrorType2.DateMinimumTimestamp, schema2, path, value)
			}
			if (
				IsDefined(schema2.multipleOfTimestamp) &&
				!(value.getTime() % schema2.multipleOfTimestamp === 0)
			) {
				yield Create(ValueErrorType2.DateMultipleOfTimestamp, schema2, path, value)
			}
		}
		function* TFunction(schema2, references, path, value) {
			if (!(0, guard_1.IsFunction)(value))
				yield Create(ValueErrorType2.Function, schema2, path, value)
		}
		function* TInteger(schema2, references, path, value) {
			if (!(0, guard_1.IsInteger)(value))
				return yield Create(ValueErrorType2.Integer, schema2, path, value)
			if (IsDefined(schema2.exclusiveMaximum) && !(value < schema2.exclusiveMaximum)) {
				yield Create(ValueErrorType2.IntegerExclusiveMaximum, schema2, path, value)
			}
			if (IsDefined(schema2.exclusiveMinimum) && !(value > schema2.exclusiveMinimum)) {
				yield Create(ValueErrorType2.IntegerExclusiveMinimum, schema2, path, value)
			}
			if (IsDefined(schema2.maximum) && !(value <= schema2.maximum)) {
				yield Create(ValueErrorType2.IntegerMaximum, schema2, path, value)
			}
			if (IsDefined(schema2.minimum) && !(value >= schema2.minimum)) {
				yield Create(ValueErrorType2.IntegerMinimum, schema2, path, value)
			}
			if (IsDefined(schema2.multipleOf) && !(value % schema2.multipleOf === 0)) {
				yield Create(ValueErrorType2.IntegerMultipleOf, schema2, path, value)
			}
		}
		function* TIntersect(schema2, references, path, value) {
			for (const inner of schema2.allOf) {
				const next = Visit(inner, references, path, value).next()
				if (!next.done) {
					yield Create(ValueErrorType2.Intersect, schema2, path, value)
					yield next.value
				}
			}
			if (schema2.unevaluatedProperties === false) {
				const keyCheck = new RegExp(Types.KeyResolver.ResolvePattern(schema2))
				for (const valueKey of Object.getOwnPropertyNames(value)) {
					if (!keyCheck.test(valueKey)) {
						yield Create(
							ValueErrorType2.IntersectUnevaluatedProperties,
							schema2,
							`${path}/${valueKey}`,
							value
						)
					}
				}
			}
			if (typeof schema2.unevaluatedProperties === "object") {
				const keyCheck = new RegExp(Types.KeyResolver.ResolvePattern(schema2))
				for (const valueKey of Object.getOwnPropertyNames(value)) {
					if (!keyCheck.test(valueKey)) {
						const next = Visit(
							schema2.unevaluatedProperties,
							references,
							`${path}/${valueKey}`,
							value[valueKey]
						).next()
						if (!next.done) yield next.value
					}
				}
			}
		}
		function* TIterator(schema2, references, path, value) {
			if (!(0, guard_1.IsIterator)(value))
				yield Create(ValueErrorType2.Iterator, schema2, path, value)
		}
		function* TLiteral(schema2, references, path, value) {
			if (!(value === schema2.const)) yield Create(ValueErrorType2.Literal, schema2, path, value)
		}
		function* TNever(schema2, references, path, value) {
			yield Create(ValueErrorType2.Never, schema2, path, value)
		}
		function* TNot(schema2, references, path, value) {
			if (Visit(schema2.not, references, path, value).next().done === true)
				yield Create(ValueErrorType2.Not, schema2, path, value)
		}
		function* TNull(schema2, references, path, value) {
			if (!(0, guard_1.IsNull)(value)) yield Create(ValueErrorType2.Null, schema2, path, value)
		}
		function* TNumber(schema2, references, path, value) {
			if (!system_1.TypeSystemPolicy.IsNumberLike(value))
				return yield Create(ValueErrorType2.Number, schema2, path, value)
			if (IsDefined(schema2.exclusiveMaximum) && !(value < schema2.exclusiveMaximum)) {
				yield Create(ValueErrorType2.NumberExclusiveMaximum, schema2, path, value)
			}
			if (IsDefined(schema2.exclusiveMinimum) && !(value > schema2.exclusiveMinimum)) {
				yield Create(ValueErrorType2.NumberExclusiveMinimum, schema2, path, value)
			}
			if (IsDefined(schema2.maximum) && !(value <= schema2.maximum)) {
				yield Create(ValueErrorType2.NumberMaximum, schema2, path, value)
			}
			if (IsDefined(schema2.minimum) && !(value >= schema2.minimum)) {
				yield Create(ValueErrorType2.NumberMinimum, schema2, path, value)
			}
			if (IsDefined(schema2.multipleOf) && !(value % schema2.multipleOf === 0)) {
				yield Create(ValueErrorType2.NumberMultipleOf, schema2, path, value)
			}
		}
		function* TObject(schema2, references, path, value) {
			if (!system_1.TypeSystemPolicy.IsObjectLike(value))
				return yield Create(ValueErrorType2.Object, schema2, path, value)
			if (
				IsDefined(schema2.minProperties) &&
				!(Object.getOwnPropertyNames(value).length >= schema2.minProperties)
			) {
				yield Create(ValueErrorType2.ObjectMinProperties, schema2, path, value)
			}
			if (
				IsDefined(schema2.maxProperties) &&
				!(Object.getOwnPropertyNames(value).length <= schema2.maxProperties)
			) {
				yield Create(ValueErrorType2.ObjectMaxProperties, schema2, path, value)
			}
			const requiredKeys = Array.isArray(schema2.required) ? schema2.required : []
			const knownKeys = Object.getOwnPropertyNames(schema2.properties)
			const unknownKeys = Object.getOwnPropertyNames(value)
			for (const requiredKey of requiredKeys) {
				if (unknownKeys.includes(requiredKey)) continue
				yield Create(
					ValueErrorType2.ObjectRequiredProperty,
					schema2.properties[requiredKey],
					`${path}/${EscapeKey(requiredKey)}`,
					void 0
				)
			}
			if (schema2.additionalProperties === false) {
				for (const valueKey of unknownKeys) {
					if (!knownKeys.includes(valueKey)) {
						yield Create(
							ValueErrorType2.ObjectAdditionalProperties,
							schema2,
							`${path}/${EscapeKey(valueKey)}`,
							value[valueKey]
						)
					}
				}
			}
			if (typeof schema2.additionalProperties === "object") {
				for (const valueKey of unknownKeys) {
					if (knownKeys.includes(valueKey)) continue
					yield* Visit(
						schema2.additionalProperties,
						references,
						`${path}/${EscapeKey(valueKey)}`,
						value[valueKey]
					)
				}
			}
			for (const knownKey of knownKeys) {
				const property = schema2.properties[knownKey]
				if (schema2.required && schema2.required.includes(knownKey)) {
					yield* Visit(property, references, `${path}/${EscapeKey(knownKey)}`, value[knownKey])
					if (Types.ExtendsUndefined.Check(schema2) && !(knownKey in value)) {
						yield Create(
							ValueErrorType2.ObjectRequiredProperty,
							property,
							`${path}/${EscapeKey(knownKey)}`,
							void 0
						)
					}
				} else {
					if (system_1.TypeSystemPolicy.IsExactOptionalProperty(value, knownKey)) {
						yield* Visit(property, references, `${path}/${EscapeKey(knownKey)}`, value[knownKey])
					}
				}
			}
		}
		function* TPromise(schema2, references, path, value) {
			if (!(0, guard_1.IsPromise)(value))
				yield Create(ValueErrorType2.Promise, schema2, path, value)
		}
		function* TRecord(schema2, references, path, value) {
			if (!system_1.TypeSystemPolicy.IsRecordLike(value))
				return yield Create(ValueErrorType2.Object, schema2, path, value)
			if (
				IsDefined(schema2.minProperties) &&
				!(Object.getOwnPropertyNames(value).length >= schema2.minProperties)
			) {
				yield Create(ValueErrorType2.ObjectMinProperties, schema2, path, value)
			}
			if (
				IsDefined(schema2.maxProperties) &&
				!(Object.getOwnPropertyNames(value).length <= schema2.maxProperties)
			) {
				yield Create(ValueErrorType2.ObjectMaxProperties, schema2, path, value)
			}
			const [patternKey, patternSchema] = Object.entries(schema2.patternProperties)[0]
			const regex = new RegExp(patternKey)
			for (const [propertyKey, propertyValue] of Object.entries(value)) {
				if (regex.test(propertyKey))
					yield* Visit(
						patternSchema,
						references,
						`${path}/${EscapeKey(propertyKey)}`,
						propertyValue
					)
			}
			if (typeof schema2.additionalProperties === "object") {
				for (const [propertyKey, propertyValue] of Object.entries(value)) {
					if (!regex.test(propertyKey))
						yield* Visit(
							schema2.additionalProperties,
							references,
							`${path}/${EscapeKey(propertyKey)}`,
							propertyValue
						)
				}
			}
			if (schema2.additionalProperties === false) {
				for (const [propertyKey, propertyValue] of Object.entries(value)) {
					if (regex.test(propertyKey)) continue
					return yield Create(
						ValueErrorType2.ObjectAdditionalProperties,
						schema2,
						`${path}/${EscapeKey(propertyKey)}`,
						propertyValue
					)
				}
			}
		}
		function* TRef(schema2, references, path, value) {
			yield* Visit((0, deref_1.Deref)(schema2, references), references, path, value)
		}
		function* TString(schema2, references, path, value) {
			if (!(0, guard_1.IsString)(value))
				return yield Create(ValueErrorType2.String, schema2, path, value)
			if (IsDefined(schema2.minLength) && !(value.length >= schema2.minLength)) {
				yield Create(ValueErrorType2.StringMinLength, schema2, path, value)
			}
			if (IsDefined(schema2.maxLength) && !(value.length <= schema2.maxLength)) {
				yield Create(ValueErrorType2.StringMaxLength, schema2, path, value)
			}
			if ((0, guard_1.IsString)(schema2.pattern)) {
				const regex = new RegExp(schema2.pattern)
				if (!regex.test(value)) {
					yield Create(ValueErrorType2.StringPattern, schema2, path, value)
				}
			}
			if ((0, guard_1.IsString)(schema2.format)) {
				if (!Types.FormatRegistry.Has(schema2.format)) {
					yield Create(ValueErrorType2.StringFormatUnknown, schema2, path, value)
				} else {
					const format = Types.FormatRegistry.Get(schema2.format)
					if (!format(value)) {
						yield Create(ValueErrorType2.StringFormat, schema2, path, value)
					}
				}
			}
		}
		function* TSymbol(schema2, references, path, value) {
			if (!(0, guard_1.IsSymbol)(value)) yield Create(ValueErrorType2.Symbol, schema2, path, value)
		}
		function* TTemplateLiteral(schema2, references, path, value) {
			if (!(0, guard_1.IsString)(value))
				return yield Create(ValueErrorType2.String, schema2, path, value)
			const regex = new RegExp(schema2.pattern)
			if (!regex.test(value)) {
				yield Create(ValueErrorType2.StringPattern, schema2, path, value)
			}
		}
		function* TThis(schema2, references, path, value) {
			yield* Visit((0, deref_1.Deref)(schema2, references), references, path, value)
		}
		function* TTuple(schema2, references, path, value) {
			if (!(0, guard_1.IsArray)(value))
				return yield Create(ValueErrorType2.Tuple, schema2, path, value)
			if (schema2.items === void 0 && !(value.length === 0)) {
				return yield Create(ValueErrorType2.TupleLength, schema2, path, value)
			}
			if (!(value.length === schema2.maxItems)) {
				return yield Create(ValueErrorType2.TupleLength, schema2, path, value)
			}
			if (!schema2.items) {
				return
			}
			for (let i = 0; i < schema2.items.length; i++) {
				yield* Visit(schema2.items[i], references, `${path}/${i}`, value[i])
			}
		}
		function* TUndefined(schema2, references, path, value) {
			if (!(0, guard_1.IsUndefined)(value))
				yield Create(ValueErrorType2.Undefined, schema2, path, value)
		}
		function* TUnion(schema2, references, path, value) {
			let count = 0
			for (const subschema of schema2.anyOf) {
				const errors = [...Visit(subschema, references, path, value)]
				if (errors.length === 0) return
				count += errors.length
			}
			if (count > 0) {
				yield Create(ValueErrorType2.Union, schema2, path, value)
			}
		}
		function* TUint8Array(schema2, references, path, value) {
			if (!(0, guard_1.IsUint8Array)(value))
				return yield Create(ValueErrorType2.Uint8Array, schema2, path, value)
			if (IsDefined(schema2.maxByteLength) && !(value.length <= schema2.maxByteLength)) {
				yield Create(ValueErrorType2.Uint8ArrayMaxByteLength, schema2, path, value)
			}
			if (IsDefined(schema2.minByteLength) && !(value.length >= schema2.minByteLength)) {
				yield Create(ValueErrorType2.Uint8ArrayMinByteLength, schema2, path, value)
			}
		}
		function* TUnknown(schema2, references, path, value) {}
		function* TVoid(schema2, references, path, value) {
			if (!system_1.TypeSystemPolicy.IsVoidLike(value))
				yield Create(ValueErrorType2.Void, schema2, path, value)
		}
		function* TKind(schema2, references, path, value) {
			const check = Types.TypeRegistry.Get(schema2[Types.Kind])
			if (!check(schema2, value)) yield Create(ValueErrorType2.Kind, schema2, path, value)
		}
		function* Visit(schema2, references, path, value) {
			const references_ = IsDefined(schema2.$id) ? [...references, schema2] : references
			const schema_ = schema2
			switch (schema_[Types.Kind]) {
				case "Any":
					return yield* TAny(schema_, references_, path, value)
				case "Array":
					return yield* TArray(schema_, references_, path, value)
				case "AsyncIterator":
					return yield* TAsyncIterator(schema_, references_, path, value)
				case "BigInt":
					return yield* TBigInt(schema_, references_, path, value)
				case "Boolean":
					return yield* TBoolean(schema_, references_, path, value)
				case "Constructor":
					return yield* TConstructor(schema_, references_, path, value)
				case "Date":
					return yield* TDate(schema_, references_, path, value)
				case "Function":
					return yield* TFunction(schema_, references_, path, value)
				case "Integer":
					return yield* TInteger(schema_, references_, path, value)
				case "Intersect":
					return yield* TIntersect(schema_, references_, path, value)
				case "Iterator":
					return yield* TIterator(schema_, references_, path, value)
				case "Literal":
					return yield* TLiteral(schema_, references_, path, value)
				case "Never":
					return yield* TNever(schema_, references_, path, value)
				case "Not":
					return yield* TNot(schema_, references_, path, value)
				case "Null":
					return yield* TNull(schema_, references_, path, value)
				case "Number":
					return yield* TNumber(schema_, references_, path, value)
				case "Object":
					return yield* TObject(schema_, references_, path, value)
				case "Promise":
					return yield* TPromise(schema_, references_, path, value)
				case "Record":
					return yield* TRecord(schema_, references_, path, value)
				case "Ref":
					return yield* TRef(schema_, references_, path, value)
				case "String":
					return yield* TString(schema_, references_, path, value)
				case "Symbol":
					return yield* TSymbol(schema_, references_, path, value)
				case "TemplateLiteral":
					return yield* TTemplateLiteral(schema_, references_, path, value)
				case "This":
					return yield* TThis(schema_, references_, path, value)
				case "Tuple":
					return yield* TTuple(schema_, references_, path, value)
				case "Undefined":
					return yield* TUndefined(schema_, references_, path, value)
				case "Union":
					return yield* TUnion(schema_, references_, path, value)
				case "Uint8Array":
					return yield* TUint8Array(schema_, references_, path, value)
				case "Unknown":
					return yield* TUnknown(schema_, references_, path, value)
				case "Void":
					return yield* TVoid(schema_, references_, path, value)
				default:
					if (!Types.TypeRegistry.Has(schema_[Types.Kind]))
						throw new ValueErrorsUnknownTypeError(schema2)
					return yield* TKind(schema_, references_, path, value)
			}
		}
		function Errors2(...args) {
			const iterator =
				args.length === 3 ? Visit(args[0], args[1], "", args[2]) : Visit(args[0], [], "", args[1])
			return new ValueErrorIterator(iterator)
		}
		exports2.Errors = Errors2
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/errors/index.js
var require_errors3 = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/errors/index.js"(
		exports2
	) {
		"use strict"
		var __createBinding =
			(exports2 && exports2.__createBinding) ||
			(Object.create
				? function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						var desc = Object.getOwnPropertyDescriptor(m, k)
						if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
							desc = {
								enumerable: true,
								get: function () {
									return m[k]
								},
							}
						}
						Object.defineProperty(o, k2, desc)
				  }
				: function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						o[k2] = m[k]
				  })
		var __exportStar =
			(exports2 && exports2.__exportStar) ||
			function (m, exports3) {
				for (var p in m)
					if (p !== "default" && !Object.prototype.hasOwnProperty.call(exports3, p))
						__createBinding(exports3, m, p)
			}
		Object.defineProperty(exports2, "__esModule", { value: true })
		__exportStar(require_errors2(), exports2)
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/pointer.js
var require_pointer = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/pointer.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.ValuePointer =
			exports2.ValuePointerRootDeleteError =
			exports2.ValuePointerRootSetError =
				void 0
		var ValuePointerRootSetError = class extends Error {
			constructor(value, path, update) {
				super("Cannot set root value")
				this.value = value
				this.path = path
				this.update = update
			}
		}
		exports2.ValuePointerRootSetError = ValuePointerRootSetError
		var ValuePointerRootDeleteError = class extends Error {
			constructor(value, path) {
				super("Cannot delete root value")
				this.value = value
				this.path = path
			}
		}
		exports2.ValuePointerRootDeleteError = ValuePointerRootDeleteError
		var ValuePointer
		;(function (ValuePointer2) {
			function Escape(component) {
				return !component.includes("~")
					? component
					: component.replace(/~1/g, "/").replace(/~0/g, "~")
			}
			function* Format(pointer) {
				if (pointer === "") return
				let [start, end] = [0, 0]
				for (let i = 0; i < pointer.length; i++) {
					const char = pointer.charAt(i)
					if (char === "/") {
						if (i === 0) {
							start = i + 1
						} else {
							end = i
							yield Escape(pointer.slice(start, end))
							start = i + 1
						}
					} else {
						end = i
					}
				}
				yield Escape(pointer.slice(start))
			}
			ValuePointer2.Format = Format
			function Set2(value, pointer, update) {
				if (pointer === "") throw new ValuePointerRootSetError(value, pointer, update)
				let [owner, next, key] = [null, value, ""]
				for (const component of Format(pointer)) {
					if (next[component] === void 0) next[component] = {}
					owner = next
					next = next[component]
					key = component
				}
				owner[key] = update
			}
			ValuePointer2.Set = Set2
			function Delete(value, pointer) {
				if (pointer === "") throw new ValuePointerRootDeleteError(value, pointer)
				let [owner, next, key] = [null, value, ""]
				for (const component of Format(pointer)) {
					if (next[component] === void 0 || next[component] === null) return
					owner = next
					next = next[component]
					key = component
				}
				if (Array.isArray(owner)) {
					const index2 = parseInt(key)
					owner.splice(index2, 1)
				} else {
					delete owner[key]
				}
			}
			ValuePointer2.Delete = Delete
			function Has(value, pointer) {
				if (pointer === "") return true
				let [owner, next, key] = [null, value, ""]
				for (const component of Format(pointer)) {
					if (next[component] === void 0) return false
					owner = next
					next = next[component]
					key = component
				}
				return Object.getOwnPropertyNames(owner).includes(key)
			}
			ValuePointer2.Has = Has
			function Get(value, pointer) {
				if (pointer === "") return value
				let current = value
				for (const component of Format(pointer)) {
					if (current[component] === void 0) return void 0
					current = current[component]
				}
				return current
			}
			ValuePointer2.Get = Get
		})(ValuePointer || (exports2.ValuePointer = ValuePointer = {}))
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/clone.js
var require_clone = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/clone.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.Clone = void 0
		var guard_1 = require_guard()
		function ObjectType(value) {
			const keys = [...Object.getOwnPropertyNames(value), ...Object.getOwnPropertySymbols(value)]
			return Object.fromEntries(keys.map((key) => [key, Clone(value[key])]))
		}
		function ArrayType(value) {
			return value.map((element) => Clone(element))
		}
		function TypedArrayType(value) {
			return [...value]
		}
		function DateType(value) {
			return new Date(value.toISOString())
		}
		function ValueType(value) {
			return value
		}
		function Clone(value) {
			if ((0, guard_1.IsArray)(value)) return ArrayType(value)
			if ((0, guard_1.IsDate)(value)) return DateType(value)
			if ((0, guard_1.IsPlainObject)(value)) return ObjectType(value)
			if ((0, guard_1.IsTypedArray)(value)) return TypedArrayType(value)
			if ((0, guard_1.IsValueType)(value)) return ValueType(value)
			throw new Error("ValueClone: Unable to clone value")
		}
		exports2.Clone = Clone
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/delta.js
var require_delta = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/delta.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.Patch =
			exports2.Diff =
			exports2.ValueDeltaUnableToDiffUnknownValue =
			exports2.ValueDeltaObjectWithSymbolKeyError =
			exports2.Edit =
			exports2.Delete =
			exports2.Update =
			exports2.Insert =
				void 0
		var guard_1 = require_guard()
		var typebox_1 = require_typebox()
		var pointer_1 = require_pointer()
		var clone_1 = require_clone()
		exports2.Insert = typebox_1.Type.Object({
			type: typebox_1.Type.Literal("insert"),
			path: typebox_1.Type.String(),
			value: typebox_1.Type.Unknown(),
		})
		exports2.Update = typebox_1.Type.Object({
			type: typebox_1.Type.Literal("update"),
			path: typebox_1.Type.String(),
			value: typebox_1.Type.Unknown(),
		})
		exports2.Delete = typebox_1.Type.Object({
			type: typebox_1.Type.Literal("delete"),
			path: typebox_1.Type.String(),
		})
		exports2.Edit = typebox_1.Type.Union([exports2.Insert, exports2.Update, exports2.Delete])
		var ValueDeltaObjectWithSymbolKeyError = class extends Error {
			constructor(key) {
				super("Cannot diff objects with symbol keys")
				this.key = key
			}
		}
		exports2.ValueDeltaObjectWithSymbolKeyError = ValueDeltaObjectWithSymbolKeyError
		var ValueDeltaUnableToDiffUnknownValue = class extends Error {
			constructor(value) {
				super("Unable to create diff edits for unknown value")
				this.value = value
			}
		}
		exports2.ValueDeltaUnableToDiffUnknownValue = ValueDeltaUnableToDiffUnknownValue
		function CreateUpdate(path, value) {
			return { type: "update", path, value }
		}
		function CreateInsert(path, value) {
			return { type: "insert", path, value }
		}
		function CreateDelete(path) {
			return { type: "delete", path }
		}
		function* ObjectType(path, current, next) {
			if (!(0, guard_1.IsPlainObject)(next)) return yield CreateUpdate(path, next)
			const currentKeys = [...Object.keys(current), ...Object.getOwnPropertySymbols(current)]
			const nextKeys = [...Object.keys(next), ...Object.getOwnPropertySymbols(next)]
			for (const key of currentKeys) {
				if ((0, guard_1.IsSymbol)(key)) throw new ValueDeltaObjectWithSymbolKeyError(key)
				if ((0, guard_1.IsUndefined)(next[key]) && nextKeys.includes(key))
					yield CreateUpdate(`${path}/${String(key)}`, void 0)
			}
			for (const key of nextKeys) {
				if ((0, guard_1.IsUndefined)(current[key]) || (0, guard_1.IsUndefined)(next[key])) continue
				if ((0, guard_1.IsSymbol)(key)) throw new ValueDeltaObjectWithSymbolKeyError(key)
				yield* Visit(`${path}/${String(key)}`, current[key], next[key])
			}
			for (const key of nextKeys) {
				if ((0, guard_1.IsSymbol)(key)) throw new ValueDeltaObjectWithSymbolKeyError(key)
				if ((0, guard_1.IsUndefined)(current[key]))
					yield CreateInsert(`${path}/${String(key)}`, next[key])
			}
			for (const key of currentKeys.reverse()) {
				if ((0, guard_1.IsSymbol)(key)) throw new ValueDeltaObjectWithSymbolKeyError(key)
				if ((0, guard_1.IsUndefined)(next[key]) && !nextKeys.includes(key))
					yield CreateDelete(`${path}/${String(key)}`)
			}
		}
		function* ArrayType(path, current, next) {
			if (!(0, guard_1.IsArray)(next)) return yield CreateUpdate(path, next)
			for (let i = 0; i < Math.min(current.length, next.length); i++) {
				yield* Visit(`${path}/${i}`, current[i], next[i])
			}
			for (const [i, element] of next.entries()) {
				if (i < current.length) continue
				yield CreateInsert(`${path}/${i}`, element)
			}
			for (let i = current.length - 1; i >= 0; i--) {
				if (i < next.length) continue
				yield CreateDelete(`${path}/${i}`)
			}
		}
		function* TypedArrayType(path, current, next) {
			if (
				!(0, guard_1.IsTypedArray)(next) ||
				current.length !== next.length ||
				Object.getPrototypeOf(current).constructor.name !==
					Object.getPrototypeOf(next).constructor.name
			)
				return yield CreateUpdate(path, next)
			for (let i = 0; i < Math.min(current.length, next.length); i++) {
				yield* Visit(`${path}/${i}`, current[i], next[i])
			}
		}
		function* ValueType(path, current, next) {
			if (current === next) return
			yield CreateUpdate(path, next)
		}
		function* Visit(path, current, next) {
			if ((0, guard_1.IsPlainObject)(current)) return yield* ObjectType(path, current, next)
			if ((0, guard_1.IsArray)(current)) return yield* ArrayType(path, current, next)
			if ((0, guard_1.IsTypedArray)(current)) return yield* TypedArrayType(path, current, next)
			if ((0, guard_1.IsValueType)(current)) return yield* ValueType(path, current, next)
			throw new ValueDeltaUnableToDiffUnknownValue(current)
		}
		function Diff(current, next) {
			return [...Visit("", current, next)]
		}
		exports2.Diff = Diff
		function IsRootUpdate(edits) {
			return edits.length > 0 && edits[0].path === "" && edits[0].type === "update"
		}
		function IsIdentity(edits) {
			return edits.length === 0
		}
		function Patch(current, edits) {
			if (IsRootUpdate(edits)) {
				return (0, clone_1.Clone)(edits[0].value)
			}
			if (IsIdentity(edits)) {
				return (0, clone_1.Clone)(current)
			}
			const clone2 = (0, clone_1.Clone)(current)
			for (const edit of edits) {
				switch (edit.type) {
					case "insert": {
						pointer_1.ValuePointer.Set(clone2, edit.path, edit.value)
						break
					}
					case "update": {
						pointer_1.ValuePointer.Set(clone2, edit.path, edit.value)
						break
					}
					case "delete": {
						pointer_1.ValuePointer.Delete(clone2, edit.path)
						break
					}
				}
			}
			return clone2
		}
		exports2.Patch = Patch
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/mutate.js
var require_mutate = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/mutate.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.Mutate =
			exports2.ValueMutateInvalidRootMutationError =
			exports2.ValueMutateTypeMismatchError =
				void 0
		var guard_1 = require_guard()
		var pointer_1 = require_pointer()
		var clone_1 = require_clone()
		var ValueMutateTypeMismatchError = class extends Error {
			constructor() {
				super("Cannot assign due type mismatch of assignable values")
			}
		}
		exports2.ValueMutateTypeMismatchError = ValueMutateTypeMismatchError
		var ValueMutateInvalidRootMutationError = class extends Error {
			constructor() {
				super("Only object and array types can be mutated at the root level")
			}
		}
		exports2.ValueMutateInvalidRootMutationError = ValueMutateInvalidRootMutationError
		function ObjectType(root, path, current, next) {
			if (!(0, guard_1.IsPlainObject)(current)) {
				pointer_1.ValuePointer.Set(root, path, (0, clone_1.Clone)(next))
			} else {
				const currentKeys = Object.keys(current)
				const nextKeys = Object.keys(next)
				for (const currentKey of currentKeys) {
					if (!nextKeys.includes(currentKey)) {
						delete current[currentKey]
					}
				}
				for (const nextKey of nextKeys) {
					if (!currentKeys.includes(nextKey)) {
						current[nextKey] = null
					}
				}
				for (const nextKey of nextKeys) {
					Visit(root, `${path}/${nextKey}`, current[nextKey], next[nextKey])
				}
			}
		}
		function ArrayType(root, path, current, next) {
			if (!(0, guard_1.IsArray)(current)) {
				pointer_1.ValuePointer.Set(root, path, (0, clone_1.Clone)(next))
			} else {
				for (const [index2, element] of next.entries()) {
					Visit(root, `${path}/${index2}`, current[index2], element)
				}
				current.splice(next.length)
			}
		}
		function TypedArrayType(root, path, current, next) {
			if ((0, guard_1.IsTypedArray)(current) && current.length === next.length) {
				for (let i = 0; i < current.length; i++) {
					current[i] = next[i]
				}
			} else {
				pointer_1.ValuePointer.Set(root, path, (0, clone_1.Clone)(next))
			}
		}
		function ValueType(root, path, current, next) {
			if (current === next) return
			pointer_1.ValuePointer.Set(root, path, next)
		}
		function Visit(root, path, current, next) {
			if ((0, guard_1.IsArray)(next)) return ArrayType(root, path, current, next)
			if ((0, guard_1.IsTypedArray)(next)) return TypedArrayType(root, path, current, next)
			if ((0, guard_1.IsPlainObject)(next)) return ObjectType(root, path, current, next)
			if ((0, guard_1.IsValueType)(next)) return ValueType(root, path, current, next)
		}
		function IsNonMutableValue(value) {
			return (0, guard_1.IsTypedArray)(value) || (0, guard_1.IsValueType)(value)
		}
		function IsMismatchedValue(current, next) {
			return (
				((0, guard_1.IsPlainObject)(current) && (0, guard_1.IsArray)(next)) ||
				((0, guard_1.IsArray)(current) && (0, guard_1.IsPlainObject)(next))
			)
		}
		function Mutate(current, next) {
			if (IsNonMutableValue(current) || IsNonMutableValue(next))
				throw new ValueMutateInvalidRootMutationError()
			if (IsMismatchedValue(current, next)) throw new ValueMutateTypeMismatchError()
			Visit(current, "", current, next)
		}
		exports2.Mutate = Mutate
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/equal.js
var require_equal = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/equal.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.Equal = void 0
		var guard_1 = require_guard()
		function ObjectType(left, right) {
			if (!(0, guard_1.IsPlainObject)(right)) return false
			const leftKeys = [...Object.keys(left), ...Object.getOwnPropertySymbols(left)]
			const rightKeys = [...Object.keys(right), ...Object.getOwnPropertySymbols(right)]
			if (leftKeys.length !== rightKeys.length) return false
			return leftKeys.every((key) => Equal(left[key], right[key]))
		}
		function DateType(left, right) {
			return (0, guard_1.IsDate)(right) && left.getTime() === right.getTime()
		}
		function ArrayType(left, right) {
			if (!(0, guard_1.IsArray)(right) || left.length !== right.length) return false
			return left.every((value, index2) => Equal(value, right[index2]))
		}
		function TypedArrayType(left, right) {
			if (
				!(0, guard_1.IsTypedArray)(right) ||
				left.length !== right.length ||
				Object.getPrototypeOf(left).constructor.name !==
					Object.getPrototypeOf(right).constructor.name
			)
				return false
			return left.every((value, index2) => Equal(value, right[index2]))
		}
		function ValueType(left, right) {
			return left === right
		}
		function Equal(left, right) {
			if ((0, guard_1.IsPlainObject)(left)) return ObjectType(left, right)
			if ((0, guard_1.IsDate)(left)) return DateType(left, right)
			if ((0, guard_1.IsTypedArray)(left)) return TypedArrayType(left, right)
			if ((0, guard_1.IsArray)(left)) return ArrayType(left, right)
			if ((0, guard_1.IsValueType)(left)) return ValueType(left, right)
			throw new Error("ValueEquals: Unable to compare value")
		}
		exports2.Equal = Equal
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/system/index.js
var require_system2 = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/system/index.js"(
		exports2
	) {
		"use strict"
		var __createBinding =
			(exports2 && exports2.__createBinding) ||
			(Object.create
				? function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						var desc = Object.getOwnPropertyDescriptor(m, k)
						if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
							desc = {
								enumerable: true,
								get: function () {
									return m[k]
								},
							}
						}
						Object.defineProperty(o, k2, desc)
				  }
				: function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						o[k2] = m[k]
				  })
		var __exportStar =
			(exports2 && exports2.__exportStar) ||
			function (m, exports3) {
				for (var p in m)
					if (p !== "default" && !Object.prototype.hasOwnProperty.call(exports3, p))
						__createBinding(exports3, m, p)
			}
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.ValueErrorType = void 0
		var errors_1 = require_errors2()
		Object.defineProperty(exports2, "ValueErrorType", {
			enumerable: true,
			get: function () {
				return errors_1.ValueErrorType
			},
		})
		__exportStar(require_system(), exports2)
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/check.js
var require_check = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/check.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.Check = exports2.ValueCheckUnknownTypeError = void 0
		var guard_1 = require_guard()
		var index_1 = require_system2()
		var deref_1 = require_deref()
		var hash_1 = require_hash2()
		var Types = require_typebox()
		var ValueCheckUnknownTypeError = class extends Types.TypeBoxError {
			constructor(schema2) {
				super(`Unknown type`)
				this.schema = schema2
			}
		}
		exports2.ValueCheckUnknownTypeError = ValueCheckUnknownTypeError
		function IsAnyOrUnknown(schema2) {
			return schema2[Types.Kind] === "Any" || schema2[Types.Kind] === "Unknown"
		}
		function IsDefined(value) {
			return value !== void 0
		}
		function TAny(schema2, references, value) {
			return true
		}
		function TArray(schema2, references, value) {
			if (!(0, guard_1.IsArray)(value)) return false
			if (IsDefined(schema2.minItems) && !(value.length >= schema2.minItems)) {
				return false
			}
			if (IsDefined(schema2.maxItems) && !(value.length <= schema2.maxItems)) {
				return false
			}
			if (!value.every((value2) => Visit(schema2.items, references, value2))) {
				return false
			}
			if (
				schema2.uniqueItems === true &&
				!(function () {
					const set = /* @__PURE__ */ new Set()
					for (const element of value) {
						const hashed = (0, hash_1.Hash)(element)
						if (set.has(hashed)) {
							return false
						} else {
							set.add(hashed)
						}
					}
					return true
				})()
			) {
				return false
			}
			if (
				!(
					IsDefined(schema2.contains) ||
					(0, guard_1.IsNumber)(schema2.minContains) ||
					(0, guard_1.IsNumber)(schema2.maxContains)
				)
			) {
				return true
			}
			const containsSchema = IsDefined(schema2.contains) ? schema2.contains : Types.Type.Never()
			const containsCount = value.reduce(
				(acc, value2) => (Visit(containsSchema, references, value2) ? acc + 1 : acc),
				0
			)
			if (containsCount === 0) {
				return false
			}
			if ((0, guard_1.IsNumber)(schema2.minContains) && containsCount < schema2.minContains) {
				return false
			}
			if ((0, guard_1.IsNumber)(schema2.maxContains) && containsCount > schema2.maxContains) {
				return false
			}
			return true
		}
		function TAsyncIterator(schema2, references, value) {
			return (0, guard_1.IsAsyncIterator)(value)
		}
		function TBigInt(schema2, references, value) {
			if (!(0, guard_1.IsBigInt)(value)) return false
			if (IsDefined(schema2.exclusiveMaximum) && !(value < schema2.exclusiveMaximum)) {
				return false
			}
			if (IsDefined(schema2.exclusiveMinimum) && !(value > schema2.exclusiveMinimum)) {
				return false
			}
			if (IsDefined(schema2.maximum) && !(value <= schema2.maximum)) {
				return false
			}
			if (IsDefined(schema2.minimum) && !(value >= schema2.minimum)) {
				return false
			}
			if (IsDefined(schema2.multipleOf) && !(value % schema2.multipleOf === BigInt(0))) {
				return false
			}
			return true
		}
		function TBoolean(schema2, references, value) {
			return (0, guard_1.IsBoolean)(value)
		}
		function TConstructor(schema2, references, value) {
			return Visit(schema2.returns, references, value.prototype)
		}
		function TDate(schema2, references, value) {
			if (!(0, guard_1.IsDate)(value)) return false
			if (
				IsDefined(schema2.exclusiveMaximumTimestamp) &&
				!(value.getTime() < schema2.exclusiveMaximumTimestamp)
			) {
				return false
			}
			if (
				IsDefined(schema2.exclusiveMinimumTimestamp) &&
				!(value.getTime() > schema2.exclusiveMinimumTimestamp)
			) {
				return false
			}
			if (IsDefined(schema2.maximumTimestamp) && !(value.getTime() <= schema2.maximumTimestamp)) {
				return false
			}
			if (IsDefined(schema2.minimumTimestamp) && !(value.getTime() >= schema2.minimumTimestamp)) {
				return false
			}
			if (
				IsDefined(schema2.multipleOfTimestamp) &&
				!(value.getTime() % schema2.multipleOfTimestamp === 0)
			) {
				return false
			}
			return true
		}
		function TFunction(schema2, references, value) {
			return (0, guard_1.IsFunction)(value)
		}
		function TInteger(schema2, references, value) {
			if (!(0, guard_1.IsInteger)(value)) {
				return false
			}
			if (IsDefined(schema2.exclusiveMaximum) && !(value < schema2.exclusiveMaximum)) {
				return false
			}
			if (IsDefined(schema2.exclusiveMinimum) && !(value > schema2.exclusiveMinimum)) {
				return false
			}
			if (IsDefined(schema2.maximum) && !(value <= schema2.maximum)) {
				return false
			}
			if (IsDefined(schema2.minimum) && !(value >= schema2.minimum)) {
				return false
			}
			if (IsDefined(schema2.multipleOf) && !(value % schema2.multipleOf === 0)) {
				return false
			}
			return true
		}
		function TIntersect(schema2, references, value) {
			const check1 = schema2.allOf.every((schema3) => Visit(schema3, references, value))
			if (schema2.unevaluatedProperties === false) {
				const keyPattern = new RegExp(Types.KeyResolver.ResolvePattern(schema2))
				const check2 = Object.getOwnPropertyNames(value).every((key) => keyPattern.test(key))
				return check1 && check2
			} else if (Types.TypeGuard.TSchema(schema2.unevaluatedProperties)) {
				const keyCheck = new RegExp(Types.KeyResolver.ResolvePattern(schema2))
				const check2 = Object.getOwnPropertyNames(value).every(
					(key) =>
						keyCheck.test(key) || Visit(schema2.unevaluatedProperties, references, value[key])
				)
				return check1 && check2
			} else {
				return check1
			}
		}
		function TIterator(schema2, references, value) {
			return (0, guard_1.IsIterator)(value)
		}
		function TLiteral(schema2, references, value) {
			return value === schema2.const
		}
		function TNever(schema2, references, value) {
			return false
		}
		function TNot(schema2, references, value) {
			return !Visit(schema2.not, references, value)
		}
		function TNull(schema2, references, value) {
			return (0, guard_1.IsNull)(value)
		}
		function TNumber(schema2, references, value) {
			if (!index_1.TypeSystemPolicy.IsNumberLike(value)) return false
			if (IsDefined(schema2.exclusiveMaximum) && !(value < schema2.exclusiveMaximum)) {
				return false
			}
			if (IsDefined(schema2.exclusiveMinimum) && !(value > schema2.exclusiveMinimum)) {
				return false
			}
			if (IsDefined(schema2.minimum) && !(value >= schema2.minimum)) {
				return false
			}
			if (IsDefined(schema2.maximum) && !(value <= schema2.maximum)) {
				return false
			}
			if (IsDefined(schema2.multipleOf) && !(value % schema2.multipleOf === 0)) {
				return false
			}
			return true
		}
		function TObject(schema2, references, value) {
			if (!index_1.TypeSystemPolicy.IsObjectLike(value)) return false
			if (
				IsDefined(schema2.minProperties) &&
				!(Object.getOwnPropertyNames(value).length >= schema2.minProperties)
			) {
				return false
			}
			if (
				IsDefined(schema2.maxProperties) &&
				!(Object.getOwnPropertyNames(value).length <= schema2.maxProperties)
			) {
				return false
			}
			const knownKeys = Object.getOwnPropertyNames(schema2.properties)
			for (const knownKey of knownKeys) {
				const property = schema2.properties[knownKey]
				if (schema2.required && schema2.required.includes(knownKey)) {
					if (!Visit(property, references, value[knownKey])) {
						return false
					}
					if (
						(Types.ExtendsUndefined.Check(property) || IsAnyOrUnknown(property)) &&
						!(knownKey in value)
					) {
						return false
					}
				} else {
					if (
						index_1.TypeSystemPolicy.IsExactOptionalProperty(value, knownKey) &&
						!Visit(property, references, value[knownKey])
					) {
						return false
					}
				}
			}
			if (schema2.additionalProperties === false) {
				const valueKeys = Object.getOwnPropertyNames(value)
				if (
					schema2.required &&
					schema2.required.length === knownKeys.length &&
					valueKeys.length === knownKeys.length
				) {
					return true
				} else {
					return valueKeys.every((valueKey) => knownKeys.includes(valueKey))
				}
			} else if (typeof schema2.additionalProperties === "object") {
				const valueKeys = Object.getOwnPropertyNames(value)
				return valueKeys.every(
					(key) =>
						knownKeys.includes(key) || Visit(schema2.additionalProperties, references, value[key])
				)
			} else {
				return true
			}
		}
		function TPromise(schema2, references, value) {
			return (0, guard_1.IsPromise)(value)
		}
		function TRecord(schema2, references, value) {
			if (!index_1.TypeSystemPolicy.IsRecordLike(value)) {
				return false
			}
			if (
				IsDefined(schema2.minProperties) &&
				!(Object.getOwnPropertyNames(value).length >= schema2.minProperties)
			) {
				return false
			}
			if (
				IsDefined(schema2.maxProperties) &&
				!(Object.getOwnPropertyNames(value).length <= schema2.maxProperties)
			) {
				return false
			}
			const [patternKey, patternSchema] = Object.entries(schema2.patternProperties)[0]
			const regex = new RegExp(patternKey)
			const check1 = Object.entries(value).every(([key, value2]) => {
				return regex.test(key) ? Visit(patternSchema, references, value2) : true
			})
			const check2 =
				typeof schema2.additionalProperties === "object"
					? Object.entries(value).every(([key, value2]) => {
							return !regex.test(key)
								? Visit(schema2.additionalProperties, references, value2)
								: true
					  })
					: true
			const check3 =
				schema2.additionalProperties === false
					? Object.getOwnPropertyNames(value).every((key) => {
							return regex.test(key)
					  })
					: true
			return check1 && check2 && check3
		}
		function TRef(schema2, references, value) {
			return Visit((0, deref_1.Deref)(schema2, references), references, value)
		}
		function TString(schema2, references, value) {
			if (!(0, guard_1.IsString)(value)) {
				return false
			}
			if (IsDefined(schema2.minLength) && !(value.length >= schema2.minLength)) return false
			if (IsDefined(schema2.maxLength) && !(value.length <= schema2.maxLength)) return false
			if (IsDefined(schema2.pattern)) {
				const regex = new RegExp(schema2.pattern)
				if (!regex.test(value)) return false
			}
			if (IsDefined(schema2.format)) {
				if (!Types.FormatRegistry.Has(schema2.format)) return false
				const func = Types.FormatRegistry.Get(schema2.format)
				return func(value)
			}
			return true
		}
		function TSymbol(schema2, references, value) {
			return (0, guard_1.IsSymbol)(value)
		}
		function TTemplateLiteral(schema2, references, value) {
			return (0, guard_1.IsString)(value) && new RegExp(schema2.pattern).test(value)
		}
		function TThis(schema2, references, value) {
			return Visit((0, deref_1.Deref)(schema2, references), references, value)
		}
		function TTuple(schema2, references, value) {
			if (!(0, guard_1.IsArray)(value)) {
				return false
			}
			if (schema2.items === void 0 && !(value.length === 0)) {
				return false
			}
			if (!(value.length === schema2.maxItems)) {
				return false
			}
			if (!schema2.items) {
				return true
			}
			for (let i = 0; i < schema2.items.length; i++) {
				if (!Visit(schema2.items[i], references, value[i])) return false
			}
			return true
		}
		function TUndefined(schema2, references, value) {
			return (0, guard_1.IsUndefined)(value)
		}
		function TUnion(schema2, references, value) {
			return schema2.anyOf.some((inner) => Visit(inner, references, value))
		}
		function TUint8Array(schema2, references, value) {
			if (!(0, guard_1.IsUint8Array)(value)) {
				return false
			}
			if (IsDefined(schema2.maxByteLength) && !(value.length <= schema2.maxByteLength)) {
				return false
			}
			if (IsDefined(schema2.minByteLength) && !(value.length >= schema2.minByteLength)) {
				return false
			}
			return true
		}
		function TUnknown(schema2, references, value) {
			return true
		}
		function TVoid(schema2, references, value) {
			return index_1.TypeSystemPolicy.IsVoidLike(value)
		}
		function TKind(schema2, references, value) {
			if (!Types.TypeRegistry.Has(schema2[Types.Kind])) return false
			const func = Types.TypeRegistry.Get(schema2[Types.Kind])
			return func(schema2, value)
		}
		function Visit(schema2, references, value) {
			const references_ = IsDefined(schema2.$id) ? [...references, schema2] : references
			const schema_ = schema2
			switch (schema_[Types.Kind]) {
				case "Any":
					return TAny(schema_, references_, value)
				case "Array":
					return TArray(schema_, references_, value)
				case "AsyncIterator":
					return TAsyncIterator(schema_, references_, value)
				case "BigInt":
					return TBigInt(schema_, references_, value)
				case "Boolean":
					return TBoolean(schema_, references_, value)
				case "Constructor":
					return TConstructor(schema_, references_, value)
				case "Date":
					return TDate(schema_, references_, value)
				case "Function":
					return TFunction(schema_, references_, value)
				case "Integer":
					return TInteger(schema_, references_, value)
				case "Intersect":
					return TIntersect(schema_, references_, value)
				case "Iterator":
					return TIterator(schema_, references_, value)
				case "Literal":
					return TLiteral(schema_, references_, value)
				case "Never":
					return TNever(schema_, references_, value)
				case "Not":
					return TNot(schema_, references_, value)
				case "Null":
					return TNull(schema_, references_, value)
				case "Number":
					return TNumber(schema_, references_, value)
				case "Object":
					return TObject(schema_, references_, value)
				case "Promise":
					return TPromise(schema_, references_, value)
				case "Record":
					return TRecord(schema_, references_, value)
				case "Ref":
					return TRef(schema_, references_, value)
				case "String":
					return TString(schema_, references_, value)
				case "Symbol":
					return TSymbol(schema_, references_, value)
				case "TemplateLiteral":
					return TTemplateLiteral(schema_, references_, value)
				case "This":
					return TThis(schema_, references_, value)
				case "Tuple":
					return TTuple(schema_, references_, value)
				case "Undefined":
					return TUndefined(schema_, references_, value)
				case "Union":
					return TUnion(schema_, references_, value)
				case "Uint8Array":
					return TUint8Array(schema_, references_, value)
				case "Unknown":
					return TUnknown(schema_, references_, value)
				case "Void":
					return TVoid(schema_, references_, value)
				default:
					if (!Types.TypeRegistry.Has(schema_[Types.Kind]))
						throw new ValueCheckUnknownTypeError(schema_)
					return TKind(schema_, references_, value)
			}
		}
		function Check(...args) {
			return args.length === 3 ? Visit(args[0], args[1], args[2]) : Visit(args[0], [], args[1])
		}
		exports2.Check = Check
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/create.js
var require_create = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/create.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.Create =
			exports2.ValueCreateRecursiveInstantiationError =
			exports2.ValueCreateTempateLiteralTypeError =
			exports2.ValueCreateIntersectTypeError =
			exports2.ValueCreateNotTypeError =
			exports2.ValueCreateNeverTypeError =
			exports2.ValueCreateUnknownTypeError =
				void 0
		var guard_1 = require_guard()
		var check_1 = require_check()
		var deref_1 = require_deref()
		var Types = require_typebox()
		var ValueCreateUnknownTypeError = class extends Types.TypeBoxError {
			constructor(schema2) {
				super("Unknown type")
				this.schema = schema2
			}
		}
		exports2.ValueCreateUnknownTypeError = ValueCreateUnknownTypeError
		var ValueCreateNeverTypeError = class extends Types.TypeBoxError {
			constructor(schema2) {
				super("Never types cannot be created")
				this.schema = schema2
			}
		}
		exports2.ValueCreateNeverTypeError = ValueCreateNeverTypeError
		var ValueCreateNotTypeError = class extends Types.TypeBoxError {
			constructor(schema2) {
				super("Not types must have a default value")
				this.schema = schema2
			}
		}
		exports2.ValueCreateNotTypeError = ValueCreateNotTypeError
		var ValueCreateIntersectTypeError = class extends Types.TypeBoxError {
			constructor(schema2) {
				super("Intersect produced invalid value. Consider using a default value.")
				this.schema = schema2
			}
		}
		exports2.ValueCreateIntersectTypeError = ValueCreateIntersectTypeError
		var ValueCreateTempateLiteralTypeError = class extends Types.TypeBoxError {
			constructor(schema2) {
				super(
					"Can only create template literal values from patterns that produce finite sequences. Consider using a default value."
				)
				this.schema = schema2
			}
		}
		exports2.ValueCreateTempateLiteralTypeError = ValueCreateTempateLiteralTypeError
		var ValueCreateRecursiveInstantiationError = class extends Types.TypeBoxError {
			constructor(schema2, recursiveMaxDepth2) {
				super(
					"Value cannot be created as recursive type may produce value of infinite size. Consider using a default."
				)
				this.schema = schema2
				this.recursiveMaxDepth = recursiveMaxDepth2
			}
		}
		exports2.ValueCreateRecursiveInstantiationError = ValueCreateRecursiveInstantiationError
		function TAny(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				return {}
			}
		}
		function TArray(schema2, references) {
			if (schema2.uniqueItems === true && !(0, guard_1.HasPropertyKey)(schema2, "default")) {
				throw new Error(
					"ValueCreate.Array: Array with the uniqueItems constraint requires a default value"
				)
			} else if ("contains" in schema2 && !(0, guard_1.HasPropertyKey)(schema2, "default")) {
				throw new Error(
					"ValueCreate.Array: Array with the contains constraint requires a default value"
				)
			} else if ("default" in schema2) {
				return schema2.default
			} else if (schema2.minItems !== void 0) {
				return Array.from({ length: schema2.minItems }).map((item) => {
					return Visit(schema2.items, references)
				})
			} else {
				return []
			}
		}
		function TAsyncIterator(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				return (async function* () {})()
			}
		}
		function TBigInt(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				return BigInt(0)
			}
		}
		function TBoolean(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				return false
			}
		}
		function TConstructor(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				const value = Visit(schema2.returns, references)
				if (typeof value === "object" && !Array.isArray(value)) {
					return class {
						constructor() {
							for (const [key, val] of Object.entries(value)) {
								const self2 = this
								self2[key] = val
							}
						}
					}
				} else {
					return class {}
				}
			}
		}
		function TDate(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else if (schema2.minimumTimestamp !== void 0) {
				return new Date(schema2.minimumTimestamp)
			} else {
				return /* @__PURE__ */ new Date()
			}
		}
		function TFunction(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				return () => Visit(schema2.returns, references)
			}
		}
		function TInteger(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else if (schema2.minimum !== void 0) {
				return schema2.minimum
			} else {
				return 0
			}
		}
		function TIntersect(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				const value = schema2.allOf.reduce((acc, schema3) => {
					const next = Visit(schema3, references)
					return typeof next === "object" ? { ...acc, ...next } : next
				}, {})
				if (!(0, check_1.Check)(schema2, references, value))
					throw new ValueCreateIntersectTypeError(schema2)
				return value
			}
		}
		function TIterator(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				return (function* () {})()
			}
		}
		function TLiteral(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				return schema2.const
			}
		}
		function TNever(schema2, references) {
			throw new ValueCreateNeverTypeError(schema2)
		}
		function TNot(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				throw new ValueCreateNotTypeError(schema2)
			}
		}
		function TNull(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				return null
			}
		}
		function TNumber(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else if (schema2.minimum !== void 0) {
				return schema2.minimum
			} else {
				return 0
			}
		}
		function TObject(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				const required = new Set(schema2.required)
				return (
					schema2.default ||
					Object.entries(schema2.properties).reduce((acc, [key, schema3]) => {
						return required.has(key) ? { ...acc, [key]: Visit(schema3, references) } : { ...acc }
					}, {})
				)
			}
		}
		function TPromise(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				return Promise.resolve(Visit(schema2.item, references))
			}
		}
		function TRecord(schema2, references) {
			const [keyPattern, valueSchema] = Object.entries(schema2.patternProperties)[0]
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else if (
				!(keyPattern === Types.PatternStringExact || keyPattern === Types.PatternNumberExact)
			) {
				const propertyKeys = keyPattern.slice(1, keyPattern.length - 1).split("|")
				return propertyKeys.reduce((acc, key) => {
					return { ...acc, [key]: Visit(valueSchema, references) }
				}, {})
			} else {
				return {}
			}
		}
		function TRef(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				return Visit((0, deref_1.Deref)(schema2, references), references)
			}
		}
		function TString(schema2, references) {
			if (schema2.pattern !== void 0) {
				if (!(0, guard_1.HasPropertyKey)(schema2, "default")) {
					throw new Error(
						"ValueCreate.String: String types with patterns must specify a default value"
					)
				} else {
					return schema2.default
				}
			} else if (schema2.format !== void 0) {
				if (!(0, guard_1.HasPropertyKey)(schema2, "default")) {
					throw new Error(
						"ValueCreate.String: String types with formats must specify a default value"
					)
				} else {
					return schema2.default
				}
			} else {
				if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
					return schema2.default
				} else if (schema2.minLength !== void 0) {
					return Array.from({ length: schema2.minLength })
						.map(() => ".")
						.join("")
				} else {
					return ""
				}
			}
		}
		function TSymbol(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else if ("value" in schema2) {
				return Symbol.for(schema2.value)
			} else {
				return Symbol()
			}
		}
		function TTemplateLiteral(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			}
			const expression = Types.TemplateLiteralParser.ParseExact(schema2.pattern)
			if (!Types.TemplateLiteralFinite.Check(expression))
				throw new ValueCreateTempateLiteralTypeError(schema2)
			const sequence = Types.TemplateLiteralGenerator.Generate(expression)
			return sequence.next().value
		}
		function TThis(schema2, references) {
			if (recursiveDepth++ > recursiveMaxDepth)
				throw new ValueCreateRecursiveInstantiationError(schema2, recursiveMaxDepth)
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				return Visit((0, deref_1.Deref)(schema2, references), references)
			}
		}
		function TTuple(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			}
			if (schema2.items === void 0) {
				return []
			} else {
				return Array.from({ length: schema2.minItems }).map((_, index2) =>
					Visit(schema2.items[index2], references)
				)
			}
		}
		function TUndefined(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				return void 0
			}
		}
		function TUnion(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else if (schema2.anyOf.length === 0) {
				throw new Error("ValueCreate.Union: Cannot create Union with zero variants")
			} else {
				return Visit(schema2.anyOf[0], references)
			}
		}
		function TUint8Array(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else if (schema2.minByteLength !== void 0) {
				return new Uint8Array(schema2.minByteLength)
			} else {
				return new Uint8Array(0)
			}
		}
		function TUnknown(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				return {}
			}
		}
		function TVoid(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				return void 0
			}
		}
		function TKind(schema2, references) {
			if ((0, guard_1.HasPropertyKey)(schema2, "default")) {
				return schema2.default
			} else {
				throw new Error("User defined types must specify a default value")
			}
		}
		function Visit(schema2, references) {
			const references_ = (0, guard_1.IsString)(schema2.$id) ? [...references, schema2] : references
			const schema_ = schema2
			switch (schema_[Types.Kind]) {
				case "Any":
					return TAny(schema_, references_)
				case "Array":
					return TArray(schema_, references_)
				case "AsyncIterator":
					return TAsyncIterator(schema_, references_)
				case "BigInt":
					return TBigInt(schema_, references_)
				case "Boolean":
					return TBoolean(schema_, references_)
				case "Constructor":
					return TConstructor(schema_, references_)
				case "Date":
					return TDate(schema_, references_)
				case "Function":
					return TFunction(schema_, references_)
				case "Integer":
					return TInteger(schema_, references_)
				case "Intersect":
					return TIntersect(schema_, references_)
				case "Iterator":
					return TIterator(schema_, references_)
				case "Literal":
					return TLiteral(schema_, references_)
				case "Never":
					return TNever(schema_, references_)
				case "Not":
					return TNot(schema_, references_)
				case "Null":
					return TNull(schema_, references_)
				case "Number":
					return TNumber(schema_, references_)
				case "Object":
					return TObject(schema_, references_)
				case "Promise":
					return TPromise(schema_, references_)
				case "Record":
					return TRecord(schema_, references_)
				case "Ref":
					return TRef(schema_, references_)
				case "String":
					return TString(schema_, references_)
				case "Symbol":
					return TSymbol(schema_, references_)
				case "TemplateLiteral":
					return TTemplateLiteral(schema_, references_)
				case "This":
					return TThis(schema_, references_)
				case "Tuple":
					return TTuple(schema_, references_)
				case "Undefined":
					return TUndefined(schema_, references_)
				case "Union":
					return TUnion(schema_, references_)
				case "Uint8Array":
					return TUint8Array(schema_, references_)
				case "Unknown":
					return TUnknown(schema_, references_)
				case "Void":
					return TVoid(schema_, references_)
				default:
					if (!Types.TypeRegistry.Has(schema_[Types.Kind]))
						throw new ValueCreateUnknownTypeError(schema_)
					return TKind(schema_, references_)
			}
		}
		var recursiveMaxDepth = 512
		var recursiveDepth = 0
		function Create(...args) {
			recursiveDepth = 0
			return args.length === 2 ? Visit(args[0], args[1]) : Visit(args[0], [])
		}
		exports2.Create = Create
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/cast.js
var require_cast = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/cast.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.Cast =
			exports2.Default =
			exports2.DefaultClone =
			exports2.ValueCastUnknownTypeError =
			exports2.ValueCastRecursiveTypeError =
			exports2.ValueCastNeverTypeError =
			exports2.ValueCastArrayUniqueItemsTypeError =
				void 0
		var guard_1 = require_guard()
		var create_1 = require_create()
		var check_1 = require_check()
		var clone_1 = require_clone()
		var deref_1 = require_deref()
		var Types = require_typebox()
		var ValueCastArrayUniqueItemsTypeError = class extends Types.TypeBoxError {
			constructor(schema2, value) {
				super("Array cast produced invalid data due to uniqueItems constraint")
				this.schema = schema2
				this.value = value
			}
		}
		exports2.ValueCastArrayUniqueItemsTypeError = ValueCastArrayUniqueItemsTypeError
		var ValueCastNeverTypeError = class extends Types.TypeBoxError {
			constructor(schema2) {
				super("Never types cannot be cast")
				this.schema = schema2
			}
		}
		exports2.ValueCastNeverTypeError = ValueCastNeverTypeError
		var ValueCastRecursiveTypeError = class extends Types.TypeBoxError {
			constructor(schema2) {
				super("Cannot cast recursive schemas")
				this.schema = schema2
			}
		}
		exports2.ValueCastRecursiveTypeError = ValueCastRecursiveTypeError
		var ValueCastUnknownTypeError = class extends Types.TypeBoxError {
			constructor(schema2) {
				super("Unknown type")
				this.schema = schema2
			}
		}
		exports2.ValueCastUnknownTypeError = ValueCastUnknownTypeError
		var UnionCastCreate
		;(function (UnionCastCreate2) {
			function Score(schema2, references, value) {
				if (
					schema2[Types.Kind] === "Object" &&
					typeof value === "object" &&
					!(0, guard_1.IsNull)(value)
				) {
					const object = schema2
					const keys = Object.getOwnPropertyNames(value)
					const entries = Object.entries(object.properties)
					const [point, max] = [1 / entries.length, entries.length]
					return entries.reduce((acc, [key, schema3]) => {
						const literal =
							schema3[Types.Kind] === "Literal" && schema3.const === value[key] ? max : 0
						const checks = (0, check_1.Check)(schema3, references, value[key]) ? point : 0
						const exists = keys.includes(key) ? point : 0
						return acc + (literal + checks + exists)
					}, 0)
				} else {
					return (0, check_1.Check)(schema2, references, value) ? 1 : 0
				}
			}
			function Select(union, references, value) {
				let [select, best] = [union.anyOf[0], 0]
				for (const schema2 of union.anyOf) {
					const score = Score(schema2, references, value)
					if (score > best) {
						select = schema2
						best = score
					}
				}
				return select
			}
			function Create(union, references, value) {
				if ("default" in union) {
					return union.default
				} else {
					const schema2 = Select(union, references, value)
					return Cast(schema2, references, value)
				}
			}
			UnionCastCreate2.Create = Create
		})(UnionCastCreate || (UnionCastCreate = {}))
		function DefaultClone(schema2, references, value) {
			return (0, check_1.Check)(schema2, references, value)
				? (0, clone_1.Clone)(value)
				: (0, create_1.Create)(schema2, references)
		}
		exports2.DefaultClone = DefaultClone
		function Default(schema2, references, value) {
			return (0, check_1.Check)(schema2, references, value)
				? value
				: (0, create_1.Create)(schema2, references)
		}
		exports2.Default = Default
		function TArray(schema2, references, value) {
			if ((0, check_1.Check)(schema2, references, value)) return (0, clone_1.Clone)(value)
			const created = (0, guard_1.IsArray)(value)
				? (0, clone_1.Clone)(value)
				: (0, create_1.Create)(schema2, references)
			const minimum =
				(0, guard_1.IsNumber)(schema2.minItems) && created.length < schema2.minItems
					? [...created, ...Array.from({ length: schema2.minItems - created.length }, () => null)]
					: created
			const maximum =
				(0, guard_1.IsNumber)(schema2.maxItems) && minimum.length > schema2.maxItems
					? minimum.slice(0, schema2.maxItems)
					: minimum
			const casted = maximum.map((value2) => Visit(schema2.items, references, value2))
			if (schema2.uniqueItems !== true) return casted
			const unique = [...new Set(casted)]
			if (!(0, check_1.Check)(schema2, references, unique))
				throw new ValueCastArrayUniqueItemsTypeError(schema2, unique)
			return unique
		}
		function TConstructor(schema2, references, value) {
			if ((0, check_1.Check)(schema2, references, value))
				return (0, create_1.Create)(schema2, references)
			const required = new Set(schema2.returns.required || [])
			const result = function () {}
			for (const [key, property] of Object.entries(schema2.returns.properties)) {
				if (!required.has(key) && value.prototype[key] === void 0) continue
				result.prototype[key] = Visit(property, references, value.prototype[key])
			}
			return result
		}
		function TIntersect(schema2, references, value) {
			const created = (0, create_1.Create)(schema2, references)
			const mapped =
				(0, guard_1.IsPlainObject)(created) && (0, guard_1.IsPlainObject)(value)
					? { ...created, ...value }
					: value
			return (0, check_1.Check)(schema2, references, mapped)
				? mapped
				: (0, create_1.Create)(schema2, references)
		}
		function TNever(schema2, references, value) {
			throw new ValueCastNeverTypeError(schema2)
		}
		function TObject(schema2, references, value) {
			if ((0, check_1.Check)(schema2, references, value)) return value
			if (value === null || typeof value !== "object")
				return (0, create_1.Create)(schema2, references)
			const required = new Set(schema2.required || [])
			const result = {}
			for (const [key, property] of Object.entries(schema2.properties)) {
				if (!required.has(key) && value[key] === void 0) continue
				result[key] = Visit(property, references, value[key])
			}
			if (typeof schema2.additionalProperties === "object") {
				const propertyNames = Object.getOwnPropertyNames(schema2.properties)
				for (const propertyName of Object.getOwnPropertyNames(value)) {
					if (propertyNames.includes(propertyName)) continue
					result[propertyName] = Visit(
						schema2.additionalProperties,
						references,
						value[propertyName]
					)
				}
			}
			return result
		}
		function TRecord(schema2, references, value) {
			if ((0, check_1.Check)(schema2, references, value)) return (0, clone_1.Clone)(value)
			if (
				value === null ||
				typeof value !== "object" ||
				Array.isArray(value) ||
				value instanceof Date
			)
				return (0, create_1.Create)(schema2, references)
			const subschemaPropertyName = Object.getOwnPropertyNames(schema2.patternProperties)[0]
			const subschema = schema2.patternProperties[subschemaPropertyName]
			const result = {}
			for (const [propKey, propValue] of Object.entries(value)) {
				result[propKey] = Visit(subschema, references, propValue)
			}
			return result
		}
		function TRef(schema2, references, value) {
			return Visit((0, deref_1.Deref)(schema2, references), references, value)
		}
		function TThis(schema2, references, value) {
			return Visit((0, deref_1.Deref)(schema2, references), references, value)
		}
		function TTuple(schema2, references, value) {
			if ((0, check_1.Check)(schema2, references, value)) return (0, clone_1.Clone)(value)
			if (!(0, guard_1.IsArray)(value)) return (0, create_1.Create)(schema2, references)
			if (schema2.items === void 0) return []
			return schema2.items.map((schema3, index2) => Visit(schema3, references, value[index2]))
		}
		function TUnion(schema2, references, value) {
			return (0, check_1.Check)(schema2, references, value)
				? (0, clone_1.Clone)(value)
				: UnionCastCreate.Create(schema2, references, value)
		}
		function Visit(schema2, references, value) {
			const references_ = (0, guard_1.IsString)(schema2.$id) ? [...references, schema2] : references
			const schema_ = schema2
			switch (schema2[Types.Kind]) {
				case "Array":
					return TArray(schema_, references_, value)
				case "Constructor":
					return TConstructor(schema_, references_, value)
				case "Intersect":
					return TIntersect(schema_, references_, value)
				case "Never":
					return TNever(schema_, references_, value)
				case "Object":
					return TObject(schema_, references_, value)
				case "Record":
					return TRecord(schema_, references_, value)
				case "Ref":
					return TRef(schema_, references_, value)
				case "This":
					return TThis(schema_, references_, value)
				case "Tuple":
					return TTuple(schema_, references_, value)
				case "Union":
					return TUnion(schema_, references_, value)
				case "Date":
				case "Symbol":
				case "Uint8Array":
					return DefaultClone(schema2, references, value)
				case "Any":
				case "AsyncIterator":
				case "BigInt":
				case "Boolean":
				case "Function":
				case "Integer":
				case "Iterator":
				case "Literal":
				case "Not":
				case "Null":
				case "Number":
				case "Promise":
				case "String":
				case "TemplateLiteral":
				case "Undefined":
				case "Unknown":
				case "Void":
					return Default(schema_, references_, value)
				default:
					if (!Types.TypeRegistry.Has(schema_[Types.Kind]))
						throw new ValueCastUnknownTypeError(schema_)
					return Default(schema_, references_, value)
			}
		}
		function Cast(...args) {
			return args.length === 3 ? Visit(args[0], args[1], args[2]) : Visit(args[0], [], args[1])
		}
		exports2.Cast = Cast
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/convert.js
var require_convert = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/convert.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.Convert = exports2.Default = exports2.ValueConvertUnknownTypeError = void 0
		var guard_1 = require_guard()
		var clone_1 = require_clone()
		var check_1 = require_check()
		var deref_1 = require_deref()
		var Types = require_typebox()
		var ValueConvertUnknownTypeError = class extends Types.TypeBoxError {
			constructor(schema2) {
				super("Unknown type")
				this.schema = schema2
			}
		}
		exports2.ValueConvertUnknownTypeError = ValueConvertUnknownTypeError
		function IsStringNumeric(value) {
			return (0, guard_1.IsString)(value) && !isNaN(value) && !isNaN(parseFloat(value))
		}
		function IsValueToString(value) {
			return (
				(0, guard_1.IsBigInt)(value) ||
				(0, guard_1.IsBoolean)(value) ||
				(0, guard_1.IsNumber)(value)
			)
		}
		function IsValueTrue(value) {
			return (
				value === true ||
				((0, guard_1.IsNumber)(value) && value === 1) ||
				((0, guard_1.IsBigInt)(value) && value === BigInt("1")) ||
				((0, guard_1.IsString)(value) && (value.toLowerCase() === "true" || value === "1"))
			)
		}
		function IsValueFalse(value) {
			return (
				value === false ||
				((0, guard_1.IsNumber)(value) && (value === 0 || Object.is(value, -0))) ||
				((0, guard_1.IsBigInt)(value) && value === BigInt("0")) ||
				((0, guard_1.IsString)(value) &&
					(value.toLowerCase() === "false" || value === "0" || value === "-0"))
			)
		}
		function IsTimeStringWithTimeZone(value) {
			return (
				(0, guard_1.IsString)(value) &&
				/^(?:[0-2]\d:[0-5]\d:[0-5]\d|23:59:60)(?:\.\d+)?(?:z|[+-]\d\d(?::?\d\d)?)$/i.test(value)
			)
		}
		function IsTimeStringWithoutTimeZone(value) {
			return (0, guard_1.IsString)(value) && /^(?:[0-2]\d:[0-5]\d:[0-5]\d|23:59:60)?$/i.test(value)
		}
		function IsDateTimeStringWithTimeZone(value) {
			return (
				(0, guard_1.IsString)(value) &&
				/^\d\d\d\d-[0-1]\d-[0-3]\dt(?:[0-2]\d:[0-5]\d:[0-5]\d|23:59:60)(?:\.\d+)?(?:z|[+-]\d\d(?::?\d\d)?)$/i.test(
					value
				)
			)
		}
		function IsDateTimeStringWithoutTimeZone(value) {
			return (
				(0, guard_1.IsString)(value) &&
				/^\d\d\d\d-[0-1]\d-[0-3]\dt(?:[0-2]\d:[0-5]\d:[0-5]\d|23:59:60)?$/i.test(value)
			)
		}
		function IsDateString(value) {
			return (0, guard_1.IsString)(value) && /^\d\d\d\d-[0-1]\d-[0-3]\d$/i.test(value)
		}
		function TryConvertLiteralString(value, target) {
			const conversion = TryConvertString(value)
			return conversion === target ? conversion : value
		}
		function TryConvertLiteralNumber(value, target) {
			const conversion = TryConvertNumber(value)
			return conversion === target ? conversion : value
		}
		function TryConvertLiteralBoolean(value, target) {
			const conversion = TryConvertBoolean(value)
			return conversion === target ? conversion : value
		}
		function TryConvertLiteral(schema2, value) {
			if (typeof schema2.const === "string") {
				return TryConvertLiteralString(value, schema2.const)
			} else if (typeof schema2.const === "number") {
				return TryConvertLiteralNumber(value, schema2.const)
			} else if (typeof schema2.const === "boolean") {
				return TryConvertLiteralBoolean(value, schema2.const)
			} else {
				return (0, clone_1.Clone)(value)
			}
		}
		function TryConvertBoolean(value) {
			return IsValueTrue(value) ? true : IsValueFalse(value) ? false : value
		}
		function TryConvertBigInt(value) {
			return IsStringNumeric(value)
				? BigInt(parseInt(value))
				: (0, guard_1.IsNumber)(value)
				? BigInt(value | 0)
				: IsValueFalse(value)
				? BigInt(0)
				: IsValueTrue(value)
				? BigInt(1)
				: value
		}
		function TryConvertString(value) {
			return IsValueToString(value)
				? value.toString()
				: (0, guard_1.IsSymbol)(value) && value.description !== void 0
				? value.description.toString()
				: value
		}
		function TryConvertNumber(value) {
			return IsStringNumeric(value)
				? parseFloat(value)
				: IsValueTrue(value)
				? 1
				: IsValueFalse(value)
				? 0
				: value
		}
		function TryConvertInteger(value) {
			return IsStringNumeric(value)
				? parseInt(value)
				: (0, guard_1.IsNumber)(value)
				? value | 0
				: IsValueTrue(value)
				? 1
				: IsValueFalse(value)
				? 0
				: value
		}
		function TryConvertNull(value) {
			return (0, guard_1.IsString)(value) && value.toLowerCase() === "null" ? null : value
		}
		function TryConvertUndefined(value) {
			return (0, guard_1.IsString)(value) && value === "undefined" ? void 0 : value
		}
		function TryConvertDate(value) {
			return (0, guard_1.IsDate)(value)
				? value
				: (0, guard_1.IsNumber)(value)
				? new Date(value)
				: IsValueTrue(value)
				? /* @__PURE__ */ new Date(1)
				: IsValueFalse(value)
				? /* @__PURE__ */ new Date(0)
				: IsStringNumeric(value)
				? new Date(parseInt(value))
				: IsTimeStringWithoutTimeZone(value)
				? /* @__PURE__ */ new Date(`1970-01-01T${value}.000Z`)
				: IsTimeStringWithTimeZone(value)
				? /* @__PURE__ */ new Date(`1970-01-01T${value}`)
				: IsDateTimeStringWithoutTimeZone(value)
				? /* @__PURE__ */ new Date(`${value}.000Z`)
				: IsDateTimeStringWithTimeZone(value)
				? new Date(value)
				: IsDateString(value)
				? /* @__PURE__ */ new Date(`${value}T00:00:00.000Z`)
				: value
		}
		function Default(value) {
			return value
		}
		exports2.Default = Default
		function TArray(schema2, references, value) {
			if ((0, guard_1.IsArray)(value)) {
				return value.map((value2) => Visit(schema2.items, references, value2))
			}
			return value
		}
		function TBigInt(schema2, references, value) {
			return TryConvertBigInt(value)
		}
		function TBoolean(schema2, references, value) {
			return TryConvertBoolean(value)
		}
		function TDate(schema2, references, value) {
			return TryConvertDate(value)
		}
		function TInteger(schema2, references, value) {
			return TryConvertInteger(value)
		}
		function TIntersect(schema2, references, value) {
			return schema2.allOf.every((schema3) => Types.TypeGuard.TObject(schema3))
				? Visit(Types.Type.Composite(schema2.allOf), references, value)
				: Visit(schema2.allOf[0], references, value)
		}
		function TLiteral(schema2, references, value) {
			return TryConvertLiteral(schema2, value)
		}
		function TNull(schema2, references, value) {
			return TryConvertNull(value)
		}
		function TNumber(schema2, references, value) {
			return TryConvertNumber(value)
		}
		function TObject(schema2, references, value) {
			if ((0, guard_1.IsObject)(value))
				return Object.getOwnPropertyNames(schema2.properties).reduce((acc, key) => {
					return value[key] !== void 0
						? { ...acc, [key]: Visit(schema2.properties[key], references, value[key]) }
						: { ...acc }
				}, value)
			return value
		}
		function TRecord(schema2, references, value) {
			const propertyKey = Object.getOwnPropertyNames(schema2.patternProperties)[0]
			const property = schema2.patternProperties[propertyKey]
			const result = {}
			for (const [propKey, propValue] of Object.entries(value)) {
				result[propKey] = Visit(property, references, propValue)
			}
			return result
		}
		function TRef(schema2, references, value) {
			return Visit((0, deref_1.Deref)(schema2, references), references, value)
		}
		function TString(schema2, references, value) {
			return TryConvertString(value)
		}
		function TSymbol(schema2, references, value) {
			return (0, guard_1.IsString)(value) || (0, guard_1.IsNumber)(value) ? Symbol(value) : value
		}
		function TThis(schema2, references, value) {
			return Visit((0, deref_1.Deref)(schema2, references), references, value)
		}
		function TTuple(schema2, references, value) {
			if ((0, guard_1.IsArray)(value) && !(0, guard_1.IsUndefined)(schema2.items)) {
				return value.map((value2, index2) => {
					return index2 < schema2.items.length
						? Visit(schema2.items[index2], references, value2)
						: value2
				})
			}
			return value
		}
		function TUndefined(schema2, references, value) {
			return TryConvertUndefined(value)
		}
		function TUnion(schema2, references, value) {
			for (const subschema of schema2.anyOf) {
				const converted = Visit(subschema, references, value)
				if ((0, check_1.Check)(subschema, references, converted)) {
					return converted
				}
			}
			return value
		}
		function Visit(schema2, references, value) {
			const references_ = (0, guard_1.IsString)(schema2.$id) ? [...references, schema2] : references
			const schema_ = schema2
			switch (schema2[Types.Kind]) {
				case "Array":
					return TArray(schema_, references_, value)
				case "BigInt":
					return TBigInt(schema_, references_, value)
				case "Boolean":
					return TBoolean(schema_, references_, value)
				case "Date":
					return TDate(schema_, references_, value)
				case "Integer":
					return TInteger(schema_, references_, value)
				case "Intersect":
					return TIntersect(schema_, references_, value)
				case "Literal":
					return TLiteral(schema_, references_, value)
				case "Null":
					return TNull(schema_, references_, value)
				case "Number":
					return TNumber(schema_, references_, value)
				case "Object":
					return TObject(schema_, references_, value)
				case "Record":
					return TRecord(schema_, references_, value)
				case "Ref":
					return TRef(schema_, references_, value)
				case "String":
					return TString(schema_, references_, value)
				case "Symbol":
					return TSymbol(schema_, references_, value)
				case "This":
					return TThis(schema_, references_, value)
				case "Tuple":
					return TTuple(schema_, references_, value)
				case "Undefined":
					return TUndefined(schema_, references_, value)
				case "Union":
					return TUnion(schema_, references_, value)
				case "Any":
				case "AsyncIterator":
				case "Constructor":
				case "Function":
				case "Iterator":
				case "Never":
				case "Promise":
				case "TemplateLiteral":
				case "Uint8Array":
				case "Unknown":
				case "Void":
					return Default(value)
				default:
					if (!Types.TypeRegistry.Has(schema_[Types.Kind]))
						throw new ValueConvertUnknownTypeError(schema_)
					return Default(value)
			}
		}
		function Convert(...args) {
			return args.length === 3 ? Visit(args[0], args[1], args[2]) : Visit(args[0], [], args[1])
		}
		exports2.Convert = Convert
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/transform.js
var require_transform = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/transform.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.EncodeTransform =
			exports2.DecodeTransform =
			exports2.HasTransform =
			exports2.TransformEncodeError =
			exports2.TransformDecodeError =
			exports2.TransformEncodeCheckError =
			exports2.TransformDecodeCheckError =
				void 0
		var guard_1 = require_guard()
		var deref_1 = require_deref()
		var check_1 = require_check()
		var Types = require_typebox()
		var TransformDecodeCheckError = class extends Types.TypeBoxError {
			constructor(schema2, value, error) {
				super(`Unable to decode due to invalid value`)
				this.schema = schema2
				this.value = value
				this.error = error
			}
		}
		exports2.TransformDecodeCheckError = TransformDecodeCheckError
		var TransformEncodeCheckError = class extends Types.TypeBoxError {
			constructor(schema2, value, error) {
				super(`Unable to encode due to invalid value`)
				this.schema = schema2
				this.value = value
				this.error = error
			}
		}
		exports2.TransformEncodeCheckError = TransformEncodeCheckError
		var TransformDecodeError = class extends Types.TypeBoxError {
			constructor(schema2, value, error) {
				super(`${error instanceof Error ? error.message : "Unknown error"}`)
				this.schema = schema2
				this.value = value
			}
		}
		exports2.TransformDecodeError = TransformDecodeError
		var TransformEncodeError = class extends Types.TypeBoxError {
			constructor(schema2, value, error) {
				super(`${error instanceof Error ? error.message : "Unknown error"}`)
				this.schema = schema2
				this.value = value
			}
		}
		exports2.TransformEncodeError = TransformEncodeError
		var HasTransform
		;(function (HasTransform2) {
			function TArray(schema2, references) {
				return Types.TypeGuard.TTransform(schema2) || Visit(schema2.items, references)
			}
			function TAsyncIterator(schema2, references) {
				return Types.TypeGuard.TTransform(schema2) || Visit(schema2.items, references)
			}
			function TConstructor(schema2, references) {
				return (
					Types.TypeGuard.TTransform(schema2) ||
					Visit(schema2.returns, references) ||
					schema2.parameters.some((schema3) => Visit(schema3, references))
				)
			}
			function TFunction(schema2, references) {
				return (
					Types.TypeGuard.TTransform(schema2) ||
					Visit(schema2.returns, references) ||
					schema2.parameters.some((schema3) => Visit(schema3, references))
				)
			}
			function TIntersect(schema2, references) {
				return (
					Types.TypeGuard.TTransform(schema2) ||
					Types.TypeGuard.TTransform(schema2.unevaluatedProperties) ||
					schema2.allOf.some((schema3) => Visit(schema3, references))
				)
			}
			function TIterator(schema2, references) {
				return Types.TypeGuard.TTransform(schema2) || Visit(schema2.items, references)
			}
			function TNot(schema2, references) {
				return Types.TypeGuard.TTransform(schema2) || Visit(schema2.not, references)
			}
			function TObject(schema2, references) {
				return (
					Types.TypeGuard.TTransform(schema2) ||
					Object.values(schema2.properties).some((schema3) => Visit(schema3, references)) ||
					(Types.TypeGuard.TSchema(schema2.additionalProperties) &&
						Visit(schema2.additionalProperties, references))
				)
			}
			function TPromise(schema2, references) {
				return Types.TypeGuard.TTransform(schema2) || Visit(schema2.item, references)
			}
			function TRecord(schema2, references) {
				const pattern3 = Object.getOwnPropertyNames(schema2.patternProperties)[0]
				const property = schema2.patternProperties[pattern3]
				return (
					Types.TypeGuard.TTransform(schema2) ||
					Visit(property, references) ||
					(Types.TypeGuard.TSchema(schema2.additionalProperties) &&
						Types.TypeGuard.TTransform(schema2.additionalProperties))
				)
			}
			function TRef(schema2, references) {
				if (Types.TypeGuard.TTransform(schema2)) return true
				return Visit((0, deref_1.Deref)(schema2, references), references)
			}
			function TThis(schema2, references) {
				if (Types.TypeGuard.TTransform(schema2)) return true
				return Visit((0, deref_1.Deref)(schema2, references), references)
			}
			function TTuple(schema2, references) {
				return (
					Types.TypeGuard.TTransform(schema2) ||
					(!(0, guard_1.IsUndefined)(schema2.items) &&
						schema2.items.some((schema3) => Visit(schema3, references)))
				)
			}
			function TUnion(schema2, references) {
				return (
					Types.TypeGuard.TTransform(schema2) ||
					schema2.anyOf.some((schema3) => Visit(schema3, references))
				)
			}
			function Visit(schema2, references) {
				const references_ = (0, guard_1.IsString)(schema2.$id)
					? [...references, schema2]
					: references
				const schema_ = schema2
				if (schema2.$id && visited.has(schema2.$id)) return false
				if (schema2.$id) visited.add(schema2.$id)
				switch (schema2[Types.Kind]) {
					case "Array":
						return TArray(schema_, references_)
					case "AsyncIterator":
						return TAsyncIterator(schema_, references_)
					case "Constructor":
						return TConstructor(schema_, references_)
					case "Function":
						return TFunction(schema_, references_)
					case "Intersect":
						return TIntersect(schema_, references_)
					case "Iterator":
						return TIterator(schema_, references_)
					case "Not":
						return TNot(schema_, references_)
					case "Object":
						return TObject(schema_, references_)
					case "Promise":
						return TPromise(schema_, references_)
					case "Record":
						return TRecord(schema_, references_)
					case "Ref":
						return TRef(schema_, references_)
					case "This":
						return TThis(schema_, references_)
					case "Tuple":
						return TTuple(schema_, references_)
					case "Union":
						return TUnion(schema_, references_)
					default:
						return Types.TypeGuard.TTransform(schema2)
				}
			}
			const visited = /* @__PURE__ */ new Set()
			function Has(schema2, references) {
				visited.clear()
				return Visit(schema2, references)
			}
			HasTransform2.Has = Has
		})(HasTransform || (exports2.HasTransform = HasTransform = {}))
		var DecodeTransform
		;(function (DecodeTransform2) {
			function Default(schema2, value) {
				try {
					return Types.TypeGuard.TTransform(schema2)
						? schema2[Types.Transform].Decode(value)
						: value
				} catch (error) {
					throw new TransformDecodeError(schema2, value, error)
				}
			}
			function TArray(schema2, references, value) {
				return (0, guard_1.IsArray)(value)
					? Default(
							schema2,
							value.map((value2) => Visit(schema2.items, references, value2))
					  )
					: Default(schema2, value)
			}
			function TIntersect(schema2, references, value) {
				if (!(0, guard_1.IsPlainObject)(value) || (0, guard_1.IsValueType)(value))
					return Default(schema2, value)
				const knownKeys = Types.KeyResolver.ResolveKeys(schema2, { includePatterns: false })
				const knownProperties = knownKeys.reduce((value2, key) => {
					return key in value2
						? {
								...value2,
								[key]: Visit(
									Types.IndexedAccessor.Resolve(schema2, [key]),
									references,
									value2[key]
								),
						  }
						: value2
				}, value)
				if (!Types.TypeGuard.TTransform(schema2.unevaluatedProperties)) {
					return Default(schema2, knownProperties)
				}
				const unknownKeys = Object.getOwnPropertyNames(knownProperties)
				const unevaluatedProperties = schema2.unevaluatedProperties
				const unknownProperties = unknownKeys.reduce((value2, key) => {
					return !knownKeys.includes(key)
						? { ...value2, [key]: Default(unevaluatedProperties, value2[key]) }
						: value2
				}, knownProperties)
				return Default(schema2, unknownProperties)
			}
			function TNot(schema2, references, value) {
				return Default(schema2, Visit(schema2.not, references, value))
			}
			function TObject(schema2, references, value) {
				if (!(0, guard_1.IsPlainObject)(value)) return Default(schema2, value)
				const knownKeys = Types.KeyResolver.ResolveKeys(schema2, { includePatterns: false })
				const knownProperties = knownKeys.reduce((value2, key) => {
					return key in value2
						? { ...value2, [key]: Visit(schema2.properties[key], references, value2[key]) }
						: value2
				}, value)
				if (!Types.TypeGuard.TSchema(schema2.additionalProperties)) {
					return Default(schema2, knownProperties)
				}
				const unknownKeys = Object.getOwnPropertyNames(knownProperties)
				const additionalProperties = schema2.additionalProperties
				const unknownProperties = unknownKeys.reduce((value2, key) => {
					return !knownKeys.includes(key)
						? { ...value2, [key]: Default(additionalProperties, value2[key]) }
						: value2
				}, knownProperties)
				return Default(schema2, unknownProperties)
			}
			function TRecord(schema2, references, value) {
				if (!(0, guard_1.IsPlainObject)(value)) return Default(schema2, value)
				const pattern3 = Object.getOwnPropertyNames(schema2.patternProperties)[0]
				const knownKeys = new RegExp(pattern3)
				const knownProperties = Object.getOwnPropertyNames(value).reduce((value2, key) => {
					return knownKeys.test(key)
						? {
								...value2,
								[key]: Visit(schema2.patternProperties[pattern3], references, value2[key]),
						  }
						: value2
				}, value)
				if (!Types.TypeGuard.TSchema(schema2.additionalProperties)) {
					return Default(schema2, knownProperties)
				}
				const unknownKeys = Object.getOwnPropertyNames(knownProperties)
				const additionalProperties = schema2.additionalProperties
				const unknownProperties = unknownKeys.reduce((value2, key) => {
					return !knownKeys.test(key)
						? { ...value2, [key]: Default(additionalProperties, value2[key]) }
						: value2
				}, knownProperties)
				return Default(schema2, unknownProperties)
			}
			function TRef(schema2, references, value) {
				const target = (0, deref_1.Deref)(schema2, references)
				return Default(schema2, Visit(target, references, value))
			}
			function TThis(schema2, references, value) {
				const target = (0, deref_1.Deref)(schema2, references)
				return Default(schema2, Visit(target, references, value))
			}
			function TTuple(schema2, references, value) {
				return (0, guard_1.IsArray)(value) && (0, guard_1.IsArray)(schema2.items)
					? Default(
							schema2,
							schema2.items.map((schema3, index2) => Visit(schema3, references, value[index2]))
					  )
					: Default(schema2, value)
			}
			function TUnion(schema2, references, value) {
				const defaulted = Default(schema2, value)
				for (const subschema of schema2.anyOf) {
					if (!(0, check_1.Check)(subschema, references, defaulted)) continue
					return Visit(subschema, references, defaulted)
				}
				return defaulted
			}
			function Visit(schema2, references, value) {
				const references_ = typeof schema2.$id === "string" ? [...references, schema2] : references
				const schema_ = schema2
				switch (schema2[Types.Kind]) {
					case "Array":
						return TArray(schema_, references_, value)
					case "Intersect":
						return TIntersect(schema_, references_, value)
					case "Not":
						return TNot(schema_, references_, value)
					case "Object":
						return TObject(schema_, references_, value)
					case "Record":
						return TRecord(schema_, references_, value)
					case "Ref":
						return TRef(schema_, references_, value)
					case "Symbol":
						return Default(schema_, value)
					case "This":
						return TThis(schema_, references_, value)
					case "Tuple":
						return TTuple(schema_, references_, value)
					case "Union":
						return TUnion(schema_, references_, value)
					default:
						return Default(schema_, value)
				}
			}
			function Decode(schema2, references, value) {
				return Visit(schema2, references, value)
			}
			DecodeTransform2.Decode = Decode
		})(DecodeTransform || (exports2.DecodeTransform = DecodeTransform = {}))
		var EncodeTransform
		;(function (EncodeTransform2) {
			function Default(schema2, value) {
				try {
					return Types.TypeGuard.TTransform(schema2)
						? schema2[Types.Transform].Encode(value)
						: value
				} catch (error) {
					throw new TransformEncodeError(schema2, value, error)
				}
			}
			function TArray(schema2, references, value) {
				const defaulted = Default(schema2, value)
				return (0, guard_1.IsArray)(defaulted)
					? defaulted.map((value2) => Visit(schema2.items, references, value2))
					: defaulted
			}
			function TIntersect(schema2, references, value) {
				const defaulted = Default(schema2, value)
				if (!(0, guard_1.IsPlainObject)(value) || (0, guard_1.IsValueType)(value)) return defaulted
				const knownKeys = Types.KeyResolver.ResolveKeys(schema2, { includePatterns: false })
				const knownProperties = knownKeys.reduce((value2, key) => {
					return key in defaulted
						? {
								...value2,
								[key]: Visit(
									Types.IndexedAccessor.Resolve(schema2, [key]),
									references,
									value2[key]
								),
						  }
						: value2
				}, defaulted)
				if (!Types.TypeGuard.TTransform(schema2.unevaluatedProperties)) {
					return Default(schema2, knownProperties)
				}
				const unknownKeys = Object.getOwnPropertyNames(knownProperties)
				const unevaluatedProperties = schema2.unevaluatedProperties
				return unknownKeys.reduce((value2, key) => {
					return !knownKeys.includes(key)
						? { ...value2, [key]: Default(unevaluatedProperties, value2[key]) }
						: value2
				}, knownProperties)
			}
			function TNot(schema2, references, value) {
				return Default(schema2.not, Default(schema2, value))
			}
			function TObject(schema2, references, value) {
				const defaulted = Default(schema2, value)
				if (!(0, guard_1.IsPlainObject)(value)) return defaulted
				const knownKeys = Types.KeyResolver.ResolveKeys(schema2, { includePatterns: false })
				const knownProperties = knownKeys.reduce((value2, key) => {
					return key in value2
						? { ...value2, [key]: Visit(schema2.properties[key], references, value2[key]) }
						: value2
				}, defaulted)
				if (!Types.TypeGuard.TSchema(schema2.additionalProperties)) {
					return knownProperties
				}
				const unknownKeys = Object.getOwnPropertyNames(knownProperties)
				const additionalProperties = schema2.additionalProperties
				return unknownKeys.reduce((value2, key) => {
					return !knownKeys.includes(key)
						? { ...value2, [key]: Default(additionalProperties, value2[key]) }
						: value2
				}, knownProperties)
			}
			function TRecord(schema2, references, value) {
				const defaulted = Default(schema2, value)
				if (!(0, guard_1.IsPlainObject)(value)) return defaulted
				const pattern3 = Object.getOwnPropertyNames(schema2.patternProperties)[0]
				const knownKeys = new RegExp(pattern3)
				const knownProperties = Object.getOwnPropertyNames(value).reduce((value2, key) => {
					return knownKeys.test(key)
						? {
								...value2,
								[key]: Visit(schema2.patternProperties[pattern3], references, value2[key]),
						  }
						: value2
				}, defaulted)
				if (!Types.TypeGuard.TSchema(schema2.additionalProperties)) {
					return Default(schema2, knownProperties)
				}
				const unknownKeys = Object.getOwnPropertyNames(knownProperties)
				const additionalProperties = schema2.additionalProperties
				return unknownKeys.reduce((value2, key) => {
					return !knownKeys.test(key)
						? { ...value2, [key]: Default(additionalProperties, value2[key]) }
						: value2
				}, knownProperties)
			}
			function TRef(schema2, references, value) {
				const target = (0, deref_1.Deref)(schema2, references)
				const resolved = Visit(target, references, value)
				return Default(schema2, resolved)
			}
			function TThis(schema2, references, value) {
				const target = (0, deref_1.Deref)(schema2, references)
				const resolved = Visit(target, references, value)
				return Default(schema2, resolved)
			}
			function TTuple(schema2, references, value) {
				const value1 = Default(schema2, value)
				return (0, guard_1.IsArray)(schema2.items)
					? schema2.items.map((schema3, index2) => Visit(schema3, references, value1[index2]))
					: []
			}
			function TUnion(schema2, references, value) {
				for (const subschema of schema2.anyOf) {
					if (!(0, check_1.Check)(subschema, references, value)) continue
					const value1 = Visit(subschema, references, value)
					return Default(schema2, value1)
				}
				for (const subschema of schema2.anyOf) {
					const value1 = Visit(subschema, references, value)
					if (!(0, check_1.Check)(schema2, references, value1)) continue
					return Default(schema2, value1)
				}
				return Default(schema2, value)
			}
			function Visit(schema2, references, value) {
				const references_ = typeof schema2.$id === "string" ? [...references, schema2] : references
				const schema_ = schema2
				switch (schema2[Types.Kind]) {
					case "Array":
						return TArray(schema_, references_, value)
					case "Intersect":
						return TIntersect(schema_, references_, value)
					case "Not":
						return TNot(schema_, references_, value)
					case "Object":
						return TObject(schema_, references_, value)
					case "Record":
						return TRecord(schema_, references_, value)
					case "Ref":
						return TRef(schema_, references_, value)
					case "This":
						return TThis(schema_, references_, value)
					case "Tuple":
						return TTuple(schema_, references_, value)
					case "Union":
						return TUnion(schema_, references_, value)
					default:
						return Default(schema_, value)
				}
			}
			function Encode(schema2, references, value) {
				return Visit(schema2, references, value)
			}
			EncodeTransform2.Encode = Encode
		})(EncodeTransform || (exports2.EncodeTransform = EncodeTransform = {}))
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/value.js
var require_value = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/value.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.Value = void 0
		var ValueErrors = require_errors3()
		var ValueMutate = require_mutate()
		var ValueHash = require_hash2()
		var ValueEqual = require_equal()
		var ValueCast = require_cast()
		var ValueClone = require_clone()
		var ValueConvert = require_convert()
		var ValueCreate = require_create()
		var ValueCheck = require_check()
		var ValueDelta = require_delta()
		var ValueTransform = require_transform()
		var Value3
		;(function (Value4) {
			function Cast(...args) {
				return ValueCast.Cast.apply(ValueCast, args)
			}
			Value4.Cast = Cast
			function Create(...args) {
				return ValueCreate.Create.apply(ValueCreate, args)
			}
			Value4.Create = Create
			function Check(...args) {
				return ValueCheck.Check.apply(ValueCheck, args)
			}
			Value4.Check = Check
			function Convert(...args) {
				return ValueConvert.Convert.apply(ValueConvert, args)
			}
			Value4.Convert = Convert
			function Clone(value) {
				return ValueClone.Clone(value)
			}
			Value4.Clone = Clone
			function Decode(...args) {
				const [schema2, references, value] =
					args.length === 3 ? [args[0], args[1], args[2]] : [args[0], [], args[1]]
				if (!Check(schema2, references, value))
					throw new ValueTransform.TransformDecodeCheckError(
						schema2,
						value,
						Errors2(schema2, references, value).First()
					)
				return ValueTransform.DecodeTransform.Decode(schema2, references, value)
			}
			Value4.Decode = Decode
			function Encode(...args) {
				const [schema2, references, value] =
					args.length === 3 ? [args[0], args[1], args[2]] : [args[0], [], args[1]]
				const encoded = ValueTransform.EncodeTransform.Encode(schema2, references, value)
				if (!Check(schema2, references, encoded))
					throw new ValueTransform.TransformEncodeCheckError(
						schema2,
						value,
						Errors2(schema2, references, value).First()
					)
				return encoded
			}
			Value4.Encode = Encode
			function Errors2(...args) {
				return ValueErrors.Errors.apply(ValueErrors, args)
			}
			Value4.Errors = Errors2
			function Equal(left, right) {
				return ValueEqual.Equal(left, right)
			}
			Value4.Equal = Equal
			function Diff(current, next) {
				return ValueDelta.Diff(current, next)
			}
			Value4.Diff = Diff
			function Hash2(value) {
				return ValueHash.Hash(value)
			}
			Value4.Hash = Hash2
			function Patch(current, edits) {
				return ValueDelta.Patch(current, edits)
			}
			Value4.Patch = Patch
			function Mutate(current, next) {
				ValueMutate.Mutate(current, next)
			}
			Value4.Mutate = Mutate
		})(Value3 || (exports2.Value = Value3 = {}))
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/index.js
var require_value2 = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/value/index.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.Value =
			exports2.ValuePointer =
			exports2.Delete =
			exports2.Update =
			exports2.Insert =
			exports2.Edit =
			exports2.ValueErrorIterator =
			exports2.ValueErrorType =
				void 0
		var index_1 = require_errors3()
		Object.defineProperty(exports2, "ValueErrorType", {
			enumerable: true,
			get: function () {
				return index_1.ValueErrorType
			},
		})
		Object.defineProperty(exports2, "ValueErrorIterator", {
			enumerable: true,
			get: function () {
				return index_1.ValueErrorIterator
			},
		})
		var delta_1 = require_delta()
		Object.defineProperty(exports2, "Edit", {
			enumerable: true,
			get: function () {
				return delta_1.Edit
			},
		})
		Object.defineProperty(exports2, "Insert", {
			enumerable: true,
			get: function () {
				return delta_1.Insert
			},
		})
		Object.defineProperty(exports2, "Update", {
			enumerable: true,
			get: function () {
				return delta_1.Update
			},
		})
		Object.defineProperty(exports2, "Delete", {
			enumerable: true,
			get: function () {
				return delta_1.Delete
			},
		})
		var pointer_1 = require_pointer()
		Object.defineProperty(exports2, "ValuePointer", {
			enumerable: true,
			get: function () {
				return pointer_1.ValuePointer
			},
		})
		var value_1 = require_value()
		Object.defineProperty(exports2, "Value", {
			enumerable: true,
			get: function () {
				return value_1.Value
			},
		})
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/compiler/compiler.js
var require_compiler = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/compiler/compiler.js"(
		exports2
	) {
		"use strict"
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.TypeCompiler =
			exports2.Policy =
			exports2.TypeCompilerTypeGuardError =
			exports2.TypeCompilerUnknownTypeError =
			exports2.TypeCheck =
				void 0
		var transform_1 = require_transform()
		var guard_1 = require_guard()
		var errors_1 = require_errors2()
		var index_1 = require_system2()
		var deref_1 = require_deref()
		var hash_1 = require_hash2()
		var Types = require_typebox()
		var TypeCheck = class {
			constructor(schema2, references, checkFunc, code) {
				this.schema = schema2
				this.references = references
				this.checkFunc = checkFunc
				this.code = code
				this.hasTransform = transform_1.HasTransform.Has(schema2, references)
			}
			/** Returns the generated assertion code used to validate this type. */
			Code() {
				return this.code
			}
			/** Returns an iterator for each error in this value. */
			Errors(value) {
				return (0, errors_1.Errors)(this.schema, this.references, value)
			}
			/** Returns true if the value matches the compiled type. */
			Check(value) {
				return this.checkFunc(value)
			}
			/** Decodes a value or throws if error */
			Decode(value) {
				if (!this.checkFunc(value))
					throw new transform_1.TransformDecodeCheckError(
						this.schema,
						value,
						this.Errors(value).First()
					)
				return this.hasTransform
					? transform_1.DecodeTransform.Decode(this.schema, this.references, value)
					: value
			}
			/** Encodes a value or throws if error */
			Encode(value) {
				const encoded = this.hasTransform
					? transform_1.EncodeTransform.Encode(this.schema, this.references, value)
					: value
				if (!this.checkFunc(encoded))
					throw new transform_1.TransformEncodeCheckError(
						this.schema,
						value,
						this.Errors(value).First()
					)
				return encoded
			}
		}
		exports2.TypeCheck = TypeCheck
		var Character
		;(function (Character2) {
			function DollarSign(code) {
				return code === 36
			}
			Character2.DollarSign = DollarSign
			function IsUnderscore(code) {
				return code === 95
			}
			Character2.IsUnderscore = IsUnderscore
			function IsAlpha(code) {
				return (code >= 65 && code <= 90) || (code >= 97 && code <= 122)
			}
			Character2.IsAlpha = IsAlpha
			function IsNumeric(code) {
				return code >= 48 && code <= 57
			}
			Character2.IsNumeric = IsNumeric
		})(Character || (Character = {}))
		var MemberExpression
		;(function (MemberExpression2) {
			function IsFirstCharacterNumeric(value) {
				if (value.length === 0) return false
				return Character.IsNumeric(value.charCodeAt(0))
			}
			function IsAccessor(value) {
				if (IsFirstCharacterNumeric(value)) return false
				for (let i = 0; i < value.length; i++) {
					const code = value.charCodeAt(i)
					const check =
						Character.IsAlpha(code) ||
						Character.IsNumeric(code) ||
						Character.DollarSign(code) ||
						Character.IsUnderscore(code)
					if (!check) return false
				}
				return true
			}
			function EscapeHyphen(key) {
				return key.replace(/'/g, "\\'")
			}
			function Encode(object, key) {
				return IsAccessor(key) ? `${object}.${key}` : `${object}['${EscapeHyphen(key)}']`
			}
			MemberExpression2.Encode = Encode
		})(MemberExpression || (MemberExpression = {}))
		var Identifier
		;(function (Identifier2) {
			function Encode($id) {
				const buffer = []
				for (let i = 0; i < $id.length; i++) {
					const code = $id.charCodeAt(i)
					if (Character.IsNumeric(code) || Character.IsAlpha(code)) {
						buffer.push($id.charAt(i))
					} else {
						buffer.push(`_${code}_`)
					}
				}
				return buffer.join("").replace(/__/g, "_")
			}
			Identifier2.Encode = Encode
		})(Identifier || (Identifier = {}))
		var LiteralString
		;(function (LiteralString2) {
			function Escape(content) {
				return content.replace(/'/g, "\\'")
			}
			LiteralString2.Escape = Escape
		})(LiteralString || (LiteralString = {}))
		var TypeCompilerUnknownTypeError = class extends Types.TypeBoxError {
			constructor(schema2) {
				super("Unknown type")
				this.schema = schema2
			}
		}
		exports2.TypeCompilerUnknownTypeError = TypeCompilerUnknownTypeError
		var TypeCompilerTypeGuardError = class extends Types.TypeBoxError {
			constructor(schema2) {
				super("Preflight validation check failed to guard for the given schema")
				this.schema = schema2
			}
		}
		exports2.TypeCompilerTypeGuardError = TypeCompilerTypeGuardError
		var Policy
		;(function (Policy2) {
			function IsExactOptionalProperty(value, key, expression) {
				return index_1.TypeSystemPolicy.ExactOptionalPropertyTypes
					? `('${key}' in ${value} ? ${expression} : true)`
					: `(${MemberExpression.Encode(value, key)} !== undefined ? ${expression} : true)`
			}
			Policy2.IsExactOptionalProperty = IsExactOptionalProperty
			function IsObjectLike(value) {
				return !index_1.TypeSystemPolicy.AllowArrayObject
					? `(typeof ${value} === 'object' && ${value} !== null && !Array.isArray(${value}))`
					: `(typeof ${value} === 'object' && ${value} !== null)`
			}
			Policy2.IsObjectLike = IsObjectLike
			function IsRecordLike(value) {
				return !index_1.TypeSystemPolicy.AllowArrayObject
					? `(typeof ${value} === 'object' && ${value} !== null && !Array.isArray(${value}) && !(${value} instanceof Date) && !(${value} instanceof Uint8Array))`
					: `(typeof ${value} === 'object' && ${value} !== null && !(${value} instanceof Date) && !(${value} instanceof Uint8Array))`
			}
			Policy2.IsRecordLike = IsRecordLike
			function IsNumberLike(value) {
				return !index_1.TypeSystemPolicy.AllowNaN
					? `(typeof ${value} === 'number' && Number.isFinite(${value}))`
					: `typeof ${value} === 'number'`
			}
			Policy2.IsNumberLike = IsNumberLike
			function IsVoidLike(value) {
				return index_1.TypeSystemPolicy.AllowNullVoid
					? `(${value} === undefined || ${value} === null)`
					: `${value} === undefined`
			}
			Policy2.IsVoidLike = IsVoidLike
		})(Policy || (exports2.Policy = Policy = {}))
		var TypeCompiler4
		;(function (TypeCompiler5) {
			function IsAnyOrUnknown(schema2) {
				return schema2[Types.Kind] === "Any" || schema2[Types.Kind] === "Unknown"
			}
			function* TAny(schema2, references, value) {
				yield "true"
			}
			function* TArray(schema2, references, value) {
				yield `Array.isArray(${value})`
				const [parameter, accumulator] = [
					CreateParameter("value", "any"),
					CreateParameter("acc", "number"),
				]
				if ((0, guard_1.IsNumber)(schema2.maxItems)) yield `${value}.length <= ${schema2.maxItems}`
				if ((0, guard_1.IsNumber)(schema2.minItems)) yield `${value}.length >= ${schema2.minItems}`
				const elementExpression = CreateExpression(schema2.items, references, "value")
				yield `${value}.every((${parameter}) => ${elementExpression})`
				if (
					Types.TypeGuard.TSchema(schema2.contains) ||
					(0, guard_1.IsNumber)(schema2.minContains) ||
					(0, guard_1.IsNumber)(schema2.maxContains)
				) {
					const containsSchema = Types.TypeGuard.TSchema(schema2.contains)
						? schema2.contains
						: Types.Type.Never()
					const checkExpression = CreateExpression(containsSchema, references, "value")
					const checkMinContains = (0, guard_1.IsNumber)(schema2.minContains)
						? [`(count >= ${schema2.minContains})`]
						: []
					const checkMaxContains = (0, guard_1.IsNumber)(schema2.maxContains)
						? [`(count <= ${schema2.maxContains})`]
						: []
					const checkCount = `const count = value.reduce((${accumulator}, ${parameter}) => ${checkExpression} ? acc + 1 : acc, 0)`
					const check = [`(count > 0)`, ...checkMinContains, ...checkMaxContains].join(" && ")
					yield `((${parameter}) => { ${checkCount}; return ${check}})(${value})`
				}
				if (schema2.uniqueItems === true) {
					const check = `const hashed = hash(element); if(set.has(hashed)) { return false } else { set.add(hashed) } } return true`
					const block = `const set = new Set(); for(const element of value) { ${check} }`
					yield `((${parameter}) => { ${block} )(${value})`
				}
			}
			function* TAsyncIterator(schema2, references, value) {
				yield `(typeof value === 'object' && Symbol.asyncIterator in ${value})`
			}
			function* TBigInt(schema2, references, value) {
				yield `(typeof ${value} === 'bigint')`
				if ((0, guard_1.IsBigInt)(schema2.exclusiveMaximum))
					yield `${value} < BigInt(${schema2.exclusiveMaximum})`
				if ((0, guard_1.IsBigInt)(schema2.exclusiveMinimum))
					yield `${value} > BigInt(${schema2.exclusiveMinimum})`
				if ((0, guard_1.IsBigInt)(schema2.maximum)) yield `${value} <= BigInt(${schema2.maximum})`
				if ((0, guard_1.IsBigInt)(schema2.minimum)) yield `${value} >= BigInt(${schema2.minimum})`
				if ((0, guard_1.IsBigInt)(schema2.multipleOf))
					yield `(${value} % BigInt(${schema2.multipleOf})) === 0`
			}
			function* TBoolean(schema2, references, value) {
				yield `(typeof ${value} === 'boolean')`
			}
			function* TConstructor(schema2, references, value) {
				yield* Visit(schema2.returns, references, `${value}.prototype`)
			}
			function* TDate(schema2, references, value) {
				yield `(${value} instanceof Date) && Number.isFinite(${value}.getTime())`
				if ((0, guard_1.IsNumber)(schema2.exclusiveMaximumTimestamp))
					yield `${value}.getTime() < ${schema2.exclusiveMaximumTimestamp}`
				if ((0, guard_1.IsNumber)(schema2.exclusiveMinimumTimestamp))
					yield `${value}.getTime() > ${schema2.exclusiveMinimumTimestamp}`
				if ((0, guard_1.IsNumber)(schema2.maximumTimestamp))
					yield `${value}.getTime() <= ${schema2.maximumTimestamp}`
				if ((0, guard_1.IsNumber)(schema2.minimumTimestamp))
					yield `${value}.getTime() >= ${schema2.minimumTimestamp}`
				if ((0, guard_1.IsNumber)(schema2.multipleOfTimestamp))
					yield `(${value}.getTime() % ${schema2.multipleOfTimestamp}) === 0`
			}
			function* TFunction(schema2, references, value) {
				yield `(typeof ${value} === 'function')`
			}
			function* TInteger(schema2, references, value) {
				yield `(typeof ${value} === 'number' && Number.isInteger(${value}))`
				if ((0, guard_1.IsNumber)(schema2.exclusiveMaximum))
					yield `${value} < ${schema2.exclusiveMaximum}`
				if ((0, guard_1.IsNumber)(schema2.exclusiveMinimum))
					yield `${value} > ${schema2.exclusiveMinimum}`
				if ((0, guard_1.IsNumber)(schema2.maximum)) yield `${value} <= ${schema2.maximum}`
				if ((0, guard_1.IsNumber)(schema2.minimum)) yield `${value} >= ${schema2.minimum}`
				if ((0, guard_1.IsNumber)(schema2.multipleOf))
					yield `(${value} % ${schema2.multipleOf}) === 0`
			}
			function* TIntersect(schema2, references, value) {
				const check1 = schema2.allOf
					.map((schema3) => CreateExpression(schema3, references, value))
					.join(" && ")
				if (schema2.unevaluatedProperties === false) {
					const keyCheck = CreateVariable(
						`${new RegExp(Types.KeyResolver.ResolvePattern(schema2))};`
					)
					const check2 = `Object.getOwnPropertyNames(${value}).every(key => ${keyCheck}.test(key))`
					yield `(${check1} && ${check2})`
				} else if (Types.TypeGuard.TSchema(schema2.unevaluatedProperties)) {
					const keyCheck = CreateVariable(
						`${new RegExp(Types.KeyResolver.ResolvePattern(schema2))};`
					)
					const check2 = `Object.getOwnPropertyNames(${value}).every(key => ${keyCheck}.test(key) || ${CreateExpression(
						schema2.unevaluatedProperties,
						references,
						`${value}[key]`
					)})`
					yield `(${check1} && ${check2})`
				} else {
					yield `(${check1})`
				}
			}
			function* TIterator(schema2, references, value) {
				yield `(typeof value === 'object' && Symbol.iterator in ${value})`
			}
			function* TLiteral(schema2, references, value) {
				if (typeof schema2.const === "number" || typeof schema2.const === "boolean") {
					yield `(${value} === ${schema2.const})`
				} else {
					yield `(${value} === '${LiteralString.Escape(schema2.const)}')`
				}
			}
			function* TNever(schema2, references, value) {
				yield `false`
			}
			function* TNot(schema2, references, value) {
				const expression = CreateExpression(schema2.not, references, value)
				yield `(!${expression})`
			}
			function* TNull(schema2, references, value) {
				yield `(${value} === null)`
			}
			function* TNumber(schema2, references, value) {
				yield Policy.IsNumberLike(value)
				if ((0, guard_1.IsNumber)(schema2.exclusiveMaximum))
					yield `${value} < ${schema2.exclusiveMaximum}`
				if ((0, guard_1.IsNumber)(schema2.exclusiveMinimum))
					yield `${value} > ${schema2.exclusiveMinimum}`
				if ((0, guard_1.IsNumber)(schema2.maximum)) yield `${value} <= ${schema2.maximum}`
				if ((0, guard_1.IsNumber)(schema2.minimum)) yield `${value} >= ${schema2.minimum}`
				if ((0, guard_1.IsNumber)(schema2.multipleOf))
					yield `(${value} % ${schema2.multipleOf}) === 0`
			}
			function* TObject(schema2, references, value) {
				yield Policy.IsObjectLike(value)
				if ((0, guard_1.IsNumber)(schema2.minProperties))
					yield `Object.getOwnPropertyNames(${value}).length >= ${schema2.minProperties}`
				if ((0, guard_1.IsNumber)(schema2.maxProperties))
					yield `Object.getOwnPropertyNames(${value}).length <= ${schema2.maxProperties}`
				const knownKeys = Object.getOwnPropertyNames(schema2.properties)
				for (const knownKey of knownKeys) {
					const memberExpression = MemberExpression.Encode(value, knownKey)
					const property = schema2.properties[knownKey]
					if (schema2.required && schema2.required.includes(knownKey)) {
						yield* Visit(property, references, memberExpression)
						if (Types.ExtendsUndefined.Check(property) || IsAnyOrUnknown(property))
							yield `('${knownKey}' in ${value})`
					} else {
						const expression = CreateExpression(property, references, memberExpression)
						yield Policy.IsExactOptionalProperty(value, knownKey, expression)
					}
				}
				if (schema2.additionalProperties === false) {
					if (schema2.required && schema2.required.length === knownKeys.length) {
						yield `Object.getOwnPropertyNames(${value}).length === ${knownKeys.length}`
					} else {
						const keys = `[${knownKeys.map((key) => `'${key}'`).join(", ")}]`
						yield `Object.getOwnPropertyNames(${value}).every(key => ${keys}.includes(key))`
					}
				}
				if (typeof schema2.additionalProperties === "object") {
					const expression = CreateExpression(
						schema2.additionalProperties,
						references,
						`${value}[key]`
					)
					const keys = `[${knownKeys.map((key) => `'${key}'`).join(", ")}]`
					yield `(Object.getOwnPropertyNames(${value}).every(key => ${keys}.includes(key) || ${expression}))`
				}
			}
			function* TPromise(schema2, references, value) {
				yield `(typeof value === 'object' && typeof ${value}.then === 'function')`
			}
			function* TRecord(schema2, references, value) {
				yield Policy.IsRecordLike(value)
				if ((0, guard_1.IsNumber)(schema2.minProperties))
					yield `Object.getOwnPropertyNames(${value}).length >= ${schema2.minProperties}`
				if ((0, guard_1.IsNumber)(schema2.maxProperties))
					yield `Object.getOwnPropertyNames(${value}).length <= ${schema2.maxProperties}`
				const [patternKey, patternSchema] = Object.entries(schema2.patternProperties)[0]
				const variable = CreateVariable(`${new RegExp(patternKey)}`)
				const check1 = CreateExpression(patternSchema, references, "value")
				const check2 = Types.TypeGuard.TSchema(schema2.additionalProperties)
					? CreateExpression(schema2.additionalProperties, references, value)
					: schema2.additionalProperties === false
					? "false"
					: "true"
				const expression = `(${variable}.test(key) ? ${check1} : ${check2})`
				yield `(Object.entries(${value}).every(([key, value]) => ${expression}))`
			}
			function* TRef(schema2, references, value) {
				const target = (0, deref_1.Deref)(schema2, references)
				if (state.functions.has(schema2.$ref))
					return yield `${CreateFunctionName(schema2.$ref)}(${value})`
				yield* Visit(target, references, value)
			}
			function* TString(schema2, references, value) {
				yield `(typeof ${value} === 'string')`
				if ((0, guard_1.IsNumber)(schema2.maxLength))
					yield `${value}.length <= ${schema2.maxLength}`
				if ((0, guard_1.IsNumber)(schema2.minLength))
					yield `${value}.length >= ${schema2.minLength}`
				if (schema2.pattern !== void 0) {
					const variable = CreateVariable(`${new RegExp(schema2.pattern)};`)
					yield `${variable}.test(${value})`
				}
				if (schema2.format !== void 0) {
					yield `format('${schema2.format}', ${value})`
				}
			}
			function* TSymbol(schema2, references, value) {
				yield `(typeof ${value} === 'symbol')`
			}
			function* TTemplateLiteral(schema2, references, value) {
				yield `(typeof ${value} === 'string')`
				const variable = CreateVariable(`${new RegExp(schema2.pattern)};`)
				yield `${variable}.test(${value})`
			}
			function* TThis(schema2, references, value) {
				yield `${CreateFunctionName(schema2.$ref)}(${value})`
			}
			function* TTuple(schema2, references, value) {
				yield `Array.isArray(${value})`
				if (schema2.items === void 0) return yield `${value}.length === 0`
				yield `(${value}.length === ${schema2.maxItems})`
				for (let i = 0; i < schema2.items.length; i++) {
					const expression = CreateExpression(schema2.items[i], references, `${value}[${i}]`)
					yield `${expression}`
				}
			}
			function* TUndefined(schema2, references, value) {
				yield `${value} === undefined`
			}
			function* TUnion(schema2, references, value) {
				const expressions = schema2.anyOf.map((schema3) =>
					CreateExpression(schema3, references, value)
				)
				yield `(${expressions.join(" || ")})`
			}
			function* TUint8Array(schema2, references, value) {
				yield `${value} instanceof Uint8Array`
				if ((0, guard_1.IsNumber)(schema2.maxByteLength))
					yield `(${value}.length <= ${schema2.maxByteLength})`
				if ((0, guard_1.IsNumber)(schema2.minByteLength))
					yield `(${value}.length >= ${schema2.minByteLength})`
			}
			function* TUnknown(schema2, references, value) {
				yield "true"
			}
			function* TVoid(schema2, references, value) {
				yield Policy.IsVoidLike(value)
			}
			function* TKind(schema2, references, value) {
				const instance = state.instances.size
				state.instances.set(instance, schema2)
				yield `kind('${schema2[Types.Kind]}', ${instance}, ${value})`
			}
			function* Visit(schema2, references, value, useHoisting = true) {
				const references_ = (0, guard_1.IsString)(schema2.$id)
					? [...references, schema2]
					: references
				const schema_ = schema2
				if (useHoisting && (0, guard_1.IsString)(schema2.$id)) {
					const functionName = CreateFunctionName(schema2.$id)
					if (state.functions.has(functionName)) {
						return yield `${functionName}(${value})`
					} else {
						const functionCode = CreateFunction(functionName, schema2, references, "value", false)
						state.functions.set(functionName, functionCode)
						return yield `${functionName}(${value})`
					}
				}
				switch (schema_[Types.Kind]) {
					case "Any":
						return yield* TAny(schema_, references_, value)
					case "Array":
						return yield* TArray(schema_, references_, value)
					case "AsyncIterator":
						return yield* TAsyncIterator(schema_, references_, value)
					case "BigInt":
						return yield* TBigInt(schema_, references_, value)
					case "Boolean":
						return yield* TBoolean(schema_, references_, value)
					case "Constructor":
						return yield* TConstructor(schema_, references_, value)
					case "Date":
						return yield* TDate(schema_, references_, value)
					case "Function":
						return yield* TFunction(schema_, references_, value)
					case "Integer":
						return yield* TInteger(schema_, references_, value)
					case "Intersect":
						return yield* TIntersect(schema_, references_, value)
					case "Iterator":
						return yield* TIterator(schema_, references_, value)
					case "Literal":
						return yield* TLiteral(schema_, references_, value)
					case "Never":
						return yield* TNever(schema_, references_, value)
					case "Not":
						return yield* TNot(schema_, references_, value)
					case "Null":
						return yield* TNull(schema_, references_, value)
					case "Number":
						return yield* TNumber(schema_, references_, value)
					case "Object":
						return yield* TObject(schema_, references_, value)
					case "Promise":
						return yield* TPromise(schema_, references_, value)
					case "Record":
						return yield* TRecord(schema_, references_, value)
					case "Ref":
						return yield* TRef(schema_, references_, value)
					case "String":
						return yield* TString(schema_, references_, value)
					case "Symbol":
						return yield* TSymbol(schema_, references_, value)
					case "TemplateLiteral":
						return yield* TTemplateLiteral(schema_, references_, value)
					case "This":
						return yield* TThis(schema_, references_, value)
					case "Tuple":
						return yield* TTuple(schema_, references_, value)
					case "Undefined":
						return yield* TUndefined(schema_, references_, value)
					case "Union":
						return yield* TUnion(schema_, references_, value)
					case "Uint8Array":
						return yield* TUint8Array(schema_, references_, value)
					case "Unknown":
						return yield* TUnknown(schema_, references_, value)
					case "Void":
						return yield* TVoid(schema_, references_, value)
					default:
						if (!Types.TypeRegistry.Has(schema_[Types.Kind]))
							throw new TypeCompilerUnknownTypeError(schema2)
						return yield* TKind(schema_, references_, value)
				}
			}
			const state = {
				language: "javascript",
				functions: /* @__PURE__ */ new Map(),
				variables: /* @__PURE__ */ new Map(),
				instances: /* @__PURE__ */ new Map(),
				// exterior kind instances
			}
			function CreateExpression(schema2, references, value, useHoisting = true) {
				return `(${[...Visit(schema2, references, value, useHoisting)].join(" && ")})`
			}
			function CreateFunctionName($id) {
				return `check_${Identifier.Encode($id)}`
			}
			function CreateVariable(expression) {
				const variableName = `local_${state.variables.size}`
				state.variables.set(variableName, `const ${variableName} = ${expression}`)
				return variableName
			}
			function CreateFunction(name, schema2, references, value, useHoisting = true) {
				const [newline, pad] = ["\n", (length) => "".padStart(length, " ")]
				const parameter = CreateParameter("value", "any")
				const returns = CreateReturns("boolean")
				const expression = [...Visit(schema2, references, value, useHoisting)]
					.map((expression2) => `${pad(4)}${expression2}`)
					.join(` &&${newline}`)
				return `function ${name}(${parameter})${returns} {${newline}${pad(
					2
				)}return (${newline}${expression}${newline}${pad(2)})
}`
			}
			function CreateParameter(name, type) {
				const annotation = state.language === "typescript" ? `: ${type}` : ""
				return `${name}${annotation}`
			}
			function CreateReturns(type) {
				return state.language === "typescript" ? `: ${type}` : ""
			}
			function Build(schema2, references, options) {
				const functionCode = CreateFunction("check", schema2, references, "value")
				const parameter = CreateParameter("value", "any")
				const returns = CreateReturns("boolean")
				const functions = [...state.functions.values()]
				const variables = [...state.variables.values()]
				const checkFunction = (0, guard_1.IsString)(schema2.$id)
					? `return function check(${parameter})${returns} {
  return ${CreateFunctionName(schema2.$id)}(value)
}`
					: `return ${functionCode}`
				return [...variables, ...functions, checkFunction].join("\n")
			}
			function Code(...args) {
				const defaults = { language: "javascript" }
				const [schema2, references, options] =
					args.length === 2 && (0, guard_1.IsArray)(args[1])
						? [args[0], args[1], defaults]
						: args.length === 2 && !(0, guard_1.IsArray)(args[1])
						? [args[0], [], args[1]]
						: args.length === 3
						? [args[0], args[1], args[2]]
						: args.length === 1
						? [args[0], [], defaults]
						: [null, [], defaults]
				state.language = options.language
				state.variables.clear()
				state.functions.clear()
				state.instances.clear()
				if (!Types.TypeGuard.TSchema(schema2)) throw new TypeCompilerTypeGuardError(schema2)
				for (const schema3 of references)
					if (!Types.TypeGuard.TSchema(schema3)) throw new TypeCompilerTypeGuardError(schema3)
				return Build(schema2, references, options)
			}
			TypeCompiler5.Code = Code
			function Compile(schema2, references = []) {
				const generatedCode = Code(schema2, references, { language: "javascript" })
				const compiledFunction = globalThis.Function("kind", "format", "hash", generatedCode)
				const instances = new Map(state.instances)
				function typeRegistryFunction(kind, instance, value) {
					if (!Types.TypeRegistry.Has(kind) || !instances.has(instance)) return false
					const checkFunc = Types.TypeRegistry.Get(kind)
					const schema3 = instances.get(instance)
					return checkFunc(schema3, value)
				}
				function formatRegistryFunction(format, value) {
					if (!Types.FormatRegistry.Has(format)) return false
					const checkFunc = Types.FormatRegistry.Get(format)
					return checkFunc(value)
				}
				function hashFunction(value) {
					return (0, hash_1.Hash)(value)
				}
				const checkFunction = compiledFunction(
					typeRegistryFunction,
					formatRegistryFunction,
					hashFunction
				)
				return new TypeCheck(schema2, references, checkFunction, generatedCode)
			}
			TypeCompiler5.Compile = Compile
		})(TypeCompiler4 || (exports2.TypeCompiler = TypeCompiler4 = {}))
	},
})

// ../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/compiler/index.js
var require_compiler2 = __commonJS({
	"../../../node_modules/.pnpm/@sinclair+typebox@0.31.28/node_modules/@sinclair/typebox/compiler/index.js"(
		exports2
	) {
		"use strict"
		var __createBinding =
			(exports2 && exports2.__createBinding) ||
			(Object.create
				? function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						var desc = Object.getOwnPropertyDescriptor(m, k)
						if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
							desc = {
								enumerable: true,
								get: function () {
									return m[k]
								},
							}
						}
						Object.defineProperty(o, k2, desc)
				  }
				: function (o, m, k, k2) {
						if (k2 === void 0) k2 = k
						o[k2] = m[k]
				  })
		var __exportStar =
			(exports2 && exports2.__exportStar) ||
			function (m, exports3) {
				for (var p in m)
					if (p !== "default" && !Object.prototype.hasOwnProperty.call(exports3, p))
						__createBinding(exports3, m, p)
			}
		Object.defineProperty(exports2, "__esModule", { value: true })
		exports2.ValueErrorIterator = exports2.ValueErrorType = void 0
		var index_1 = require_errors3()
		Object.defineProperty(exports2, "ValueErrorType", {
			enumerable: true,
			get: function () {
				return index_1.ValueErrorType
			},
		})
		Object.defineProperty(exports2, "ValueErrorIterator", {
			enumerable: true,
			get: function () {
				return index_1.ValueErrorIterator
			},
		})
		__exportStar(require_compiler(), exports2)
	},
})

// ../../../node_modules/.pnpm/ms@2.1.2/node_modules/ms/index.js
var require_ms2 = __commonJS({
	"../../../node_modules/.pnpm/ms@2.1.2/node_modules/ms/index.js"(exports2, module2) {
		var s = 1e3
		var m = s * 60
		var h = m * 60
		var d = h * 24
		var w = d * 7
		var y = d * 365.25
		module2.exports = function (val, options) {
			options = options || {}
			var type = typeof val
			if (type === "string" && val.length > 0) {
				return parse2(val)
			} else if (type === "number" && isFinite(val)) {
				return options.long ? fmtLong(val) : fmtShort(val)
			}
			throw new Error("val is not a non-empty string or a valid number. val=" + JSON.stringify(val))
		}
		function parse2(str) {
			str = String(str)
			if (str.length > 100) {
				return
			}
			var match =
				/^(-?(?:\d+)?\.?\d+) *(milliseconds?|msecs?|ms|seconds?|secs?|s|minutes?|mins?|m|hours?|hrs?|h|days?|d|weeks?|w|years?|yrs?|y)?$/i.exec(
					str
				)
			if (!match) {
				return
			}
			var n = parseFloat(match[1])
			var type = (match[2] || "ms").toLowerCase()
			switch (type) {
				case "years":
				case "year":
				case "yrs":
				case "yr":
				case "y":
					return n * y
				case "weeks":
				case "week":
				case "w":
					return n * w
				case "days":
				case "day":
				case "d":
					return n * d
				case "hours":
				case "hour":
				case "hrs":
				case "hr":
				case "h":
					return n * h
				case "minutes":
				case "minute":
				case "mins":
				case "min":
				case "m":
					return n * m
				case "seconds":
				case "second":
				case "secs":
				case "sec":
				case "s":
					return n * s
				case "milliseconds":
				case "millisecond":
				case "msecs":
				case "msec":
				case "ms":
					return n
				default:
					return void 0
			}
		}
		function fmtShort(ms) {
			var msAbs = Math.abs(ms)
			if (msAbs >= d) {
				return Math.round(ms / d) + "d"
			}
			if (msAbs >= h) {
				return Math.round(ms / h) + "h"
			}
			if (msAbs >= m) {
				return Math.round(ms / m) + "m"
			}
			if (msAbs >= s) {
				return Math.round(ms / s) + "s"
			}
			return ms + "ms"
		}
		function fmtLong(ms) {
			var msAbs = Math.abs(ms)
			if (msAbs >= d) {
				return plural(ms, msAbs, d, "day")
			}
			if (msAbs >= h) {
				return plural(ms, msAbs, h, "hour")
			}
			if (msAbs >= m) {
				return plural(ms, msAbs, m, "minute")
			}
			if (msAbs >= s) {
				return plural(ms, msAbs, s, "second")
			}
			return ms + " ms"
		}
		function plural(ms, msAbs, n, name) {
			var isPlural = msAbs >= n * 1.5
			return Math.round(ms / n) + " " + name + (isPlural ? "s" : "")
		}
	},
})

// ../../../node_modules/.pnpm/debug@4.3.5/node_modules/debug/src/common.js
var require_common2 = __commonJS({
	"../../../node_modules/.pnpm/debug@4.3.5/node_modules/debug/src/common.js"(exports2, module2) {
		function setup(env) {
			createDebug.debug = createDebug
			createDebug.default = createDebug
			createDebug.coerce = coerce
			createDebug.disable = disable
			createDebug.enable = enable
			createDebug.enabled = enabled
			createDebug.humanize = require_ms2()
			createDebug.destroy = destroy
			for (const key of Object.keys(env)) {
				createDebug[key] = env[key]
			}
			createDebug.names = []
			createDebug.skips = []
			createDebug.formatters = {}
			function selectColor(namespace) {
				let hash2 = 0
				for (let i = 0; i < namespace.length; i++) {
					hash2 = (hash2 << 5) - hash2 + namespace.charCodeAt(i)
					hash2 |= 0
				}
				return createDebug.colors[Math.abs(hash2) % createDebug.colors.length]
			}
			createDebug.selectColor = selectColor
			function createDebug(namespace) {
				let prevTime
				let enableOverride = null
				let namespacesCache
				let enabledCache
				function debug10(...args) {
					if (!debug10.enabled) {
						return
					}
					const self2 = debug10
					const curr = Number(/* @__PURE__ */ new Date())
					const ms = curr - (prevTime || curr)
					self2.diff = ms
					self2.prev = prevTime
					self2.curr = curr
					prevTime = curr
					args[0] = createDebug.coerce(args[0])
					if (typeof args[0] !== "string") {
						args.unshift("%O")
					}
					let index2 = 0
					args[0] = args[0].replace(/%([a-zA-Z%])/g, (match, format) => {
						if (match === "%%") {
							return "%"
						}
						index2++
						const formatter = createDebug.formatters[format]
						if (typeof formatter === "function") {
							const val = args[index2]
							match = formatter.call(self2, val)
							args.splice(index2, 1)
							index2--
						}
						return match
					})
					createDebug.formatArgs.call(self2, args)
					const logFn = self2.log || createDebug.log
					logFn.apply(self2, args)
				}
				debug10.namespace = namespace
				debug10.useColors = createDebug.useColors()
				debug10.color = createDebug.selectColor(namespace)
				debug10.extend = extend
				debug10.destroy = createDebug.destroy
				Object.defineProperty(debug10, "enabled", {
					enumerable: true,
					configurable: false,
					get: () => {
						if (enableOverride !== null) {
							return enableOverride
						}
						if (namespacesCache !== createDebug.namespaces) {
							namespacesCache = createDebug.namespaces
							enabledCache = createDebug.enabled(namespace)
						}
						return enabledCache
					},
					set: (v) => {
						enableOverride = v
					},
				})
				if (typeof createDebug.init === "function") {
					createDebug.init(debug10)
				}
				return debug10
			}
			function extend(namespace, delimiter) {
				const newDebug = createDebug(
					this.namespace + (typeof delimiter === "undefined" ? ":" : delimiter) + namespace
				)
				newDebug.log = this.log
				return newDebug
			}
			function enable(namespaces) {
				createDebug.save(namespaces)
				createDebug.namespaces = namespaces
				createDebug.names = []
				createDebug.skips = []
				let i
				const split = (typeof namespaces === "string" ? namespaces : "").split(/[\s,]+/)
				const len = split.length
				for (i = 0; i < len; i++) {
					if (!split[i]) {
						continue
					}
					namespaces = split[i].replace(/\*/g, ".*?")
					if (namespaces[0] === "-") {
						createDebug.skips.push(new RegExp("^" + namespaces.slice(1) + "$"))
					} else {
						createDebug.names.push(new RegExp("^" + namespaces + "$"))
					}
				}
			}
			function disable() {
				const namespaces = [
					...createDebug.names.map(toNamespace),
					...createDebug.skips.map(toNamespace).map((namespace) => "-" + namespace),
				].join(",")
				createDebug.enable("")
				return namespaces
			}
			function enabled(name) {
				if (name.at(-1) === "*") {
					return true
				}
				let i
				let len
				for (i = 0, len = createDebug.skips.length; i < len; i++) {
					if (createDebug.skips[i].test(name)) {
						return false
					}
				}
				for (i = 0, len = createDebug.names.length; i < len; i++) {
					if (createDebug.names[i].test(name)) {
						return true
					}
				}
				return false
			}
			function toNamespace(regexp) {
				return regexp
					.toString()
					.substring(2, regexp.toString().length - 2)
					.replace(/\.\*\?$/, "*")
			}
			function coerce(val) {
				if (val instanceof Error) {
					return val.stack || val.message
				}
				return val
			}
			function destroy() {
				console.warn(
					"Instance method `debug.destroy()` is deprecated and no longer does anything. It will be removed in the next major version of `debug`."
				)
			}
			createDebug.enable(createDebug.load())
			return createDebug
		}
		module2.exports = setup
	},
})

// ../../../node_modules/.pnpm/debug@4.3.5/node_modules/debug/src/browser.js
var require_browser = __commonJS({
	"../../../node_modules/.pnpm/debug@4.3.5/node_modules/debug/src/browser.js"(exports2, module2) {
		exports2.formatArgs = formatArgs
		exports2.save = save
		exports2.load = load
		exports2.useColors = useColors
		exports2.storage = localstorage()
		exports2.destroy = /* @__PURE__ */ (() => {
			let warned = false
			return () => {
				if (!warned) {
					warned = true
					console.warn(
						"Instance method `debug.destroy()` is deprecated and no longer does anything. It will be removed in the next major version of `debug`."
					)
				}
			}
		})()
		exports2.colors = [
			"#0000CC",
			"#0000FF",
			"#0033CC",
			"#0033FF",
			"#0066CC",
			"#0066FF",
			"#0099CC",
			"#0099FF",
			"#00CC00",
			"#00CC33",
			"#00CC66",
			"#00CC99",
			"#00CCCC",
			"#00CCFF",
			"#3300CC",
			"#3300FF",
			"#3333CC",
			"#3333FF",
			"#3366CC",
			"#3366FF",
			"#3399CC",
			"#3399FF",
			"#33CC00",
			"#33CC33",
			"#33CC66",
			"#33CC99",
			"#33CCCC",
			"#33CCFF",
			"#6600CC",
			"#6600FF",
			"#6633CC",
			"#6633FF",
			"#66CC00",
			"#66CC33",
			"#9900CC",
			"#9900FF",
			"#9933CC",
			"#9933FF",
			"#99CC00",
			"#99CC33",
			"#CC0000",
			"#CC0033",
			"#CC0066",
			"#CC0099",
			"#CC00CC",
			"#CC00FF",
			"#CC3300",
			"#CC3333",
			"#CC3366",
			"#CC3399",
			"#CC33CC",
			"#CC33FF",
			"#CC6600",
			"#CC6633",
			"#CC9900",
			"#CC9933",
			"#CCCC00",
			"#CCCC33",
			"#FF0000",
			"#FF0033",
			"#FF0066",
			"#FF0099",
			"#FF00CC",
			"#FF00FF",
			"#FF3300",
			"#FF3333",
			"#FF3366",
			"#FF3399",
			"#FF33CC",
			"#FF33FF",
			"#FF6600",
			"#FF6633",
			"#FF9900",
			"#FF9933",
			"#FFCC00",
			"#FFCC33",
		]
		function useColors() {
			if (
				typeof window !== "undefined" &&
				window.process &&
				(window.process.type === "renderer" || window.process.__nwjs)
			) {
				return true
			}
			if (
				typeof navigator !== "undefined" &&
				navigator.userAgent &&
				navigator.userAgent.toLowerCase().match(/(edge|trident)\/(\d+)/)
			) {
				return false
			}
			return (
				(typeof document !== "undefined" &&
					document.documentElement &&
					document.documentElement.style &&
					document.documentElement.style.WebkitAppearance) || // Is firebug? http://stackoverflow.com/a/398120/376773
				(typeof window !== "undefined" &&
					window.console &&
					(window.console.firebug || (window.console.exception && window.console.table))) || // Is firefox >= v31?
				// https://developer.mozilla.org/en-US/docs/Tools/Web_Console#Styling_messages
				(typeof navigator !== "undefined" &&
					navigator.userAgent &&
					navigator.userAgent.toLowerCase().match(/firefox\/(\d+)/) &&
					parseInt(RegExp.$1, 10) >= 31) || // Double check webkit in userAgent just in case we are in a worker
				(typeof navigator !== "undefined" &&
					navigator.userAgent &&
					navigator.userAgent.toLowerCase().match(/applewebkit\/(\d+)/))
			)
		}
		function formatArgs(args) {
			args[0] =
				(this.useColors ? "%c" : "") +
				this.namespace +
				(this.useColors ? " %c" : " ") +
				args[0] +
				(this.useColors ? "%c " : " ") +
				"+" +
				module2.exports.humanize(this.diff)
			if (!this.useColors) {
				return
			}
			const c = "color: " + this.color
			args.splice(1, 0, c, "color: inherit")
			let index2 = 0
			let lastC = 0
			args[0].replace(/%[a-zA-Z%]/g, (match) => {
				if (match === "%%") {
					return
				}
				index2++
				if (match === "%c") {
					lastC = index2
				}
			})
			args.splice(lastC, 0, c)
		}
		exports2.log = console.debug || console.log || (() => {})
		function save(namespaces) {
			try {
				if (namespaces) {
					exports2.storage.setItem("debug", namespaces)
				} else {
					exports2.storage.removeItem("debug")
				}
			} catch (error) {}
		}
		function load() {
			let r
			try {
				r = exports2.storage.getItem("debug")
			} catch (error) {}
			if (!r && typeof process !== "undefined" && "env" in process) {
				r = process.env.DEBUG
			}
			return r
		}
		function localstorage() {
			try {
				return localStorage
			} catch (error) {}
		}
		module2.exports = require_common2()(exports2)
		var { formatters } = module2.exports
		formatters.j = function (v) {
			try {
				return JSON.stringify(v)
			} catch (error) {
				return "[UnexpectedJSONParseError]: " + error.message
			}
		}
	},
})

// ../../../node_modules/.pnpm/has-flag@4.0.0/node_modules/has-flag/index.js
var require_has_flag = __commonJS({
	"../../../node_modules/.pnpm/has-flag@4.0.0/node_modules/has-flag/index.js"(exports2, module2) {
		"use strict"
		module2.exports = (flag, argv = process.argv) => {
			const prefix = flag.startsWith("-") ? "" : flag.length === 1 ? "-" : "--"
			const position = argv.indexOf(prefix + flag)
			const terminatorPosition = argv.indexOf("--")
			return position !== -1 && (terminatorPosition === -1 || position < terminatorPosition)
		}
	},
})

// ../../../node_modules/.pnpm/supports-color@8.1.1/node_modules/supports-color/index.js
var require_supports_color = __commonJS({
	"../../../node_modules/.pnpm/supports-color@8.1.1/node_modules/supports-color/index.js"(
		exports2,
		module2
	) {
		"use strict"
		var os = require("node:os")
		var tty = require("node:tty")
		var hasFlag = require_has_flag()
		var { env } = process
		var flagForceColor
		if (
			hasFlag("no-color") ||
			hasFlag("no-colors") ||
			hasFlag("color=false") ||
			hasFlag("color=never")
		) {
			flagForceColor = 0
		} else if (
			hasFlag("color") ||
			hasFlag("colors") ||
			hasFlag("color=true") ||
			hasFlag("color=always")
		) {
			flagForceColor = 1
		}
		function envForceColor() {
			if ("FORCE_COLOR" in env) {
				if (env.FORCE_COLOR === "true") {
					return 1
				}
				if (env.FORCE_COLOR === "false") {
					return 0
				}
				return env.FORCE_COLOR.length === 0 ? 1 : Math.min(Number.parseInt(env.FORCE_COLOR, 10), 3)
			}
		}
		function translateLevel(level) {
			if (level === 0) {
				return false
			}
			return {
				level,
				hasBasic: true,
				has256: level >= 2,
				has16m: level >= 3,
			}
		}
		function supportsColor(haveStream, { streamIsTTY, sniffFlags = true } = {}) {
			const noFlagForceColor = envForceColor()
			if (noFlagForceColor !== void 0) {
				flagForceColor = noFlagForceColor
			}
			const forceColor = sniffFlags ? flagForceColor : noFlagForceColor
			if (forceColor === 0) {
				return 0
			}
			if (sniffFlags) {
				if (hasFlag("color=16m") || hasFlag("color=full") || hasFlag("color=truecolor")) {
					return 3
				}
				if (hasFlag("color=256")) {
					return 2
				}
			}
			if (haveStream && !streamIsTTY && forceColor === void 0) {
				return 0
			}
			const min = forceColor || 0
			if (env.TERM === "dumb") {
				return min
			}
			if (process.platform === "win32") {
				const osRelease = os.release().split(".")
				if (Number(osRelease[0]) >= 10 && Number(osRelease[2]) >= 10586) {
					return Number(osRelease[2]) >= 14931 ? 3 : 2
				}
				return 1
			}
			if ("CI" in env) {
				if (
					[
						"TRAVIS",
						"CIRCLECI",
						"APPVEYOR",
						"GITLAB_CI",
						"GITHUB_ACTIONS",
						"BUILDKITE",
						"DRONE",
					].some((sign) => sign in env) ||
					env.CI_NAME === "codeship"
				) {
					return 1
				}
				return min
			}
			if ("TEAMCITY_VERSION" in env) {
				return /^(9\.(0*[1-9]\d*)\.|\d{2,}\.)/.test(env.TEAMCITY_VERSION) ? 1 : 0
			}
			if (env.COLORTERM === "truecolor") {
				return 3
			}
			if ("TERM_PROGRAM" in env) {
				const version3 = Number.parseInt((env.TERM_PROGRAM_VERSION || "").split(".")[0], 10)
				switch (env.TERM_PROGRAM) {
					case "iTerm.app":
						return version3 >= 3 ? 3 : 2
					case "Apple_Terminal":
						return 2
				}
			}
			if (/-256(color)?$/i.test(env.TERM)) {
				return 2
			}
			if (/^screen|^xterm|^vt100|^vt220|^rxvt|color|ansi|cygwin|linux/i.test(env.TERM)) {
				return 1
			}
			if ("COLORTERM" in env) {
				return 1
			}
			return min
		}
		function getSupportLevel(stream, options = {}) {
			const level = supportsColor(stream, {
				streamIsTTY: stream && stream.isTTY,
				...options,
			})
			return translateLevel(level)
		}
		module2.exports = {
			supportsColor: getSupportLevel,
			stdout: getSupportLevel({ isTTY: tty.isatty(1) }),
			stderr: getSupportLevel({ isTTY: tty.isatty(2) }),
		}
	},
})

// ../../../node_modules/.pnpm/debug@4.3.5/node_modules/debug/src/node.js
var require_node = __commonJS({
	"../../../node_modules/.pnpm/debug@4.3.5/node_modules/debug/src/node.js"(exports2, module2) {
		var tty = require("node:tty")
		var util = require("node:util")
		exports2.init = init2
		exports2.log = log3
		exports2.formatArgs = formatArgs
		exports2.save = save
		exports2.load = load
		exports2.useColors = useColors
		exports2.destroy = util.deprecate(() => {},
		"Instance method `debug.destroy()` is deprecated and no longer does anything. It will be removed in the next major version of `debug`.")
		exports2.colors = [6, 2, 3, 4, 5, 1]
		try {
			const supportsColor = require_supports_color()
			if (supportsColor && (supportsColor.stderr || supportsColor).level >= 2) {
				exports2.colors = [
					20, 21, 26, 27, 32, 33, 38, 39, 40, 41, 42, 43, 44, 45, 56, 57, 62, 63, 68, 69, 74, 75,
					76, 77, 78, 79, 80, 81, 92, 93, 98, 99, 112, 113, 128, 129, 134, 135, 148, 149, 160, 161,
					162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 178, 179, 184, 185, 196, 197,
					198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 214, 215, 220, 221,
				]
			}
		} catch (error) {}
		exports2.inspectOpts = Object.keys(process.env)
			.filter((key) => {
				return /^debug_/i.test(key)
			})
			.reduce((obj, key) => {
				const prop = key
					.slice(6)
					.toLowerCase()
					.replace(/_([a-z])/g, (_, k) => {
						return k.toUpperCase()
					})
				let val = process.env[key]
				if (/^(yes|on|true|enabled)$/i.test(val)) {
					val = true
				} else if (/^(no|off|false|disabled)$/i.test(val)) {
					val = false
				} else if (val === "null") {
					val = null
				} else {
					val = Number(val)
				}
				obj[prop] = val
				return obj
			}, {})
		function useColors() {
			return "colors" in exports2.inspectOpts
				? Boolean(exports2.inspectOpts.colors)
				: tty.isatty(process.stderr.fd)
		}
		function formatArgs(args) {
			const { namespace: name, useColors: useColors2 } = this
			if (useColors2) {
				const c = this.color
				const colorCode = "\x1B[3" + (c < 8 ? c : "8;5;" + c)
				const prefix = `  ${colorCode};1m${name} \x1B[0m`
				args[0] = prefix + args[0].split("\n").join("\n" + prefix)
				args.push(colorCode + "m+" + module2.exports.humanize(this.diff) + "\x1B[0m")
			} else {
				args[0] = getDate() + name + " " + args[0]
			}
		}
		function getDate() {
			if (exports2.inspectOpts.hideDate) {
				return ""
			}
			return /* @__PURE__ */ new Date().toISOString() + " "
		}
		function log3(...args) {
			return process.stderr.write(util.formatWithOptions(exports2.inspectOpts, ...args) + "\n")
		}
		function save(namespaces) {
			if (namespaces) {
				process.env.DEBUG = namespaces
			} else {
				delete process.env.DEBUG
			}
		}
		function load() {
			return process.env.DEBUG
		}
		function init2(debug10) {
			debug10.inspectOpts = {}
			const keys = Object.keys(exports2.inspectOpts)
			for (const key of keys) {
				debug10.inspectOpts[key] = exports2.inspectOpts[key]
			}
		}
		module2.exports = require_common2()(exports2)
		var { formatters } = module2.exports
		formatters.o = function (v) {
			this.inspectOpts.colors = this.useColors
			return util
				.inspect(v, this.inspectOpts)
				.split("\n")
				.map((str) => str.trim())
				.join(" ")
		}
		formatters.O = function (v) {
			this.inspectOpts.colors = this.useColors
			return util.inspect(v, this.inspectOpts)
		}
	},
})

// ../../../node_modules/.pnpm/debug@4.3.5/node_modules/debug/src/index.js
var require_src = __commonJS({
	"../../../node_modules/.pnpm/debug@4.3.5/node_modules/debug/src/index.js"(exports2, module2) {
		if (
			typeof process === "undefined" ||
			process.type === "renderer" ||
			process.browser === true ||
			process.__nwjs
		) {
			module2.exports = require_browser()
		} else {
			module2.exports = require_node()
		}
	},
})

// ../../../node_modules/.pnpm/murmurhash3js@3.0.1/node_modules/murmurhash3js/lib/murmurHash3js.js
var require_murmurHash3js = __commonJS({
	"../../../node_modules/.pnpm/murmurhash3js@3.0.1/node_modules/murmurhash3js/lib/murmurHash3js.js"(
		exports2,
		module2
	) {
		;(function (root, undefined2) {
			"use strict"
			var library = {
				version: "3.0.1",
				x86: {},
				x64: {},
			}
			function _x86Multiply(m, n) {
				return (m & 65535) * n + ((((m >>> 16) * n) & 65535) << 16)
			}
			function _x86Rotl(m, n) {
				return (m << n) | (m >>> (32 - n))
			}
			function _x86Fmix(h) {
				h ^= h >>> 16
				h = _x86Multiply(h, 2246822507)
				h ^= h >>> 13
				h = _x86Multiply(h, 3266489909)
				h ^= h >>> 16
				return h
			}
			function _x64Add(m, n) {
				m = [m[0] >>> 16, m[0] & 65535, m[1] >>> 16, m[1] & 65535]
				n = [n[0] >>> 16, n[0] & 65535, n[1] >>> 16, n[1] & 65535]
				var o = [0, 0, 0, 0]
				o[3] += m[3] + n[3]
				o[2] += o[3] >>> 16
				o[3] &= 65535
				o[2] += m[2] + n[2]
				o[1] += o[2] >>> 16
				o[2] &= 65535
				o[1] += m[1] + n[1]
				o[0] += o[1] >>> 16
				o[1] &= 65535
				o[0] += m[0] + n[0]
				o[0] &= 65535
				return [(o[0] << 16) | o[1], (o[2] << 16) | o[3]]
			}
			function _x64Multiply(m, n) {
				m = [m[0] >>> 16, m[0] & 65535, m[1] >>> 16, m[1] & 65535]
				n = [n[0] >>> 16, n[0] & 65535, n[1] >>> 16, n[1] & 65535]
				var o = [0, 0, 0, 0]
				o[3] += m[3] * n[3]
				o[2] += o[3] >>> 16
				o[3] &= 65535
				o[2] += m[2] * n[3]
				o[1] += o[2] >>> 16
				o[2] &= 65535
				o[2] += m[3] * n[2]
				o[1] += o[2] >>> 16
				o[2] &= 65535
				o[1] += m[1] * n[3]
				o[0] += o[1] >>> 16
				o[1] &= 65535
				o[1] += m[2] * n[2]
				o[0] += o[1] >>> 16
				o[1] &= 65535
				o[1] += m[3] * n[1]
				o[0] += o[1] >>> 16
				o[1] &= 65535
				o[0] += m[0] * n[3] + m[1] * n[2] + m[2] * n[1] + m[3] * n[0]
				o[0] &= 65535
				return [(o[0] << 16) | o[1], (o[2] << 16) | o[3]]
			}
			function _x64Rotl(m, n) {
				n %= 64
				if (n === 32) {
					return [m[1], m[0]]
				} else if (n < 32) {
					return [(m[0] << n) | (m[1] >>> (32 - n)), (m[1] << n) | (m[0] >>> (32 - n))]
				} else {
					n -= 32
					return [(m[1] << n) | (m[0] >>> (32 - n)), (m[0] << n) | (m[1] >>> (32 - n))]
				}
			}
			function _x64LeftShift(m, n) {
				n %= 64
				if (n === 0) {
					return m
				} else if (n < 32) {
					return [(m[0] << n) | (m[1] >>> (32 - n)), m[1] << n]
				} else {
					return [m[1] << (n - 32), 0]
				}
			}
			function _x64Xor(m, n) {
				return [m[0] ^ n[0], m[1] ^ n[1]]
			}
			function _x64Fmix(h) {
				h = _x64Xor(h, [0, h[0] >>> 1])
				h = _x64Multiply(h, [4283543511, 3981806797])
				h = _x64Xor(h, [0, h[0] >>> 1])
				h = _x64Multiply(h, [3301882366, 444984403])
				h = _x64Xor(h, [0, h[0] >>> 1])
				return h
			}
			library.x86.hash32 = function (key, seed) {
				key = key || ""
				seed = seed || 0
				var remainder = key.length % 4
				var bytes = key.length - remainder
				var h1 = seed
				var k1 = 0
				var c1 = 3432918353
				var c2 = 461845907
				for (var i = 0; i < bytes; i = i + 4) {
					k1 =
						(key.charCodeAt(i) & 255) |
						((key.charCodeAt(i + 1) & 255) << 8) |
						((key.charCodeAt(i + 2) & 255) << 16) |
						((key.charCodeAt(i + 3) & 255) << 24)
					k1 = _x86Multiply(k1, c1)
					k1 = _x86Rotl(k1, 15)
					k1 = _x86Multiply(k1, c2)
					h1 ^= k1
					h1 = _x86Rotl(h1, 13)
					h1 = _x86Multiply(h1, 5) + 3864292196
				}
				k1 = 0
				switch (remainder) {
					case 3:
						k1 ^= (key.charCodeAt(i + 2) & 255) << 16
					case 2:
						k1 ^= (key.charCodeAt(i + 1) & 255) << 8
					case 1:
						k1 ^= key.charCodeAt(i) & 255
						k1 = _x86Multiply(k1, c1)
						k1 = _x86Rotl(k1, 15)
						k1 = _x86Multiply(k1, c2)
						h1 ^= k1
				}
				h1 ^= key.length
				h1 = _x86Fmix(h1)
				return h1 >>> 0
			}
			library.x86.hash128 = function (key, seed) {
				key = key || ""
				seed = seed || 0
				var remainder = key.length % 16
				var bytes = key.length - remainder
				var h1 = seed
				var h2 = seed
				var h3 = seed
				var h4 = seed
				var k1 = 0
				var k2 = 0
				var k3 = 0
				var k4 = 0
				var c1 = 597399067
				var c2 = 2869860233
				var c3 = 951274213
				var c4 = 2716044179
				for (var i = 0; i < bytes; i = i + 16) {
					k1 =
						(key.charCodeAt(i) & 255) |
						((key.charCodeAt(i + 1) & 255) << 8) |
						((key.charCodeAt(i + 2) & 255) << 16) |
						((key.charCodeAt(i + 3) & 255) << 24)
					k2 =
						(key.charCodeAt(i + 4) & 255) |
						((key.charCodeAt(i + 5) & 255) << 8) |
						((key.charCodeAt(i + 6) & 255) << 16) |
						((key.charCodeAt(i + 7) & 255) << 24)
					k3 =
						(key.charCodeAt(i + 8) & 255) |
						((key.charCodeAt(i + 9) & 255) << 8) |
						((key.charCodeAt(i + 10) & 255) << 16) |
						((key.charCodeAt(i + 11) & 255) << 24)
					k4 =
						(key.charCodeAt(i + 12) & 255) |
						((key.charCodeAt(i + 13) & 255) << 8) |
						((key.charCodeAt(i + 14) & 255) << 16) |
						((key.charCodeAt(i + 15) & 255) << 24)
					k1 = _x86Multiply(k1, c1)
					k1 = _x86Rotl(k1, 15)
					k1 = _x86Multiply(k1, c2)
					h1 ^= k1
					h1 = _x86Rotl(h1, 19)
					h1 += h2
					h1 = _x86Multiply(h1, 5) + 1444728091
					k2 = _x86Multiply(k2, c2)
					k2 = _x86Rotl(k2, 16)
					k2 = _x86Multiply(k2, c3)
					h2 ^= k2
					h2 = _x86Rotl(h2, 17)
					h2 += h3
					h2 = _x86Multiply(h2, 5) + 197830471
					k3 = _x86Multiply(k3, c3)
					k3 = _x86Rotl(k3, 17)
					k3 = _x86Multiply(k3, c4)
					h3 ^= k3
					h3 = _x86Rotl(h3, 15)
					h3 += h4
					h3 = _x86Multiply(h3, 5) + 2530024501
					k4 = _x86Multiply(k4, c4)
					k4 = _x86Rotl(k4, 18)
					k4 = _x86Multiply(k4, c1)
					h4 ^= k4
					h4 = _x86Rotl(h4, 13)
					h4 += h1
					h4 = _x86Multiply(h4, 5) + 850148119
				}
				k1 = 0
				k2 = 0
				k3 = 0
				k4 = 0
				switch (remainder) {
					case 15:
						k4 ^= key.charCodeAt(i + 14) << 16
					case 14:
						k4 ^= key.charCodeAt(i + 13) << 8
					case 13:
						k4 ^= key.charCodeAt(i + 12)
						k4 = _x86Multiply(k4, c4)
						k4 = _x86Rotl(k4, 18)
						k4 = _x86Multiply(k4, c1)
						h4 ^= k4
					case 12:
						k3 ^= key.charCodeAt(i + 11) << 24
					case 11:
						k3 ^= key.charCodeAt(i + 10) << 16
					case 10:
						k3 ^= key.charCodeAt(i + 9) << 8
					case 9:
						k3 ^= key.charCodeAt(i + 8)
						k3 = _x86Multiply(k3, c3)
						k3 = _x86Rotl(k3, 17)
						k3 = _x86Multiply(k3, c4)
						h3 ^= k3
					case 8:
						k2 ^= key.charCodeAt(i + 7) << 24
					case 7:
						k2 ^= key.charCodeAt(i + 6) << 16
					case 6:
						k2 ^= key.charCodeAt(i + 5) << 8
					case 5:
						k2 ^= key.charCodeAt(i + 4)
						k2 = _x86Multiply(k2, c2)
						k2 = _x86Rotl(k2, 16)
						k2 = _x86Multiply(k2, c3)
						h2 ^= k2
					case 4:
						k1 ^= key.charCodeAt(i + 3) << 24
					case 3:
						k1 ^= key.charCodeAt(i + 2) << 16
					case 2:
						k1 ^= key.charCodeAt(i + 1) << 8
					case 1:
						k1 ^= key.charCodeAt(i)
						k1 = _x86Multiply(k1, c1)
						k1 = _x86Rotl(k1, 15)
						k1 = _x86Multiply(k1, c2)
						h1 ^= k1
				}
				h1 ^= key.length
				h2 ^= key.length
				h3 ^= key.length
				h4 ^= key.length
				h1 += h2
				h1 += h3
				h1 += h4
				h2 += h1
				h3 += h1
				h4 += h1
				h1 = _x86Fmix(h1)
				h2 = _x86Fmix(h2)
				h3 = _x86Fmix(h3)
				h4 = _x86Fmix(h4)
				h1 += h2
				h1 += h3
				h1 += h4
				h2 += h1
				h3 += h1
				h4 += h1
				return (
					("00000000" + (h1 >>> 0).toString(16)).slice(-8) +
					("00000000" + (h2 >>> 0).toString(16)).slice(-8) +
					("00000000" + (h3 >>> 0).toString(16)).slice(-8) +
					("00000000" + (h4 >>> 0).toString(16)).slice(-8)
				)
			}
			library.x64.hash128 = function (key, seed) {
				key = key || ""
				seed = seed || 0
				var remainder = key.length % 16
				var bytes = key.length - remainder
				var h1 = [0, seed]
				var h2 = [0, seed]
				var k1 = [0, 0]
				var k2 = [0, 0]
				var c1 = [2277735313, 289559509]
				var c2 = [1291169091, 658871167]
				for (var i = 0; i < bytes; i = i + 16) {
					k1 = [
						(key.charCodeAt(i + 4) & 255) |
							((key.charCodeAt(i + 5) & 255) << 8) |
							((key.charCodeAt(i + 6) & 255) << 16) |
							((key.charCodeAt(i + 7) & 255) << 24),
						(key.charCodeAt(i) & 255) |
							((key.charCodeAt(i + 1) & 255) << 8) |
							((key.charCodeAt(i + 2) & 255) << 16) |
							((key.charCodeAt(i + 3) & 255) << 24),
					]
					k2 = [
						(key.charCodeAt(i + 12) & 255) |
							((key.charCodeAt(i + 13) & 255) << 8) |
							((key.charCodeAt(i + 14) & 255) << 16) |
							((key.charCodeAt(i + 15) & 255) << 24),
						(key.charCodeAt(i + 8) & 255) |
							((key.charCodeAt(i + 9) & 255) << 8) |
							((key.charCodeAt(i + 10) & 255) << 16) |
							((key.charCodeAt(i + 11) & 255) << 24),
					]
					k1 = _x64Multiply(k1, c1)
					k1 = _x64Rotl(k1, 31)
					k1 = _x64Multiply(k1, c2)
					h1 = _x64Xor(h1, k1)
					h1 = _x64Rotl(h1, 27)
					h1 = _x64Add(h1, h2)
					h1 = _x64Add(_x64Multiply(h1, [0, 5]), [0, 1390208809])
					k2 = _x64Multiply(k2, c2)
					k2 = _x64Rotl(k2, 33)
					k2 = _x64Multiply(k2, c1)
					h2 = _x64Xor(h2, k2)
					h2 = _x64Rotl(h2, 31)
					h2 = _x64Add(h2, h1)
					h2 = _x64Add(_x64Multiply(h2, [0, 5]), [0, 944331445])
				}
				k1 = [0, 0]
				k2 = [0, 0]
				switch (remainder) {
					case 15:
						k2 = _x64Xor(k2, _x64LeftShift([0, key.charCodeAt(i + 14)], 48))
					case 14:
						k2 = _x64Xor(k2, _x64LeftShift([0, key.charCodeAt(i + 13)], 40))
					case 13:
						k2 = _x64Xor(k2, _x64LeftShift([0, key.charCodeAt(i + 12)], 32))
					case 12:
						k2 = _x64Xor(k2, _x64LeftShift([0, key.charCodeAt(i + 11)], 24))
					case 11:
						k2 = _x64Xor(k2, _x64LeftShift([0, key.charCodeAt(i + 10)], 16))
					case 10:
						k2 = _x64Xor(k2, _x64LeftShift([0, key.charCodeAt(i + 9)], 8))
					case 9:
						k2 = _x64Xor(k2, [0, key.charCodeAt(i + 8)])
						k2 = _x64Multiply(k2, c2)
						k2 = _x64Rotl(k2, 33)
						k2 = _x64Multiply(k2, c1)
						h2 = _x64Xor(h2, k2)
					case 8:
						k1 = _x64Xor(k1, _x64LeftShift([0, key.charCodeAt(i + 7)], 56))
					case 7:
						k1 = _x64Xor(k1, _x64LeftShift([0, key.charCodeAt(i + 6)], 48))
					case 6:
						k1 = _x64Xor(k1, _x64LeftShift([0, key.charCodeAt(i + 5)], 40))
					case 5:
						k1 = _x64Xor(k1, _x64LeftShift([0, key.charCodeAt(i + 4)], 32))
					case 4:
						k1 = _x64Xor(k1, _x64LeftShift([0, key.charCodeAt(i + 3)], 24))
					case 3:
						k1 = _x64Xor(k1, _x64LeftShift([0, key.charCodeAt(i + 2)], 16))
					case 2:
						k1 = _x64Xor(k1, _x64LeftShift([0, key.charCodeAt(i + 1)], 8))
					case 1:
						k1 = _x64Xor(k1, [0, key.charCodeAt(i)])
						k1 = _x64Multiply(k1, c1)
						k1 = _x64Rotl(k1, 31)
						k1 = _x64Multiply(k1, c2)
						h1 = _x64Xor(h1, k1)
				}
				h1 = _x64Xor(h1, [0, key.length])
				h2 = _x64Xor(h2, [0, key.length])
				h1 = _x64Add(h1, h2)
				h2 = _x64Add(h2, h1)
				h1 = _x64Fmix(h1)
				h2 = _x64Fmix(h2)
				h1 = _x64Add(h1, h2)
				h2 = _x64Add(h2, h1)
				return (
					("00000000" + (h1[0] >>> 0).toString(16)).slice(-8) +
					("00000000" + (h1[1] >>> 0).toString(16)).slice(-8) +
					("00000000" + (h2[0] >>> 0).toString(16)).slice(-8) +
					("00000000" + (h2[1] >>> 0).toString(16)).slice(-8)
				)
			}
			if (typeof exports2 !== "undefined") {
				if (typeof module2 !== "undefined" && module2.exports) {
					exports2 = module2.exports = library
				}
				exports2.murmurHash3 = library
			} else if (typeof define === "function" && define.amd) {
				define([], function () {
					return library
				})
			} else {
				library._murmurHash3 = root.murmurHash3
				library.noConflict = function () {
					root.murmurHash3 = library._murmurHash3
					library._murmurHash3 = undefined2
					library.noConflict = undefined2
					return library
				}
				root.murmurHash3 = library
			}
		})(exports2)
	},
})

// ../../../node_modules/.pnpm/murmurhash3js@3.0.1/node_modules/murmurhash3js/index.js
var require_murmurhash3js = __commonJS({
	"../../../node_modules/.pnpm/murmurhash3js@3.0.1/node_modules/murmurhash3js/index.js"(
		exports2,
		module2
	) {
		module2.exports = require_murmurHash3js()
	},
})

// src/main.ts
var fs = __toESM(require("node:fs/promises"), 1)
var core = __toESM(require_core(), 1)
var github = __toESM(require_github(), 1)

// ../../../lix/packages/client/dist/git/commit.js
init_isomorphic_git()

// ../../../lix/packages/client/dist/git/add.js
init_isomorphic_git()
async function add2(ctx, state, filepath) {
	await state.ensureFirstBatch()
	return await isomorphic_git_default.add({
		fs: ctx.rawFs,
		parallel: true,
		dir: ctx.dir,
		cache: ctx.cache,
		filepath,
	})
}

// ../../../lix/packages/client/dist/git/remove.js
init_isomorphic_git()
async function remove2(ctx, state, filepath) {
	await state.ensureFirstBatch()
	return await isomorphic_git_default.remove({
		fs: ctx.rawFs,
		dir: ctx.dir,
		cache: ctx.cache,
		filepath,
	})
}

// ../../../lix/packages/client/dist/git/commit.js
init_dist()
var isoCommit = (ctx, { author, message }) =>
	commit({
		fs: ctx.rawFs,
		dir: ctx.dir,
		cache: ctx.cache,
		author: author || ctx.author,
		message,
	})
async function commit2(ctx, state, { author: overrideAuthor, message, include }) {
	if (include) {
		const additions = []
		const deletions = []
		for (const entry of include) {
			if (await ctx.rawFs.lstat(entry).catch(() => void 0)) {
				additions.push(entry)
			} else {
				deletions.push(entry)
			}
		}
		additions.length && (await add2(ctx, state, additions))
		deletions.length && (await Promise.all(deletions.map((del) => remove2(ctx, state, del))))
	} else {
	}
	const commitArgs = {
		fs: state.nodeishFs,
		dir: ctx.dir,
		cache: ctx.cache,
		author: overrideAuthor || ctx.author,
		message,
	}
	if (ctx.experimentalFeatures.lixCommit) {
		console.warn("using experimental commit for this repo.")
		return doCommit(commitArgs)
	} else {
		return commit(commitArgs)
	}
}
async function doCommit({ cache: cache2, fs: fs2, dir, ref, author, message }) {
	const fileStates = {}
	async function createTree(currentFolder, fileStates2) {
		const entries = []
		const currentFolderStates = fileStates2[currentFolder]
		if (!currentFolderStates) {
			throw new Error("couldn't find folder " + currentFolder + " in file states")
		}
		for (const entry of currentFolderStates) {
			if (entry.type === "tree") {
				entries.push({
					mode: "040000",
					path: entry.path,
					type: entry.type,
					oid: await createTree(currentFolder + entry.path + "/", fileStates2),
				})
			} else {
				if (!entry.oid) {
					throw new Error("OID should be set for types except tree")
				}
				entries.push({
					mode: entry.mode,
					path: entry.path,
					type: entry.type,
					oid: entry.oid,
				})
			}
		}
		return await writeTree({ fs: fs2, dir, tree: entries })
	}
	await walk({
		fs: fs2,
		dir,
		cache: cache2,
		// gitdir,
		trees: [TREE({ ref }), STAGE()],
		// @ts-ignore FIXME
		map: async function (filepath, [refState, stagingState]) {
			if (!refState && !stagingState) {
				throw new Error("At least one of the trees should contain an entry")
			}
			const refStateType = refState ? await refState.type() : void 0
			const stagingStateType = stagingState ? await stagingState.type() : void 0
			if (refStateType === "commit" || stagingStateType === "commit") {
				throw new Error("Submodule found in " + filepath + " currently not supported")
			}
			if (refStateType === "special" || stagingStateType === "special") {
				throw new Error("type special should not occure in ref or staging")
			}
			if (filepath === ".") {
				return
			}
			const fileDir = getDirname(filepath)
			if (fileStates[fileDir] === void 0) {
				fileStates[fileDir] = []
			}
			if (!stagingState && refState) {
				if (refStateType === "tree" || (fs2._isPlaceholder && fs2._isPlaceholder(filepath))) {
					fileStates[fileDir]?.push({
						mode: (await refState.mode()).toString(8),
						path: getBasename(filepath),
						type: refStateType,
						oid: await refState.oid(),
					})
					return
				}
				return
			}
			if (stagingState && !refState) {
				const stMode = await stagingState.mode()
				fileStates[fileDir]?.push({
					mode: stMode?.toString(8),
					path: getBasename(filepath),
					type: stagingStateType,
					oid: await stagingState.oid(),
				})
				return
			}
			if (stagingState && refState) {
				const stagingMode = await stagingState.mode()
				const stagingType = await stagingState.type()
				fileStates[fileDir]?.push({
					mode: stagingType === "tree" ? "040000" : stagingMode.toString(8),
					path: getBasename(filepath),
					type: stagingStateType,
					oid: await stagingState.oid(),
				})
				return
			}
		},
		// TODO: use reduce to build datastructure? reduce: async function (parent, children) {},
	})
	const tree = await createTree("/", fileStates)
	return commit({
		cache: cache2,
		fs: fs2,
		dir,
		author,
		message,
		tree,
	})
}

// ../../../lix/packages/client/dist/git/status-list.js
init_isomorphic_git()

// ../../../lix/packages/client/dist/git/helpers.js
var fileModeTypeMapping = {
	40: "folder",
	10: "file",
	12: "symlink",
}
function modeToFileType(mode) {
	const fileMode = mode.toString(8).slice(0, 2)
	return fileModeTypeMapping[fileMode] || "unknown"
}

// ../../../lix/packages/client/dist/git/status-list.js
var {
	walk: walk2,
	// _walk expects cache to always exist.
	TREE: TREE2,
	WORKDIR: WORKDIR2,
	STAGE: STAGE2,
	isIgnored: isIgnored2,
} = isomorphic_git_default
function isoNormalizePath(path) {
	return path
		.replace(/\/\.\//g, "/")
		.replace(/\/{2,}/g, "/")
		.replace(/^\/\.$/, "/")
		.replace(/^\.\/$/, ".")
		.replace(/^\.\//, "")
		.replace(/\/\.$/, "")
		.replace(/(.+)\/$/, "$1")
		.replace(/^$/, ".")
}
function join2(...parts) {
	return isoNormalizePath(parts.map(isoNormalizePath).join("/"))
}
async function statusList(ctx, state, statusArg) {
	return await _statusList({
		fs: ctx.rawFs,
		ensureFirstBatch: state.ensureFirstBatch,
		dir: ctx.dir,
		cache: ctx.cache,
		sparseFilter: state.sparseFilter,
		filter: statusArg?.filter,
		filepaths: statusArg?.filepaths,
		includeStatus: statusArg?.includeStatus,
	})
}
async function _statusList({
	fs: fs2,
	ensureFirstBatch,
	dir = "/",
	gitdir = join2(dir, ".git"),
	ref = "HEAD",
	filepaths = ["."],
	filter,
	sparseFilter,
	// experimental, not yet exposed as lix api!
	cache: cache2,
	includeStatus = [],
	addHashes = false,
}) {
	try {
		await ensureFirstBatch()
		const ignoredRes = []
		const walkRes = await walk2({
			fs: fs2,
			cache: cache2,
			dir,
			gitdir,
			trees: [TREE2({ ref }), WORKDIR2(), STAGE2()],
			/**
			 * Taken from the isomorphic-git documentation of walk's map function:
			 *
			 * This is the function that is called once per entry BEFORE visiting the children of that node.
			 * (1) If you return null for a tree entry, then none of the children of that tree entry will be walked - but the tree entry is still part of the results.
			 * (2) If you do not return a value (or return undefined) that entry will be filtered from the results.
			 */
			map: async function (filepath, [head, workdir, stage]) {
				if (!head && !stage && workdir) {
					const ignored = await isIgnored2({ fs: fs2, dir, filepath })
					if (ignored) {
						if (includeStatus.includes("ignored") || filepaths.includes(filepath)) {
							ignoredRes.push([
								filepath,
								"ignored",
								{ headOid: void 0, workdirOid: "ignored", stageOid: void 0 },
							])
						}
						return null
					}
				}
				if (!filepaths.some((base) => worthWalking(filepath, base))) {
					return null
				}
				if (filter && !filter(filepath)) {
					return void 0
				}
				if (fs2._isPlaceholder && fs2._isPlaceholder(filepath)) {
					if (includeStatus.includes("unmodified") || filepaths.includes(filepath)) {
						const headType2 = head && (await head.type())
						if (headType2 !== "blob") {
							throw new Error("Placeholder file is not a blob in head: " + filepath)
						}
						const headOid2 = await head?.oid()
						return [
							filepath,
							"unmodified",
							{ headOid: headOid2, workdirOid: headOid2, stageOid: headOid2, placeholder: true },
						]
					}
					return null
				}
				let types2
				try {
					types2 = await Promise.all([
						head && head.type(),
						workdir && workdir.type(),
						stage && stage.type(),
					])
				} catch (error) {
					return null
				}
				const [headType, workdirType, stageType] = types2
				const isBlob = [headType, workdirType, stageType].includes("blob")
				if (sparseFilter) {
					const fileMode = await (head || workdir)?.mode()
					const fileType = modeToFileType(fileMode)
					if (
						!sparseFilter({
							filename: filepath,
							type: fileType,
						})
					) {
						return null
					}
				}
				if ((headType === "tree" || headType === "special") && !isBlob) {
					return void 0
				}
				if (headType === "commit") {
					return null
				}
				if ((workdirType === "tree" || workdirType === "special") && !isBlob) {
					return void 0
				}
				if (stageType === "commit") {
					return null
				}
				if ((stageType === "tree" || stageType === "special") && !isBlob) {
					return void 0
				}
				const headOid = headType === "blob" ? await head?.oid() : void 0
				const stageOid = stageType === "blob" ? await stage?.oid() : void 0
				let workdirOid
				if (headType !== "blob" && workdirType === "blob" && stageType !== "blob") {
					workdirOid = addHashes ? await workdir?.oid() : "42"
				} else if (workdirType === "blob") {
					workdirOid = await workdir?.oid()
				}
				const entry = { headOid, workdirOid, stageOid }
				if (entry.headOid === entry.workdirOid && entry.workdirOid === entry.stageOid) {
					if (
						includeStatus.includes("unmodified") ||
						filepaths.includes(filepath) ||
						includeStatus.includes("materialized")
					) {
						return [filepath, "unmodified", entry]
					} else {
						return void 0
					}
				}
				if (!entry.headOid && !entry.stageOid && entry.workdirOid) {
					return [filepath, "*untracked", entry]
				}
				if (!entry.headOid && entry.workdirOid && entry.workdirOid === entry.stageOid) {
					return [filepath, "added", entry]
				}
				if (
					!entry.headOid &&
					entry.stageOid &&
					entry.workdirOid &&
					entry.workdirOid !== entry.stageOid
				) {
					return [filepath, "*added2", entry]
				}
				if (
					entry.headOid &&
					entry.workdirOid &&
					entry.headOid === entry.stageOid &&
					entry.headOid !== entry.workdirOid
				) {
					return [filepath, "*modified", entry]
				}
				if (
					entry.headOid &&
					entry.workdirOid &&
					entry.stageOid &&
					entry.stageOid !== entry.headOid &&
					entry.stageOid !== entry.workdirOid &&
					entry.headOid !== entry.workdirOid
				) {
					return [filepath, "*modified2", entry]
				}
				if (
					entry.headOid &&
					entry.workdirOid &&
					entry.headOid !== entry.stageOid &&
					entry.stageOid === entry.workdirOid
				) {
					return [filepath, "modified", entry]
				}
				if (entry.headOid && !entry.workdirOid && entry.headOid === entry.stageOid) {
					return [filepath, "*deleted", entry]
				}
				if (entry.headOid && !entry.workdirOid && !entry.stageOid) {
					return [filepath, "deleted", entry]
				}
				if (!entry.headOid && !entry.workdirOid && entry.stageOid) {
					return [filepath, "*absent", entry]
				}
				if (
					entry.headOid &&
					entry.workdirOid &&
					!entry.stageOid &&
					entry.headOid !== entry.workdirOid
				) {
					return [filepath, "*undeletemodified", entry]
				}
				if (
					entry.headOid &&
					entry.workdirOid &&
					entry.stageOid &&
					entry.stageOid !== entry.headOid &&
					entry.headOid === entry.workdirOid
				) {
					return [filepath, "*unmodified", entry]
				}
				if (
					entry.headOid &&
					entry.workdirOid &&
					!entry.stageOid &&
					entry.headOid === entry.workdirOid
				) {
					return [filepath, "*undeleted", entry]
				}
				return [filepath, "unknown", entry]
			},
		})
		return [...walkRes, ...ignoredRes]
	} catch (err) {
		err.caller = "lix.status"
		throw err
	}
}

// ../../../lix/packages/client/dist/git/status.js
async function status2(ctx, state, filepath) {
	if (typeof filepath !== "string") {
		throw new Error("parameter must be a string")
	}
	const statusList2 = await statusList(ctx, state, {
		filepaths: [filepath],
	})
	const maybeStatusEntry = statusList2[0] || [filepath, "unknown"]
	return maybeStatusEntry?.[1]
}

// ../../../lix/packages/client/dist/git/push.js
init_isomorphic_git()

// ../../../lix/packages/client/dist/git-http/client.js
function fromValue2(value) {
	let queue = [value]
	return {
		next() {
			return Promise.resolve({ done: queue.length === 0, value: queue.pop() })
		},
		return() {
			queue = []
			return {}
		},
		[Symbol.asyncIterator]() {
			return this
		},
	}
}
function getIterator2(iterable) {
	if (iterable[Symbol.asyncIterator]) {
		return iterable[Symbol.asyncIterator]()
	}
	if (iterable[Symbol.iterator]) {
		return iterable[Symbol.iterator]()
	}
	if (iterable.next) {
		return iterable
	}
	return fromValue2(iterable)
}
async function forAwait2(iterable, cb) {
	const iter = getIterator2(iterable)
	while (true) {
		const { value, done } = await iter.next()
		if (value) await cb(value)
		if (done) break
	}
	if (iter.return) iter.return()
}
async function collect2(iterable) {
	let size = 0
	const buffers = []
	await forAwait2(iterable, (value) => {
		buffers.push(value)
		size += value.byteLength
	})
	const result = new Uint8Array(size)
	let nextIndex = 0
	for (const buffer of buffers) {
		result.set(buffer, nextIndex)
		nextIndex += buffer.byteLength
	}
	return result
}
var cache = /* @__PURE__ */ new Map()
var cacheDisabler
function makeHttpClient({ noCache, debug: debug10, description, onReq, onRes }) {
	async function request({ url, method = "GET", headers = {}, body: rawBody }) {
		let body = rawBody ? await collect2(rawBody) : void 0
		const origUrl = url
		const origMethod = method
		if (!noCache && cache && origMethod === "GET" && cache.has(origUrl)) {
			const { resHeaders: resHeaders2, resBody: resBody2 } = cache.get(origUrl)
			return {
				url: origUrl,
				method: origMethod,
				statusCode: 200,
				statusMessage: "OK",
				body: resBody2,
				headers: resHeaders2,
			}
		}
		if (onReq) {
			const rewritten = await onReq({ body, url, method })
			method = rewritten?.method || method
			headers = rewritten?.headers || headers
			body = rewritten?.body || body
			url = rewritten?.url || url
		}
		const res = await fetch(url, { method, headers, body, credentials: "include" })
		let resHeaders = {}
		for (const [key, value] of res.headers.entries()) {
			resHeaders[key] = value
		}
		if (debug10) {
			console.warn(`${description} git req:`, origUrl)
		}
		const statusCode = res.status
		let resBody
		const uint8Array = res.body && new Uint8Array(await res.arrayBuffer())
		if (debug10 && statusCode === 200 && uint8Array) {
			const { inflatePackResponse: inflatePackResponse2 } = await Promise.resolve().then(
				() => (init_packfile(), packfile_exports)
			)
			console.info(await inflatePackResponse2(uint8Array).catch((err) => err))
		}
		if (onRes) {
			const rewritten = await onRes({
				origUrl,
				usedUrl: url,
				resBody: uint8Array,
				statusCode,
				resHeaders,
			})
			resHeaders = rewritten?.resHeaders || resHeaders
			resBody = rewritten?.resBody || [uint8Array]
		}
		if (!resBody) {
			resBody = [uint8Array]
		}
		if (!noCache && cache && statusCode === 200 && origMethod === "GET") {
			if (!cacheDisabler) {
				cacheDisabler = setTimeout(() => {
					cache?.clear()
					cache = void 0
				}, 15e3)
			}
			cache.set(origUrl, { resHeaders, resBody })
		}
		return {
			url: origUrl,
			method: origMethod,
			statusCode,
			statusMessage: res.statusText,
			body: resBody,
			headers: resHeaders,
		}
	}
	return { request }
}

// ../../../lix/packages/client/dist/git/push.js
var push2 = async (ctx) => {
	if (!ctx.gitUrl) {
		throw new Error("Could not find repo url, only github supported for push at the moment")
	}
	return await isomorphic_git_default.push({
		fs: ctx.rawFs,
		url: ctx.gitUrl,
		cache: ctx.cache,
		corsProxy: ctx.gitProxyUrl,
		http: makeHttpClient({ debug: ctx.debug, description: "push" }),
		dir: ctx.dir,
	})
}

// ../../../lix/packages/client/dist/git/pull.js
init_isomorphic_git()

// ../../../lix/packages/client/dist/git/checkout.js
init_isomorphic_git()

// ../../../lix/packages/client/dist/git/__checkout.js
init_isomorphic_git()
init_isomorphic_git()
init_isomorphic_git()
async function __checkout({
	fs: fs2,
	cache: cache2,
	onProgress,
	dir,
	gitdir,
	remote,
	ref,
	filepaths,
	noCheckout,
	noUpdateHead,
	dryRun,
	force,
	track = true,
}) {
	let oid
	try {
		oid = await GitRefManager.resolve({ fs: fs2, gitdir, ref })
	} catch (err) {
		if (ref === "HEAD") {
			throw err
		}
		const remoteRef = `${remote}/${ref}`
		oid = await GitRefManager.resolve({
			fs: fs2,
			gitdir,
			ref: remoteRef,
		})
		if (track) {
			const config = await GitConfigManager.get({ fs: fs2, gitdir })
			await config.set(`branch.${ref}.remote`, remote)
			await config.set(`branch.${ref}.merge`, `refs/heads/${ref}`)
			await GitConfigManager.save({ fs: fs2, gitdir, config })
		}
		await GitRefManager.writeRef({
			fs: fs2,
			gitdir,
			ref: `refs/heads/${ref}`,
			value: oid,
		})
	}
	if (!noCheckout) {
		let ops
		try {
			ops = await analyze2({
				fs: fs2,
				cache: cache2,
				onProgress,
				dir,
				gitdir,
				ref,
				force,
				filepaths,
			})
		} catch (err) {
			if (err instanceof Errors.NotFoundError && err.data.what === oid) {
				throw new Errors.CommitNotFetchedError(ref, oid)
			} else {
				throw err
			}
		}
		const conflicts = ops
			.filter(([method]) => method === "conflict")
			.map(([method, fullpath]) => fullpath)
		if (conflicts.length > 0) {
			throw new Errors.CheckoutConflictError(conflicts)
		}
		const errors = ops
			.filter(([method]) => method === "error")
			.map(([method, fullpath]) => fullpath)
		if (errors.length > 0) {
			throw new Errors.InternalError(errors.join(", "))
		}
		if (dryRun) {
			return
		}
		let count = 0
		const total = ops.length
		await GitIndexManager.acquire({ fs: fs2, gitdir, cache: cache2 }, async function (index2) {
			await Promise.all(
				// @ts-ignore
				ops
					.filter(([method]) => method === "delete" || method === "delete-index")
					.map(async function ([method, fullpath]) {
						const filepath = `${dir}/${fullpath}`
						if (method === "delete") {
							await fs2.rm(filepath)
						}
						index2.delete({ filepath: fullpath })
						if (onProgress) {
							await onProgress({
								phase: "Updating workdir",
								loaded: ++count,
								total,
							})
						}
					})
			)
		})
		await GitIndexManager.acquire({ fs: fs2, gitdir, cache: cache2 }, async function (index2) {
			for (const [method, fullpath] of ops) {
				if (method === "rmdir" || method === "rmdir-index") {
					const filepath = `${dir}/${fullpath}`
					try {
						if (method === "rmdir-index") {
							index2.delete({ filepath: fullpath })
						}
						await fs2.rmdir(filepath)
						if (onProgress) {
							await onProgress({
								phase: "Updating workdir",
								loaded: ++count,
								total,
							})
						}
					} catch (e) {
						if (e.code === "ENOTEMPTY") {
							console.log(`Did not delete ${fullpath} because directory is not empty`)
						} else {
							throw e
						}
					}
				}
			}
		})
		await Promise.all(
			ops
				.filter(([method]) => method === "mkdir" || method === "mkdir-index")
				.map(async function ([_, fullpath]) {
					const filepath = `${dir}/${fullpath}`
					await fs2.mkdir(filepath)
					if (onProgress) {
						await onProgress({
							phase: "Updating workdir",
							loaded: ++count,
							total,
						})
					}
				})
		)
		await GitIndexManager.acquire({ fs: fs2, gitdir, cache: cache2 }, async function (index2) {
			await Promise.all(
				// @ts-ignore
				ops
					.filter(
						// @ts-ignore
						([method]) =>
							method === "create" ||
							method === "create-index" ||
							method === "update" ||
							method === "mkdir-index"
					)
					.map(async function ([method, fullpath, oid2, mode, chmod]) {
						const filepath = `${dir}/${fullpath}`
						try {
							if (method !== "create-index" && method !== "mkdir-index") {
								const { object } = await _readObject({ fs: fs2, cache: cache2, gitdir, oid: oid2 })
								if (chmod) {
									await fs2.rm(filepath)
								}
								if (mode === 33188) {
									await fs2.write(filepath, object)
								} else if (mode === 33261) {
									await fs2.write(filepath, object, { mode: 511 })
								} else if (mode === 40960) {
									await fs2.writelink(filepath, object)
								} else {
									throw new Errors.InternalError(
										`Invalid mode 0o${mode.toString(8)} detected in blob ${oid2}`
									)
								}
							}
							const stats = await fs2.lstat(filepath)
							if (mode === 33261) {
								stats.mode = 493
							}
							if (method === "mkdir-index") {
								stats.mode = 57344
							}
							index2.insert({
								filepath: fullpath,
								stats,
								oid: oid2,
							})
							if (onProgress) {
								await onProgress({
									phase: "Updating workdir",
									loaded: ++count,
									total,
								})
							}
						} catch (e) {
							console.log(e)
						}
					})
			)
		})
	}
	if (!noUpdateHead) {
		const fullRef = await GitRefManager.expand({ fs: fs2, gitdir, ref })
		if (fullRef.startsWith("refs/heads")) {
			await GitRefManager.writeSymbolicRef({
				fs: fs2,
				gitdir,
				ref: "HEAD",
				value: fullRef,
			})
		} else {
			await GitRefManager.writeRef({ fs: fs2, gitdir, ref: "HEAD", value: oid })
		}
	}
}
async function analyze2({
	fs: fs2,
	cache: cache2,
	onProgress,
	dir,
	gitdir,
	ref,
	force,
	filepaths,
}) {
	let count = 0
	return _walk({
		fs: fs2,
		cache: cache2,
		dir,
		gitdir,
		trees: [TREE({ ref }), WORKDIR(), STAGE()],
		// @ts-ignore
		map: async function (fullpath, [commit3, workdir, stage]) {
			if (fullpath === ".") return
			if (filepaths && !filepaths.some((base) => worthWalking(fullpath, base))) {
				return null
			}
			if (onProgress) {
				await onProgress({ phase: "Analyzing workdir", loaded: ++count })
			}
			const key = [!!stage, !!commit3, !!workdir].map(Number).join("")
			switch (key) {
				case "000":
					return
				case "001":
					if (force && filepaths && filepaths.includes(fullpath)) {
						return ["delete", fullpath]
					}
					return
				case "010": {
					switch (await commit3.type()) {
						case "tree": {
							return ["mkdir", fullpath]
						}
						case "blob": {
							return ["create", fullpath, await commit3.oid(), await commit3.mode()]
						}
						case "commit": {
							return ["mkdir-index", fullpath, await commit3.oid(), await commit3.mode()]
						}
						default: {
							return ["error", `new entry Unhandled type ${await commit3.type()}`]
						}
					}
				}
				case "011": {
					switch (`${await commit3.type()}-${await workdir.type()}`) {
						case "tree-tree": {
							return
						}
						case "tree-blob":
						case "blob-tree": {
							return ["conflict", fullpath]
						}
						case "blob-blob": {
							if ((await commit3.oid()) !== (await workdir.oid())) {
								if (force) {
									return [
										"update",
										fullpath,
										// @ts-ignore
										await commit3.oid(),
										// @ts-ignore
										await commit3.mode(),
										// @ts-ignore
										(await commit3.mode()) !== (await workdir.mode()),
									]
								} else {
									return ["conflict", fullpath]
								}
							} else {
								if ((await commit3.mode()) !== (await workdir.mode())) {
									if (force) {
										return ["update", fullpath, await commit3.oid(), await commit3.mode(), true]
									} else {
										return ["conflict", fullpath]
									}
								} else {
									return ["create-index", fullpath, await commit3.oid(), await commit3.mode()]
								}
							}
						}
						case "commit-tree": {
							return
						}
						case "commit-blob": {
							return ["conflict", fullpath]
						}
						default: {
							return ["error", `new entry Unhandled type ${commit3.type}`]
						}
					}
				}
				case "100": {
					return ["delete-index", fullpath]
				}
				case "101": {
					switch (await stage.type()) {
						case "tree": {
							return ["rmdir", fullpath]
						}
						case "blob": {
							if ((await stage.oid()) !== (await workdir.oid())) {
								if (force) {
									return ["delete", fullpath]
								} else {
									return ["conflict", fullpath]
								}
							} else {
								return ["delete", fullpath]
							}
						}
						case "commit": {
							return ["rmdir-index", fullpath]
						}
						default: {
							return ["error", `delete entry Unhandled type ${await stage.type()}`]
						}
					}
				}
				case "110":
				case "111": {
					switch (`${await stage.type()}-${await commit3.type()}`) {
						case "tree-tree": {
							return
						}
						case "blob-blob": {
							if (
								// @ts-ignore
								(await stage.oid()) === (await commit3.oid()) && // @ts-ignore
								(await stage.mode()) === (await commit3.mode()) &&
								!force
							) {
								return
							}
							if (workdir) {
								if (
									// @ts-ignore
									(await workdir.oid()) !== (await stage.oid()) && // @ts-ignore
									(await workdir.oid()) !== (await commit3.oid())
								) {
									if (force) {
										return [
											"update",
											fullpath,
											// @ts-ignore
											await commit3.oid(),
											// @ts-ignore
											await commit3.mode(),
											// @ts-ignore
											(await commit3.mode()) !== (await workdir.mode()),
										]
									} else {
										return ["conflict", fullpath]
									}
								}
							} else if (force) {
								return [
									"update",
									fullpath,
									// @ts-ignore
									await commit3.oid(),
									// @ts-ignore
									await commit3.mode(),
									// @ts-ignore
									(await commit3.mode()) !== (await stage.mode()),
								]
							}
							if ((await commit3.mode()) !== (await stage.mode())) {
								return ["update", fullpath, await commit3.oid(), await commit3.mode(), true]
							}
							if ((await commit3.oid()) !== (await stage.oid())) {
								return ["update", fullpath, await commit3.oid(), await commit3.mode(), false]
							} else {
								return
							}
						}
						case "tree-blob": {
							return ["update-dir-to-blob", fullpath, await commit3.oid()]
						}
						case "blob-tree": {
							return ["update-blob-to-tree", fullpath]
						}
						case "commit-commit": {
							return ["mkdir-index", fullpath, await commit3.oid(), await commit3.mode()]
						}
						default: {
							return [
								"error",
								// @ts-ignore
								`update entry Unhandled type ${await stage.type()}-${await commit3.type()}`,
							]
						}
					}
				}
			}
		},
		// Modify the default flat mapping
		reduce: async function (parent, children2) {
			children2 = flat(children2)
			if (!parent) {
				return children2
			} else if (parent && parent[0] === "rmdir") {
				children2.push(parent)
				return children2
			} else {
				children2.unshift(parent)
				return children2
			}
		},
	})
}

// ../../../lix/packages/client/dist/git/checkout.js
async function checkout2(ctx, state, { branch: branch2 }) {
	state.branchName = branch2
	if (ctx.useLazyFS) {
		throw new Error(
			"not implemented for lazy lix mode yet, use openRepo with different branch instead"
		)
	}
	return await _checkout2({
		fs: ctx.rawFs,
		cache: ctx.cache,
		dir: ctx.dir,
		ref: state.branchName,
	})
}
async function _checkout2({
	fs: fs2,
	onProgress,
	dir,
	gitdir = join(dir, ".git"),
	remote = "origin",
	ref: _ref,
	filepaths,
	noCheckout = false,
	noUpdateHead = _ref === void 0,
	dryRun = false,
	force = false,
	track = true,
	cache: cache2 = {},
}) {
	try {
		assertParameter("fs", fs2)
		assertParameter("dir", dir)
		assertParameter("gitdir", gitdir)
		const ref = _ref || "HEAD"
		return await __checkout({
			fs: new FileSystem(fs2),
			cache: cache2,
			onProgress,
			dir,
			gitdir,
			remote,
			ref,
			filepaths,
			noCheckout,
			noUpdateHead,
			dryRun,
			force,
			track,
		})
	} catch (err) {
		err.caller = "git.checkout"
		throw err
	}
}

// ../../../lix/packages/client/dist/lix/emptyWorkdir.js
init_isomorphic_git()
async function emptyWorkdir(ctx, state) {
	const { rawFs, cache: cache2 } = ctx
	state.pending && (await state.pending)
	const statusResult = await statusList(ctx, state, { includeStatus: ["materialized", "ignored"] })
	const ignored = []
	const materialized = []
	const dirty = []
	for (const [path, status3] of statusResult) {
		if (status3 === "unmodified") {
			materialized.push(path)
		} else if (status3 === "ignored") {
			ignored.push("/" + path)
		} else {
			dirty.push(path)
		}
	}
	if (dirty.length > 0) {
		console.error(dirty)
		throw new Error("could not empty the workdir, uncommitted changes")
	}
	const listing = await allFiles(rawFs, ignored)
	await Promise.all(
		listing.map((entry) =>
			rawFs
				.rm(entry)
				.catch((err) => {
					console.warn(err)
				})
				.then(() =>
					isomorphic_git_default.remove({
						fs: rawFs,
						dir: "/",
						cache: cache2,
						filepath: entry,
					})
				)
		)
	)
	return materialized
}
async function allFiles(fs2, ignored, root = "/") {
	const entries = await fs2.readdir(root)
	const notIgnored = entries.filter((entry) => !ignored.includes(root + entry))
	const withMeta = await Promise.all(
		notIgnored.map(async (entry) => ({
			name: entry,
			isDir: (
				await fs2.lstat(root + entry).catch((err) => {
					console.warn(err)
				})
			)?.isDirectory?.(),
		}))
	)
	const withChildren = await Promise.all(
		withMeta.map(async ({ name, isDir }) =>
			isDir ? allFiles(fs2, ignored, root + name + "/") : root + name
		)
	)
	return withChildren.flat().map((entry) => entry.replace(/^\//, ""))
}

// ../../../lix/packages/client/dist/git-http/helpers.js
init_isomorphic_git()
init_isomorphic_git()
function padHex2(pad, n) {
	const s = n.toString(16)
	return "0".repeat(pad - s.length) + s
}
var WANT_PREFIX = "want "
function overrideWants(lines, oids) {
	oids = [...new Set(oids)]
	const newLines = []
	let wantsCount = 0
	let lastLineWasAWants = false
	for (const line of lines) {
		if (line.startsWith(WANT_PREFIX)) {
			lastLineWasAWants = true
			if (oids.length > wantsCount) {
				const postOidCurrentLine = line.slice(
					Math.max(0, WANT_PREFIX.length + oids[wantsCount].length)
				)
				newLines.push(`${WANT_PREFIX}${oids[wantsCount]}${postOidCurrentLine}`)
			}
			wantsCount += 1
		}
		if (!line.startsWith(WANT_PREFIX)) {
			if (lastLineWasAWants && oids.length > wantsCount) {
				while (oids.length > wantsCount) {
					newLines.push(WANT_PREFIX + oids[wantsCount] + "\n")
					wantsCount += 1
				}
				lastLineWasAWants = false
			}
			newLines.push(line)
		}
	}
	return newLines
}
function addWantsCapabilities(lines) {
	let capabilitiesAdded = false
	const updatedLines = []
	for (let line of lines) {
		if (line.startsWith(WANT_PREFIX) && !capabilitiesAdded) {
			line =
				line.slice(0, Math.max(0, line.length - 1)) +
				" allow-tip-sha1-in-want allow-reachable-sha1-in-want\n"
			line = line.replace("ofs-delta", "")
			capabilitiesAdded = true
		}
		updatedLines.push(line)
	}
	return updatedLines
}
function addNoProgress(lines) {
	let capabilitiesAdded = false
	const updatedLines = []
	for (let line of lines) {
		if (line.startsWith(WANT_PREFIX) && !capabilitiesAdded) {
			line = line.slice(0, Math.max(0, line.length - 1)) + " no-progress\n"
			capabilitiesAdded = true
		}
		updatedLines.push(line)
	}
	return updatedLines
}
function addBlobNoneFilter(lines) {
	let filterCapabilityAdded = false
	let filterAdded = false
	const updatedLines = []
	const flushLine = ""
	for (let line of lines) {
		if (line.startsWith("want") && !filterCapabilityAdded) {
			line = line.slice(0, Math.max(0, line.length - 1)) + " filter\n"
			filterCapabilityAdded = true
		}
		if (
			!filterAdded &&
			(line.startsWith("deepen-since") || line.startsWith("deepen-not") || line === flushLine)
		) {
			updatedLines.push("filter blob:none\n")
			filterAdded = true
		}
		updatedLines.push(line)
	}
	return updatedLines
}
function overrideHaves(lines, oids) {
	oids = [...new Set(oids)]
	const linesWithoutHaves = []
	const flushLine = ""
	for (const line of lines) {
		if (!line.startsWith("have ")) {
			linesWithoutHaves.push(line)
		}
	}
	const updatedLines = []
	for (const line of linesWithoutHaves) {
		updatedLines.push(line)
		if (line === flushLine) {
			for (const oid of oids) {
				updatedLines.push("have " + oid + "\n")
			}
		}
	}
	return updatedLines
}
async function blobExistsLocaly({ fs: fs2, oid, gitdir, cache: cache2 }) {
	try {
		return await hasObject({
			// fs must not be intercepted - we don't want to intercept calls of read Object  // TODO #1459 can we check this by type checking or an added flag property for better dx?
			fs: new FileSystem(fs2),
			oid,
			gitdir,
			cache: cache2,
			// NOTE: we use deflated to stop early in _readObject no hashing etc is happening for format deflated
			format: "deflated",
		})
	} catch (err) {
		if (err.code !== "ENOENT" && err.code !== "NotFoundError") {
			throw err
		}
	}
	return false
}
function encodePackLine(line) {
	const flushLine = ""
	if (line === flushLine) {
		const paddedHex = padHex2(4, 0)
		return paddedHex
	}
	const length = line.length + 4
	const hexLength = padHex2(4, length)
	const lineWithLength = hexLength + line
	return lineWithLength
}
function decodeGitPackLines(concatenatedUint8Array) {
	const strings = []
	let offset = 0
	while (offset + 4 < concatenatedUint8Array.length) {
		const hexLength = new TextDecoder().decode(concatenatedUint8Array.subarray(offset, offset + 4))
		const packLineLength = parseInt(hexLength, 16)
		if (packLineLength === 0) {
			strings.push("")
			offset += 4
		} else if (packLineLength === 1) {
			throw new Error("decodeGitPackLines does not support delimiter yet")
		} else {
			const contentStart = offset + 4
			const stringData = new TextDecoder().decode(
				concatenatedUint8Array.subarray(contentStart, offset + packLineLength)
			)
			offset += packLineLength
			strings.push(stringData)
		}
	}
	return strings
}

// ../../../lix/packages/client/dist/git-http/optimizeReq.js
async function optimizeReq(gitConfig = {}, { method, url, body }) {
	if (url.endsWith("info/refs?service=git-upload-pack") && gitConfig.addRefs !== void 0) {
		const uploadPackUrl = url.replace("info/refs?service=git-upload-pack", "git-upload-pack")
		const lines = []
		lines.push(encodePackLine("command=ls-refs"))
		lines.push(encodePackLine("agent=lix") + "0001")
		if (gitConfig.addRefs?.length > 0) {
			for (let ref of gitConfig.addRefs) {
				if (!ref.startsWith("refs/")) {
					ref = "refs/heads/" + ref
				}
				lines.push(encodePackLine("ref-prefix " + ref))
			}
		}
		lines.push(encodePackLine("ref-prefix HEAD"))
		lines.push(encodePackLine("symrefs"))
		lines.push(encodePackLine(""))
		return {
			url: uploadPackUrl,
			body: lines.join(""),
			method: "POST",
			headers: {
				accept: "application/x-git-upload-pack-result",
				"content-type": "application/x-git-upload-pack-request",
				"git-protocol": "version=2",
			},
		}
	}
	if (method === "POST") {
		let rawLines = decodeGitPackLines(body)
		if (gitConfig.noBlobs) {
			rawLines = addBlobNoneFilter(rawLines)
		}
		if (gitConfig.overrideHaves) {
			rawLines = overrideHaves(rawLines, gitConfig.overrideHaves)
		}
		rawLines = addNoProgress(rawLines)
		if (gitConfig.overrideWants) {
			rawLines = addWantsCapabilities(rawLines)
			rawLines = overrideWants(rawLines, gitConfig.overrideWants)
		}
		const newBody = rawLines.map((updatedRawLine) => encodePackLine(updatedRawLine)).join("")
		return {
			body: newBody,
		}
	}
	return void 0
}
async function optimizeRes({ origUrl, resBody, statusCode, resHeaders }) {
	if (!origUrl.endsWith("info/refs?service=git-upload-pack") || statusCode !== 200) {
		return void 0
	}
	const capabilites = [
		"multi_ack",
		"thin-pack",
		"side-band",
		"side-band-64k",
		"ofs-delta",
		"shallow",
		"deepen-since",
		"deepen-not",
		"deepen-relative",
		"no-progress",
		"include-tag",
		"multi_ack_detailed",
		"allow-tip-sha1-in-want",
		"allow-reachable-sha1-in-want",
		"no-done",
		"filter",
		"object-format=sha1",
	]
	const origLines = decodeGitPackLines(resBody)
	const rewrittenLines = ["# service=git-upload-pack\n", ""]
	let headSymref = ""
	for (const line of origLines) {
		if (line.includes("HEAD symref-target")) {
			headSymref = "refs" + line.slice(64).replace("\n", "")
			const headBlob = line.slice(0, 40)
			rewrittenLines.push(
				headBlob + " HEAD\0" + capabilites.join(" ") + " symref=HEAD:" + headSymref
			)
			rewrittenLines.push(headBlob + " " + headSymref)
		} else {
			rewrittenLines.push(line.replace("\n", ""))
		}
	}
	rewrittenLines.push("")
	resHeaders["content-type"] = "application/x-git-upload-pack-advertisement"
	const bodyString = rewrittenLines.map((updatedRawLine) => encodePackLine(updatedRawLine)).join("")
	return {
		resHeaders,
		resBody: [new TextEncoder().encode(bodyString)],
	}
}

// ../../../lix/packages/client/dist/lix/checkoutPlaceholders.js
init_isomorphic_git()
async function checkOutPlaceholders(
	ctx,
	state,
	{ materializeGitignores = true, preload = [] } = {}
) {
	const { rawFs, cache: cache2, dir } = ctx
	const { branchName, checkedOut, sparseFilter } = state
	await _checkout2({
		fs: ctx.rawFs,
		cache: cache2,
		dir,
		ref: branchName,
		filepaths: [],
	})
	const fs2 = rawFs
	const gitignoreFiles = []
	let rootHash
	await isomorphic_git_default.walk({
		fs: fs2,
		dir,
		cache: cache2,
		gitdir: ".git",
		trees: [isomorphic_git_default.TREE({ ref: branchName })],
		map: async function (fullpath, [commit3]) {
			if (!commit3) {
				return void 0
			}
			if (fullpath.endsWith(".gitignore")) {
				gitignoreFiles.push(fullpath)
			}
			const fileMode = await commit3.mode()
			const oid = await commit3.oid()
			if (fullpath === ".") {
				rootHash = oid
			}
			const fileType = modeToFileType(fileMode)
			if (
				sparseFilter &&
				!sparseFilter({
					filename: fullpath,
					type: fileType,
				})
			) {
				return void 0
			}
			if (fileType === "folder" && !checkedOut.has(fullpath)) {
				return fullpath
			}
			if (fileType === "file" && !checkedOut.has(fullpath)) {
				await fs2._createPlaceholder(fullpath, { mode: fileMode, oid, rootHash })
				return fullpath
			}
			if (fileType === "symlink" && !checkedOut.has(fullpath)) {
				await fs2._createPlaceholder(fullpath, { mode: fileMode, oid, rootHash })
				return fullpath
			}
			console.warn("ignored checkout palcholder path", fullpath, fileType)
			return void 0
		},
	})
	if (gitignoreFiles.length && materializeGitignores) {
		preload = [...gitignoreFiles, ...preload]
	}
	await state.ensureFirstBatch({ preload })
	return { gitignoreFiles }
}

// ../../../lix/packages/client/dist/git/pull.js
async function pull2(ctx, state, cmdArgs) {
	if (!ctx.gitUrl) {
		throw new Error("Could not find repo url, only github supported for pull at the moment")
	}
	const branchName =
		state.branchName ||
		(await isomorphic_git_default.currentBranch({ fs: ctx.rawFs, dir: "/" })) ||
		"HEAD"
	const oid = await isomorphic_git_default.resolveRef({
		fs: ctx.rawFs,
		dir: "/",
		ref: "refs/remotes/origin/" + branchName,
	})
	const { commit: commit3 } = await isomorphic_git_default.readCommit({
		fs: ctx.rawFs,
		dir: "/",
		oid,
	})
	const since = new Date(commit3.committer.timestamp * 1e3)
	const { fetchHead, fetchHeadDescription } = await isomorphic_git_default.fetch({
		since,
		fs: ctx.rawFs,
		cache: ctx.cache,
		http: makeHttpClient({
			debug: ctx.debug,
			description: "pull",
			onReq: ctx.experimentalFeatures.lazyClone
				? optimizeReq.bind(null, {
						noBlobs: true,
						addRefs: [branchName],
				  })
				: void 0,
			onRes: ctx.experimentalFeatures.lazyClone ? optimizeRes : void 0,
		}),
		corsProxy: ctx.gitProxyUrl,
		ref: branchName,
		tags: false,
		dir: ctx.dir,
		url: ctx.gitUrl,
		// remote: "origin",
		// remoteRef,
		singleBranch: cmdArgs.singleBranch || true,
	})
	if (!fetchHead) {
		throw new Error("could not fetch head")
	}
	let materialized = []
	if (ctx.experimentalFeatures.lazyClone) {
		materialized = await emptyWorkdir(ctx, state)
		ctx.debug && console.info("experimental checkout after pull preload:", materialized)
		state.checkedOut.clear()
	}
	const mergeDriver = ({ branches, contents, path }) => {
		console.info("mergeDriver", branches, contents, path)
		ctx.rawFs.writeFile(path + `.${branches[2].slice(0, 4)}.conflict`, contents[2] || "")
		return { cleanMerge: true, mergedText: contents[1] || "" }
	}
	const mergeRes = await isomorphic_git_default
		.merge({
			fs: state.nodeishFs,
			cache: ctx.cache,
			dir: ctx.dir,
			ours: branchName,
			theirs: fetchHead,
			fastForward: cmdArgs.fastForward === false ? false : true,
			message: `Merge ${fetchHeadDescription}`,
			author: cmdArgs.author || ctx.author,
			dryRun: false,
			noUpdateBranch: false,
			abortOnConflict: true,
			mergeDriver: ctx.experimentalFeatures.lixMerge ? mergeDriver : void 0,
			// different to native git this replaces the default 3 way merge
			// committer,
			// signingKey,
			// fastForwardOnly,
		})
		.catch((error) => ({ error }))
	console.info("mergeRes", { data: mergeRes.data, code: mergeRes.code, error: mergeRes.error })
	if (ctx.experimentalFeatures.lazyClone) {
		await checkOutPlaceholders(ctx, state, { preload: materialized })
	} else {
		await _checkout2({
			fs: ctx.rawFs,
			cache: ctx.cache,
			dir: ctx.dir,
			ref: branchName,
			noCheckout: false,
		})
	}
}

// ../../../lix/packages/client/dist/git/listRemotes.js
init_isomorphic_git()
async function listRemotes2(ctx, state) {
	try {
		const remotes = await isomorphic_git_default.listRemotes({
			fs: state.nodeishFs,
			dir: ctx.dir,
		})
		return remotes
	} catch (_err) {
		return void 0
	}
}

// ../../../lix/packages/client/dist/git/log.js
init_isomorphic_git()
async function log2(ctx, cmdArgs) {
	return await isomorphic_git_default.log({
		fs: ctx.rawFs,
		depth: cmdArgs?.depth,
		filepath: cmdArgs?.filepath,
		dir: ctx.dir,
		ref: cmdArgs?.ref,
		cache: ctx.cache,
		since: cmdArgs?.since,
		force: cmdArgs?.force,
		follow: cmdArgs?.follow,
	})
}

// ../../../lix/packages/client/dist/helpers.js
var withProxy = ({ nodeishFs, verbose = false, description, intercept }) => {
	return new Proxy(nodeishFs, {
		get(getTarget, prop, receiver) {
			if (getTarget[prop]) {
				return new Proxy(getTarget[prop], {
					apply(callTarget, thisArg, argumentsList) {
						if (verbose) {
							console.warn(`${description} fs:`, prop, argumentsList)
						}
						const execute = () => Reflect.apply(callTarget, thisArg, argumentsList)
						return intercept ? intercept({ prop, argumentsList, execute }) : execute()
					},
				})
			}
			return Reflect.get(getTarget, prop, receiver)
		},
	})
}
function parseOrigin({ remotes }) {
	const origin = remotes?.find((elements) => elements.remote === "origin")
	if (origin === void 0) {
		return void 0
	}
	let result = origin.url
	if (result.endsWith(".git") === false) {
		result += ".git"
	}
	return transformRemote(result)
}
function transformRemote(remote) {
	const regex = /(?:https:\/\/|@|git:\/\/)([^/]+)\/(.+?)(?:\.git)?$/
	const matches = remote.match(regex)
	if (matches && matches[1] && matches[2]) {
		let host = matches[1].replace(/:/g, "/")
		const repo = matches[2]
		const hostRegex = /(ghp_|ghs_)[\w]+@/
		host = host.replace(hostRegex, "")
		return `${host}/${repo}.git`
	}
	return void 0
}
function parseLixUri(uriText) {
	let url
	try {
		url = new URL(uriText)
	} catch (error) {
		return {
			error,
			username: "",
			password: "",
			protocol: "",
			lixHost: "",
			namespace: "",
			repoHost: "",
			owner: "",
			repoName: "",
		}
	}
	const { protocol, host, pathname, username, password } = url
	const pathParts = pathname.split("/")
	let lixHost = ""
	let namespace = ""
	let repoHost = ""
	let owner = ""
	let repoName = ""
	if (host === "github.com") {
		repoHost = host
		owner = pathParts[1] || ""
		repoName = pathParts[2] || ""
		if (!repoHost || !owner || !repoName) {
			return {
				error: new Error(
					`Invalid url format for '${uriText}' for direct cloning repository from github, please use the format of https://github.com/opral/monorepo.`
				),
				username,
				password,
				protocol,
				lixHost,
				namespace,
				repoHost,
				owner,
				repoName,
			}
		}
	} else {
		lixHost = host
		namespace = pathParts[1] || ""
		repoHost = pathParts[2] || ""
		owner = pathParts[3] || ""
		repoName = pathParts[4] || ""
		if (!namespace || !host || !owner || !repoName) {
			return {
				error: new Error(
					`Invalid url format for '${uriText}' for cloning repository, please use the format of https://lix.inlang.com/git/github.com/opral/monorepo.`
				),
				username,
				password,
				protocol,
				lixHost,
				namespace,
				repoHost,
				owner,
				repoName,
			}
		}
	}
	return {
		username,
		password,
		protocol,
		lixHost,
		namespace,
		repoHost,
		owner,
		repoName,
	}
}
async function hash(inputStr) {
	let usedCrypto
	if (typeof crypto === "undefined" && typeof process !== "undefined" && process?.versions?.node) {
		const modName = "crypto"
		usedCrypto = globalThis?.crypto || (await import(`node:${modName}`))
	} else if (typeof crypto !== "undefined") {
		usedCrypto = crypto
	}
	if (!usedCrypto) {
		throw new Error("Could not find crypto features in runtime")
	}
	const idDigest = await usedCrypto.subtle.digest("SHA-256", new TextEncoder().encode(inputStr))
	return [...new Uint8Array(idDigest)].map((b) => ("00" + b.toString(16)).slice(-2)).join("")
}

// ../../../lix/packages/client/dist/git/getOrigin.js
async function getOrigin(ctx, state) {
	const remotes = (await listRemotes2(ctx, state)) || []
	return await parseOrigin({ remotes })
}

// ../../../lix/packages/client/dist/git/getBranches.js
init_isomorphic_git()
async function getBranches(ctx) {
	if (!ctx.gitUrl) {
		throw new Error("Could not find repo url, only github supported for getBranches at the moment")
	}
	let serverRefs
	try {
		serverRefs = await isomorphic_git_default.listServerRefs({
			url: ctx.gitUrl,
			corsProxy: ctx.gitProxyUrl,
			prefix: "refs/heads",
			http: makeHttpClient({ debug: ctx.debug, noCache: true, description: "getBranches" }),
		})
	} catch (_error) {
		return void 0
	}
	return (
		serverRefs
			.filter((ref) => !ref.ref.startsWith("refs/heads/gh-readonly-queue/"))
			.map((ref) => ref.ref.replace("refs/heads/", "")) || void 0
	)
}

// ../../../lix/packages/client/dist/git/getCurrentBranch.js
init_isomorphic_git()
async function getCurrentBranch(ctx, state) {
	return (
		(await isomorphic_git_default.currentBranch({
			fs: state.nodeishFs,
			dir: ctx.dir,
		})) || void 0
	)
}

// ../../../lix/packages/client/dist/github/getMeta.js
async function getMeta(ctx) {
	const { gitUrl, owner, repoName, githubClient } = ctx
	if (!gitUrl) {
		throw new Error("Could not find repo url, only github supported for getMeta at the moment")
	}
	const res = await githubClient.getRepo({ repoName, owner })
	let isInstalled = false
	const installResult = await githubClient.getInstallations()
	if (!("error" in installResult)) {
		const found = installResult.installations.find((i) => i.account.login === owner)
		if (found?.repository_selection === "all") {
			isInstalled = true
		} else if (found) {
			const repoResult = await githubClient.getAvailableRepos(found.id)
			if (
				!("error" in repoResult) &&
				repoResult.repositories.find((r) => r.full_name === `${owner}/${repoName}`)
			) {
				isInstalled = true
			}
		}
	}
	if ("error" in res) {
		return { error: res.error }
	} else {
		return {
			allowForking: res.data.allow_forking,
			name: res.data.name,
			isPrivate: res.data.private,
			isFork: res.data.fork,
			isInstalled,
			permissions: {
				admin: res.data.permissions?.admin || false,
				push: res.data.permissions?.push || false,
				pull: res.data.permissions?.pull || false,
			},
			owner: {
				type: res.data.owner.type.toLowerCase(),
				name: res.data.owner.name || void 0,
				email: res.data.owner.email || void 0,
				login: res.data.owner.login,
			},
			parent: res.data.parent
				? {
						url: transformRemote(res.data.parent.git_url) || "unknown",
						fullName: res.data.parent.full_name,
				  }
				: void 0,
		}
	}
}

// ../../../lix/packages/client/dist/github/forkStatus.js
init_isomorphic_git()
async function forkStatus(ctx) {
	const {
		gitUrl,
		debug: debug10,
		dir,
		cache: cache2,
		owner,
		repoName,
		githubClient,
		gitProxyUrl,
	} = ctx
	if (!gitUrl) {
		throw new Error("Could not find repo url, only github supported for forkStatus at the moment")
	}
	const { isFork, parent, error } = await getMeta(ctx)
	if (error) {
		return { error: "could check fork status of repo" }
	}
	if (!isFork) {
		return { error: "repo is not a fork" }
	}
	const useBranchName = await isomorphic_git_default.currentBranch({
		fs: ctx.rawFs,
		dir,
		fullname: false,
	})
	if (!useBranchName) {
		return { error: "could not get fork status for detached head" }
	}
	await isomorphic_git_default.addRemote({
		dir,
		remote: "upstream",
		url: "https://" + parent.url,
		fs: ctx.rawFs,
	})
	try {
		await isomorphic_git_default.fetch({
			depth: 1,
			singleBranch: true,
			dir,
			cache: cache2,
			ref: useBranchName,
			remote: "upstream",
			http: makeHttpClient({
				debug: debug10,
				description: "forkStatus",
				onReq: ctx.experimentalFeatures.lazyClone
					? optimizeReq.bind(null, {
							noBlobs: true,
							addRefs: [useBranchName || "HEAD"],
					  })
					: void 0,
				onRes: ctx.experimentalFeatures.lazyClone ? optimizeRes : void 0,
			}),
			tags: false,
			fs: ctx.rawFs,
		})
	} catch (err) {
		return { error: err }
	}
	const currentUpstreamCommit = await isomorphic_git_default.resolveRef({
		fs: ctx.rawFs,
		dir: "/",
		ref: "upstream/" + useBranchName,
	})
	const currentOriginCommit = await isomorphic_git_default.resolveRef({
		fs: ctx.rawFs,
		dir: "/",
		ref: useBranchName,
	})
	if (currentUpstreamCommit === currentOriginCommit) {
		return { ahead: 0, behind: 0, conflicts: void 0 }
	}
	const compare = await githubClient
		.compare({
			owner,
			repoName,
			base: currentUpstreamCommit,
			head: currentOriginCommit,
		})
		.catch((newError) => {
			return { error: newError }
		})
	if ("error" in compare || !("data" in compare)) {
		return { error: compare.error || "could not diff repos on github" }
	}
	const ahead = compare.data.ahead_by
	const behind = compare.data.behind_by
	await isomorphic_git_default.fetch({
		depth: behind + 1,
		remote: "upstream",
		cache: cache2,
		singleBranch: true,
		dir,
		ref: useBranchName,
		http: makeHttpClient({ debug: debug10, description: "forkStatus" }),
		fs: ctx.rawFs,
	})
	await isomorphic_git_default.fetch({
		depth: ahead + 1,
		cache: cache2,
		singleBranch: true,
		ref: useBranchName,
		dir,
		http: makeHttpClient({ debug: debug10, description: "forkStatus" }),
		corsProxy: gitProxyUrl,
		fs: ctx.rawFs,
	})
	let conflicts
	try {
		await isomorphic_git_default.merge({
			fs: ctx.rawFs,
			cache: cache2,
			author: { name: "lix" },
			dir,
			ours: useBranchName,
			dryRun: true,
			theirs: "upstream/" + useBranchName,
			noUpdateBranch: true,
			abortOnConflict: true,
		})
	} catch (err) {
		conflicts = {
			data: err.data,
			code: err.code,
		}
		console.warn(conflicts)
	}
	return { ahead, behind, conflicts }
}

// ../../../lix/packages/client/dist/github/createFork.js
async function createFork(ctx) {
	return await ctx.githubClient.createFork({
		owner: ctx.owner,
		repo: ctx.repoName,
	})
}

// ../../../lix/packages/client/dist/github/mergeUpstream.js
init_isomorphic_git()
async function mergeUpstream(ctx, cmdArgs = {}) {
	if (!ctx.gitUrl) {
		throw new Error(
			"Could not find repo url, only github supported for mergeUpstream at the moment"
		)
	}
	const branch2 =
		cmdArgs?.branch ||
		(await isomorphic_git_default.currentBranch({
			fs: ctx.rawFs,
			dir: ctx.dir,
			fullname: false,
		}))
	if (typeof branch2 !== "string") {
		throw "could not get current branch"
	}
	let response
	try {
		response = await ctx.githubClient.mergeUpstream({
			branch: branch2,
			owner: ctx.owner,
			repoName: ctx.repoName,
		})
	} catch (error) {
		return { error }
	}
	return response?.data || response.error
}

// ../../../lix/packages/client/dist/lix/getFirstCommitHash.js
init_isomorphic_git()
async function getFirstCommitHash(ctx) {
	const getFirstCommitFs = ctx.rawFs
	const maybeShallow = !!(await getFirstCommitFs
		.readFile(ctx.dir + "/.git/shallow", { encoding: "utf-8" })
		.catch(() => void 0))
	if (maybeShallow) {
		console.warn("shallow clone detected, not generating first commit hash.")
		return void 0
	}
	if (ctx.useLazyFS) {
		try {
			await isomorphic_git_default.fetch({
				singleBranch: true,
				dir: ctx.dir,
				depth: 2147483647,
				http: makeHttpClient({ debug: ctx.debug, description: "getFirstCommitHash" }),
				corsProxy: ctx.gitProxyUrl,
				fs: getFirstCommitFs,
			})
		} catch {
			return void 0
		}
	}
	let firstCommitHash = "HEAD"
	for (;;) {
		const commits = await isomorphic_git_default
			.log({
				fs: getFirstCommitFs,
				depth: 550,
				dir: ctx.dir,
				ref: firstCommitHash,
			})
			.catch((error) => {
				return { error }
			})
		if ("error" in commits) {
			firstCommitHash = void 0
			break
		}
		const lastHashInPage = commits.at(-1)?.oid
		if (lastHashInPage) {
			firstCommitHash = lastHashInPage
		}
		if (commits.length < 550) {
			break
		}
	}
	return firstCommitHash
}

// ../../../lix/packages/client/dist/repoContext.js
init_isomorphic_git()

// ../../../lix/packages/client/dist/allowedRepos.js
var allowedRepos_default = [
	"inlang/example",
	"opral/example",
	"inlang/ci-test-repo",
	"opral/ci-test-repo",
	"opral/monorepo",
	"inlang/example-test",
	"opral/example-test",
	"janfjohannes/inlang-example",
	"janfjohannes/cal.com",
	"niklasbuchfink/appflowy",
	"jldec/load-test",
]

// ../../../lix/packages/client/dist/github/client.js
var import_octokit = __toESM(require_dist_node26(), 1)
function makeGithubClient({ gitHubProxyUrl } = {}) {
	const githubClient = new import_octokit.Octokit({
		request: {
			fetch: (...ghArgs) => {
				ghArgs[0] = gitHubProxyUrl + "/" + ghArgs[0]
				if (!ghArgs[1]) {
					ghArgs[1] = {}
				}
				if (gitHubProxyUrl) {
					ghArgs[1].credentials = "include"
				}
				return fetch(...ghArgs)
			},
		},
	})
	const getRepo = async ({ repoName, owner }) =>
		await githubClient
			.request("GET /repos/{owner}/{repo}", {
				owner,
				repo: repoName,
			})
			.catch((newError) => {
				return { error: newError }
			})
	const getAvailableRepos = async (installation_id) =>
		await githubClient
			.request(`GET /user/installations/{installation_id}/repositories`, {
				installation_id,
			})
			.then(({ data }) => data)
			.catch((newError) => {
				return { error: newError }
			})
	const getInstallations = async () =>
		await githubClient
			.request("GET /user/installations")
			.then(({ data }) => data)
			.catch((newError) => {
				return { error: newError }
			})
	const createFork2 = githubClient.rest.repos.createFork
	const mergeUpstream2 = async ({ branch: branch2, owner, repoName }) =>
		await githubClient.request("POST /repos/{owner}/{repo}/merge-upstream", {
			branch: branch2,
			owner,
			repo: repoName,
		})
	const compare = async ({ owner, repoName, base, head }) => {
		return await githubClient.request("GET /repos/{owner}/{repo}/compare/{base}...{head}", {
			owner,
			repo: repoName,
			base,
			head,
		})
	}
	return {
		getInstallations,
		getAvailableRepos,
		getRepo,
		createFork: createFork2,
		mergeUpstream: mergeUpstream2,
		compare,
	}
}

// ../../../lix/packages/client/dist/repoContext.js
async function repoContext(url, args) {
	const rawFs =
		args.nodeishFs ||
		(await Promise.resolve().then(() => (init_dist(), dist_exports))).createNodeishMemoryFs()
	const author = args.author
	let debug10 = args.debug || false
	if (
		!url ||
		(!url.startsWith("file://") && !url.startsWith("https://") && !url.startsWith("http://"))
	) {
		throw new Error("repo url is required, use file:// for local repos")
	}
	if (debug10 && typeof window !== "undefined") {
		window["rawFs"] = rawFs
	}
	let freshClone = false
	let dir = "/"
	if (url.startsWith("file:")) {
		dir = url.replace("file://", "")
		const remotes = await isomorphic_git_default
			.listRemotes({
				fs: rawFs,
				dir,
			})
			.catch(() => [])
		const origin = remotes.find(({ remote }) => remote === "origin")?.url || ""
		if (origin.startsWith("git@githubClient.com:")) {
			url = origin.replace("git@githubClient.com:", "https://githubClient.com/")
		} else {
			url = origin
		}
	} else {
		const maybeGitDir = await rawFs.stat(".git").catch((error) => ({ error }))
		if ("error" in maybeGitDir) {
			freshClone = true
		}
	}
	const { protocol, lixHost, repoHost, owner, repoName, username, password, namespace } =
		parseLixUri(url)
	if (debug10 && (username || password)) {
		console.warn(
			"username and password and providers other than github are not supported yet. Only local commands will work."
		)
	}
	const isWhitelistedRepo = allowedRepos_default.includes(
		`${owner}/${repoName}`.toLocaleLowerCase()
	)
	const experimentalFeatures =
		args.experimentalFeatures ||
		(isWhitelistedRepo ? { lazyClone: freshClone, lixCommit: true } : {})
	const useLazyFS = experimentalFeatures?.lazyClone && rawFs?._createPlaceholder
	const cache2 = useLazyFS ? {} : void 0
	let gitProxyUrl
	let gitHubProxyUrl
	if (namespace === "git") {
		gitProxyUrl = lixHost ? `${protocol}//${lixHost}/git-proxy` : ""
		gitHubProxyUrl = lixHost ? `${protocol}//${lixHost}/github-proxy` : ""
	}
	debug10 &&
		console.info({
			gitProxyUrl,
			gitHubProxyUrl,
			protocol,
			lixHost,
			repoHost,
			owner,
			repoName,
			username,
			password,
		})
	const githubClient = makeGithubClient({ gitHubProxyUrl })
	const gitUrl = repoName ? `https://${repoHost}/${owner}/${repoName}` : ""
	if (!gitUrl && debug10) {
		console.warn(
			"valid repo url / local repo not found, only fs features available outside of repo"
		)
	}
	const expFeatures = Object.entries(experimentalFeatures)
		.filter(([_, value]) => value)
		.map(([key]) => key)
	if (expFeatures.length) {
		console.warn("using experimental git features for this repo.", expFeatures)
	}
	return {
		gitUrl,
		gitProxyUrl,
		protocol,
		lixHost,
		repoHost,
		owner,
		repoName,
		username,
		password,
		namespace,
		useLazyFS,
		githubClient,
		debug: debug10,
		experimentalFeatures,
		author,
		freshClone,
		dir,
		// maybe handle these different when touching lixFS impl.
		rawFs,
		cache: cache2,
	}
}

// ../../../lix/packages/client/dist/repoState.js
init_isomorphic_git()
async function repoState(ctx, args) {
	const {
		gitUrl,
		debug: debug10,
		rawFs,
		experimentalFeatures,
		gitProxyUrl,
		freshClone,
		useLazyFS,
		dir,
		cache: cache2,
	} = ctx
	const nodeishFs = withProxy({
		nodeishFs: rawFs,
		verbose: debug10,
		description: "app",
		intercept: useLazyFS ? delayedAction : void 0,
	})
	let preloads = []
	let nextBatch = []
	const state = {
		ensureFirstBatch,
		pending: void 0,
		nodeishFs,
		checkedOut: /* @__PURE__ */ new Set(),
		branchName: args.branch,
		currentRef: "HEAD",
		defaultBranch: "refs/remotes/origin/HEAD",
		sparseFilter: args.sparseFilter,
	}
	async function ensureFirstBatch(args2) {
		if (!useLazyFS) {
			return
		}
		preloads = preloads.concat(args2?.preload || [])
		if (state.pending) {
			await state.pending.catch((error) => console.error(error))
		} else {
			if (preloads.length) {
				nextBatch.push("")
				state.pending = doCheckout().finally(() => {
					state.pending = void 0
				})
			}
		}
	}
	async function doCheckout() {
		if (nextBatch.length < 1) {
			return
		}
		const thisBatch = []
		const oidPromises = []
		for (const entry of nextBatch) {
			if (entry === "") {
				continue
			}
			if (typeof entry === "string") {
				if (!state.checkedOut.has(entry)) {
					thisBatch.push(entry)
				}
			} else {
				oidPromises.push(entry)
			}
		}
		nextBatch = []
		if (debug10) {
			oidPromises.length && console.warn("fetching oids ", oidPromises)
		}
		if (oidPromises.length > 0) {
			await Promise.all(oidPromises).catch(console.error)
		}
		const allBatchFiles = [.../* @__PURE__ */ new Set([...preloads, ...thisBatch])]
		preloads = []
		if (debug10) {
			console.warn("checking out ", JSON.stringify(allBatchFiles))
		}
		const oids = []
		const placeholders = allBatchFiles.filter((entry) => rawFs._isPlaceholder?.(entry))
		for (const placeholder of placeholders) {
			const stats = await rawFs.stat(placeholder)
			oids.push(stats._oid)
		}
		if (useLazyFS && oids.length > 0) {
			const toFetch = (
				await Promise.all(
					oids.map((oid) =>
						blobExistsLocaly({
							fs: rawFs,
							cache: cache2,
							oid,
							gitdir: ".git",
						}).then((exists) => (exists ? false : oid))
					)
				)
			).filter((a) => a !== false)
			if (toFetch.length) {
				await isomorphic_git_default
					.fetch({
						cache: cache2,
						fs: rawFs,
						dir: "/",
						http: makeHttpClient({
							debug: debug10,
							description: "lazy fetch",
							onReq: optimizeReq.bind(null, {
								noBlobs: false,
								addRefs: [state.branchName || "HEAD"],
								overrideWants: toFetch,
							}),
							onRes: optimizeRes,
						}),
						depth: 1,
						singleBranch: true,
						tags: false,
					})
					.catch((error) => {
						console.error({ error, toFetch })
					})
			}
		}
		let res
		if (allBatchFiles.length > 0) {
			await Promise.all(placeholders.map((placeholder) => rawFs.rm(placeholder).catch(() => {})))
			res = await _checkout2({
				fs: rawFs,
				dir,
				cache: cache2,
				ref: state.branchName,
				filepaths: allBatchFiles,
			}).catch((error) => {
				console.error({ error, allBatchFiles })
			})
		}
		for (const entry of allBatchFiles) {
			state.checkedOut.add(entry)
		}
		if (debug10) {
			console.warn("checked out ", allBatchFiles)
		}
		if (nextBatch.length) {
			return doCheckout()
		}
		return res
	}
	if (freshClone) {
		if (!rawFs._createPlaceholder && !rawFs._isPlaceholder) {
			throw new Error("fs provider does not support placeholders")
		}
		console.info("Using lix for cloning repo")
		await isomorphic_git_default
			.clone({
				fs: rawFs,
				http: makeHttpClient({
					debug: debug10,
					description: "clone",
					onReq: experimentalFeatures.lazyClone
						? optimizeReq.bind(null, {
								noBlobs: true,
								addRefs: [state.branchName || "HEAD"],
						  })
						: void 0,
					onRes: experimentalFeatures.lazyClone ? optimizeRes : void 0,
				}),
				dir,
				cache: cache2,
				corsProxy: gitProxyUrl,
				url: gitUrl,
				singleBranch: false,
				noCheckout: experimentalFeatures.lazyClone,
				ref: state.branchName,
				// TODO: use only first and last commit in lazy clone? (we need first commit for repo id)
				depth: 1,
				noTags: true,
			})
			.then(async () => {
				if (!experimentalFeatures.lazyClone) {
					return
				}
				const { gitignoreFiles } = await checkOutPlaceholders(
					ctx,
					{
						branchName: state.branchName,
						checkedOut: state.checkedOut,
						sparseFilter: state.sparseFilter,
						ensureFirstBatch,
					},
					{ materializeGitignores: false }
				)
				preloads = gitignoreFiles
			})
	} else {
		console.info("Using existing cloned repo")
	}
	function delayedAction({ execute, prop, argumentsList }) {
		const filename = argumentsList?.[0]?.replace(/^(\.)?(\/)?\//, "")
		const pathParts = filename?.split("/") || []
		const rootObject = pathParts[0]
		if (
			experimentalFeatures.lazyClone &&
			typeof rootObject !== "undefined" &&
			rootObject !== ".git" &&
			["readFile", "readlink", "writeFile", "readdir"].includes(prop) &&
			!state.checkedOut.has(rootObject) &&
			!state.checkedOut.has(filename)
		) {
			if (debug10) {
				console.info("delayedAction", {
					prop,
					argumentsList,
					rootObject,
					checkedOut: state.checkedOut,
					filename,
					pending: state.pending,
					nextBatch,
				})
			}
			if (prop !== "readdir") {
				nextBatch.push(filename)
			}
			if (!state.pending && nextBatch.length > 0) {
				state.pending = doCheckout()
			}
		} else if (
			experimentalFeatures.lazyClone &&
			typeof rootObject !== "undefined" &&
			rootObject === ".git" && // TODO #1459 more solid check for git folder !filePath.startsWith(gitdir))
			pathParts[1] === "objects" &&
			pathParts[2] !== "pack" &&
			pathParts.length === 4 &&
			prop === "readFile"
		) {
			const oid = pathParts[2] + pathParts[3]
			nextBatch.push(
				blobExistsLocaly({
					fs: rawFs,
					cache: cache2,
					oid,
					gitdir: ".git",
				}).then((existsLocaly) => {
					if (!existsLocaly) {
						console.warn("missing oid!! in git object store interceptor: ", oid)
						return isomorphic_git_default.fetch({
							cache: cache2,
							fs: rawFs,
							dir: "/",
							http: makeHttpClient({
								debug: debug10,
								description: "lazy fetch",
								onReq: optimizeReq.bind(null, {
									noBlobs: false,
									addRefs: [state.branchName || "HEAD"],
									// we don't need to override the haves any more since adding the capabilities
									// allow-tip-sha1-in-want allow-reachable-sha1-in-want to the request enable us to request objects explicetly
									overrideWants: [oid],
								}),
								onRes: optimizeRes,
							}),
							depth: 1,
							singleBranch: true,
							tags: false,
						})
					}
					return void 0
				})
			)
			if (!state.pending) {
				state.pending = doCheckout()
			}
		} else {
			return execute()
		}
		if (state.pending) {
			return state.pending.then(execute).finally(() => {
				state.pending = void 0
				if (debug10) {
					console.warn("executed", filename, prop)
				}
			})
		}
		return execute()
	}
	return state
}

// ../../../lix/packages/client/dist/lixFs.js
var lixFs = (nodeishFs) => ({
	read(path) {
		return nodeishFs.readFile(path, { encoding: "utf-8" })
	},
	write(path, content) {
		return nodeishFs.writeFile(path, content)
	},
	listDir(path) {
		return nodeishFs.readdir(path)
	},
})

// ../../../lix/packages/client/dist/openRepository.js
async function openRepository(url, args) {
	const ctx = await repoContext(url, args)
	const state = await repoState(ctx, args)
	return {
		_experimentalFeatures: ctx.experimentalFeatures,
		_rawFs: ctx.rawFs,
		nodeishFs: state.nodeishFs,
		commit: commit2.bind(void 0, ctx, state),
		status: status2.bind(void 0, ctx, state),
		statusList: statusList.bind(void 0, ctx, state),
		forkStatus: forkStatus.bind(void 0, ctx),
		getMeta: getMeta.bind(void 0, ctx),
		listRemotes: listRemotes2.bind(void 0, ctx, state),
		log: log2.bind(void 0, ctx),
		getOrigin: getOrigin.bind(void 0, ctx, state),
		getBranches: getBranches.bind(void 0, ctx),
		getCurrentBranch: getCurrentBranch.bind(void 0, ctx, state),
		getFirstCommitHash: getFirstCommitHash.bind(void 0, ctx),
		checkout: checkout2.bind(void 0, ctx, state),
		createFork: createFork.bind(void 0, ctx),
		mergeUpstream: mergeUpstream.bind(void 0, ctx),
		push: push2.bind(void 0, ctx),
		pull: pull2.bind(void 0, ctx, state),
		...(ctx.experimentalFeatures.lixFs ? lixFs(state.nodeishFs) : {}),
		// only exposed for testing
		_emptyWorkdir: emptyWorkdir.bind(void 0, ctx, state),
		_checkOutPlaceholders: checkOutPlaceholders.bind(void 0, ctx, state),
		_add: add2.bind(void 0, ctx, state),
		_remove: remove2.bind(void 0, ctx, state),
		_isoCommit: isoCommit.bind(void 0, ctx),
	}
}

// ../../../lix/packages/client/dist/index.js
init_dist()

// ../../../lix/packages/client/dist/mockRepo.js
init_dist()

// ../versioned-interfaces/language-tag/dist/interface.js
var import_typebox = __toESM(require_typebox(), 1)
var pattern =
	"^((?<grandfathered>(en-GB-oed|i-ami|i-bnn|i-default|i-enochian|i-hak|i-klingon|i-lux|i-mingo|i-navajo|i-pwn|i-tao|i-tay|i-tsu|sgn-BE-FR|sgn-BE-NL|sgn-CH-DE)|(art-lojban|cel-gaulish|no-bok|no-nyn|zh-guoyu|zh-hakka|zh-min|zh-min-nan|zh-xiang))|((?<language>([A-Za-z]{2,3}(-(?<extlang>[A-Za-z]{3}(-[A-Za-z]{3}){0,2}))?))(-(?<script>[A-Za-z]{4}))?(-(?<region>[A-Za-z]{2}|[0-9]{3}))?(-(?<variant>[A-Za-z0-9]{5,8}|[0-9][A-Za-z0-9]{3}))*))$"
var LanguageTag = import_typebox.Type.String({
	pattern,
	description: "The language tag must be a valid IETF BCP 47 language tag.",
	examples: ["en", "de", "en-US", "zh-Hans", "es-419"],
})

// ../versioned-interfaces/translatable/dist/interface.js
var import_typebox2 = __toESM(require_typebox(), 1)
var Translatable = (type) =>
	import_typebox2.Type.Union([
		type,
		import_typebox2.Type.Intersect([
			import_typebox2.Type.Object({ en: type }),
			import_typebox2.Type.Record(LanguageTag, type),
		]),
	])

// ../versioned-interfaces/message-lint-rule/dist/interface.js
var import_typebox5 = __toESM(require_typebox(), 1)

// ../versioned-interfaces/project-settings/dist/interface.js
var import_typebox4 = __toESM(require_typebox(), 1)

// ../json-types/dist/interface.js
var import_typebox3 = __toESM(require_typebox(), 1)
var JSONValue1 = import_typebox3.Type.Union([
	import_typebox3.Type.String(),
	import_typebox3.Type.Number(),
	import_typebox3.Type.Boolean(),
	import_typebox3.Type.Null(),
])
var JSONArray1 = import_typebox3.Type.Array(JSONValue1)
var JSONObject1 = import_typebox3.Type.Record(
	import_typebox3.Type.String(),
	import_typebox3.Type.Union([JSONArray1, JSONValue1])
)
var JSONValue2 = import_typebox3.Type.Union([JSONValue1, JSONObject1])
var JSONArray2 = import_typebox3.Type.Array(JSONValue2)
var JSONObject2 = import_typebox3.Type.Record(
	import_typebox3.Type.String(),
	import_typebox3.Type.Union([JSONValue2, JSONArray2])
)
var JSONValue3 = import_typebox3.Type.Union([JSONValue2, JSONObject2])
var JSONArray3 = import_typebox3.Type.Array(JSONValue3)
var JSONObject3 = import_typebox3.Type.Record(
	import_typebox3.Type.String(),
	import_typebox3.Type.Union([JSONValue3, JSONArray3])
)
var JSONValue4 = import_typebox3.Type.Union([JSONValue3, JSONObject3])
var JSONArray4 = import_typebox3.Type.Array(JSONValue4)
var JSONObject4 = import_typebox3.Type.Record(
	import_typebox3.Type.String(),
	import_typebox3.Type.Union([JSONValue4, JSONArray4])
)
var JSON2 = import_typebox3.Type.Union([JSONObject4, JSONValue4, JSONArray4])

// ../versioned-interfaces/project-settings/dist/interface.js
var _MessageLintRuleId = import_typebox4.Type.String({
	pattern: "^messageLintRule\\.([a-z][a-zA-Z0-9]*)\\.([a-z][a-zA-Z0-9]*(?:[A-Z][a-z0-9]*)*)$",
	description: "The key must be conform to `messageLintRule.{namespace}.{id}` pattern.",
	examples: [
		"messageLintRule.namespace.patternInvalid",
		"messageLintRule.namespace.missingTranslation",
	],
})
var _MessageLintRuleLevel = import_typebox4.Type.Union([
	import_typebox4.Type.Literal("error"),
	import_typebox4.Type.Literal("warning"),
])
var SourceLanguageTag = LanguageTag
SourceLanguageTag.title = "Source language tag"
SourceLanguageTag.description =
	"Set the reference language for your project. It needs to be a valid BCP-47 language tag."
var InternalProjectSettings = import_typebox4.Type.Object({
	$schema: import_typebox4.Type.Optional(
		import_typebox4.Type.Literal("https://inlang.com/schema/project-settings")
	),
	sourceLanguageTag: SourceLanguageTag,
	languageTags: import_typebox4.Type.Array(LanguageTag, {
		uniqueItems: true,
		title: "Language tags",
		description:
			"Set the languages that are available in your project. All language tags needs to be a valid BCP-47 language tag. Needs to include the source language tag.",
	}),
	/**
	 * The modules to load.
	 *
	 * @example
	 *  modules: [
	 * 	  "https://cdn.jsdelivr.net/npm/@inlang/plugin-i18next@3/dist/index.js",
	 * 	  "https://cdn.jsdelivr.net/npm/@inlang/plugin-csv@1/dist/index.js",
	 *  ]
	 */
	modules: import_typebox4.Type.Array(
		import_typebox4.Type.Intersect([
			import_typebox4.Type.String({
				pattern:
					"(?:[A-Za-z][A-Za-z0-9+.-]*:/{2})?(?:(?:[A-Za-z0-9-._~]|%[A-Fa-f0-9]{2})+(?::([A-Za-z0-9-._~]?|[%][A-Fa-f0-9]{2})+)?@)?(?:[A-Za-z0-9](?:[A-Za-z0-9-]*[A-Za-z0-9])?\\.){1,126}[A-Za-z0-9](?:[A-Za-z0-9-]*[A-Za-z0-9])?(?::[0-9]+)?(?:/(?:[A-Za-z0-9-._~]|%[A-Fa-f0-9]{2})*)*(?:\\?(?:[A-Za-z0-9-._~]+(?:=(?:[A-Za-z0-9-._~+]|%[A-Fa-f0-9]{2})+)?)(?:&|;[A-Za-z0-9-._~]+(?:=(?:[A-Za-z0-9-._~+]|%[A-Fa-f0-9]{2})+)?)*)?",
				description: "The module must be a valid URI according to RFC 3986.",
			}),
			import_typebox4.Type.String({
				pattern: ".*\\.js$",
				description: "The module must end with `.js`.",
			}),
			import_typebox4.Type.String({
				pattern: "^(?!.*@\\d\\.)[^]*$",
				description: "The module can only contain a major version number.",
			}),
		]),
		{
			uniqueItems: true,
			description: "The modules to load. Must be a valid URI but can be relative.",
			examples: [
				"https://cdn.jsdelivr.net/npm/@inlang/plugin-i18next@3/dist/index.js",
				"https://cdn.jsdelivr.net/npm/@inlang/plugin-csv@1/dist/index.js",
				"./local-testing-plugin.js",
			],
		}
	),
	messageLintRuleLevels: import_typebox4.Type.Optional(
		import_typebox4.Type.Record(_MessageLintRuleId, _MessageLintRuleLevel, {
			title: "Levels for lint rules",
			description:
				"Adjust the lint rule levels in your project to choose between 'warning' and 'error'. If set to 'error', you can configure a CI process to prevent merging with existing reports. (When you want to configure your lint rules visit inlang.com/c/lint-rules)",
			examples: [
				{
					"messageLintRule.inlang.missingTranslation": "error",
					"messageLintRule.inlang.patternInvalid": "warning",
				},
			],
		})
	),
	experimental: import_typebox4.Type.Optional(
		import_typebox4.Type.Record(import_typebox4.Type.String(), import_typebox4.Type.Literal(true), {
			title: "Experimental settings",
			description: "Experimental settings that are used for product development.",
		})
	),
})
var ExternalProjectSettings = import_typebox4.Type.Record(
	import_typebox4.Type.String({
		// pattern includes ProjectSettings keys
		pattern: `^((messageLintRule|plugin|app|library)\\.([a-z][a-zA-Z0-9]*)\\.([a-z][a-zA-Z0-9]*(?:[A-Z][a-z0-9]*)*)|\\$schema|${Object.keys(
			InternalProjectSettings.properties
		)
			.map((key) => key.replaceAll(".", "\\."))
			.join("|")})$`,
		description:
			"The key must be conform to `{type:app|plugin|messageLintRule}.{namespace:string}.{id:string}`.",
		examples: ["plugin.publisher.sqlite", "messageLintRule.inlang.missingTranslation"],
	}),
	// Using JSON (array and object) as a workaround to make the
	// intersection between `InternalSettings`, which contains an array,
	// and `ExternalSettings` which are objects possible
	JSON2,
	{ description: "Settings defined by apps, plugins, etc." }
)
var ProjectSettings = import_typebox4.Type.Intersect([
	InternalProjectSettings,
	ExternalProjectSettings,
])

// ../versioned-interfaces/message-lint-rule/dist/interface.js
var MessageLintRule = import_typebox5.Type.Object({
	id: _MessageLintRuleId,
	displayName: Translatable(import_typebox5.Type.String()),
	description: Translatable(import_typebox5.Type.String()),
	/**
	 * Tyepbox is must be used to validate the Json Schema.
	 * Github discussion to upvote a plain Json Schema validator and read the benefits of Typebox
	 * https://github.com/opral/monorepo/discussions/1503
	 */
	settingsSchema: import_typebox5.Type.Optional(
		import_typebox5.Type.Object({}, { additionalProperties: true })
	),
})

// ../versioned-interfaces/plugin/dist/customApis/app.inlang.ideExtension.js
var import_typebox6 = __toESM(require_typebox(), 1)
var MessageReferenceMatch = import_typebox6.Type.Object({
	/**
	 * The messages id.
	 */
	messageId: import_typebox6.Type.String(),
	/**
	 * The position from where to where the reference can be found.
	 */
	position: import_typebox6.Type.Object({
		start: import_typebox6.Type.Object({
			line: import_typebox6.Type.Number(),
			character: import_typebox6.Type.Number(),
		}),
		end: import_typebox6.Type.Object({
			line: import_typebox6.Type.Number(),
			character: import_typebox6.Type.Number(),
		}),
	}),
})
var IdeExtensionConfigSchema = import_typebox6.Type.Object({
	/**
	 * Defines matchers for message references inside the code.
	 *
	 * @param args represents the data to conduct the search on
	 * @returns a promise with matched message references
	 */
	messageReferenceMatchers: import_typebox6.Type.Array(
		import_typebox6.Type.Function(
			[
				import_typebox6.Type.Object({
					documentText: import_typebox6.Type.String(),
				}),
			],
			import_typebox6.Type.Promise(import_typebox6.Type.Array(MessageReferenceMatch))
		)
	),
	/**
	 * Defines the options to extract messages.
	 */
	extractMessageOptions: import_typebox6.Type.Array(
		import_typebox6.Type.Object({
			/**
			 * Function which is called, when the user finished the message extraction command.
			 *
			 * @param messageId is the message identifier entered by the user
			 * @param selection is the text which was extracted
			 * @returns the code which is inserted into the document
			 */
			callback: import_typebox6.Type.Function(
				[
					import_typebox6.Type.Object({
						messageId: import_typebox6.Type.String(),
						selection: import_typebox6.Type.String(),
					}),
				],
				import_typebox6.Type.Object({
					messageId: import_typebox6.Type.String(),
					messageReplacement: import_typebox6.Type.String(),
				})
			),
		})
	),
	/**
	 * An array of Visual Studio Code DocumentSelectors.
	 *
	 * The document selectors specify for which files/programming languages
	 * (typescript, svelte, etc.) the extension should be activated.
	 *
	 * See https://code.visualstudio.com/api/references/document-selector
	 */
	documentSelectors: import_typebox6.Type.Optional(
		import_typebox6.Type.Array(
			import_typebox6.Type.Object({
				language: import_typebox6.Type.Optional(import_typebox6.Type.String()),
			})
		)
	),
})

// ../versioned-interfaces/plugin/dist/interface.js
var import_typebox7 = __toESM(require_typebox(), 1)
var Plugin = import_typebox7.Type.Object({
	id: import_typebox7.Type.String({
		pattern: "^plugin\\.([a-z][a-zA-Z0-9]*)\\.([a-z][a-zA-Z0-9]*(?:[A-Z][a-z0-9]*)*)$",
		examples: ["plugin.namespace.id"],
	}),
	displayName: Translatable(import_typebox7.Type.String()),
	description: Translatable(import_typebox7.Type.String()),
	/**
	 * Tyepbox is must be used to validate the Json Schema.
	 * Github discussion to upvote a plain Json Schema validator and read the benefits of Typebox
	 * https://github.com/opral/monorepo/discussions/1503
	 */
	settingsSchema: import_typebox7.Type.Optional(
		import_typebox7.Type.Object({}, { additionalProperties: true })
	),
	loadMessages: import_typebox7.Type.Optional(import_typebox7.Type.Any()),
	saveMessages: import_typebox7.Type.Optional(import_typebox7.Type.Any()),
	/**
	 * @deprecated removed
	 */
	detectedLanguageTags: import_typebox7.Type.Optional(import_typebox7.Type.Any()),
	addCustomApi: import_typebox7.Type.Optional(import_typebox7.Type.Any()),
})

// ../versioned-interfaces/message/dist/interface.js
var import_typebox8 = __toESM(require_typebox(), 1)
var Text = import_typebox8.Type.Object({
	type: import_typebox8.Type.Literal("Text"),
	value: import_typebox8.Type.String(),
})
var VariableReference = import_typebox8.Type.Object({
	type: import_typebox8.Type.Literal("VariableReference"),
	name: import_typebox8.Type.String(),
})
var Expression = import_typebox8.Type.Union([VariableReference])
var Pattern = import_typebox8.Type.Array(import_typebox8.Type.Union([Text, Expression]))
var Variant = import_typebox8.Type.Object({
	languageTag: LanguageTag,
	/**
	 * The number of keys in each variant match MUST equal the number of expressions in the selectors.
	 *
	 * Inspired by: https://github.com/unicode-org/message-format-wg/blob/main/spec/formatting.md#pattern-selection
	 */
	// a match can always only be string-based because a string is what is rendered to the UI
	match: import_typebox8.Type.Array(import_typebox8.Type.String()),
	pattern: Pattern,
})
var Message = import_typebox8.Type.Object({
	id: import_typebox8.Type.String(),
	alias: import_typebox8.Type.Record(import_typebox8.Type.String(), import_typebox8.Type.String()),
	/**
	 * The order in which the selectors are placed determines the precedence of patterns.
	 */
	selectors: import_typebox8.Type.Array(Expression),
	variants: import_typebox8.Type.Array(Variant),
})

// ../versioned-interfaces/module/dist/interface.js
var import_typebox9 = __toESM(require_typebox(), 1)
var InlangModule = import_typebox9.Type.Object({
	default: import_typebox9.Type.Union([Plugin, MessageLintRule]),
})

// ../sdk/dist/resolve-modules/plugins/errors.js
var PluginHasInvalidIdError = class extends Error {
	constructor(options) {
		super(`Plugin "${options.id}" has an invalid id. The id must:
1) Start with "plugin."
2) camelCase
3) Contain a namespace.
An example would be "plugin.namespace.myPlugin".`)
		this.name = "PluginHasInvalidIdError"
	}
}
var PluginHasInvalidSchemaError = class extends Error {
	constructor(options) {
		super(`Plugin "${options.id}" has an invalid schema:

${options.errors
	.map((error) => `Path "${error.path}" with value "${error.value}": "${error.message}"`)
	.join("\n")})}

Please refer to the documentation for the correct schema.`)
		this.name = "PluginHasInvalidSchemaError"
	}
}
var PluginLoadMessagesFunctionAlreadyDefinedError = class extends Error {
	constructor(options) {
		super(`Plugin "${options.id}" defines the \`loadMessages()\` function, but it was already defined by another plugin.

Inlang only allows one plugin to define the \`loadMessages()\` function.`)
		this.name = "PluginLoadMessagesFunctionAlreadyDefinedError"
	}
}
var PluginSaveMessagesFunctionAlreadyDefinedError = class extends Error {
	constructor(options) {
		super(`Plugin "${options.id}" defines the \`saveMessages()\` function, but it was already defined by another plugin.

Inlang only allows one plugin to define the \`saveMessages()\` function.`)
		this.name = "PluginSaveMessagesFunctionAlreadyDefinedError"
	}
}
var PluginReturnedInvalidCustomApiError = class extends Error {
	constructor(options) {
		super(
			`Plugin "${options.id}" returned an invalid custom API:

${options.cause}`,
			options
		)
		this.name = "PluginReturnedInvalidCustomApiError"
	}
}
var PluginsDoNotProvideLoadOrSaveMessagesError = class extends Error {
	constructor() {
		super(`No plugin provides a \`loadMessages()\` or \`saveMessages()\` function

In case no plugin threw an error, you likely forgot to add a plugin that handles the loading and saving of messages. Refer to the marketplace for available plugins https://inlang.com/marketplace.`)
		this.name = "PluginsDoNotProvideLoadOrSaveMessagesError"
	}
}

// ../sdk/dist/resolve-modules/message-lint-rules/errors.js
var MessageLintRuleIsInvalidError = class extends Error {
	constructor(options) {
		super(`The message lint rule "${options.id}" is invalid:

${options.errors.join("\n")}`)
		this.name = "MessageLintRuleIsInvalidError"
	}
}

// ../sdk/dist/resolve-modules/errors.js
var ModuleError = class extends Error {
	module
	constructor(message, options) {
		super(message)
		this.name = "ModuleError"
		this.module = options.module
		this.cause = options.cause
	}
}
var ModuleHasNoExportsError = class extends ModuleError {
	constructor(options) {
		super(
			`Module "${options.module}" has no exports. Every module must have an "export default".`,
			options
		)
		this.name = "ModuleHasNoExportsError"
	}
}
var ModuleImportError = class extends ModuleError {
	constructor(options) {
		super(
			`Couldn't import the plugin "${options.module}":

${options.cause}`,
			options
		)
		this.name = "ModuleImportError"
	}
}
var ModuleExportIsInvalidError = class extends ModuleError {
	constructor(options) {
		super(
			`The export(s) of "${options.module}" are invalid:

${options.errors
	.map((error) => `"${error.path}" "${JSON.stringify(error.value, void 0, 2)}": "${error.message}"`)
	.join("\n")}`,
			options
		)
		this.name = "ModuleExportIsInvalidError"
	}
}
var ModuleSettingsAreInvalidError = class extends ModuleError {
	constructor(options) {
		super(
			`The settings are invalid of "${options.module}" are invalid:

${options.errors
	.map(
		(error) =>
			`Path "${error.path}" with value "${JSON.stringify(error.value, void 0, 2)}": "${
				error.message
			}"`
	)
	.join("\n")}`,
			options
		)
		this.name = "ModuleSettingsAreInvalidError"
	}
}

// ../result/dist/tryCatch.js
function tryCatch(callback) {
	try {
		const callbackResult = callback()
		if (isAsync(callbackResult)) {
			return callbackResult.then((data) => ({ data })).catch(getErrorResponse)
		}
		return { data: callbackResult }
	} catch (e) {
		return getErrorResponse(e)
	}
}
var getErrorResponse = (error) => {
	if (error instanceof Error) {
		return { error }
	}
	return { error: new Error(`Unknown error has been caught: ${error}`, { cause: error }) }
}
var isAsync = (p) => !!p && typeof p === "object" && typeof p.then === "function"

// ../sdk/dist/resolve-modules/message-lint-rules/resolveMessageLintRules.js
var import_value = __toESM(require_value2(), 1)
var resolveMessageLintRules = (args) => {
	const result = {
		data: [],
		errors: [],
	}
	for (const rule of args.messageLintRules) {
		if (import_value.Value.Check(MessageLintRule, rule) === false) {
			const errors = [...import_value.Value.Errors(MessageLintRule, rule)]
			result.errors.push(
				new MessageLintRuleIsInvalidError({
					// @ts-ignore
					id: rule.id,
					errors,
				})
			)
			continue
		} else {
			result.data.push(rule)
		}
	}
	return result
}

// ../../../node_modules/.pnpm/dedent@1.5.1/node_modules/dedent/dist/dedent.mjs
function ownKeys(object, enumerableOnly) {
	var keys = Object.keys(object)
	if (Object.getOwnPropertySymbols) {
		var symbols = Object.getOwnPropertySymbols(object)
		enumerableOnly &&
			(symbols = symbols.filter(function (sym) {
				return Object.getOwnPropertyDescriptor(object, sym).enumerable
			})),
			keys.push.apply(keys, symbols)
	}
	return keys
}
function _objectSpread(target) {
	for (var i = 1; i < arguments.length; i++) {
		var source = undefined != arguments[i] ? arguments[i] : {}
		i % 2
			? ownKeys(Object(source), true).forEach(function (key) {
					_defineProperty(target, key, source[key])
			  })
			: Object.getOwnPropertyDescriptors
			? Object.defineProperties(target, Object.getOwnPropertyDescriptors(source))
			: ownKeys(Object(source)).forEach(function (key) {
					Object.defineProperty(target, key, Object.getOwnPropertyDescriptor(source, key))
			  })
	}
	return target
}
function _defineProperty(obj, key, value) {
	key = _toPropertyKey(key)
	if (key in obj) {
		Object.defineProperty(obj, key, { value, enumerable: true, configurable: true, writable: true })
	} else {
		obj[key] = value
	}
	return obj
}
function _toPropertyKey(arg) {
	var key = _toPrimitive(arg, "string")
	return typeof key === "symbol" ? key : String(key)
}
function _toPrimitive(input, hint) {
	if (typeof input !== "object" || input === null) return input
	var prim = input[Symbol.toPrimitive]
	if (prim !== void 0) {
		var res = prim.call(input, hint || "default")
		if (typeof res !== "object") return res
		throw new TypeError("@@toPrimitive must return a primitive value.")
	}
	return (hint === "string" ? String : Number)(input)
}
var dedent_default = createDedent({})
function createDedent(options) {
	dedent.withOptions = (newOptions) =>
		createDedent(_objectSpread(_objectSpread({}, options), newOptions))
	return dedent
	function dedent(strings, ...values) {
		const raw = typeof strings === "string" ? [strings] : strings.raw
		const { escapeSpecialCharacters = Array.isArray(strings) } = options
		let result = ""
		for (let [i, next] of raw.entries()) {
			if (escapeSpecialCharacters) {
				next = next
					.replace(/\\\n[ \t]*/g, "")
					.replace(/\\`/g, "`")
					.replace(/\\\$/g, "$")
					.replace(/\\{/g, "{")
			}
			result += next
			if (i < values.length) {
				result += values[i]
			}
		}
		const lines = result.split("\n")
		let mindent = null
		for (const l of lines) {
			const m = l.match(/^(\s+)\S+/)
			if (m) {
				const indent2 = m[1].length
				if (!mindent) {
					mindent = indent2
				} else {
					mindent = Math.min(mindent, indent2)
				}
			}
		}
		if (mindent !== null) {
			const m = mindent
			result = lines.map((l) => (l[0] === " " || l[0] === "	" ? l.slice(m) : l)).join("\n")
		}
		return result.trim().replace(/\\n/g, "\n")
	}
}

// ../sdk/dist/resolve-modules/cache.js
function escape(url) {
	const bytes = new TextEncoder().encode(url)
	const hash2 = bytes.reduce(
		(hash3, byte) => BigInt.asUintN(64, (hash3 ^ BigInt(byte)) * 1099511628211n),
		14695981039346656037n
	)
	return hash2.toString(36)
}
async function readModuleFromCache(moduleURI, projectPath, readFile) {
	const moduleHash = escape(moduleURI)
	const filePath = projectPath + `/cache/modules/${moduleHash}`
	return await tryCatch(async () => await readFile(filePath, { encoding: "utf-8" }))
}
async function writeModuleToCache(moduleURI, moduleContent, projectPath, writeFile, mkdir) {
	const moduleHash = escape(moduleURI)
	const filePath = projectPath + `/cache/modules/${moduleHash}`
	const writeFileResult = await tryCatch(() => writeFile(filePath, moduleContent))
	if (writeFileResult.error) {
		const dirPath = projectPath + `/cache/modules`
		const createDirResult = await tryCatch(() => mkdir(dirPath, { recursive: true }))
		if (createDirResult.error && createDirResult.error.code !== "EEXIST")
			throw new Error("[sdk:module-cacke] failed to create cache-directory. Path: " + dirPath, {
				cause: createDirResult.error,
			})
		const writeFileResult2 = await tryCatch(() => writeFile(filePath, moduleContent))
		if (writeFileResult2.error)
			throw new Error("[sdk:module-cacke] failed to write cache-file. Path: " + filePath, {
				cause: writeFileResult2.error,
			})
	}
}
function withCache(moduleLoader, projectPath, nodeishFs) {
	return async (uri) => {
		const cachePromise = readModuleFromCache(uri, projectPath, nodeishFs.readFile)
		const loaderResult = await tryCatch(async () => await moduleLoader(uri))
		if (loaderResult.error) {
			const cacheResult = await cachePromise
			if (!cacheResult.error) return cacheResult.data
			else throw loaderResult.error
		} else {
			const moduleAsText = loaderResult.data
			try {
				await writeModuleToCache(
					uri,
					moduleAsText,
					projectPath,
					nodeishFs.writeFile,
					nodeishFs.mkdir
				)
			} catch (error) {}
			return moduleAsText
		}
	}
}

// ../sdk/dist/resolve-modules/import.js
function createImport(projectPath, nodeishFs) {
	return (uri) => $import(uri, projectPath, nodeishFs)
}
async function $import(uri, projectPath, nodeishFs) {
	const moduleAsText = uri.startsWith("http")
		? await withCache(readModuleFromCDN, projectPath, nodeishFs)(uri)
		: await readModulefromDisk(uri, nodeishFs.readFile)
	const moduleWithMimeType = "data:application/javascript," + encodeURIComponent(moduleAsText)
	try {
		return await import(
			/* @vite-ignore */
			moduleWithMimeType
		)
	} catch (error) {
		if (error instanceof SyntaxError && uri.includes("jsdelivr")) {
			error.message += dedent_default`\n\n
				Are you sure that the file exists on JSDelivr?

				The error indicates that the imported file does not exist on JSDelivr. For non-existent files, JSDelivr returns a 404 text that JS cannot parse as a module and throws a SyntaxError.`
		}
		throw new ModuleImportError({ module: uri, cause: error })
	}
}
async function readModulefromDisk(uri, readFile) {
	try {
		return await readFile(uri, { encoding: "utf-8" })
	} catch (error) {
		throw new ModuleImportError({ module: uri, cause: error })
	}
}
async function readModuleFromCDN(uri) {
	if (!isValidUrl(uri))
		throw new ModuleImportError({ module: uri, cause: new Error("Malformed URL") })
	const result = await tryCatch(async () => await fetch(uri))
	if (result.error) {
		throw new ModuleImportError({
			module: uri,
			cause: result.error,
		})
	}
	const response = result.data
	if (!response.ok) {
		throw new ModuleImportError({
			module: uri,
			cause: new Error(
				`Failed to fetch module. HTTP status: ${response.status}, Message: ${response.statusText}`
			),
		})
	}
	const JS_CONTENT_TYPES = [
		"application/javascript",
		"text/javascript",
		"application/x-javascript",
		"text/x-javascript",
	]
	const contentType = response.headers.get("content-type")?.toLowerCase()
	if (contentType && !JS_CONTENT_TYPES.some((knownType) => contentType.includes(knownType))) {
		throw new ModuleImportError({
			module: uri,
			cause: new Error(`Server responded with ${contentType} insetad of a JavaScript module`),
		})
	}
	return await response.text()
}
function isValidUrl(url) {
	const URLConstructor = URL
	if ("canParse" in URL) {
		return URL.canParse(url)
	}
	try {
		new URLConstructor(url)
		return true
	} catch (e) {
		console.warn(`Invalid URL: ${url}`)
		return false
	}
}

// ../../../node_modules/.pnpm/deepmerge-ts@5.1.0/node_modules/deepmerge-ts/dist/node/index.mjs
var actions = {
	defaultMerge: Symbol("deepmerge-ts: default merge"),
	skip: Symbol("deepmerge-ts: skip"),
}
var actionsInto = {
	defaultMerge: actions.defaultMerge,
}
function defaultMetaDataUpdater(previousMeta, metaMeta) {
	return metaMeta
}
function getObjectType(object) {
	if (typeof object !== "object" || object === null) {
		return 0
	}
	if (Array.isArray(object)) {
		return 2
	}
	if (isRecord(object)) {
		return 1
	}
	if (object instanceof Set) {
		return 3
	}
	if (object instanceof Map) {
		return 4
	}
	return 5
}
function getKeys(objects) {
	const keys = /* @__PURE__ */ new Set()
	for (const object of objects) {
		for (const key of [...Object.keys(object), ...Object.getOwnPropertySymbols(object)]) {
			keys.add(key)
		}
	}
	return keys
}
function objectHasProperty(object, property) {
	return typeof object === "object" && Object.prototype.propertyIsEnumerable.call(object, property)
}
function getIterableOfIterables(iterables) {
	return {
		// eslint-disable-next-line functional/functional-parameters
		*[Symbol.iterator]() {
			for (const iterable of iterables) {
				for (const value of iterable) {
					yield value
				}
			}
		},
	}
}
var validRecordToStringValues = /* @__PURE__ */ new Set(["[object Object]", "[object Module]"])
function isRecord(value) {
	if (!validRecordToStringValues.has(Object.prototype.toString.call(value))) {
		return false
	}
	const { constructor } = value
	if (constructor === void 0) {
		return true
	}
	const prototype = constructor.prototype
	if (
		prototype === null ||
		typeof prototype !== "object" ||
		!validRecordToStringValues.has(Object.prototype.toString.call(prototype))
	) {
		return false
	}
	if (!prototype.hasOwnProperty("isPrototypeOf")) {
		return false
	}
	return true
}
function mergeRecords$2(values, utils, meta) {
	const result = {}
	for (const key of getKeys(values)) {
		const propValues = []
		for (const value of values) {
			if (objectHasProperty(value, key)) {
				propValues.push(value[key])
			}
		}
		if (propValues.length === 0) {
			continue
		}
		const updatedMeta = utils.metaDataUpdater(meta, {
			key,
			parents: values,
		})
		const propertyResult = mergeUnknowns(propValues, utils, updatedMeta)
		if (propertyResult === actions.skip) {
			continue
		}
		if (key === "__proto__") {
			Object.defineProperty(result, key, {
				value: propertyResult,
				configurable: true,
				enumerable: true,
				writable: true,
			})
		} else {
			result[key] = propertyResult
		}
	}
	return result
}
function mergeArrays$2(values) {
	return values.flat()
}
function mergeSets$2(values) {
	return new Set(getIterableOfIterables(values))
}
function mergeMaps$2(values) {
	return new Map(getIterableOfIterables(values))
}
function mergeOthers$2(values) {
	return values.at(-1)
}
var defaultMergeFunctions = /* @__PURE__ */ Object.freeze({
	__proto__: null,
	mergeArrays: mergeArrays$2,
	mergeMaps: mergeMaps$2,
	mergeOthers: mergeOthers$2,
	mergeRecords: mergeRecords$2,
	mergeSets: mergeSets$2,
})
function deepmerge(...objects) {
	return deepmergeCustom({})(...objects)
}
function deepmergeCustom(options, rootMetaData) {
	const utils = getUtils(options, customizedDeepmerge)
	function customizedDeepmerge(...objects) {
		return mergeUnknowns(objects, utils, rootMetaData)
	}
	return customizedDeepmerge
}
function getUtils(options, customizedDeepmerge) {
	return {
		defaultMergeFunctions,
		mergeFunctions: {
			...defaultMergeFunctions,
			...Object.fromEntries(
				Object.entries(options)
					.filter(([key, option]) => Object.hasOwn(defaultMergeFunctions, key))
					.map(([key, option]) => (option === false ? [key, mergeOthers$2] : [key, option]))
			),
		},
		metaDataUpdater: options.metaDataUpdater ?? defaultMetaDataUpdater,
		deepmerge: customizedDeepmerge,
		useImplicitDefaultMerging: options.enableImplicitDefaultMerging ?? false,
		actions,
	}
}
function mergeUnknowns(values, utils, meta) {
	if (values.length === 0) {
		return void 0
	}
	if (values.length === 1) {
		return mergeOthers$1(values, utils, meta)
	}
	const type = getObjectType(values[0])
	if (type !== 0 && type !== 5) {
		for (let m_index = 1; m_index < values.length; m_index++) {
			if (getObjectType(values[m_index]) === type) {
				continue
			}
			return mergeOthers$1(values, utils, meta)
		}
	}
	switch (type) {
		case 1: {
			return mergeRecords$1(values, utils, meta)
		}
		case 2: {
			return mergeArrays$1(values, utils, meta)
		}
		case 3: {
			return mergeSets$1(values, utils, meta)
		}
		case 4: {
			return mergeMaps$1(values, utils, meta)
		}
		default: {
			return mergeOthers$1(values, utils, meta)
		}
	}
}
function mergeRecords$1(values, utils, meta) {
	const result = utils.mergeFunctions.mergeRecords(values, utils, meta)
	if (
		result === actions.defaultMerge ||
		(utils.useImplicitDefaultMerging &&
			result === void 0 &&
			utils.mergeFunctions.mergeRecords !== utils.defaultMergeFunctions.mergeRecords)
	) {
		return utils.defaultMergeFunctions.mergeRecords(values, utils, meta)
	}
	return result
}
function mergeArrays$1(values, utils, meta) {
	const result = utils.mergeFunctions.mergeArrays(values, utils, meta)
	if (
		result === actions.defaultMerge ||
		(utils.useImplicitDefaultMerging &&
			result === void 0 &&
			utils.mergeFunctions.mergeArrays !== utils.defaultMergeFunctions.mergeArrays)
	) {
		return utils.defaultMergeFunctions.mergeArrays(values)
	}
	return result
}
function mergeSets$1(values, utils, meta) {
	const result = utils.mergeFunctions.mergeSets(values, utils, meta)
	if (
		result === actions.defaultMerge ||
		(utils.useImplicitDefaultMerging &&
			result === void 0 &&
			utils.mergeFunctions.mergeSets !== utils.defaultMergeFunctions.mergeSets)
	) {
		return utils.defaultMergeFunctions.mergeSets(values)
	}
	return result
}
function mergeMaps$1(values, utils, meta) {
	const result = utils.mergeFunctions.mergeMaps(values, utils, meta)
	if (
		result === actions.defaultMerge ||
		(utils.useImplicitDefaultMerging &&
			result === void 0 &&
			utils.mergeFunctions.mergeMaps !== utils.defaultMergeFunctions.mergeMaps)
	) {
		return utils.defaultMergeFunctions.mergeMaps(values)
	}
	return result
}
function mergeOthers$1(values, utils, meta) {
	const result = utils.mergeFunctions.mergeOthers(values, utils, meta)
	if (
		result === actions.defaultMerge ||
		(utils.useImplicitDefaultMerging &&
			result === void 0 &&
			utils.mergeFunctions.mergeOthers !== utils.defaultMergeFunctions.mergeOthers)
	) {
		return utils.defaultMergeFunctions.mergeOthers(values)
	}
	return result
}

// ../sdk/dist/resolve-modules/plugins/resolvePlugins.js
var import_compiler = __toESM(require_compiler2(), 1)
var import_debug = __toESM(require_src(), 1)
var debug = (0, import_debug.default)("sdk:resolvePlugins")
var PluginCompiler = import_compiler.TypeCompiler.Compile(Plugin)
var resolvePlugins = async (args) => {
	const result = {
		data: {
			loadMessages: void 0,
			saveMessages: void 0,
			customApi: {},
		},
		errors: [],
	}
	const experimentalPersistence = !!args.settings.experimental?.persistence
	if (experimentalPersistence) {
		debug("Using experimental persistence")
	}
	for (const plugin of args.plugins) {
		const errors = [...PluginCompiler.Errors(plugin)]
		const hasInvalidId = errors.some((error) => error.path === "/id")
		if (hasInvalidId) {
			result.errors.push(new PluginHasInvalidIdError({ id: plugin.id }))
		}
		if (errors.length > 0) {
			result.errors.push(
				new PluginHasInvalidSchemaError({
					id: plugin.id,
					errors,
				})
			)
		}
		if (typeof plugin.loadMessages === "function" && result.data.loadMessages !== void 0) {
			result.errors.push(new PluginLoadMessagesFunctionAlreadyDefinedError({ id: plugin.id }))
		}
		if (typeof plugin.saveMessages === "function" && result.data.saveMessages !== void 0) {
			result.errors.push(new PluginSaveMessagesFunctionAlreadyDefinedError({ id: plugin.id }))
		}
		if (typeof plugin.addCustomApi === "function") {
			const { data: customApi, error } = tryCatch(() =>
				plugin.addCustomApi({
					settings: args.settings,
				})
			)
			if (error) {
				result.errors.push(new PluginReturnedInvalidCustomApiError({ id: plugin.id, cause: error }))
			} else if (typeof customApi !== "object") {
				result.errors.push(
					new PluginReturnedInvalidCustomApiError({
						id: plugin.id,
						cause: new Error(`The return value must be an object. Received "${typeof customApi}".`),
					})
				)
			}
		}
		if (result.errors.length > 0) {
			continue
		}
		if (typeof plugin.loadMessages === "function") {
			result.data.loadMessages = plugin.loadMessages
		}
		if (typeof plugin.saveMessages === "function") {
			result.data.saveMessages = plugin.saveMessages
		}
		if (typeof plugin.addCustomApi === "function") {
			const { data: customApi } = tryCatch(() =>
				plugin.addCustomApi({
					settings: args.settings,
				})
			)
			if (customApi) {
				result.data.customApi = deepmerge(result.data.customApi, customApi)
			}
		}
	}
	if (
		!experimentalPersistence &&
		(typeof result.data.loadMessages !== "function" ||
			typeof result.data.saveMessages !== "function")
	) {
		result.errors.push(new PluginsDoNotProvideLoadOrSaveMessagesError())
	}
	return result
}

// ../sdk/dist/resolve-modules/resolveModules.js
var import_compiler2 = __toESM(require_compiler2(), 1)

// ../sdk/dist/resolve-modules/validatedModuleSettings.js
var import_value2 = __toESM(require_value2(), 1)
var validatedModuleSettings = (args) => {
	if (args.settingsSchema && args.moduleSettings) {
		const hasValidSettings = import_value2.Value.Check(args.settingsSchema, args.moduleSettings)
		if (hasValidSettings === false) {
			const errors = [...import_value2.Value.Errors(args.settingsSchema, args.moduleSettings)]
			return errors
		}
	}
	return "isValid"
}

// ../sdk/dist/resolve-modules/resolveModules.js
var ModuleCompiler = import_compiler2.TypeCompiler.Compile(InlangModule)
var resolveModules = async (args) => {
	const _import = args._import ?? createImport(args.projectPath, args.nodeishFs)
	const allPlugins = []
	const allMessageLintRules = []
	const meta = []
	const moduleErrors = []
	async function resolveModule(module2) {
		const importedModule = await tryCatch(() => _import(module2))
		if (importedModule.error) {
			moduleErrors.push(
				new ModuleImportError({
					module: module2,
					cause: importedModule.error,
				})
			)
			return
		}
		if (importedModule.data?.default === void 0) {
			moduleErrors.push(
				new ModuleHasNoExportsError({
					module: module2,
				})
			)
			return
		}
		const isValidModule = ModuleCompiler.Check(importedModule.data)
		if (isValidModule === false) {
			const errors = [...ModuleCompiler.Errors(importedModule.data)]
			moduleErrors.push(
				new ModuleExportIsInvalidError({
					module: module2,
					errors,
				})
			)
			return
		}
		const result = validatedModuleSettings({
			settingsSchema: importedModule.data.default.settingsSchema,
			moduleSettings: args.settings[importedModule.data.default.id],
		})
		if (result !== "isValid") {
			moduleErrors.push(new ModuleSettingsAreInvalidError({ module: module2, errors: result }))
			return
		}
		meta.push({
			module: module2,
			id: importedModule.data.default.id,
		})
		if (importedModule.data.default.id.startsWith("plugin.")) {
			allPlugins.push(importedModule.data.default)
		} else if (importedModule.data.default.id.startsWith("messageLintRule.")) {
			allMessageLintRules.push(importedModule.data.default)
		} else {
			moduleErrors.push(
				new ModuleError(
					`Unimplemented module type ${importedModule.data.default.id}.The module has not been installed.`,
					{ module: module2 }
				)
			)
		}
	}
	await Promise.all(args.settings.modules.map(resolveModule))
	const resolvedPlugins = await resolvePlugins({
		plugins: allPlugins,
		settings: args.settings,
		nodeishFs: args.nodeishFs,
	})
	const resolvedLintRules = resolveMessageLintRules({ messageLintRules: allMessageLintRules })
	return {
		meta,
		messageLintRules: allMessageLintRules,
		plugins: allPlugins,
		resolvedPluginApi: resolvedPlugins.data,
		errors: [...moduleErrors, ...resolvedLintRules.errors, ...resolvedPlugins.errors],
	}
}

// ../sdk/dist/validateProjectPath.js
function assertValidProjectPath(projectPath) {
	if (!isAbsolutePath(projectPath)) {
		throw new Error(`Expected an absolute path but received "${projectPath}".`)
	}
	if (!isInlangProjectPath(projectPath)) {
		throw new Error(`Expected a path ending in "{name}.inlang" but received "${projectPath}".

Valid examples: 
- "/path/to/micky-mouse.inlang"
- "/path/to/green-elephant.inlang
`)
	}
}
function isInlangProjectPath(path) {
	return /[^\\/]+\.inlang$/.test(path)
}
function isAbsolutePath(path) {
	return /^\/|^[A-Za-z]:[\\/]/.test(path)
}

// ../sdk/dist/loadProject.js
var import_compiler3 = __toESM(require_compiler2(), 1)

// ../sdk/dist/errors.js
var ProjectSettingsInvalidError = class extends Error {
	constructor(options) {
		super(`The project settings are invalid:
${options.errors
	.filter((error) => error.path)
	.map(FormatProjectSettingsError)
	.join("\n")}`)
		this.name = "ProjectSettingsInvalidError"
	}
}
function FormatProjectSettingsError(error) {
	let msg = `${error.message} at ${error.path}`
	if (error.path.startsWith("/modules/")) {
		msg += `
value = "${error.value}"
- ${error.schema.allOf.map((o) => `${o.description ?? ""}`).join("\n- ")}`
	}
	return msg
}
var ProjectSettingsFileJSONSyntaxError = class extends Error {
	constructor(options) {
		super(
			`The settings file at "${options.path}" is not a valid JSON file:

${options.cause}`,
			options
		)
		this.name = "ProjectSettingsFileJSONSyntaxError"
	}
}
var ProjectSettingsFileNotFoundError = class extends Error {
	constructor(options) {
		super(`The file at "${options.path}" could not be read. Does the file exists?`, options)
		this.name = "ProjectSettingsFileNotFoundError"
	}
}
var PluginSaveMessagesError = class extends Error {
	constructor(options) {
		super(`An error occured in saveMessages() caused by ${options.cause}.`, options)
		this.name = "PluginSaveMessagesError"
	}
}
var PluginLoadMessagesError = class extends Error {
	constructor(options) {
		super(`An error occured in loadMessages() caused by ${options.cause}.`, options)
		this.name = "PluginLoadMessagesError"
	}
}

// ../../../node_modules/.pnpm/solid-js@1.6.12/node_modules/solid-js/dist/solid.js
var sharedConfig = {}
function setHydrateContext(context2) {
	sharedConfig.context = context2
}
var equalFn = (a, b) => a === b
var $PROXY = Symbol("solid-proxy")
var $TRACK = Symbol("solid-track")
var $DEVCOMP = Symbol("solid-dev-component")
var signalOptions = {
	equals: equalFn,
}
var ERROR = null
var runEffects = runQueue
var STALE = 1
var PENDING = 2
var UNOWNED = {
	owned: null,
	cleanups: null,
	context: null,
	owner: null,
}
var Owner = null
var Transition = null
var Scheduler = null
var ExternalSourceFactory = null
var Listener = null
var Updates = null
var Effects = null
var ExecCount = 0
var [transPending, setTransPending] = /* @__PURE__ */ createSignal(false)
function createRoot(fn, detachedOwner) {
	const listener = Listener,
		owner = Owner,
		unowned = fn.length === 0,
		root = unowned
			? UNOWNED
			: {
					owned: null,
					cleanups: null,
					context: null,
					owner: detachedOwner === void 0 ? owner : detachedOwner,
			  },
		updateFn = unowned ? fn : () => fn(() => untrack(() => cleanNode(root)))
	Owner = root
	Listener = null
	try {
		return runUpdates(updateFn, true)
	} finally {
		Listener = listener
		Owner = owner
	}
}
function createSignal(value, options) {
	options = options ? Object.assign({}, signalOptions, options) : signalOptions
	const s = {
		value,
		observers: null,
		observerSlots: null,
		comparator: options.equals || void 0,
	}
	const setter = (value2) => {
		if (typeof value2 === "function") {
			if (Transition && Transition.running && Transition.sources.has(s)) value2 = value2(s.tValue)
			else value2 = value2(s.value)
		}
		return writeSignal(s, value2)
	}
	return [readSignal.bind(s), setter]
}
function createRenderEffect(fn, value, options) {
	const c = createComputation(fn, value, false, STALE)
	if (Scheduler && Transition && Transition.running) Updates.push(c)
	else updateComputation(c)
}
function createEffect(fn, value, options) {
	runEffects = runUserEffects
	const c = createComputation(fn, value, false, STALE),
		s = SuspenseContext && lookup2(Owner, SuspenseContext.id)
	if (s) c.suspense = s
	c.user = true
	Effects ? Effects.push(c) : updateComputation(c)
}
function createMemo(fn, value, options) {
	options = options ? Object.assign({}, signalOptions, options) : signalOptions
	const c = createComputation(fn, value, true, 0)
	c.observers = null
	c.observerSlots = null
	c.comparator = options.equals || void 0
	if (Scheduler && Transition && Transition.running) {
		c.tState = STALE
		Updates.push(c)
	} else updateComputation(c)
	return readSignal.bind(c)
}
function batch(fn) {
	return runUpdates(fn, false)
}
function untrack(fn) {
	if (Listener === null) return fn()
	const listener = Listener
	Listener = null
	try {
		return fn()
	} finally {
		Listener = listener
	}
}
function onCleanup(fn) {
	if (Owner === null);
	else if (Owner.cleanups === null) Owner.cleanups = [fn]
	else Owner.cleanups.push(fn)
	return fn
}
function getListener() {
	return Listener
}
function startTransition(fn) {
	if (Transition && Transition.running) {
		fn()
		return Transition.done
	}
	const l = Listener
	const o = Owner
	return Promise.resolve().then(() => {
		Listener = l
		Owner = o
		let t
		if (Scheduler || SuspenseContext) {
			t =
				Transition ||
				(Transition = {
					sources: /* @__PURE__ */ new Set(),
					effects: [],
					promises: /* @__PURE__ */ new Set(),
					disposed: /* @__PURE__ */ new Set(),
					queue: /* @__PURE__ */ new Set(),
					running: true,
				})
			t.done || (t.done = new Promise((res) => (t.resolve = res)))
			t.running = true
		}
		runUpdates(fn, false)
		Listener = Owner = null
		return t ? t.done : void 0
	})
}
function createContext(defaultValue, options) {
	const id = Symbol("context")
	return {
		id,
		Provider: createProvider(id),
		defaultValue,
	}
}
function children(fn) {
	const children2 = createMemo(fn)
	const memo2 = createMemo(() => resolveChildren(children2()))
	memo2.toArray = () => {
		const c = memo2()
		return Array.isArray(c) ? c : c != undefined ? [c] : []
	}
	return memo2
}
var SuspenseContext
function readSignal() {
	const runningTransition = Transition && Transition.running
	if (this.sources && ((!runningTransition && this.state) || (runningTransition && this.tState))) {
		if (
			(!runningTransition && this.state === STALE) ||
			(runningTransition && this.tState === STALE)
		)
			updateComputation(this)
		else {
			const updates = Updates
			Updates = null
			runUpdates(() => lookUpstream(this), false)
			Updates = updates
		}
	}
	if (Listener) {
		const sSlot = this.observers ? this.observers.length : 0
		if (!Listener.sources) {
			Listener.sources = [this]
			Listener.sourceSlots = [sSlot]
		} else {
			Listener.sources.push(this)
			Listener.sourceSlots.push(sSlot)
		}
		if (!this.observers) {
			this.observers = [Listener]
			this.observerSlots = [Listener.sources.length - 1]
		} else {
			this.observers.push(Listener)
			this.observerSlots.push(Listener.sources.length - 1)
		}
	}
	if (runningTransition && Transition.sources.has(this)) return this.tValue
	return this.value
}
function writeSignal(node, value, isComp) {
	let current =
		Transition && Transition.running && Transition.sources.has(node) ? node.tValue : node.value
	if (!node.comparator || !node.comparator(current, value)) {
		if (Transition) {
			const TransitionRunning = Transition.running
			if (TransitionRunning || (!isComp && Transition.sources.has(node))) {
				Transition.sources.add(node)
				node.tValue = value
			}
			if (!TransitionRunning) node.value = value
		} else node.value = value
		if (node.observers && node.observers.length) {
			runUpdates(() => {
				for (let i = 0; i < node.observers.length; i += 1) {
					const o = node.observers[i]
					const TransitionRunning = Transition && Transition.running
					if (TransitionRunning && Transition.disposed.has(o)) continue
					if ((TransitionRunning && !o.tState) || (!TransitionRunning && !o.state)) {
						if (o.pure) Updates.push(o)
						else Effects.push(o)
						if (o.observers) markDownstream(o)
					}
					if (TransitionRunning) o.tState = STALE
					else o.state = STALE
				}
				if (Updates.length > 1e6) {
					Updates = []
					if (false);
					throw new Error()
				}
			}, false)
		}
	}
	return value
}
function updateComputation(node) {
	if (!node.fn) return
	cleanNode(node)
	const owner = Owner,
		listener = Listener,
		time = ExecCount
	Listener = Owner = node
	runComputation(
		node,
		Transition && Transition.running && Transition.sources.has(node) ? node.tValue : node.value,
		time
	)
	if (Transition && !Transition.running && Transition.sources.has(node)) {
		queueMicrotask(() => {
			runUpdates(() => {
				Transition && (Transition.running = true)
				Listener = Owner = node
				runComputation(node, node.tValue, time)
				Listener = Owner = null
			}, false)
		})
	}
	Listener = listener
	Owner = owner
}
function runComputation(node, value, time) {
	let nextValue
	try {
		nextValue = node.fn(value)
	} catch (err) {
		if (node.pure) {
			if (Transition && Transition.running) {
				node.tState = STALE
				node.tOwned && node.tOwned.forEach(cleanNode)
				node.tOwned = void 0
			} else {
				node.state = STALE
				node.owned && node.owned.forEach(cleanNode)
				node.owned = null
			}
		}
		node.updatedAt = time + 1
		return handleError(err)
	}
	if (!node.updatedAt || node.updatedAt <= time) {
		if (node.updatedAt != undefined && "observers" in node) {
			writeSignal(node, nextValue, true)
		} else if (Transition && Transition.running && node.pure) {
			Transition.sources.add(node)
			node.tValue = nextValue
		} else node.value = nextValue
		node.updatedAt = time
	}
}
function createComputation(fn, init2, pure, state = STALE, options) {
	const c = {
		fn,
		state,
		updatedAt: null,
		owned: null,
		sources: null,
		sourceSlots: null,
		cleanups: null,
		value: init2,
		owner: Owner,
		context: null,
		pure,
	}
	if (Transition && Transition.running) {
		c.state = 0
		c.tState = state
	}
	if (Owner === null);
	else if (Owner !== UNOWNED) {
		if (Transition && Transition.running && Owner.pure) {
			if (!Owner.tOwned) Owner.tOwned = [c]
			else Owner.tOwned.push(c)
		} else {
			if (!Owner.owned) Owner.owned = [c]
			else Owner.owned.push(c)
		}
	}
	if (ExternalSourceFactory) {
		const [track, trigger] = createSignal(void 0, {
			equals: false,
		})
		const ordinary = ExternalSourceFactory(c.fn, trigger)
		onCleanup(() => ordinary.dispose())
		const triggerInTransition = () => startTransition(trigger).then(() => inTransition.dispose())
		const inTransition = ExternalSourceFactory(c.fn, triggerInTransition)
		c.fn = (x) => {
			track()
			return Transition && Transition.running ? inTransition.track(x) : ordinary.track(x)
		}
	}
	return c
}
function runTop(node) {
	const runningTransition = Transition && Transition.running
	if ((!runningTransition && node.state === 0) || (runningTransition && node.tState === 0)) return
	if (
		(!runningTransition && node.state === PENDING) ||
		(runningTransition && node.tState === PENDING)
	)
		return lookUpstream(node)
	if (node.suspense && untrack(node.suspense.inFallback)) return node.suspense.effects.push(node)
	const ancestors = [node]
	while ((node = node.owner) && (!node.updatedAt || node.updatedAt < ExecCount)) {
		if (runningTransition && Transition.disposed.has(node)) return
		if ((!runningTransition && node.state) || (runningTransition && node.tState))
			ancestors.push(node)
	}
	for (let i = ancestors.length - 1; i >= 0; i--) {
		node = ancestors[i]
		if (runningTransition) {
			let top = node,
				prev = ancestors[i + 1]
			while ((top = top.owner) && top !== prev) {
				if (Transition.disposed.has(top)) return
			}
		}
		if (
			(!runningTransition && node.state === STALE) ||
			(runningTransition && node.tState === STALE)
		) {
			updateComputation(node)
		} else if (
			(!runningTransition && node.state === PENDING) ||
			(runningTransition && node.tState === PENDING)
		) {
			const updates = Updates
			Updates = null
			runUpdates(() => lookUpstream(node, ancestors[0]), false)
			Updates = updates
		}
	}
}
function runUpdates(fn, init2) {
	if (Updates) return fn()
	let wait = false
	if (!init2) Updates = []
	if (Effects) wait = true
	else Effects = []
	ExecCount++
	try {
		const res = fn()
		completeUpdates(wait)
		return res
	} catch (err) {
		if (!wait) Effects = null
		Updates = null
		handleError(err)
	}
}
function completeUpdates(wait) {
	if (Updates) {
		if (Scheduler && Transition && Transition.running) scheduleQueue(Updates)
		else runQueue(Updates)
		Updates = null
	}
	if (wait) return
	let res
	if (Transition) {
		if (!Transition.promises.size && !Transition.queue.size) {
			const sources = Transition.sources
			const disposed = Transition.disposed
			Effects.push.apply(Effects, Transition.effects)
			res = Transition.resolve
			for (const e2 of Effects) {
				"tState" in e2 && (e2.state = e2.tState)
				delete e2.tState
			}
			Transition = null
			runUpdates(() => {
				for (const d of disposed) cleanNode(d)
				for (const v of sources) {
					v.value = v.tValue
					if (v.owned) {
						for (let i = 0, len = v.owned.length; i < len; i++) cleanNode(v.owned[i])
					}
					if (v.tOwned) v.owned = v.tOwned
					delete v.tValue
					delete v.tOwned
					v.tState = 0
				}
				setTransPending(false)
			}, false)
		} else if (Transition.running) {
			Transition.running = false
			Transition.effects.push.apply(Transition.effects, Effects)
			Effects = null
			setTransPending(true)
			return
		}
	}
	const e = Effects
	Effects = null
	if (e.length) runUpdates(() => runEffects(e), false)
	if (res) res()
}
function runQueue(queue) {
	for (let i = 0; i < queue.length; i++) runTop(queue[i])
}
function scheduleQueue(queue) {
	for (const item of queue) {
		const tasks = Transition.queue
		if (!tasks.has(item)) {
			tasks.add(item)
			Scheduler(() => {
				tasks.delete(item)
				runUpdates(() => {
					Transition.running = true
					runTop(item)
				}, false)
				Transition && (Transition.running = false)
			})
		}
	}
}
function runUserEffects(queue) {
	let i,
		userLength = 0
	for (i = 0; i < queue.length; i++) {
		const e = queue[i]
		if (!e.user) runTop(e)
		else queue[userLength++] = e
	}
	if (sharedConfig.context) setHydrateContext()
	for (i = 0; i < userLength; i++) runTop(queue[i])
}
function lookUpstream(node, ignore2) {
	const runningTransition = Transition && Transition.running
	if (runningTransition) node.tState = 0
	else node.state = 0
	for (let i = 0; i < node.sources.length; i += 1) {
		const source = node.sources[i]
		if (source.sources) {
			if (
				(!runningTransition && source.state === STALE) ||
				(runningTransition && source.tState === STALE)
			) {
				if (source !== ignore2 && (!source.updatedAt || source.updatedAt < ExecCount))
					runTop(source)
			} else if (
				(!runningTransition && source.state === PENDING) ||
				(runningTransition && source.tState === PENDING)
			)
				lookUpstream(source, ignore2)
		}
	}
}
function markDownstream(node) {
	const runningTransition = Transition && Transition.running
	for (let i = 0; i < node.observers.length; i += 1) {
		const o = node.observers[i]
		if ((!runningTransition && !o.state) || (runningTransition && !o.tState)) {
			if (runningTransition) o.tState = PENDING
			else o.state = PENDING
			if (o.pure) Updates.push(o)
			else Effects.push(o)
			o.observers && markDownstream(o)
		}
	}
}
function cleanNode(node) {
	let i
	if (node.sources) {
		while (node.sources.length) {
			const source = node.sources.pop(),
				index2 = node.sourceSlots.pop(),
				obs = source.observers
			if (obs && obs.length) {
				const n = obs.pop(),
					s = source.observerSlots.pop()
				if (index2 < obs.length) {
					n.sourceSlots[s] = index2
					obs[index2] = n
					source.observerSlots[index2] = s
				}
			}
		}
	}
	if (Transition && Transition.running && node.pure) {
		if (node.tOwned) {
			for (i = 0; i < node.tOwned.length; i++) cleanNode(node.tOwned[i])
			delete node.tOwned
		}
		reset(node, true)
	} else if (node.owned) {
		for (i = 0; i < node.owned.length; i++) cleanNode(node.owned[i])
		node.owned = null
	}
	if (node.cleanups) {
		for (i = 0; i < node.cleanups.length; i++) node.cleanups[i]()
		node.cleanups = null
	}
	if (Transition && Transition.running) node.tState = 0
	else node.state = 0
	node.context = null
}
function reset(node, top) {
	if (!top) {
		node.tState = 0
		Transition.disposed.add(node)
	}
	if (node.owned) {
		for (let i = 0; i < node.owned.length; i++) reset(node.owned[i])
	}
}
function castError(err) {
	if (err instanceof Error || typeof err === "string") return err
	return new Error("Unknown error")
}
function runErrors(fns, err) {
	for (const f of fns) f(err)
}
function handleError(err) {
	err = castError(err)
	const fns = ERROR && lookup2(Owner, ERROR)
	if (!fns) throw err
	if (Effects)
		Effects.push({
			fn() {
				runErrors(fns, err)
			},
			state: STALE,
		})
	else runErrors(fns, err)
}
function lookup2(owner, key) {
	return owner
		? owner.context && owner.context[key] !== void 0
			? owner.context[key]
			: lookup2(owner.owner, key)
		: void 0
}
function resolveChildren(children2) {
	if (typeof children2 === "function" && !children2.length) return resolveChildren(children2())
	if (Array.isArray(children2)) {
		const results = []
		for (const element of children2) {
			const result = resolveChildren(element)
			Array.isArray(result) ? results.push.apply(results, result) : results.push(result)
		}
		return results
	}
	return children2
}
function createProvider(id, options) {
	return function provider(props) {
		let res
		createRenderEffect(
			() =>
				(res = untrack(() => {
					Owner.context = {
						[id]: props.value,
					}
					return children(() => props.children)
				})),
			void 0
		)
		return res
	}
}
var FALLBACK = Symbol("fallback")
var SuspenseListContext = createContext()
var DEV

// ../sdk/dist/reactivity/solid.js
var createSignal2 = createSignal
var createMemo2 = createMemo
var createRoot2 = createRoot
var createEffect2 = createEffect
var batch2 = batch
var getListener2 = getListener
var onCleanup2 = onCleanup
var untrack2 = untrack

// ../sdk/dist/reactivity/trigger.js
var isServer = false
var triggerOptions = !isServer && DEV ? { equals: false, name: "trigger" } : { equals: false }
var triggerCacheOptions = !isServer && DEV ? { equals: false, internal: true } : triggerOptions
var TriggerCache = class {
	#map
	constructor(mapConstructor = Map) {
		this.#map = new mapConstructor()
	}
	dirty(key) {
		if (isServer) return
		this.#map.get(key)?.$$()
	}
	track(key) {
		if (!getListener2()) return
		let trigger = this.#map.get(key)
		if (!trigger) {
			const [$, $$] = createSignal2(void 0, triggerCacheOptions)
			this.#map.set(key, (trigger = { $, $$, n: 1 }))
		} else trigger.n++
		onCleanup2(() => {
			if (trigger.n-- === 1) queueMicrotask(() => trigger.n === 0 && this.#map.delete(key))
		})
		trigger.$()
	}
}

// ../sdk/dist/reactivity/map.js
var $KEYS = Symbol("track-keys")
var ReactiveMap = class extends Map {
	#keyTriggers = new TriggerCache()
	#valueTriggers = new TriggerCache()
	constructor(initial) {
		super()
		if (initial) for (const v of initial) super.set(v[0], v[1])
	}
	// reads
	has(key) {
		this.#keyTriggers.track(key)
		return super.has(key)
	}
	get(key) {
		this.#valueTriggers.track(key)
		return super.get(key)
	}
	get size() {
		this.#keyTriggers.track($KEYS)
		return super.size
	}
	keys() {
		this.#keyTriggers.track($KEYS)
		return super.keys()
	}
	values() {
		this.#keyTriggers.track($KEYS)
		for (const v of super.keys()) this.#valueTriggers.track(v)
		return super.values()
	}
	entries() {
		this.#keyTriggers.track($KEYS)
		for (const v of super.keys()) this.#valueTriggers.track(v)
		return super.entries()
	}
	// writes
	set(key, value) {
		batch2(() => {
			if (super.has(key)) {
				if (super.get(key) === value) return
			} else {
				this.#keyTriggers.dirty(key)
				this.#keyTriggers.dirty($KEYS)
			}
			this.#valueTriggers.dirty(key)
			super.set(key, value)
		})
		return this
	}
	delete(key) {
		const r = super.delete(key)
		if (r) {
			batch2(() => {
				this.#keyTriggers.dirty(key)
				this.#keyTriggers.dirty($KEYS)
				this.#valueTriggers.dirty(key)
			})
		}
		return r
	}
	clear() {
		if (super.size) {
			batch2(() => {
				for (const v of super.keys()) {
					this.#keyTriggers.dirty(v)
					this.#valueTriggers.dirty(v)
				}
				super.clear()
				this.#keyTriggers.dirty($KEYS)
			})
		}
	}
	// callback
	forEach(callbackfn) {
		this.#keyTriggers.track($KEYS)
		for (const [key, value] of super.entries()) callbackfn(value, key, this)
	}
	[Symbol.iterator]() {
		return this.entries()
	}
}

// ../sdk/dist/createNodeishFsWithWatcher.js
var createNodeishFsWithWatcher = (args) => {
	const pathList = /* @__PURE__ */ new Set()
	const abortControllers = /* @__PURE__ */ new Set()
	const stopWatching = () => {
		for (const ac of abortControllers) {
			ac.abort()
			abortControllers.delete(ac)
		}
	}
	const makeWatcher = async (path) => {
		try {
			const ac = new AbortController()
			abortControllers.add(ac)
			const watcher = args.nodeishFs.watch(path, {
				signal: ac.signal,
				recursive: true,
				persistent: false,
			})
			if (watcher) {
				for await (const event of watcher) {
					args.onChange()
				}
			}
		} catch (err) {
			if (err.name === "AbortError") return
			else if (err.code === "ENOENT") return
			throw err
		}
	}
	const watched = (fn) => {
		return (path, ...rest) => {
			if (!pathList.has(path)) {
				makeWatcher(path)
				pathList.add(path)
			}
			return fn(path, ...rest)
		}
	}
	return {
		...args.nodeishFs,
		/**
		 * Reads the file and automatically adds it to the list of watched files.
		 * Any changes to the file will trigger a message update.
		 */
		// @ts-expect-error
		readFile: watched(args.nodeishFs.readFile),
		/**
		 * Reads the directory and automatically adds it to the list of watched files.
		 * Any changes to the directory will trigger a message update.
		 */
		readdir: watched(args.nodeishFs.readdir),
		stopWatching,
	}
}

// ../sdk/dist/storage/helper.js
function normalizeMessage(message) {
	const messageWithSortedKeys = {}
	for (const key of Object.keys(message).sort()) {
		messageWithSortedKeys[key] = message[key]
	}
	messageWithSortedKeys["variants"] = messageWithSortedKeys["variants"]
		.sort((variantA, variantB) => {
			const languageComparison = variantA.languageTag.localeCompare(variantB.languageTag)
			if (languageComparison === 0) {
				return variantA.match.join("-").localeCompare(variantB.match.join("-"))
			}
			return languageComparison
		})
		.map((variant) => {
			const variantWithSortedKeys = {}
			for (const variantKey of Object.keys(variant).sort()) {
				if (variantKey === "pattern") {
					variantWithSortedKeys[variantKey] = variant["pattern"].map((token) => {
						const tokenWithSortedKey = {}
						for (const tokenKey of Object.keys(token).sort()) {
							tokenWithSortedKey[tokenKey] = token[tokenKey]
						}
						return tokenWithSortedKey
					})
				} else {
					variantWithSortedKeys[variantKey] = variant[variantKey]
				}
			}
			return variantWithSortedKeys
		})
	return messageWithSortedKeys
}
function stringifyMessage(message) {
	return JSON.stringify(normalizeMessage(message), void 0, 4)
}

// ../sdk/dist/persistence/filelock/acquireFileLock.js
init_dist()
var import_debug2 = __toESM(require_src(), 1)
var debug2 = (0, import_debug2.default)("sdk:fileLock")
var maxRetries = 10
var nProbes = 50
var probeInterval = 100
async function acquireFileLock(fs2, lockDirPath, lockOrigin, tryCount = 0) {
	if (tryCount > maxRetries) {
		throw new Error(
			`${lockOrigin} exceeded maximum retries (${maxRetries}) to acquire lockfile ${tryCount}`
		)
	}
	try {
		debug2(lockOrigin + " tries to acquire a lockfile Retry Nr.: " + tryCount)
		await fs2.mkdir(lockDirPath)
		const stats = await fs2.stat(lockDirPath)
		debug2(lockOrigin + " acquired a lockfile Retry Nr.: " + tryCount)
		return stats.mtimeMs
	} catch (error) {
		if (error.code !== "EEXIST") {
			throw error
		}
	}
	let currentLockTime
	try {
		const stats = await fs2.stat(lockDirPath)
		currentLockTime = stats.mtimeMs
	} catch (fstatError) {
		if (fstatError.code === "ENOENT") {
			debug2(lockOrigin + " tryCount++ lock file seems to be gone :) - lets try again " + tryCount)
			return acquireFileLock(fs2, lockDirPath, lockOrigin, tryCount + 1)
		}
		throw fstatError
	}
	debug2(
		lockOrigin +
			" tries to acquire a lockfile  - lock currently in use... starting probe phase " +
			tryCount
	)
	return new Promise((resolve, reject) => {
		let probeCounts = 0
		const scheduleProbationTimeout = () => {
			setTimeout(async () => {
				probeCounts += 1
				let lockFileStats = void 0
				try {
					debug2(
						lockOrigin + " tries to acquire a lockfile - check if the lock is free now " + tryCount
					)
					lockFileStats = await fs2.stat(lockDirPath)
				} catch (fstatError) {
					if (fstatError.code === "ENOENT") {
						debug2(
							lockOrigin +
								" tryCount++ in Promise - tries to acquire a lockfile - lock file seems to be free now - try to acquire " +
								tryCount
						)
						const lock2 = acquireFileLock(fs2, lockDirPath, lockOrigin, tryCount + 1)
						return resolve(lock2)
					}
					return reject(fstatError)
				}
				if (lockFileStats.mtimeMs === currentLockTime) {
					if (probeCounts >= nProbes) {
						debug2(
							lockOrigin +
								" tries to acquire a lockfile  - lock not free - but stale lets drop it" +
								tryCount
						)
						try {
							await fs2.rmdir(lockDirPath)
						} catch (rmLockError) {
							if (rmLockError.code === "ENOENT") {
							}
							return reject(rmLockError)
						}
						try {
							debug2(
								lockOrigin +
									" tryCount++ same locker - try to acquire again after removing stale lock " +
									tryCount
							)
							const lock2 = await acquireFileLock(fs2, lockDirPath, lockOrigin, tryCount + 1)
							return resolve(lock2)
						} catch (lockAquireException) {
							return reject(lockAquireException)
						}
					} else {
						return scheduleProbationTimeout()
					}
				} else {
					try {
						debug2(lockOrigin + " tryCount++ different locker - try to acquire again " + tryCount)
						const lock2 = await acquireFileLock(fs2, lockDirPath, lockOrigin, tryCount + 1)
						return resolve(lock2)
					} catch (error) {
						return reject(error)
					}
				}
			}, probeInterval)
		}
		scheduleProbationTimeout()
	})
}

// ../sdk/dist/createMessagesQuery.js
var import_debug4 = __toESM(require_src(), 1)

// ../sdk/dist/persistence/filelock/releaseLock.js
init_dist()
var import_debug3 = __toESM(require_src(), 1)
var debug3 = (0, import_debug3.default)("sdk:fileLock")
async function releaseLock(fs2, lockDirPath, lockOrigin, lockTime) {
	debug3(lockOrigin + " releasing the lock ")
	try {
		const stats = await fs2.stat(lockDirPath)
		if (stats.mtimeMs === lockTime) {
			await fs2.rmdir(lockDirPath)
		}
	} catch (statError) {
		debug3(lockOrigin + " couldn't release the lock")
		if (statError.code === "ENOENT") {
			debug3(lockOrigin + " WARNING - the lock was released by a different process")
			return
		}
		debug3(statError)
		throw statError
	}
}

// ../sdk/dist/storage/human-id/human-readable-id.js
var import_murmurhash3js = __toESM(require_murmurhash3js(), 1)

// ../sdk/dist/storage/human-id/words.js
var animals = [
	"albatross",
	"alligator",
	"alpaca",
	"anaconda",
	"angelfish",
	"ant",
	"anteater",
	"antelope",
	"ape",
	"baboon",
	"badger",
	"barbel",
	"bat",
	"bear",
	"beaver",
	"bee",
	"beetle",
	"bird",
	"bison",
	"blackbird",
	"boar",
	"bobcat",
	"bulldog",
	"bullock",
	"bumblebee",
	"butterfly",
	"buzzard",
	"camel",
	"canary",
	"capybara",
	"carp",
	"cat",
	"cheetah",
	"chicken",
	"chipmunk",
	"clownfish",
	"cobra",
	"cockroach",
	"cod",
	"cougar",
	"cow",
	"cowfish",
	"coyote",
	"crab",
	"crocodile",
	"crossbill",
	"crow",
	"cuckoo",
	"dachshund",
	"deer",
	"dingo",
	"dog",
	"dolphin",
	"donkey",
	"dove",
	"dragonfly",
	"duck",
	"eagle",
	"earthworm",
	"eel",
	"elephant",
	"elk",
	"emu",
	"falcon",
	"felix",
	"finch",
	"fireant",
	"firefox",
	"fish",
	"flamingo",
	"flea",
	"florian",
	"fly",
	"fox",
	"frog",
	"gadfly",
	"gazelle",
	"gecko",
	"gibbon",
	"giraffe",
	"goat",
	"goldfish",
	"goose",
	"gopher",
	"gorilla",
	"grebe",
	"grizzly",
	"gull",
	"guppy",
	"haddock",
	"halibut",
	"hamster",
	"hare",
	"hawk",
	"hedgehog",
	"herring",
	"hornet",
	"horse",
	"hound",
	"husky",
	"hyena",
	"ibex",
	"iguana",
	"impala",
	"insect",
	"jackal",
	"jackdaw",
	"jaguar",
	"jan",
	"jannes",
	"javelina",
	"jay",
	"jellyfish",
	"jurgen",
	"kangaroo",
	"kestrel",
	"kitten",
	"koala",
	"kudu",
	"ladybug",
	"lamb",
	"lark",
	"larva",
	"lemming",
	"lemur",
	"leopard",
	"liger",
	"lion",
	"lionfish",
	"lizard",
	"llama",
	"lobster",
	"loris",
	"lynx",
	"macaw",
	"maggot",
	"mallard",
	"mammoth",
	"manatee",
	"mantis",
	"mare",
	"marlin",
	"marmot",
	"marten",
	"martin",
	"mayfly",
	"meerkat",
	"midge",
	"millipede",
	"mink",
	"mole",
	"mongoose",
	"monkey",
	"moose",
	"moth",
	"mouse",
	"mule",
	"myna",
	"newt",
	"niklas",
	"nils",
	"nuthatch",
	"ocelot",
	"octopus",
	"okapi",
	"opossum",
	"orangutan",
	"oryx",
	"osprey",
	"ostrich",
	"otter",
	"owl",
	"ox",
	"panda",
	"panther",
	"parakeet",
	"parrot",
	"peacock",
	"pelican",
	"penguin",
	"pig",
	"pigeon",
	"piranha",
	"platypus",
	"polecat",
	"pony",
	"poodle",
	"porpoise",
	"puffin",
	"pug",
	"puma",
	"quail",
	"rabbit",
	"racoon",
	"rat",
	"raven",
	"ray",
	"reindeer",
	"robin",
	"rook",
	"rooster",
	"salmon",
	"samuel",
	"sawfish",
	"scallop",
	"seahorse",
	"seal",
	"shad",
	"shark",
	"sheep",
	"shell",
	"shrike",
	"shrimp",
	"skate",
	"skunk",
	"sloth",
	"slug",
	"snail",
	"snake",
	"sparrow",
	"spider",
	"squid",
	"squirrel",
	"starfish",
	"stingray",
	"stork",
	"swallow",
	"swan",
	"tadpole",
	"tapir",
	"termite",
	"tern",
	"thrush",
	"tiger",
	"toad",
	"tortoise",
	"toucan",
	"trout",
	"tuna",
	"turkey",
	"turtle",
	"vole",
	"vulture",
	"wallaby",
	"walrus",
	"warbler",
	"warthog",
	"wasp",
	"weasel",
	"whale",
	"wolf",
	"wombat",
	"worm",
	"wren",
	"yak",
	"zebra",
]
var adjectives = [
	"acidic",
	"active",
	"actual",
	"agent",
	"ago",
	"alert",
	"alive",
	"aloof",
	"antsy",
	"any",
	"aqua",
	"arable",
	"awake",
	"aware",
	"away",
	"awful",
	"bad",
	"bald",
	"basic",
	"best",
	"big",
	"bland",
	"blue",
	"bold",
	"born",
	"brave",
	"brief",
	"bright",
	"broad",
	"busy",
	"calm",
	"candid",
	"careful",
	"caring",
	"chunky",
	"civil",
	"clean",
	"clear",
	"close",
	"cool",
	"cozy",
	"crazy",
	"crisp",
	"cuddly",
	"curly",
	"cute",
	"dark",
	"day",
	"deft",
	"direct",
	"dirty",
	"dizzy",
	"drab",
	"dry",
	"due",
	"dull",
	"each",
	"early",
	"east",
	"elegant",
	"empty",
	"equal",
	"even",
	"every",
	"extra",
	"factual",
	"fair",
	"fancy",
	"few",
	"fine",
	"fit",
	"flaky",
	"flat",
	"fluffy",
	"formal",
	"frail",
	"free",
	"fresh",
	"front",
	"full",
	"fun",
	"funny",
	"fuzzy",
	"game",
	"gaudy",
	"giant",
	"glad",
	"good",
	"grand",
	"grassy",
	"gray",
	"great",
	"green",
	"gross",
	"happy",
	"heavy",
	"helpful",
	"heroic",
	"home",
	"honest",
	"hour",
	"house",
	"icy",
	"ideal",
	"inclusive",
	"inner",
	"jolly",
	"jumpy",
	"just",
	"keen",
	"key",
	"kind",
	"knotty",
	"known",
	"large",
	"last",
	"late",
	"lazy",
	"least",
	"left",
	"legal",
	"less",
	"level",
	"light",
	"lime",
	"livid",
	"lofty",
	"long",
	"loose",
	"lost",
	"loud",
	"loved",
	"low",
	"lower",
	"lucky",
	"mad",
	"main",
	"major",
	"male",
	"many",
	"maroon",
	"mealy",
	"mean",
	"mellow",
	"merry",
	"mild",
	"minor",
	"misty",
	"moving",
	"muddy",
	"mushy",
	"neat",
	"new",
	"next",
	"nice",
	"nimble",
	"noble",
	"noisy",
	"north",
	"novel",
	"odd",
	"ok",
	"only",
	"orange",
	"ornate",
	"patchy",
	"patient",
	"petty",
	"pink",
	"plain",
	"plane",
	"polite",
	"pretty",
	"proof",
	"proud",
	"quaint",
	"quick",
	"quiet",
	"raw",
	"real",
	"red",
	"round",
	"royal",
	"sad",
	"safe",
	"salty",
	"same",
	"sea",
	"seemly",
	"sharp",
	"short",
	"shy",
	"silly",
	"simple",
	"sleek",
	"slimy",
	"slow",
	"small",
	"smart",
	"smug",
	"soft",
	"solid",
	"sound",
	"sour",
	"spare",
	"spicy",
	"spry",
	"stale",
	"steep",
	"still",
	"stock",
	"stout",
	"strong",
	"suave",
	"such",
	"sunny",
	"super",
	"sweet",
	"swift",
	"tame",
	"tangy",
	"tasty",
	"teal",
	"teary",
	"tense",
	"that",
	"these",
	"this",
	"tidy",
	"tiny",
	"tired",
	"top",
	"topical",
	"tough",
	"trick",
	"trite",
	"true",
	"upper",
	"vexed",
	"vivid",
	"wacky",
	"warm",
	"watery",
	"weak",
	"weary",
	"weird",
	"white",
	"whole",
	"wide",
	"wild",
	"wise",
	"witty",
	"yummy",
	"zany",
	"zesty",
	"zippy",
]
var adverbs = [
	"ablaze",
	"about",
	"above",
	"abroad",
	"across",
	"adrift",
	"afloat",
	"after",
	"again",
	"ahead",
	"alike",
	"all",
	"almost",
	"alone",
	"along",
	"aloud",
	"always",
	"amazing",
	"anxious",
	"anywhere",
	"apart",
	"around",
	"arrogant",
	"aside",
	"asleep",
	"awkward",
	"back",
	"bashful",
	"beautiful",
	"before",
	"behind",
	"below",
	"beside",
	"besides",
	"beyond",
	"bitter",
	"bleak",
	"blissful",
	"boldly",
	"bravely",
	"briefly",
	"brightly",
	"brisk",
	"busily",
	"calmly",
	"carefully",
	"careless",
	"cautious",
	"certain",
	"cheerful",
	"clearly",
	"clever",
	"closely",
	"closer",
	"colorful",
	"common",
	"correct",
	"cross",
	"cruel",
	"curious",
	"daily",
	"dainty",
	"daring",
	"dear",
	"desperate",
	"diligent",
	"doubtful",
	"doubtless",
	"down",
	"downwards",
	"dreamily",
	"eager",
	"easily",
	"either",
	"elegantly",
	"else",
	"elsewhere",
	"enormous",
	"enough",
	"ever",
	"famous",
	"far",
	"fast",
	"fervent",
	"fierce",
	"fondly",
	"foolish",
	"forever",
	"forth",
	"fortunate",
	"forward",
	"frank",
	"freely",
	"frequent",
	"fully",
	"general",
	"generous",
	"gladly",
	"graceful",
	"grateful",
	"gratis",
	"half",
	"happily",
	"hard",
	"harsh",
	"hearty",
	"helpless",
	"here",
	"highly",
	"hitherto",
	"how",
	"however",
	"hurried",
	"immediate",
	"in",
	"indeed",
	"inland",
	"innocent",
	"inside",
	"instant",
	"intense",
	"inward",
	"jealous",
	"jovial",
	"joyful",
	"jubilant",
	"keenly",
	"kindly",
	"knowing",
	"lately",
	"lazily",
	"lightly",
	"likely",
	"little",
	"live",
	"loftily",
	"longing",
	"loosely",
	"loudly",
	"loving",
	"loyal",
	"luckily",
	"madly",
	"maybe",
	"meanwhile",
	"mocking",
	"monthly",
	"moreover",
	"much",
	"near",
	"neatly",
	"neither",
	"nervous",
	"never",
	"noisily",
	"normal",
	"not",
	"now",
	"nowadays",
	"nowhere",
	"oddly",
	"off",
	"official",
	"often",
	"on",
	"once",
	"open",
	"openly",
	"opposite",
	"otherwise",
	"out",
	"outside",
	"over",
	"overall",
	"overhead",
	"overnight",
	"overseas",
	"parallel",
	"partial",
	"past",
	"patiently",
	"perfect",
	"perhaps",
	"physical",
	"playful",
	"politely",
	"potential",
	"powerful",
	"presto",
	"profound",
	"prompt",
	"proper",
	"proudly",
	"punctual",
	"quickly",
	"quizzical",
	"rare",
	"ravenous",
	"ready",
	"really",
	"reckless",
	"regular",
	"repeated",
	"restful",
	"rightful",
	"rigid",
	"rude",
	"sadly",
	"safely",
	"scarce",
	"scary",
	"searching",
	"seeming",
	"seldom",
	"selfish",
	"separate",
	"serious",
	"shaky",
	"sheepish",
	"silent",
	"sleepy",
	"smooth",
	"softly",
	"solemn",
	"solidly",
	"sometimes",
	"speedy",
	"stealthy",
	"stern",
	"strict",
	"stubborn",
	"sudden",
	"supposed",
	"sweetly",
	"swiftly",
	"tender",
	"tensely",
	"thankful",
	"tight",
	"too",
	"twice",
	"under",
	"untrue",
	"uphill",
	"upward",
	"vaguely",
	"vainly",
	"vastly",
	"warmly",
	"wearily",
	"weekly",
	"well",
	"wisely",
	"within",
	"wrongly",
	"yonder",
]
var verbs = [
	"absorb",
	"accept",
	"achieve",
	"adapt",
	"adore",
	"advise",
	"affirm",
	"agree",
	"aid",
	"aim",
	"amaze",
	"amuse",
	"animate",
	"approve",
	"arise",
	"arrive",
	"ascend",
	"ask",
	"aspire",
	"assure",
	"attend",
	"bake",
	"bask",
	"beam",
	"believe",
	"belong",
	"bend",
	"blend",
	"bless",
	"blink",
	"bloom",
	"boil",
	"boost",
	"borrow",
	"breathe",
	"bubble",
	"build",
	"bump",
	"burn",
	"buy",
	"buzz",
	"care",
	"catch",
	"charm",
	"cheer",
	"cherish",
	"chop",
	"clap",
	"clasp",
	"climb",
	"clip",
	"coax",
	"comfort",
	"commend",
	"compose",
	"conquer",
	"cook",
	"create",
	"cry",
	"cuddle",
	"cure",
	"cut",
	"dance",
	"dare",
	"dart",
	"dash",
	"dazzle",
	"delight",
	"devour",
	"dial",
	"dig",
	"dine",
	"dream",
	"drip",
	"drop",
	"drum",
	"dust",
	"earn",
	"edit",
	"embrace",
	"emerge",
	"empower",
	"enchant",
	"endure",
	"engage",
	"enjoy",
	"enrich",
	"evoke",
	"exhale",
	"expand",
	"explore",
	"express",
	"fade",
	"fall",
	"favor",
	"fear",
	"feast",
	"feel",
	"fetch",
	"file",
	"find",
	"flip",
	"flop",
	"flow",
	"fold",
	"fond",
	"forgive",
	"foster",
	"fry",
	"fulfill",
	"gasp",
	"gaze",
	"gleam",
	"glow",
	"grace",
	"grasp",
	"greet",
	"grin",
	"grip",
	"grow",
	"gulp",
	"hack",
	"harbor",
	"heal",
	"heart",
	"hike",
	"hint",
	"honor",
	"hope",
	"hug",
	"hunt",
	"hurl",
	"hush",
	"imagine",
	"inspire",
	"intend",
	"jest",
	"jolt",
	"jump",
	"kick",
	"kiss",
	"laugh",
	"launch",
	"lead",
	"leap",
	"learn",
	"lend",
	"lift",
	"link",
	"list",
	"lock",
	"loop",
	"love",
	"mend",
	"mix",
	"mop",
	"nail",
	"nourish",
	"nudge",
	"nurture",
	"offer",
	"pat",
	"pause",
	"pave",
	"peek",
	"peel",
	"persist",
	"pet",
	"pick",
	"pinch",
	"play",
	"pop",
	"pout",
	"praise",
	"pray",
	"pride",
	"promise",
	"propel",
	"prosper",
	"pull",
	"push",
	"quell",
	"quiz",
	"race",
	"radiate",
	"read",
	"reap",
	"relish",
	"renew",
	"reside",
	"rest",
	"revive",
	"ripple",
	"rise",
	"roam",
	"roar",
	"rush",
	"sail",
	"savor",
	"scold",
	"scoop",
	"seek",
	"sew",
	"shine",
	"sing",
	"skip",
	"slide",
	"slurp",
	"smile",
	"snap",
	"snip",
	"soar",
	"spark",
	"spin",
	"splash",
	"sprout",
	"spur",
	"stab",
	"startle",
	"stir",
	"stop",
	"strive",
	"succeed",
	"support",
	"surge",
	"sway",
	"swim",
	"talk",
	"tap",
	"taste",
	"tear",
	"tend",
	"thrive",
	"tickle",
	"transform",
	"treasure",
	"treat",
	"trim",
	"trip",
	"trust",
	"twirl",
	"twist",
	"type",
	"urge",
	"value",
	"vent",
	"view",
	"walk",
	"wave",
	"win",
	"wish",
	"work",
	"yell",
	"zap",
	"zip",
	"zoom",
]

// ../sdk/dist/storage/human-id/human-readable-id.js
function humanIdHash(value, offset = 0) {
	const seed = 42
	const hash32 = import_murmurhash3js.default.x86.hash32(value, seed)
	const hash32WithOffset = (hash32 + offset) >>> 0
	const part1 = (hash32WithOffset >>> 24) & 255
	const part2 = (hash32WithOffset >>> 16) & 255
	const part3 = (hash32WithOffset >>> 8) & 255
	const part4 = hash32WithOffset & 255
	return `${adjectives[part1]}_${animals[part2]}_${verbs[part3]}_${adverbs[part4]}`
}

// ../sdk/dist/createMessagesQuery.js
var debug4 = (0, import_debug4.default)("sdk:messages")
function createMessagesQuery({
	projectPath,
	nodeishFs,
	settings,
	resolvedModules,
	onInitialMessageLoadResult,
	onLoadMessageResult,
	onSaveMessageResult,
}) {
	const index2 = new ReactiveMap()
	let loaded = false
	const messageLockDirPath = projectPath + "/messagelock"
	let delegate = void 0
	const setDelegate = (newDelegate, onLoad) => {
		delegate = newDelegate
		if (newDelegate && loaded && onLoad) {
			newDelegate.onLoaded([...index2.values()])
		}
	}
	const defaultAliasIndex = new ReactiveMap()
	const messageStates = {
		messageDirtyFlags: {},
		messageLoadHash: {},
		isSaving: false,
		currentSaveMessagesViaPlugin: void 0,
		sheduledSaveMessages: void 0,
		isLoading: false,
		sheduledLoadMessagesViaPlugin: void 0,
	}
	createEffect2(() => {
		index2.clear()
		defaultAliasIndex.clear()
		loaded = false
		const _settings = settings()
		if (!_settings) return
		const resolvedPluginApi = resolvedModules()?.resolvedPluginApi
		if (!resolvedPluginApi) return
		const fsWithWatcher = createNodeishFsWithWatcher({
			nodeishFs,
			// this message is called whenever a file changes that was read earlier by this filesystem
			// - the plugin loads messages -> reads the file messages.json -> start watching on messages.json -> updateMessages
			onChange: () => {
				loadMessagesViaPlugin(
					fsWithWatcher,
					messageLockDirPath,
					messageStates,
					index2,
					defaultAliasIndex,
					delegate,
					_settings,
					// NOTE we bang here - we don't expect the settings to become null during the livetime of a project
					resolvedPluginApi
				)
					.catch((e) => {
						onLoadMessageResult(e)
					})
					.then(() => {
						onLoadMessageResult()
					})
			},
		})
		onCleanup2(() => {
			fsWithWatcher.stopWatching()
			delegate?.onCleanup()
		})
		if (!resolvedPluginApi.loadMessages) {
			onInitialMessageLoadResult(new Error("no loadMessages in resolved Modules found"))
			return
		}
		loadMessagesViaPlugin(
			fsWithWatcher,
			messageLockDirPath,
			messageStates,
			index2,
			defaultAliasIndex,
			void 0,
			_settings,
			// NOTE we bang here - we don't expect the settings to become null during the livetime of a project
			resolvedPluginApi
		)
			.catch((e) => {
				onInitialMessageLoadResult(new PluginLoadMessagesError({ cause: e }))
			})
			.then(() => {
				onInitialMessageLoadResult()
				delegate?.onLoaded([...index2.values()])
				loaded = true
			})
	})
	const get = (args) => index2.get(args.where.id)
	const getByDefaultAlias = (alias) => defaultAliasIndex.get(alias)
	const scheduleSave = function () {
		const _settings = settings()
		if (!_settings) return
		const resolvedPluginApi = resolvedModules()?.resolvedPluginApi
		if (!resolvedPluginApi) return
		saveMessagesViaPlugin(
			nodeishFs,
			messageLockDirPath,
			messageStates,
			index2,
			defaultAliasIndex,
			delegate,
			_settings,
			// NOTE we bang here - we don't expect the settings to become null during the livetime of a project
			resolvedPluginApi
		)
			.catch((e) => {
				debug4.log("error during saveMessagesViaPlugin")
				debug4.log(e)
			})
			.catch((e) => {
				onSaveMessageResult(e)
			})
			.then(() => {
				onSaveMessageResult()
			})
	}
	return {
		setDelegate,
		create: ({ data }) => {
			if (index2.has(data.id)) return false
			index2.set(data.id, data)
			if ("default" in data.alias) {
				defaultAliasIndex.set(data.alias.default, data)
			}
			messageStates.messageDirtyFlags[data.id] = true
			delegate?.onMessageCreate(data.id, index2.get(data.id), [...index2.values()])
			scheduleSave()
			return true
		},
		get: Object.assign(get, {
			subscribe: (args, callback) => createSubscribable(() => get(args)).subscribe(callback),
		}),
		getByDefaultAlias: Object.assign(getByDefaultAlias, {
			subscribe: (alias, callback) =>
				createSubscribable(() => getByDefaultAlias(alias)).subscribe(callback),
		}),
		includedMessageIds: createSubscribable(() => {
			return [...index2.keys()]
		}),
		getAll: createSubscribable(() => {
			return [...index2.values()]
		}),
		update: ({ where, data }) => {
			const message = index2.get(where.id)
			if (message === void 0) return false
			index2.set(where.id, { ...message, ...data })
			if (data.alias && "default" in data.alias) {
				defaultAliasIndex.set(data.alias.default, data)
			}
			messageStates.messageDirtyFlags[where.id] = true
			delegate?.onMessageUpdate(where.id, index2.get(data.id), [...index2.values()])
			scheduleSave()
			return true
		},
		upsert: ({ where, data }) => {
			const message = index2.get(where.id)
			if (message === void 0) {
				index2.set(where.id, data)
				if ("default" in data.alias) {
					defaultAliasIndex.set(data.alias.default, data)
				}
				messageStates.messageDirtyFlags[where.id] = true
				delegate?.onMessageCreate(data.id, index2.get(data.id), [...index2.values()])
			} else {
				index2.set(where.id, { ...message, ...data })
				defaultAliasIndex.set(data.alias.default, { ...message, ...data })
				messageStates.messageDirtyFlags[where.id] = true
				delegate?.onMessageUpdate(data.id, index2.get(data.id), [...index2.values()])
			}
			scheduleSave()
			return true
		},
		delete: ({ where }) => {
			const message = index2.get(where.id)
			if (message === void 0) return false
			if ("default" in message.alias) {
				defaultAliasIndex.delete(message.alias.default)
			}
			index2.delete(where.id)
			messageStates.messageDirtyFlags[where.id] = true
			delegate?.onMessageDelete(where.id, [...index2.values()])
			scheduleSave()
			return true
		},
	}
}
async function loadMessagesViaPlugin(
	fs2,
	lockDirPath,
	messageState,
	messages,
	aliaseToMessageMap,
	delegate,
	settingsValue,
	resolvedPluginApi
) {
	const experimentalAliases = !!settingsValue.experimental?.aliases
	if (messageState.isLoading) {
		if (!messageState.sheduledLoadMessagesViaPlugin) {
			messageState.sheduledLoadMessagesViaPlugin = createAwaitable()
		}
		return messageState.sheduledLoadMessagesViaPlugin.promise
	}
	messageState.isLoading = true
	let lockTime = void 0
	try {
		lockTime = await acquireFileLock(fs2, lockDirPath, "loadMessage")
		const loadedMessages = await makeTrulyAsync(
			resolvedPluginApi.loadMessages({
				settings: settingsValue,
				nodeishFs: fs2,
			})
		)
		const deletedMessages = new Set(messages.keys())
		batch2(() => {
			for (const loadedMessage of loadedMessages) {
				const loadedMessageClone = structuredClone(loadedMessage)
				const currentMessages = [...messages.values()].filter(
					(message) =>
						(experimentalAliases ? message.alias["default"] : message.id) === loadedMessage.id
				)
				if (currentMessages.length > 1) {
					throw new Error("more than one message with the same id or alias found ")
				} else if (currentMessages.length === 1) {
					deletedMessages.delete(currentMessages[0].id)
					loadedMessageClone.alias = {}
					if (experimentalAliases) {
						loadedMessageClone.alias["default"] = loadedMessageClone.id
						loadedMessageClone.id = currentMessages[0].id
					}
					const importedEnecoded = stringifyMessage(loadedMessageClone)
					if (messageState.messageLoadHash[loadedMessageClone.id] === importedEnecoded) {
						continue
					}
					messages.set(loadedMessageClone.id, loadedMessageClone)
					if (loadedMessageClone.alias["default"]) {
						aliaseToMessageMap.set(loadedMessageClone.alias["default"], loadedMessageClone)
					}
					messageState.messageLoadHash[loadedMessageClone.id] = importedEnecoded
					delegate?.onMessageUpdate(loadedMessageClone.id, loadedMessageClone, [
						...messages.values(),
					])
				} else {
					loadedMessageClone.alias = {}
					if (experimentalAliases) {
						loadedMessageClone.alias["default"] = loadedMessageClone.id
						let currentOffset = 0
						let messsageId
						do {
							messsageId = humanIdHash(loadedMessageClone.id, currentOffset)
							if (messages.get(messsageId)) {
								currentOffset += 1
								messsageId = void 0
							}
						} while (messsageId === void 0)
						loadedMessageClone.id = messsageId
						aliaseToMessageMap.set(loadedMessageClone.alias["default"], loadedMessageClone)
					}
					const importedEnecoded = stringifyMessage(loadedMessageClone)
					messages.set(loadedMessageClone.id, loadedMessageClone)
					messageState.messageLoadHash[loadedMessageClone.id] = importedEnecoded
					delegate?.onMessageUpdate(loadedMessageClone.id, loadedMessageClone, [
						...messages.values(),
					])
				}
			}
			for (const deletedMessageId of deletedMessages) {
				messages.delete(deletedMessageId)
				delegate?.onMessageDelete(deletedMessageId, [...messages.values()])
			}
		})
		await releaseLock(fs2, lockDirPath, "loadMessage", lockTime)
		lockTime = void 0
		debug4("loadMessagesViaPlugin: " + loadedMessages.length + " Messages processed ")
		messageState.isLoading = false
	} finally {
		if (lockTime !== void 0) {
			await releaseLock(fs2, lockDirPath, "loadMessage", lockTime)
		}
		messageState.isLoading = false
	}
	const executingScheduledMessages = messageState.sheduledLoadMessagesViaPlugin
	if (executingScheduledMessages) {
		messageState.sheduledLoadMessagesViaPlugin = void 0
		loadMessagesViaPlugin(
			fs2,
			lockDirPath,
			messageState,
			messages,
			aliaseToMessageMap,
			delegate,
			settingsValue,
			resolvedPluginApi
		)
			.then(() => {
				executingScheduledMessages.resolve()
			})
			.catch((e) => {
				executingScheduledMessages.reject(e)
			})
	}
}
async function saveMessagesViaPlugin(
	fs2,
	lockDirPath,
	messageState,
	messages,
	aliaseToMessageMap,
	delegate,
	settingsValue,
	resolvedPluginApi
) {
	if (messageState.isSaving) {
		if (!messageState.sheduledSaveMessages) {
			messageState.sheduledSaveMessages = createAwaitable()
		}
		return messageState.sheduledSaveMessages.promise
	}
	messageState.isSaving = true
	messageState.currentSaveMessagesViaPlugin = (async function () {
		const saveMessageHashes = {}
		if (Object.keys(messageState.messageDirtyFlags).length == 0) {
			debug4("save was skipped - no messages marked as dirty... build!")
			messageState.isSaving = false
			return
		}
		let messageDirtyFlagsBeforeSave
		let lockTime
		try {
			lockTime = await acquireFileLock(fs2, lockDirPath, "saveMessage")
			if (Object.keys(messageState.messageDirtyFlags).length == 0) {
				debug4("save was skipped - no messages marked as dirty... releasing lock again")
				messageState.isSaving = false
				return
			}
			const currentMessages = [...messages.values()]
			const messagesToExport = []
			for (const message of currentMessages) {
				if (messageState.messageDirtyFlags[message.id]) {
					const importedEnecoded = stringifyMessage(message)
					saveMessageHashes[message.id] = importedEnecoded
				}
				const fixedExportMessage = { ...message }
				if (settingsValue.experimental?.aliases) {
					fixedExportMessage.id = fixedExportMessage.alias["default"] ?? fixedExportMessage.id
				}
				messagesToExport.push(fixedExportMessage)
			}
			messageDirtyFlagsBeforeSave = { ...messageState.messageDirtyFlags }
			messageState.messageDirtyFlags = {}
			await resolvedPluginApi.saveMessages({
				settings: settingsValue,
				messages: messagesToExport,
				nodeishFs: fs2,
			})
			for (const [messageId, messageHash] of Object.entries(saveMessageHashes)) {
				messageState.messageLoadHash[messageId] = messageHash
			}
			if (lockTime !== void 0) {
				await releaseLock(fs2, lockDirPath, "saveMessage", lockTime)
				lockTime = void 0
			}
			if (messageState.sheduledLoadMessagesViaPlugin) {
				debug4("saveMessagesViaPlugin calling queued loadMessagesViaPlugin to share lock")
				await loadMessagesViaPlugin(
					fs2,
					lockDirPath,
					messageState,
					messages,
					aliaseToMessageMap,
					delegate,
					settingsValue,
					resolvedPluginApi
				)
			}
			messageState.isSaving = false
		} catch (err) {
			if (messageDirtyFlagsBeforeSave !== void 0) {
				for (const dirtyMessageId of Object.keys(messageDirtyFlagsBeforeSave)) {
					messageState.messageDirtyFlags[dirtyMessageId] = true
				}
			}
			if (lockTime !== void 0) {
				await releaseLock(fs2, lockDirPath, "saveMessage", lockTime)
				lockTime = void 0
			}
			messageState.isSaving = false
			throw new PluginSaveMessagesError({
				cause: err,
			})
		} finally {
			if (lockTime !== void 0) {
				await releaseLock(fs2, lockDirPath, "saveMessage", lockTime)
				lockTime = void 0
			}
			messageState.isSaving = false
		}
	})()
	await messageState.currentSaveMessagesViaPlugin
	if (messageState.sheduledSaveMessages) {
		const executingSheduledSaveMessages = messageState.sheduledSaveMessages
		messageState.sheduledSaveMessages = void 0
		saveMessagesViaPlugin(
			fs2,
			lockDirPath,
			messageState,
			messages,
			aliaseToMessageMap,
			delegate,
			settingsValue,
			resolvedPluginApi
		)
			.then(() => {
				executingSheduledSaveMessages.resolve()
			})
			.catch((e) => {
				executingSheduledSaveMessages.reject(e)
			})
	}
}
var makeTrulyAsync = (fn) => (async () => fn)()
var createAwaitable = () => {
	let resolve
	let reject
	const promise = new Promise((res, rej) => {
		resolve = res
		reject = rej
	})
	return { promise, resolve, reject }
}

// ../sdk/dist/lint/message/errors.js
var MessagedLintRuleThrowedError = class extends Error {
	constructor(message, options) {
		super(message, options)
		this.name = "MessagedLintRuleThrowedError"
	}
}

// ../sdk/dist/lint/message/lintSingleMessage.js
var lintSingleMessage = async (args) => {
	const reports = []
	const errors = []
	const promises = args.rules.map(async (rule) => {
		const level = args.settings.messageLintRuleLevels?.[rule.id]
		if (level === void 0) {
			throw Error("No lint level provided for lint rule: " + rule.id)
		}
		try {
			await rule.run({
				message: args.message,
				settings: args.settings,
				report: (reportArgs) => {
					reports.push({
						ruleId: rule.id,
						level,
						messageId: reportArgs.messageId,
						languageTag: reportArgs.languageTag,
						body: reportArgs.body,
					})
				},
			})
		} catch (error) {
			errors.push(
				new MessagedLintRuleThrowedError(
					`Lint rule '${rule.id}' throwed while linting message "${args.message.id}".`,
					{ cause: error }
				)
			)
		}
	})
	await Promise.all(promises)
	const sortedReports = reports.sort((r1, r2) => r1.ruleId.localeCompare(r2.ruleId))
	return { data: sortedReports, errors }
}

// ../sdk/dist/createMessageLintReportsQuery.js
var import_debug5 = __toESM(require_src(), 1)
var debug5 = (0, import_debug5.default)("sdk:lintReports")
function createMessageLintReportsQuery(
	messagesQuery,
	settings,
	installedMessageLintRules,
	resolvedModules
) {
	const index2 = new ReactiveMap()
	debug5("resetting settledReports")
	let settledReports = Promise.resolve()
	let currentBatchEnd = void 0
	const updatedReports = {}
	createMemo2(() => {
		index2.clear()
		onCleanup2(() => {
			messagesQuery.setDelegate(void 0, false)
		})
		const _settings = settings()
		if (!_settings) return
		const _resolvedModules = resolvedModules()
		if (!_resolvedModules) return
		const rulesArray = _resolvedModules.messageLintRules
		const messageLintRuleLevels = Object.fromEntries(
			installedMessageLintRules().map((rule) => [rule.id, rule.level])
		)
		const settingsObject = () => {
			return {
				...settings(),
				messageLintRuleLevels,
			}
		}
		const sheduleLintMessage = (message, messages) => {
			debug5("shedule Lint for message:", message.id)
			const updateOutstandingReportsOnLast = async () => {
				if (currentBatchEnd !== updateOutstandingReportsOnLast) {
					debug5("skip triggering reactivy", message.id)
					return
				}
				debug5("finished queue - trigger reactivity", message.id)
				batch2(() => {
					for (const [id, reports] of Object.entries(updatedReports)) {
						const currentReports = index2.get(id)
						if (!reportsEqual(currentReports, reports)) {
							debug5("lint reports for message: ", id, " now n:", reports.length)
							index2.set(id, reports)
						}
					}
				})
			}
			currentBatchEnd = updateOutstandingReportsOnLast
			const scheduledLint = lintSingleMessage({
				rules: rulesArray,
				settings: settingsObject(),
				messages,
				message,
			}).then((reportsResult) => {
				if (reportsResult.errors.length === 0) {
					debug5("lint reports for message: ", message.id, "n:", reportsResult.data.length)
					updatedReports[message.id] = reportsResult.data
				}
				return updateOutstandingReportsOnLast()
			})
			settledReports = settledReports.then(() => scheduledLint)
		}
		const messageQueryChangeDelegate = {
			onCleanup: () => {
				index2.clear()
			},
			onLoaded: (messages) => {
				debug5("sheduluing Lint for all messages - on load")
				batch2(() => {
					debug5("sheduluing Lint for all messages - subsquencial call?")
					for (const message of messages) {
						sheduleLintMessage(message, messages)
					}
				})
			},
			onMessageCreate: (messageId, message, messages) => {
				debug5("shedule Lint for message - onMessageCreate", message.id)
				sheduleLintMessage(message, messages)
			},
			onMessageUpdate: (messageId, message, messages) => {
				debug5("shedule Lint for message - onMessageUpdate", message.id)
				sheduleLintMessage(message, messages)
			},
			// eslint-disable-next-line @typescript-eslint/no-unused-vars -- TODO MESDK-105 we gonna need the mesage Property for evaluation
			onMessageDelete: (messageId, _messages) => {
				index2.delete(messageId)
			},
		}
		untrack2(() => {
			messagesQuery.setDelegate(messageQueryChangeDelegate, true)
		})
	})
	const get = (args) => {
		debug5("get", args.where.messageId)
		return structuredClone(index2.get(args.where.messageId))
	}
	const getAll = () => {
		const flatValues = [...index2.values()].flat()
		debug5("getAll", flatValues.length)
		return structuredClone(flatValues.length === 0 ? [] : flatValues)
	}
	return {
		getAll: Object.assign(createSubscribable(getAll), {
			settled: async () => {
				await settledReports
				return getAll()
			},
		}),
		get: Object.assign(get, {
			subscribe: (args, callback) => createSubscribable(() => get(args)).subscribe(callback),
		}),
	}
}
function reportsEqual(reportsA, reportsB) {
	if (reportsA === void 0 && reportsB === void 0) {
		return true
	} else if (reportsA === void 0 || reportsB === void 0) {
		return false
	}
	if (reportsA.length !== reportsB.length) {
		return false
	}
	for (const [i, element] of reportsA.entries()) {
		if (element?.languageTag !== reportsB[i]?.languageTag) {
			return false
		}
		if (element?.level !== reportsB[i]?.level) {
			return false
		}
		if (element?.ruleId !== reportsB[i]?.ruleId) {
			return false
		}
		if (typeof element?.body !== typeof reportsB[i]?.body) {
			return false
		}
		if (typeof element?.body === "string") {
			if (reportsB[i]?.body !== reportsB[i]?.body) {
				return false
			}
		} else {
			if (JSON.stringify(element?.body) !== JSON.stringify(element?.body)) {
				return false
			}
		}
	}
	return true
}

// ../versioned-interfaces/project-settings/dist/migration/1-to-2.js
var migrate1to2 = (config) => {
	const migrated = {
		$schema: "https://inlang.com/schema/project-settings",
		sourceLanguageTag: config.sourceLanguageTag,
		languageTags: config.languageTags,
		modules: config.modules,
	}
	if (config.settings["project.messageLintRuleLevels"]) {
		migrated.messageLintRuleLevels = config.settings["project.messageLintRuleLevels"]
	}
	for (const key in config.settings) {
		if (key === "project.messageLintRuleLevels") continue
		migrated[key] = config.settings[key]
	}
	return migrated
}

// ../versioned-interfaces/project-settings/dist/migration/index.js
function migrateIfOutdated(schema2) {
	if (schema2.settings) {
		return migrate1to2(schema2)
	}
	return schema2
}

// ../sdk/dist/createNodeishFsWithAbsolutePaths.js
init_dist()
var createNodeishFsWithAbsolutePaths = (args) => {
	if (!isAbsolutePath(args.projectPath)) {
		throw new Error(`Expected an absolute path but received "${args.projectPath}".`)
	}
	const basePath = normalizePath2(args.projectPath).split("/").slice(0, -1).join("/")
	const makeAbsolute = (path) => {
		if (isAbsolutePath(path)) {
			return normalizePath2(path)
		}
		return normalizePath2(basePath + "/" + path)
	}
	return {
		// @ts-expect-error
		readFile: (path, options) => args.nodeishFs.readFile(makeAbsolute(path), options),
		readdir: (path) => args.nodeishFs.readdir(makeAbsolute(path)),
		mkdir: (path, options) => args.nodeishFs.mkdir(makeAbsolute(path), options),
		writeFile: (path, data) => args.nodeishFs.writeFile(makeAbsolute(path), data),
		stat: (path) => args.nodeishFs.stat(makeAbsolute(path)),
		rm: (path) => args.nodeishFs.rm(makeAbsolute(path)),
		rmdir: (path) => args.nodeishFs.rmdir(makeAbsolute(path)),
		watch: (path, options) => args.nodeishFs.watch(makeAbsolute(path), options),
		// This might be surprising when symlinks were intended to be relative
		symlink: (target, path) => args.nodeishFs.symlink(makeAbsolute(target), makeAbsolute(path)),
		unlink: (path) => args.nodeishFs.unlink(makeAbsolute(path)),
		readlink: (path) => args.nodeishFs.readlink(makeAbsolute(path)),
		lstat: (path) => args.nodeishFs.lstat(makeAbsolute(path)),
	}
}

// ../sdk/dist/loadProject.js
init_dist()

// ../sdk/dist/migrations/migrateToDirectory.js
var maybeMigrateToDirectory = async (args) => {
	if (args.projectPath.endsWith("project.inlang") === false) {
		return
	}
	const projectDirectory = await tryCatch(() => args.nodeishFs.stat(args.projectPath))
	if (projectDirectory.data) {
		return
	}
	const settingsFile = await tryCatch(() =>
		args.nodeishFs.readFile(args.projectPath + ".json", { encoding: "utf-8" })
	)
	if (settingsFile.error) {
		return
	}
	await args.nodeishFs.mkdir(args.projectPath)
	await args.nodeishFs.writeFile(`${args.projectPath}/settings.json`, settingsFile.data)
	await args.nodeishFs.writeFile(args.projectPath + ".README.md", readme)
}
var readme = `
# DELETE THE \`project.inlang.json\` FILE

The \`project.inlang.json\` file is now contained in a project directory e.g. \`project.inlang/settings.json\`.


## What you need to do

1. Update the inlang CLI (if you use it) to use the new path \`project.inlang\` instead of \`project.inlang.json\`.
2. Delete the \`project.inlang.json\` file.


## Why is this happening?

See this RFC https://docs.google.com/document/d/1OYyA1wYfQRbIJOIBDliYoWjiUlkFBNxH_U2R4WpVRZ4/edit#heading=h.pecv6xb7ial6 
and the following GitHub issue for more information https://github.com/opral/monorepo/issues/1678.

- Monorepo support https://github.com/opral/monorepo/discussions/258. 
- Required for many other future features like caching, first class offline support, and more. 
- Stablize the inlang project format.
`

// ../sdk/dist/migrations/maybeCreateFirstProjectId.js
async function maybeCreateFirstProjectId(args) {
	if (args.repo === void 0) {
		return
	}
	try {
		await args.repo.nodeishFs.readFile(args.projectPath + "/project_id", {
			encoding: "utf-8",
		})
	} catch (error) {
		if (error.code === "ENOENT" && args.repo) {
			const projectId = await generateProjectId({ repo: args.repo, projectPath: args.projectPath })
			if (projectId) {
				await args.repo.nodeishFs
					.writeFile(args.projectPath + "/project_id", projectId)
					.catch((error2) => {
						console.error("Failed to write project_id", error2)
					})
			}
		}
	}
}
async function generateProjectId(args) {
	if (!args.repo || !args.projectPath) {
		return void 0
	}
	const firstCommitHash = await args.repo.getFirstCommitHash()
	if (firstCommitHash) {
		try {
			return await hash(`${firstCommitHash + args.projectPath}`)
		} catch (error) {
			console.error("Failed to generate project_id", error)
		}
	}
	return void 0
}

// ../sdk/dist/migrations/maybeAddModuleCache.js
var EXPECTED_IGNORES = ["cache"]
async function maybeAddModuleCache(args) {
	if (args.repo === void 0) return
	const projectExists = await directoryExists(args.projectPath, args.repo.nodeishFs)
	if (!projectExists) return
	const gitignorePath = args.projectPath + "/.gitignore"
	const moduleCache = args.projectPath + "/cache/modules/"
	const gitignoreExists = await fileExists(gitignorePath, args.repo.nodeishFs)
	const moduleCacheExists = await directoryExists(moduleCache, args.repo.nodeishFs)
	if (gitignoreExists) {
		try {
			const gitignore = await args.repo.nodeishFs.readFile(gitignorePath, { encoding: "utf-8" })
			const missingIgnores = EXPECTED_IGNORES.filter((ignore2) => !gitignore.includes(ignore2))
			if (missingIgnores.length > 0) {
				await args.repo.nodeishFs.appendFile(gitignorePath, "\n" + missingIgnores.join("\n"))
			}
		} catch (error) {
			throw new Error("[migrate:module-cache] Failed to update .gitignore", { cause: error })
		}
	} else {
		try {
			await args.repo.nodeishFs.writeFile(gitignorePath, EXPECTED_IGNORES.join("\n"))
		} catch (e) {
			if (e.code && e.code !== "EISDIR" && e.code !== "EEXIST") {
				throw new Error("[migrate:module-cache] Failed to create .gitignore", { cause: e })
			}
		}
	}
	if (!moduleCacheExists) {
		try {
			await args.repo.nodeishFs.mkdir(moduleCache, { recursive: true })
		} catch (e) {
			throw new Error("[migrate:module-cache] Failed to create cache directory", { cause: e })
		}
	}
}
async function fileExists(path, nodeishFs) {
	try {
		const stat = await nodeishFs.stat(path)
		return stat.isFile()
	} catch {
		return false
	}
}
async function directoryExists(path, nodeishFs) {
	try {
		const stat = await nodeishFs.stat(path)
		return stat.isDirectory()
	} catch {
		return false
	}
}

// ../sdk/dist/env-variables/index.js
var ENV_VARIABLES = {
	PUBLIC_POSTHOG_TOKEN: "phc_oE6Qs5mT6oXMw9Z0Om1CqIBpg5RcSLYmghoFR5irZOd",
}

// ../sdk/dist/telemetry/capture.js
var capture = async (event, args) => {
	if (ENV_VARIABLES.PUBLIC_POSTHOG_TOKEN === void 0) {
		return
	}
	try {
		await fetch("https://eu.posthog.com/capture/", {
			method: "POST",
			body: JSON.stringify({
				api_key: ENV_VARIABLES.PUBLIC_POSTHOG_TOKEN,
				event,
				// id is "unknown" because no user information is available
				distinct_id: "unknown",
				properties: {
					$groups: { project: args.projectId },
					...args.properties,
				},
			}),
		})
	} catch (e) {}
}

// ../sdk/dist/telemetry/groupIdentify.js
var identifyProject = async (args) => {
	if (ENV_VARIABLES.PUBLIC_POSTHOG_TOKEN === void 0) {
		return
	}
	try {
		await fetch("https://eu.posthog.com/capture/", {
			method: "POST",
			body: JSON.stringify({
				api_key: ENV_VARIABLES.PUBLIC_POSTHOG_TOKEN,
				event: "$groupidentify",
				// id is "unknown" because no user information is available
				distinct_id: "unknown",
				properties: {
					$group_type: "project",
					$group_key: args.projectId,
					$group_set: {
						...args.properties,
					},
				},
			}),
		})
	} catch (e) {}
}

// ../sdk/dist/v2/stubQueryApi.js
var stubMessagesQuery = {
	create: () => false,
	// @ts-expect-error
	get: subscribable(() => void 0),
	// @ts-expect-error
	getByDefaultAlias: subscribable(() => void 0),
	// @ts-expect-error
	includedMessageIds: subscribable(() => []),
	// @ts-expect-error
	getAll: subscribable(() => []),
	update: () => false,
	upsert: () => {},
	delete: () => false,
	setDelegate: () => {},
}
var stubMessageLintReportsQuery = {
	// @ts-expect-error
	get: subscribable(() => []),
	// @ts-expect-error
	getAll: settleable(subscribable(() => [])),
}
function subscribable(fn) {
	return Object.assign(fn, {
		subscribe: () => {},
	})
}
function settleable(fn) {
	return Object.assign(fn, {
		settled: async () => [],
	})
}

// ../sdk/dist/v2/types.js
var import_typebox10 = __toESM(require_typebox(), 1)
var pattern2 =
	"^((?<grandfathered>(en-GB-oed|i-ami|i-bnn|i-default|i-enochian|i-hak|i-klingon|i-lux|i-mingo|i-navajo|i-pwn|i-tao|i-tay|i-tsu|sgn-BE-FR|sgn-BE-NL|sgn-CH-DE)|(art-lojban|cel-gaulish|no-bok|no-nyn|zh-guoyu|zh-hakka|zh-min|zh-min-nan|zh-xiang))|((?<language>([A-Za-z]{2,3}(-(?<extlang>[A-Za-z]{3}(-[A-Za-z]{3}){0,2}))?))(-(?<script>[A-Za-z]{4}))?(-(?<region>[A-Za-z]{2}|[0-9]{3}))?(-(?<variant>[A-Za-z0-9]{5,8}|[0-9][A-Za-z0-9]{3}))*))$"
var LanguageTag2 = import_typebox10.Type.String({
	pattern: pattern2,
	description: "The language tag must be a valid IETF BCP 47 language tag.",
	examples: ["en", "de", "en-US", "zh-Hans", "es-419"],
})
var Literal = import_typebox10.Type.Object({
	type: import_typebox10.Type.Literal("literal"),
	value: import_typebox10.Type.String(),
})
var Text2 = import_typebox10.Type.Object({
	type: import_typebox10.Type.Literal("text"),
	value: import_typebox10.Type.String(),
})
var VariableReference2 = import_typebox10.Type.Object({
	type: import_typebox10.Type.Literal("variable"),
	name: import_typebox10.Type.String(),
})
var Option = import_typebox10.Type.Object({
	name: import_typebox10.Type.String(),
	value: import_typebox10.Type.Union([Literal, VariableReference2]),
})
var FunctionAnnotation = import_typebox10.Type.Object({
	type: import_typebox10.Type.Literal("function"),
	name: import_typebox10.Type.String(),
	options: import_typebox10.Type.Array(Option),
})
var Expression2 = import_typebox10.Type.Object({
	type: import_typebox10.Type.Literal("expression"),
	arg: import_typebox10.Type.Union([Literal, VariableReference2]),
	annotation: import_typebox10.Type.Optional(FunctionAnnotation),
})
var Pattern2 = import_typebox10.Type.Array(import_typebox10.Type.Union([Text2, Expression2]))
var Variant2 = import_typebox10.Type.Object({
	/**
	 * The number of keys in each variant match MUST equal the number of expressions in the selectors.
	 *
	 * Inspired by: https://github.com/unicode-org/message-format-wg/blob/main/spec/formatting.md#pattern-selection
	 */
	// a match can always only be string-based because a string is what is rendered to the UI
	match: import_typebox10.Type.Array(import_typebox10.Type.String()),
	pattern: Pattern2,
})
var InputDeclaration = import_typebox10.Type.Object({
	type: import_typebox10.Type.Literal("input"),
	name: import_typebox10.Type.String(),
	//TODO make this generic so that only Variable-Ref Expressions are allowed
	value: Expression2,
})
var Declaration = import_typebox10.Type.Union([InputDeclaration])
var Message2 = import_typebox10.Type.Object({
	locale: LanguageTag2,
	declarations: import_typebox10.Type.Array(Declaration),
	/**
	 * The order in which the selectors are placed determines the precedence of patterns.
	 */
	selectors: import_typebox10.Type.Array(Expression2),
	variants: import_typebox10.Type.Array(Variant2),
})
var MessageBundle = import_typebox10.Type.Object({
	id: import_typebox10.Type.String(),
	alias: import_typebox10.Type.Record(
		import_typebox10.Type.String(),
		import_typebox10.Type.String()
	),
	messages: import_typebox10.Type.Array(Message2),
})
var MessageSlot = import_typebox10.Type.Object({
	locale: LanguageTag2,
	slot: import_typebox10.Type.Literal(true),
})
var MessageBundleWithSlots = import_typebox10.Type.Object({
	id: import_typebox10.Type.String(),
	alias: import_typebox10.Type.Record(
		import_typebox10.Type.String(),
		import_typebox10.Type.String()
	),
	messages: import_typebox10.Type.Array(import_typebox10.Type.Union([Message2, MessageSlot])),
})

// ../sdk/dist/v2/helper.js
function createMessageSlot(locale) {
	return {
		locale,
		slot: true,
	}
}
function addSlots(messageBundle, locales) {
	const bundle = structuredClone(messageBundle)
	bundle.messages = locales.map((locale) => {
		return bundle.messages.find((message) => message.locale === locale) ?? createMessageSlot(locale)
	})
	return bundle
}
function removeSlots(messageBundle) {
	messageBundle.messages = messageBundle.messages.filter((message) => !("slot" in message))
	return messageBundle
}
function injectJSONNewlines(json) {
	return json
		.replace(/\{"id":"/g, '\n\n\n\n{"id":"')
		.replace(/"messages":\[\{"locale":"/g, '"messages":[\n\n\n\n{"locale":"')
		.replace(/\}\]\}\]\},\{"locale":"/g, '}]}]},\n\n\n\n{"locale":"')
		.replace(/"slot":true\},\{"locale":/g, '"slot":true},\n\n\n\n{"locale":')
}

// ../sdk/dist/persistence/store.js
init_dist()

// ../sdk/dist/persistence/batchedIO.js
var import_debug6 = __toESM(require_src(), 1)
var debug6 = (0, import_debug6.default)("sdk:batchedIO")
function batchedIO(acquireLock2, releaseLock2, save) {
	let state = "idle"
	const queue = []
	let nextBatch = void 0
	return async (id) => {
		if (state === "idle") {
			state = "acquiring"
			const lock2 = await acquireLock2()
			state = "saving"
			await save()
			await releaseLock2(lock2)
			resolveQueued()
			nextBatch = void 0
			state = "idle"
			return id
		} else if (state === "acquiring") {
			return new Promise((resolve, reject) => {
				queue.push({ id, resolve, reject })
			})
		} else {
			nextBatch = nextBatch ?? batchedIO(acquireLock2, releaseLock2, save)
			return await nextBatch(id)
		}
	}
	function resolveQueued() {
		debug6("batched", queue.length + 1)
		for (const { id, resolve } of queue) {
			resolve(id)
		}
		queue.length = 0
	}
}

// ../sdk/dist/persistence/store.js
var import_debug7 = __toESM(require_src(), 1)
var debug7 = (0, import_debug7.default)("sdk:store")
async function openStore(args) {
	const nodeishFs = args.nodeishFs
	const filePath = args.projectPath + "/messages.json"
	const lockDirPath = args.projectPath + "/messagelock"
	let index2 = await load()
	const batchedSave = batchedIO(acquireSaveLock, releaseSaveLock, save)
	return {
		messageBundles: {
			reload: async () => {
				index2.clear()
				index2 = await load()
			},
			get: async (args2) => {
				return index2.get(args2.id)
			},
			set: async (args2) => {
				index2.set(args2.data.id, args2.data)
				await batchedSave(args2.data.id)
			},
			delete: async (args2) => {
				index2.delete(args2.id)
				await batchedSave(args2.id)
			},
			getAll: async () => {
				return [...index2.values()]
			},
		},
	}
	async function load() {
		const lockTime = await acquireFileLock(nodeishFs, lockDirPath, "load")
		const messages = await readJSON({ filePath, nodeishFs })
		const index3 = new Map(messages.map((message) => [message.id, message]))
		await releaseLock(nodeishFs, lockDirPath, "load", lockTime)
		return index3
	}
	async function acquireSaveLock() {
		return await acquireFileLock(nodeishFs, lockDirPath, "save")
	}
	async function releaseSaveLock(lock2) {
		return await releaseLock(nodeishFs, lockDirPath, "save", lock2)
	}
	async function save() {
		await writeJSON({
			filePath,
			nodeishFs,
			messages: [...index2.values()],
			locales: args.locales,
		})
	}
}
async function readJSON(args) {
	let result = []
	debug7("loadAll", args.filePath)
	try {
		const file = await args.nodeishFs.readFile(args.filePath, { encoding: "utf-8" })
		result = JSON.parse(file)
	} catch (error) {
		if (error?.code !== "ENOENT") {
			debug7("loadMessages", error)
			throw error
		}
	}
	return result.map(removeSlots)
}
async function writeJSON(args) {
	debug7("saveall", args.filePath)
	try {
		await createDirectoryIfNotExits(getDirname(args.filePath), args.nodeishFs)
		const output = injectJSONNewlines(
			JSON.stringify(args.messages.map((bundle) => addSlots(bundle, args.locales)))
		)
		await args.nodeishFs.writeFile(args.filePath, output)
	} catch (error) {
		debug7("saveMessages", error)
		throw error
	}
}
async function createDirectoryIfNotExits(path, nodeishFs) {
	try {
		await nodeishFs.mkdir(path, { recursive: true })
	} catch (error) {
		if (error.code !== "EEXIST") {
			throw error
		}
	}
}

// ../sdk/dist/loadProject.js
var import_debug8 = __toESM(require_src(), 1)
var debug8 = (0, import_debug8.default)("sdk:loadProject")
var settingsCompiler = import_compiler3.TypeCompiler.Compile(ProjectSettings)
async function loadProject(args) {
	const projectPath = normalizePath2(args.projectPath)
	assertValidProjectPath(projectPath)
	debug8(projectPath)
	const nodeishFs = createNodeishFsWithAbsolutePaths({
		projectPath,
		nodeishFs: args.repo.nodeishFs,
	})
	await maybeMigrateToDirectory({ nodeishFs, projectPath })
	await maybeCreateFirstProjectId({ projectPath, repo: args.repo })
	await maybeAddModuleCache({ projectPath, repo: args.repo })
	return await createRoot2(async () => {
		const { data: projectId } = await tryCatch(() =>
			nodeishFs.readFile(args.projectPath + "/project_id", { encoding: "utf-8" })
		)
		const [initialized, markInitAsComplete, markInitAsFailed] = createAwaitable2()
		const [loadedSettings, markSettingsAsLoaded, markSettingsAsFailed] = createAwaitable2()
		const [resolvedModules, setResolvedModules] = createSignal2()
		const [settings, _setSettings] = createSignal2()
		let v2Persistence = false
		let locales = []
		const setSettings = (newSettings) => {
			try {
				const validatedSettings = parseSettings(newSettings)
				v2Persistence = !!validatedSettings.experimental?.persistence
				locales = validatedSettings.languageTags
				batch2(() => {
					setResolvedModules(void 0)
					_setSettings(validatedSettings)
				})
				return { data: validatedSettings }
			} catch (error) {
				if (error instanceof ProjectSettingsInvalidError) {
					return { error }
				}
				throw new Error(
					"Unhandled error in setSettings. This is an internal bug. Please file an issue.",
					{ cause: error }
				)
			}
		}
		const nodeishFsWithWatchersForSettings = createNodeishFsWithWatcher({
			nodeishFs,
			onChange: async () => {
				const readSettingsResult = await tryCatch(
					async () =>
						await loadSettings({
							settingsFilePath: projectPath + "/settings.json",
							nodeishFs,
						})
				)
				if (readSettingsResult.error) return
				const newSettings = readSettingsResult.data
				if (JSON.stringify(newSettings) !== JSON.stringify(settings())) {
					setSettings(newSettings)
				}
			},
		})
		const settingsResult = await tryCatch(
			async () =>
				await loadSettings({
					settingsFilePath: projectPath + "/settings.json",
					nodeishFs: nodeishFsWithWatchersForSettings,
				})
		)
		if (settingsResult.error) {
			markInitAsFailed(settingsResult.error)
			markSettingsAsFailed(settingsResult.error)
		} else {
			setSettings(settingsResult.data)
			markSettingsAsLoaded()
		}
		createEffect2(() => {
			const _settings = settings()
			if (!_settings) return
			resolveModules({
				settings: _settings,
				nodeishFs,
				_import: args._import,
				projectPath,
			})
				.then((resolvedModules2) => {
					setResolvedModules(resolvedModules2)
				})
				.catch((err) => markInitAsFailed(err))
		})
		let settingsValue
		createEffect2(() => (settingsValue = settings()))
		const installedMessageLintRules = () => {
			if (!resolvedModules()) return []
			return resolvedModules().messageLintRules.map((rule) => ({
				id: rule.id,
				displayName: rule.displayName,
				description: rule.description,
				module:
					resolvedModules()?.meta.find((m) => m.id.includes(rule.id))?.module ??
					"Unknown module. You stumbled on a bug in inlang's source code. Please open an issue.",
				// default to warning, see https://github.com/opral/monorepo/issues/1254
				level: settingsValue["messageLintRuleLevels"]?.[rule.id] ?? "warning",
				settingsSchema: rule.settingsSchema,
			}))
		}
		const installedPlugins = () => {
			if (!resolvedModules()) return []
			return resolvedModules().plugins.map((plugin) => ({
				id: plugin.id,
				displayName: plugin.displayName,
				description: plugin.description,
				module:
					resolvedModules()?.meta.find((m) => m.id.includes(plugin.id))?.module ??
					"Unknown module. You stumbled on a bug in inlang's source code. Please open an issue.",
				settingsSchema: plugin.settingsSchema,
			}))
		}
		const [loadMessagesViaPluginError, setLoadMessagesViaPluginError] = createSignal2()
		const [saveMessagesViaPluginError, setSaveMessagesViaPluginError] = createSignal2()
		let messagesQuery
		let lintReportsQuery
		let store
		await loadedSettings.catch(() => {})
		if (v2Persistence) {
			messagesQuery = stubMessagesQuery
			lintReportsQuery = stubMessageLintReportsQuery
			try {
				store = await openStore({ projectPath, nodeishFs, locales })
				markInitAsComplete()
			} catch (e) {
				markInitAsFailed(e)
			}
		} else {
			messagesQuery = createMessagesQuery({
				projectPath,
				nodeishFs,
				settings,
				resolvedModules,
				onInitialMessageLoadResult: (e) => {
					if (e) {
						markInitAsFailed(e)
					} else {
						markInitAsComplete()
					}
				},
				onLoadMessageResult: (e) => {
					setLoadMessagesViaPluginError(e)
				},
				onSaveMessageResult: (e) => {
					setSaveMessagesViaPluginError(e)
				},
			})
			lintReportsQuery = createMessageLintReportsQuery(
				messagesQuery,
				settings,
				installedMessageLintRules,
				resolvedModules
			)
			store = void 0
		}
		const initializeError = await initialized.catch((error) => error)
		let projectLoadedCapturedAlready = false
		if (projectId && projectLoadedCapturedAlready === false) {
			projectLoadedCapturedAlready = true
			await identifyProject({
				projectId,
				properties: {
					// using the id for now as a name but can be changed in the future
					// we need at least one property to make a project visible in the dashboard
					name: projectId,
				},
			})
			await capture("SDK loaded project", {
				projectId,
				properties: {
					appId: args.appId,
					settings: settings(),
					installedPluginIds: installedPlugins().map((p) => p.id),
					installedMessageLintRuleIds: installedMessageLintRules().map((r) => r.id),
					// TODO: fix for v2Persistence
					// https://github.com/opral/inlang-message-sdk/issues/78
					numberOfMessages: messagesQuery.includedMessageIds().length,
				},
			})
		}
		return {
			id: projectId,
			installed: {
				plugins: createSubscribable(() => installedPlugins()),
				messageLintRules: createSubscribable(() => installedMessageLintRules()),
			},
			errors: createSubscribable(() => [
				...(initializeError ? [initializeError] : []),
				...(resolvedModules() ? resolvedModules().errors : []),
				...(loadMessagesViaPluginError() ? [loadMessagesViaPluginError()] : []),
				...(saveMessagesViaPluginError() ? [saveMessagesViaPluginError()] : []),
				// have a query error exposed
				//...(lintErrors() ?? []),
			]),
			settings: createSubscribable(() => settings()),
			setSettings: (newSettings) => {
				const result = setSettings(newSettings)
				if (!result.error) writeSettingsToDisk({ nodeishFs, settings: result.data, projectPath })
				return result.error ? result : { data: void 0 }
			},
			customApi: createSubscribable(() => resolvedModules()?.resolvedPluginApi.customApi || {}),
			query: {
				messages: messagesQuery,
				messageLintReports: lintReportsQuery,
			},
			store,
		}
	})
}
var loadSettings = async (args) => {
	const { data: settingsFile, error: settingsFileError } = await tryCatch(
		async () => await args.nodeishFs.readFile(args.settingsFilePath, { encoding: "utf-8" })
	)
	if (settingsFileError)
		throw new ProjectSettingsFileNotFoundError({
			cause: settingsFileError,
			path: args.settingsFilePath,
		})
	const json = tryCatch(() => JSON.parse(settingsFile))
	if (json.error) {
		throw new ProjectSettingsFileJSONSyntaxError({
			cause: json.error,
			path: args.settingsFilePath,
		})
	}
	return parseSettings(json.data)
}
var parseSettings = (settings) => {
	const withMigration = migrateIfOutdated(settings)
	if (settingsCompiler.Check(withMigration) === false) {
		const typeErrors = [...settingsCompiler.Errors(settings)]
		if (typeErrors.length > 0) {
			throw new ProjectSettingsInvalidError({
				errors: typeErrors,
			})
		}
	}
	const { sourceLanguageTag, languageTags } = settings
	if (!languageTags.includes(sourceLanguageTag)) {
		throw new ProjectSettingsInvalidError({
			errors: [
				{
					message: `The sourceLanguageTag "${sourceLanguageTag}" is not included in the languageTags "${languageTags.join(
						'", "'
					)}". Please add it to the languageTags.`,
					type: import_compiler3.ValueErrorType.String,
					schema: ProjectSettings,
					value: sourceLanguageTag,
					path: "sourceLanguageTag",
				},
			],
		})
	}
	return withMigration
}
var writeSettingsToDisk = async (args) => {
	const serializeResult = tryCatch(() =>
		// TODO: this will probably not match the original formatting
		JSON.stringify(args.settings, void 0, 2)
	)
	if (serializeResult.error) throw serializeResult.error
	const serializedSettings = serializeResult.data
	const writeResult = await tryCatch(
		async () =>
			await args.nodeishFs.writeFile(args.projectPath + "/settings.json", serializedSettings)
	)
	if (writeResult.error) throw writeResult.error
}
var createAwaitable2 = () => {
	let resolve
	let reject
	const promise = new Promise((res, rej) => {
		resolve = res
		reject = rej
	})
	return [promise, resolve, reject]
}
function createSubscribable(signal) {
	return Object.assign(signal, {
		subscribe: (callback) => {
			createEffect2(() => {
				callback(signal())
			})
		},
	})
}

// ../sdk/dist/listProjects.js
var ignores = ["node_modules", ".git"]
var listProjects = async (nodeishFs, from2) => {
	const recursionLimit = 5
	const projects = []
	async function searchDir(path, depth) {
		if (depth > recursionLimit) {
			return
		}
		const files = await nodeishFs.readdir(path)
		for (const file of files) {
			const filePath = `${path}/${file}`
			try {
				const stats = await nodeishFs.stat(filePath)
				if (stats.isDirectory()) {
					if (ignores.includes(file)) {
						continue
					}
					if (file.endsWith(".inlang")) {
						projects.push({ projectPath: filePath })
					} else {
						await searchDir(filePath, depth + 1)
					}
				}
			} catch {
				continue
			}
		}
	}
	await searchDir(from2, 0)
	for (const project of projects) {
		project.projectPath = project.projectPath.replace(/\/\//g, "/")
	}
	return projects
}

// src/main.ts
async function run() {
	core.debug("Running the action")
	try {
		const token = process.env.GITHUB_TOKEN
		if (!token) {
			throw new Error("GITHUB_TOKEN is not set")
		}
		const octokit = github.getOctokit(token)
		const { owner, repo } = github.context.repo
		const prNumber = github.context.payload.pull_request?.number
		const { data } = await octokit.rest.pulls.get({
			owner,
			repo,
			pull_number: prNumber,
		})
		if (data.mergeable) {
			console.debug(`Pull Request #${prNumber} is mergeable.`)
		} else {
			console.warn(`Pull Request #${prNumber} is not mergeable. Skipping linting.`)
			return
		}
		process.chdir("target")
		core.debug(`Changed directory to target`)
		const repoTarget = await openRepository("file://" + process.cwd(), {
			nodeishFs: fs,
			branch: github.context.payload.pull_request?.head.ref,
		})
		core.debug(`Opened target repository`)
		const projectListTarget = await listProjects(repoTarget.nodeishFs, process.cwd())
		const results = projectListTarget.map((project) => ({
			projectPath: project.projectPath.replace(process.cwd(), ""),
			errorsTarget: [],
			errorsMerge: [],
			installedRules: [],
			reportsTarget: [],
			reportsMerge: [],
			lintSummary: [],
			changedIds: [],
			commentContent: "",
		}))
		for (const result of results) {
			core.debug(`Checking project: ${result.projectPath} in target repo`)
			const projectTarget = await loadProject({
				projectPath: process.cwd() + result.projectPath,
				repo: repoTarget,
				appId: "app.inlang.ninjaI18nAction",
			})
			if (projectTarget.errors().length > 0) {
				if (result) result.errorsTarget = projectTarget.errors()
				console.debug("Skip project ", result.projectPath, " in base repo because of errors")
				continue
			}
			result.installedRules.push(...projectTarget.installed.messageLintRules())
			const messageLintReports = await projectTarget.query.messageLintReports.getAll.settled()
			core.debug(`message: ${messageLintReports.length}`)
			result.reportsTarget.push(...messageLintReports)
			core.debug(`detected lint reports: ${messageLintReports.length}`)
		}
		const headMeta = {
			owner: github.context.payload.pull_request?.head.label.split(":")[0],
			repo: github.context.payload.pull_request?.head.repo.name,
			branch: github.context.payload.pull_request?.head.label.split(":")[1],
		}
		const baseMeta = {
			owner: github.context.payload.pull_request?.base.label.split(":")[0],
			repo,
			branch: github.context.payload.pull_request?.base.label.split(":")[1],
		}
		process.chdir("../merge")
		core.debug(`Changed directory to merge`)
		const repoMerge = await openRepository("file://" + process.cwd(), {
			nodeishFs: fs,
		})
		core.debug(`Opened merge repository`)
		const projectListMerge = await listProjects(repoMerge.nodeishFs, process.cwd())
		const newProjects = projectListMerge.filter(
			(project) =>
				!results.some(
					(result) => result.projectPath === project.projectPath.replace(process.cwd(), "")
				)
		)
		for (const project of newProjects) {
			results.push({
				projectPath: project.projectPath.replace(process.cwd(), ""),
				errorsTarget: [],
				errorsMerge: [],
				installedRules: [],
				reportsTarget: [],
				reportsMerge: [],
				lintSummary: [],
				changedIds: [],
				commentContent: "",
			})
		}
		for (const result of results) {
			core.debug(`Checking project: ${result.projectPath} in merge repo`)
			if (
				projectListMerge.some(
					(project) => project.projectPath.replace(process.cwd(), "") === result.projectPath
				) === false
			) {
				console.debug(`Project ${result.projectPath} not found in head repo`)
				continue
			}
			const projectMerge = await loadProject({
				projectPath: process.cwd() + result.projectPath,
				repo: repoMerge,
				appId: "app.inlang.ninjaI18nAction",
			})
			if (projectMerge.errors().length > 0) {
				if (result) result.errorsMerge = projectMerge.errors()
				console.debug("Skip project ", result.projectPath, " in head repo because of errors")
				continue
			}
			const newInstalledRules = projectMerge.installed.messageLintRules()
			for (const newRule of newInstalledRules) {
				if (!result.installedRules.some((rule) => rule.id === newRule.id)) {
					result.installedRules.push(newRule)
				}
			}
			const messageLintReports = await projectMerge.query.messageLintReports.getAll.settled()
			core.debug(`message: ${messageLintReports.length}`)
			result?.reportsMerge.push(...messageLintReports)
			core.debug(`detected lint reports: ${messageLintReports.length}`)
		}
		let projectWithNewSetupErrors = false
		let projectWithNewLintErrors = false
		for (const result of results) {
			if (result.errorsMerge.length > 0) continue
			const LintSummary = createLintSummary(
				result.reportsMerge,
				result.reportsTarget,
				result.installedRules
			)
			if (LintSummary.summary.some((lintSummary) => lintSummary.level === "error")) {
				console.debug(
					`\u2757\uFE0F New lint errors found in project ${result.projectPath}. Set workflow to fail.`
				)
				projectWithNewLintErrors = true
			}
			result.lintSummary = LintSummary.summary
			result.changedIds = LintSummary.changedIds
		}
		for (const result of results) {
			const shortenedProjectPath = () => {
				const parts = result.projectPath.split("/")
				if (parts.length > 2) {
					return `/${parts.at(-2)}/${parts.at(-1)}`
				} else {
					return result.projectPath
				}
			}
			if (result.errorsTarget.length === 0 && result.errorsMerge.length > 0) {
				console.debug(
					`\u2757\uFE0F New errors in setup of project \`${result.projectPath}\` found. Set workflow to fail.`
				)
				projectWithNewSetupErrors = true
				result.commentContent = `#### \u2757\uFE0F New errors in setup of project \`${shortenedProjectPath()}\` found
${result.errorsMerge
	.map((error) => {
		let errorLog = `<details>
<summary>${error?.name}</summary>
${error?.message}`
		if (error?.cause && error?.cause?.message) {
			errorLog += `

**Error cause**
${error?.cause.message}`
		}
		if (error?.cause && error?.cause?.message && error?.cause?.stack) {
			errorLog += `

**Stack trace**
${error?.cause.stack}`
		}
		errorLog += `</details>`
		return errorLog
	})
	.join("\n")}`
				continue
			}
			if (result.errorsTarget.length > 0 && result.errorsMerge.length === 0) {
				console.debug(`\u2705 Setup of project \`${result.projectPath}\` fixed`)
			}
			if (result.errorsMerge.length > 0) continue
			if (result.lintSummary.length === 0) continue
			const lintSummary = result.lintSummary
			const commentContent2 = `#### Project \`${shortenedProjectPath()}\`
| lint rule | new reports | level | link |
|-----------|-------------| ------|------|
${lintSummary
	.map(
		(lintSummary2) =>
			`| ${lintSummary2.name} | ${lintSummary2.count}| ${
				lintSummary2.level
			} | [contribute (via Fink \u{1F426})](https://fink.inlang.com/github.com/${headMeta.owner}/${
				headMeta.repo
			}?branch=${headMeta.branch}&project=${result.projectPath}&lint=${
				lintSummary2.id
			}&${result.changedIds.map((id) => `id=${id}`).join("&")}&ref=ninja-${baseMeta.owner}/${
				baseMeta.repo
			}/pull/${prNumber}) |`
	)
	.join("\n")}
`
			result.commentContent = commentContent2
		}
		const commentMergeline = `### \u{1F977} Ninja i18n \u2013 \u{1F6CE}\uFE0F Translations need to be updated`
		const commentResolved = `### \u{1F977} Ninja i18n \u2013 \u{1F389} Translations have been successfully updated`
		const commentContent =
			commentMergeline +
			"\n\n" +
			results
				.map((result) => result.commentContent)
				.filter((content) => content.length > 0)
				.join("\n")
		console.debug("Comment content created")
		const issue = await octokit.rest.issues.get({
			owner,
			repo,
			issue_number: prNumber,
		})
		if (issue.data.locked) return console.debug("PR is locked, comment is skipped")
		console.debug("Checked if PR is locked")
		const existingComment = await octokit.rest.issues.listComments({
			owner,
			repo,
			issue_number: prNumber,
		})
		console.debug(
			"Checking if new comment needs to be created or existing comment needs to be updated"
		)
		if (existingComment.data.length > 0) {
			const commentId = existingComment.data.find(
				(comment) =>
					(comment.body?.includes(commentMergeline) || comment.body?.includes(commentResolved)) &&
					comment.user?.login === "github-actions[bot]"
			)?.id
			if (commentId) {
				console.debug("Updating existing comment")
				if (results.every((result) => result.commentContent.length === 0)) {
					console.debug("Reports have been fixed, updating comment")
					const comment = await octokit.rest.issues.updateComment({
						owner,
						repo,
						comment_id: commentId,
						body: commentResolved,
						as: "ninja-i18n",
					})
					if (comment) {
						console.debug("Comment updated:\n", comment?.data?.body)
					}
				} else {
					console.debug("Reports have not been fixed, updating comment")
					const comment = await octokit.rest.issues.updateComment({
						owner,
						repo,
						comment_id: commentId,
						body: commentContent,
						as: "ninja-i18n",
					})
					if (comment) {
						console.debug("Comment updated:\n", comment?.data?.body)
					}
				}
			}
		} else if (results.every((result) => result.commentContent.length === 0)) {
			console.debug("No lint reports found, skipping comment")
		} else {
			console.debug("Creating a new comment")
			const comment = await octokit.rest.issues.createComment({
				owner,
				repo,
				issue_number: prNumber,
				body: commentContent,
				as: "ninja-i18n",
			})
			core.debug(`createComment API response:
${JSON.stringify(comment, null, 2)}`)
			if (!comment || comment.status !== 201) {
				throw new Error(`Failed to create a new comment [${comment?.status}]`)
			} else {
				console.debug("Comment created:\n", comment?.data?.body)
			}
		}
		console.debug("Action completed")
		if (projectWithNewSetupErrors || projectWithNewLintErrors) {
			let error_message = ""
			if (projectWithNewSetupErrors && projectWithNewLintErrors) {
				error_message = "New errors found in project setup and new lint errors found in project"
			} else if (projectWithNewSetupErrors) {
				error_message = "New errors found in project setup"
			} else if (projectWithNewLintErrors) {
				error_message = "New lint errors found in project"
			}
			core.setFailed(error_message)
		}
	} catch (error) {
		if (error instanceof Error) {
			core.setFailed(error)
		}
	}
}
function createLintSummary(reportsMerge, reportsTarget, installedRules) {
	const summary = []
	const diffReports = reportsMerge.filter(
		(report) =>
			!reportsTarget.some(
				(baseReport) =>
					baseReport.ruleId === report.ruleId &&
					baseReport.languageTag === report.languageTag &&
					baseReport.messageId === report.messageId
			)
	)
	for (const installedRule of installedRules) {
		const id = installedRule.id
		const name =
			typeof installedRule.displayName === "object"
				? installedRule.displayName.en
				: installedRule.displayName
		const count = diffReports.filter((report) => report.ruleId === id).length
		const level = installedRule.level
		if (count > 0) {
			summary.push({ id, name, count, level })
		}
	}
	const changedIds = diffReports
		.map((report) => report.messageId)
		.filter((value, index2, self2) => self2.indexOf(value) === index2)
	return { summary, changedIds }
}

// src/index.ts
run()
/*! Bundled license information:

undici/lib/fetch/body.js:
  (*! formdata-polyfill. MIT License. Jimmy Wrting <https://jimmy.warting.se/opensource> *)

undici/lib/websocket/frame.js:
  (*! ws. MIT License. Einar Otto Stangvik <einaros@gmail.com> *)

safe-buffer/index.js:
  (*! safe-buffer. MIT License. Feross Aboukhadijeh <https://feross.org/opensource> *)

crc-32/crc32.js:
  (*! crc32.js (C) 2014-present SheetJS -- http://sheetjs.com *)
*/
